,_id,Category,Link,Title,Authors,Tags,Abstract
0,679d2d0ebc21667774cae0a7,cs.CV,https://arxiv.org/pdf/2501.18444,Adaptive Object Detection for Indoor Navigation Assistance: A Performance Evaluation of Real-Time Algorithms,"Abhinav Pratap, Sushant Kumar, Suchinton Chakravarty",Computer Vision and Pattern Recognition,
1,679d2d0ebc21667774cae0a8,cs.CV,https://arxiv.org/pdf/2501.18427,SANA 1.5: Efficient Scaling of Training-Time and Inference-Time Compute in Linear Diffusion Transformer,"Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, Han Cai, Bingchen Liu, Daquan Zhou, Song Han",Computer Vision and Pattern Recognition,"Abstract:This paper presents SANA-1.5, a linear Diffusion Transformer for efficient scaling in text-to-image generation. Building upon SANA-1.0, we introduce three key innovations: (1) Efficient Training Scaling: A depth-growth paradigm that enables scaling from 1.6B to 4.8B parameters with significantly reduced computational resources, combined with a memory-efficient 8-bit optimizer.
(2) Model Depth Pruning: A block importance analysis technique for efficient model compression to arbitrary sizes with minimal quality loss.
(3) Inference-time Scaling: A repeated sampling strategy that trades computation for model capacity, enabling smaller models to match larger model quality at inference time.
Through these strategies, SANA-1.5 achieves a text-image alignment score of 0.72 on GenEval, which can be further improved to 0.80 through inference scaling, establishing a new SoTA on GenEval benchmark.
These innovations enable efficient model scaling across different compute budgets while maintaining high quality, making high-quality image generation more accessible. Our code and pre-trained models will be released.Links:Github Code|HF Models|Demo|Project Page"
2,679d2d0ebc21667774cae0aa,cs.CV,https://arxiv.org/pdf/2501.18401,MatIR: A Hybrid Mamba-Transformer Image Restoration Model,"Juan Wen, Weiyan Hou, Luc Van Gool, Radu Timofte",Computer Vision and Pattern Recognition,"In recent years, Transformers-based models have made significant progress in the field of image restoration by leveraging their inherent ability to capture complex contextual features. Recently, Mamba models have made a splash in the field of computer vision due to their ability to handle long-range dependencies and their significant computational efficiency compared to Transformers. However, Mamba currently lags behind Transformers in contextual learning capabilities. To overcome the limitations of these two models, we propose a Mamba-Transformer hybrid image restoration model called MatIR. Specifically, MatIR cross-cycles the blocks of the Transformer layer and the Mamba layer to extract features, thereby taking full advantage of the advantages of the two architectures. In the Mamba module, we introduce the Image Inpainting State Space (IRSS) module, which traverses along four scan paths to achieve efficient processing of long sequence data. In the Transformer module, we combine triangular window-based local attention with channel-based global attention to effectively activate the attention mechanism over a wider range of image pixels. Extensive experimental results and ablation studies demonstrate the effectiveness of our approach."
3,679d2d0ebc21667774cae0ac,cs.CV,https://arxiv.org/pdf/2501.18361,Video-based Surgical Tool-tip and Keypoint Tracking using Multi-frame Context-driven Deep Learning Models,"Bhargav Ghanekar, Lianne R. Johnson, Jacob L. Laughlin, Marcia K. O'Malley, Ashok Veeraraghavan",Computer Vision and Pattern Recognition,"Automated tracking of surgical tool keypoints in robotic surgery videos is an essential task for various downstream use cases such as skill assessment, expertise assessment, and the delineation of safety zones. In recent years, the explosion of deep learning for vision applications has led to many works in surgical instrument segmentation, while lesser focus has been on tracking specific tool keypoints, such as tool tips. In this work, we propose a novel, multi-frame context-driven deep learning framework to localize and track tool keypoints in surgical videos. We train and test our models on the annotated frames from the 2015 EndoVis Challenge dataset, resulting in state-of-the-art performance. By leveraging sophisticated deep learning models and multi-frame context, we achieve 90% keypoint detection accuracy and a localization RMS error of 5.27 pixels. Results on a self-annotated JIGSAWS dataset with more challenging scenarios also show that the proposed multi-frame models can accurately track tool-tip and tool-base keypoints, with<4.2absent4.2{<}4.2< 4.2-pixel RMS error overall. Such a framework paves the way for accurately tracking surgical instrument keypoints, enabling further downstream use cases. Project and dataset webpage:https://tinyurl.com/mfc-tracker"
4,679d2d4aeefbeb5e9748bab0,cs.CV,https://arxiv.org/pdf/2501.18592,Advances in Multimodal Adaptation and Generalization: From Traditional Approaches to Foundation Models,"Hao Dong, Moru Liu, Kaiyang Zhou, Eleni Chatzi, Juho Kannala, Cyrill Stachniss, Olga Fink","Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning, Robotics","In real-world scenarios, achieving domain adaptation and generalization poses significant challenges, as models must adapt to or generalize across unknown target distributions. Extending these capabilities to unseen multimodal distributions, i.e., multimodal domain adaptation and generalization, is even more challenging due to the distinct characteristics of different modalities.
Significant progress has been made over the years, with applications ranging from action recognition to semantic segmentation. Besides, the recent advent of large-scale pre-trained multimodal foundation models, such as CLIP, has inspired works leveraging these models to enhance adaptation and generalization performances or adapting them to downstream tasks. This survey provides the first comprehensive review of recent advances from traditional approaches to
foundation models, covering: (1) Multimodal domain adaptation; (2) Multimodal test-time adaptation; (3) Multimodal domain generalization;
(4) Domain adaptation and generalization with the help of multimodal foundation models; and (5) Adaptation of multimodal foundation models. For each topic, we formally define the problem and thoroughly review existing methods.
Additionally, we analyze relevant datasets and applications, highlighting open challenges and potential future research directions. We maintain an active repository that contains up-to-date literature athttps://github.com/donghao51/Awesome-Multimodal-Adaptation."
5,679d2d4aeefbeb5e9748babf,cs.CV,https://arxiv.org/pdf/2501.18453,Transfer Learning for Keypoint Detection in Low-Resolution Thermal TUG Test Images,"Wei-Lun Chen, Chia-Yeh Hsieh, Yu-Hsiang Kao, Kai-Chun Liu, Sheng-Yu Peng, Yu Tsao","Computer Vision and Pattern Recognition, Image and Video Processing","This study presents a novel approach to human keypoint detection in low-resolution thermal images using transfer learning techniques. We introduce the first application of the Timed Up and Go (TUG) test in thermal image computer vision, establishing a new paradigm for mobility assessment. Our method leverages a MobileNetV3-Small encoder and a ViTPose decoder, trained using a composite loss function that balances latent representation alignment and heatmap accuracy. The model was evaluated using the Object Keypoint Similarity (OKS) metric from the COCO Keypoint Detection Challenge. The proposed model achieves better performance with AP, AP50, and AP75 scores of 0.861, 0.942, and 0.887 respectively, outperforming traditional supervised learning approaches like Mask R-CNN and ViTPose-Base. Moreover, our model demonstrates superior computational efficiency in terms of parameter count and FLOPS. This research lays a solid foundation for future clinical applications of thermal imaging in mobility assessment and rehabilitation monitoring."
6,679d2d4aeefbeb5e9748bac4,cs.CV,https://arxiv.org/pdf/2501.18376,Cracks in concrete,"Tin Barisin, Christian Jung, Anna Nowacka, Claudia Redenbach, Katja Schladitz","Computer Vision and Pattern Recognition, Image and Video Processing, Applications","Finding and properly segmenting cracks in images of concrete is a
challenging task. Cracks are thin and rough and being air filled do yield a
very weak contrast in 3D images obtained by computed tomography.
Enhancing and segmenting dark lower-dimensional structures is already
demanding. The heterogeneous concrete matrix and the size of the images
further increase the complexity.
ML methods have proven to solve difficult segmentation problems when trained
on enough and well annotated data. However, so
far, there is not much 3D image data of cracks available at all, let alone
annotated. Interactive annotation is error-prone as humans can easily tell
cats from dogs or roads without from roads with cars but have a hard time
deciding whether a thin and dark structure seen in a 2D slice continues in
the next one. Training networks by synthetic, simulated images is an elegant
way out, bears however its own challenges.
In this contribution, we describe how to generate semi-synthetic image data
to train CNN like the well known 3D U-Net or random forests for segmenting
cracks in 3D images of concrete.
The thickness of real cracks varies widely, both, within one crack as well
as from crack to crack in the same sample. The segmentation method should
therefore be invariant with respect to scale changes. We introduce the so-called
RieszNet, designed for exactly this purpose.
Finally, we discuss how to generalize the ML crack segmentation methods to
other concrete types.111This is a preprint of the following chapter:
Tin Barisin, Christian Jung, Anna Nowacka, Claudia Redenbach, and Katja Schladitz:
Cracks in concrete, published in
Statistical Machine Learning for Engineering with Applications (Lecture Notes in Statistics),
edited by Jürgen Franke, Anita Schöbel, 2024, Springer Cham,
reproduced with permission of Springer Nature Switzerland AG 2024. The final authenticated version is available online at:
https://doi.org/10.1007/978-3-031-66253-9"
7,679d2d63e9fe38c02764065e,cs.CV,https://arxiv.org/pdf/2501.18463,A Benchmark and Evaluation for Real-World Out-of-Distribution Detection Using Vision-Language Models,"Shiho Noda, Atsuyuki Miyai, Qing Yu, Go Irie, Kiyoharu Aizawa",Computer Vision and Pattern Recognition,"Out-of-distribution (OOD) detection is a task that detects OOD samples during inference to ensure the safety of deployed models. However, conventional benchmarks have reached performance saturation, making it difficult to compare recent OOD detection methods. To address this challenge, we introduce three novel OOD detection benchmarks that enable a deeper understanding of method characteristics and reflect real-world conditions. First, we present ImageNet-X, designed to evaluate performance under challenging semantic shifts. Second, we propose ImageNet-FS-X for full-spectrum OOD detection, assessing robustness to covariate shifts (feature distribution shifts). Finally, we propose Wilds-FS-X, which extends these evaluations to real-world datasets, offering a more comprehensive testbed. Our experiments reveal that recent CLIP-based OOD detection methods struggle to varying degrees across the three proposed benchmarks, and none of them consistently outperforms the others. We hope the community goes beyond specific benchmarks and includes more challenging conditions reflecting real-world scenarios.
The code ishttps://github.com/hoshi23/OOD-X-Banchmarks."
8,679d2d63e9fe38c027640662,cs.CV,https://arxiv.org/pdf/2501.18403,Efficient Transformer for High Resolution Image Motion Deblurring,"Amanturdieva Akmaral, Muhammad Hamza Zafar","Computer Vision and Pattern Recognition, Artificial Intelligence","This paper presents a comprehensive study and improvement of the Restormer architecture for high-resolution image motion deblurring. We introduce architectural modifications that reduce model complexity by 18.4% while maintaining or improving performance through optimized attention mechanisms. Our enhanced training pipeline incorporates additional transformations including color jitter, Gaussian blur, and perspective transforms to improve model robustness as well as new Frequency loss term. Extensive experiments on the RealBlur-R, RealBlur-J[10], and Ultra-High-Definition Motion blurred (UHDM)[23]datasets demonstrate the effectiveness of our approach. The improved architecture shows better convergence behavior and reduced training time while maintaining competitive performance across challenging scenarios. We also provide detailed ablation studies analyzing the impact of our modifications on model behavior and performance. Our results suggest that thoughtful architectural simplification combined with enhanced training strategies can yield more efficient yet equally capable models for motion deblurring tasks. Code and Data Available athttps://github.com/hamzafer/image-deblurring."
9,679d2ddce2f34252dec216a9,cs.CV,https://arxiv.org/pdf/2501.18500,HSRMamba: Contextual Spatial-Spectral State Space Model for Single Hyperspectral Super-Resolution,"Shi Chen, Lefei Zhang, Liangpei Zhang","Computer Vision and Pattern Recognition, Image and Video Processing","Mamba has demonstrated exceptional performance in visual tasks due to its powerful global modeling capabilities and linear computational complexity, offering considerable potential in hyperspectral image super-resolution (HSISR). However, in HSISR, Mamba faces challenges as transforming images into 1D sequences neglects the spatial-spectral structural relationships between locally adjacent pixels, and its performance is highly sensitive to input order, which affects the restoration of both spatial and spectral details. In this paper, we propose HSRMamba, a contextual spatial-spectral modeling state space model for HSISR, to address these issues both locally and globally. Specifically, a local spatial-spectral partitioning mechanism is designed to establish patch-wise causal relationships among adjacent pixels in 3D features, mitigating the local forgetting issue. Furthermore, a global spectral reordering strategy based on spectral similarity is employed to enhance the causal representation of similar pixels across both spatial and spectral dimensions. Finally, experimental results demonstrate our HSRMamba outperforms the state-of-the-art methods in quantitative quality and visual results. Code will be available soon."
10,679d2ddce2f34252dec216aa,cs.CV,https://arxiv.org/pdf/2501.18494,Runway vs. Taxiway: Challenges in Automated Line Identification and Notation Approaches,"Parth Ganeriwala, Amy Alvarez, Abdullah AlQahtani, Siddhartha Bhattacharyya, Mohammed Abdul Hafeez Khan, Natasha Neogi","Computer Vision and Pattern Recognition, Machine Learning","The increasing complexity of autonomous systems has amplified the need for accurate and reliable labeling of runway and taxiway markings to ensure operational safety. Precise detection and labeling of these markings are critical for tasks such as navigation, landing assistance, and ground control automation. Existing labeling algorithms, like the Automated Line Identification and Notation Algorithm (ALINA), have demonstrated success in identifying taxiway markings but encounter significant challenges when applied to runway markings. This limitation arises due to notable differences in line characteristics, environmental context, and interference from elements such as shadows, tire marks, and varying surface conditions. To address these challenges, we modified ALINA by adjusting color thresholds and refining region of interest (ROI) selection to better suit runway-specific contexts. While these modifications yielded limited improvements, the algorithm still struggled with consistent runway identification, often mislabeling elements such as the horizon or non-relevant background features. This highlighted the need for a more robust solution capable of adapting to diverse visual interferences. In this paper, we propose integrating a classification step using a Convolutional Neural Network (CNN) named AssistNet. By incorporating this classification step, the detection pipeline becomes more resilient to environmental variations and misclassifications. This work not only identifies the challenges but also outlines solutions, paving the way for improved automated labeling techniques essential for autonomous aviation systems."
11,679d2ddce2f34252dec216ab,cs.CV,https://arxiv.org/pdf/2501.18487,Track-On: Transformer-based Online Point Tracking with Memory,"Görkay Aydemir, Xiongyi Cai, Weidi Xie, Fatma Güney",Computer Vision and Pattern Recognition,"In this paper, we consider the problem of long-term point tracking, which requires consistent identification of points across multiple frames in a video, despite changes in appearance, lighting, perspective, and occlusions. We target online tracking on a frame-by-frame basis, making it suitable for real-world, streaming scenarios. Specifically, we introduceTrack-On, a simple transformer-based model designed for online long-term point tracking.
Unlike prior methods that depend on full temporal modeling,
our model processes video frames causally without access to future frames, leveraging two memory modules —spatial memory and context memory— to capture temporal information and maintain reliable point tracking over long time horizons.
At inference time, it employs patch classification and refinement to identify correspondences and track points with high accuracy.
Through extensive experiments, we demonstrate thatTrack-Onsets a new state-of-the-art for online models and delivers superior or competitive results compared to offline approaches on seven datasets, including the TAP-Vid benchmark. Our method offers a robust and scalable solution for real-time tracking in diverse applications.111Project page:https://kuis-ai.github.io/track_on"
12,679d2ddce2f34252dec216ac,cs.CV,https://arxiv.org/pdf/2501.18478,SimpleDepthPose: Fast and Reliable Human Pose Estimation with RGBD-Images,"Daniel Bermuth, Alexander Poeppel, Wolfgang Reif",Computer Vision and Pattern Recognition,"In the rapidly advancing domain of computer vision, accurately estimating the poses of multiple individuals from various viewpoints remains a significant challenge, especially when reliability is a key requirement. This paper introduces a novel algorithm that excels in multi-view, multi-person pose estimation by incorporating depth information. An extensive evaluation demonstrates that the proposed algorithm not only generalizes well to unseen datasets, and shows a fast runtime performance, but also is adaptable to different keypoints. To support further research, all of the work is publicly accessible."
13,679d459debd8ffd557a2ae6d,cs.CV,https://arxiv.org/pdf/2501.18595,ROSA: Reconstructing Object Shape and Appearance Textures by Adaptive Detail Transfer,"Julian Kaltheuner, Patrick Stotko, Reinhard Klein",Computer Vision and Pattern Recognition,"Reconstructing an object’s shape and appearance in terms of a mesh textured by a spatially-varying bidirectional reflectance distribution function (SVBRDF) from a limited set of images captured under collocated light is an ill-posed problem.
Previous state-of-the-art approaches either aim to reconstruct the appearance directly on the geometry or additionally use texture normals as part of the appearance features.
However, this requires detailed but inefficiently large meshes, that would have to be simplified in a post-processing step, or suffers from well-known limitations of normal maps such as missing shadows or incorrect silhouettes.
Another limiting factor is the fixed and typically low resolution of the texture estimation resulting in loss of important surface details.
To overcome these problems, we present ROSA, an inverse rendering method that directly optimizes mesh geometry with spatially adaptive mesh resolution solely based on the image data.
In particular, we refine the mesh and locally condition the surface smoothness based on the estimated normal texture and mesh curvature.
In addition, we enable the reconstruction of fine appearance details in high-resolution textures through a pioneering tile-based method that operates on a single pre-trained decoder network but is not limited by the network output resolution."
14,679d459debd8ffd557a2ae6e,cs.CV,https://arxiv.org/pdf/2501.18594,Foundational Models for 3D Point Clouds: A Survey and Outlook,"Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, Yunpeng Li",Computer Vision and Pattern Recognition,"The 3D point cloud representation plays a crucial role in preserving the geometric fidelity of the physical world, enabling more accurate understanding and interaction with complex 3D environments.
While humans naturally comprehend the intricate relationships between objects, their spatial arrangements, and variations through a multisensory system, artificial intelligence (AI) systems have yet to fully replicate this capacity. To bridge this gap, it becomes essential to incorporate multiple modalities, such as images, text, audio, and point clouds. Models that can seamlessly integrate and reason across these modalities are known as foundation models (FMs). The development of FMs for 2D modalities, such as images and text, has seen significant progress, driven by the abundant availability of large-scale datasets. However, the 3D domain has lagged due to the scarcity of labelled data and high computational overheads. In response, recent research has begun to explore the potential of applying FMs to 3D tasks, overcoming these challenges by leveraging existing 2D knowledge.
Additionally, language, with its capacity for abstract reasoning and description of the environment, offers a promising avenue for enhancing 3D understanding through large pre-trained language models (LLMs).
Despite the rapid development and adoption of FMs for 3D vision tasks in recent years, there remains a gap in comprehensive and in-depth literature reviews. This article aims to address this gap by presenting a comprehensive overview of the state-of-the-art methods that utilize FMs for 3D visual understanding.
We start by reviewing various strategies employed in the building of various 3D FMs. Then we categorize and summarize use of different FMs for tasks such as perception tasks. Finally, the article offers insights into future directions for research and development in this field. This survey is intended to serve as a structured guide for researchers and practitioners seeking to delve into this emerging area of study, providing both a summary of existing knowledge and a roadmap for future exploration. To complement this survey, we provide a curated list of
relevant papers on the topic:https://github.com/vgthengane/Awesome-FMs-in-3D"
15,679d459debd8ffd557a2ae6f,cs.CV,https://arxiv.org/pdf/2501.18593,Diffusion Autoencoders are Scalable Image Tokenizers,"Yinbo Chen, Rohit Girdhar, Xiaolong Wang, Sai Saketh Rambhatla, Ishan Misra","Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning","Tokenizing images into compact visual representations is a key step in learning efficient and high-quality image generative models.
We present a simple diffusion tokenizer (DiTo) that learns compact visual representations for image generation models.
Our key insight is that a single learning objective, diffusion L2 loss, can be used for training scalable image tokenizers.
Since diffusion is already widely used for image generation, our insight greatly simplifies training such tokenizers.
In contrast, current state-of-the-art tokenizers rely on an empirically found combination of heuristics and losses, thus requiring a complex training recipe that relies on non-trivially balancing different losses and pretrained supervised models.
We show design decisions, along with theoretical grounding, that enable us to scale DiTo for learning competitive image representations.
Our results show that DiTo is a simpler, scalable, and self-supervised alternative to the current state-of-the-art image tokenizer which is supervised.
DiTo achieves competitive or better quality than state-of-the-art in image reconstruction and downstream image generation tasks.
Project page and code:https://yinboc.github.io/dito/."
16,679d459debd8ffd557a2ae71,cs.CV,https://arxiv.org/pdf/2501.18590,DiffusionRenderer: Neural Inverse and Forward Rendering with Video Diffusion Models,"Ruofan Liang, Zan Gojcic, Huan Ling, Jacob Munkberg, Jon Hasselgren, Zhi-Hao Lin, Jun Gao, Alexander Keller, Nandita Vijaykumar, Sanja Fidler, Zian Wang","Computer Vision and Pattern Recognition, Graphics","Understanding and modeling lighting effects are fundamental tasks in computer vision and graphics. Classic physically-based rendering (PBR) accurately simulates the light transport, but relies on precise scene representations–explicit 3D geometry, high-quality material properties, and lighting conditions–that are often impractical to obtain in real-world scenarios.
Therefore, we introduceDiffusionRenderer, a neural approach that addresses the dual problem of inverse and forward rendering within a holistic framework.
Leveraging powerful video diffusion model priors, the inverse rendering model accurately estimates G-buffers from real-world videos, providing an interface for image editing tasks, and training data for the rendering model.
Conversely,
our rendering model generates photorealistic images from G-buffers without explicit light transport simulation.
Specifically, we first train a video diffusion model for inverse rendering on synthetic data, which generalizes well to real-world videos and allows us to auto-label diverse real-world videos. We then co-train our rendering model using both synthetic and auto-labeled real-world data.
Experiments demonstrate thatDiffusionRenderereffectively approximates inverse and forwards rendering, consistently outperforming the state-of-the-art. Our model enables practical applications from a single video input—including relighting, material editing, and realistic object insertion."
17,679d459debd8ffd557a2ae72,cs.CV,https://arxiv.org/pdf/2501.18545,UDC-VIT: A Real-World Video Dataset for Under-Display Cameras,"Kyusu Ahn, JiSoo Kim, Sangik Lee, HyunGyu Lee, Byeonghyun Ko, Chanwoo Park, Jaejin Lee",Computer Vision and Pattern Recognition,"Under Display Camera (UDC) is an advanced imaging system that places a digital camera lens underneath a display panel, effectively concealing the camera. However, the display panel significantly degrades captured images or videos, introducing low transmittance, blur, noise, and flare issues. Tackling such issues is challenging because of the complex degradation of UDCs, including diverse flare patterns. Despite extensive research on UDC images and their restoration models, studies on videos have yet to be significantly explored. While two UDC video datasets exist, they primarily focus on unrealistic or synthetic UDC degradation rather than real-world UDC degradation. In this paper, we propose a real-world UDC video dataset called UDC-VIT. Unlike existing datasets, only UDC-VIT exclusively includes human motions that target facial recognition. We propose a video-capturing system to simultaneously acquire non-degraded and UDC-degraded videos of the same scene. Then, we align a pair of captured videos frame by frame, using discrete Fourier transform (DFT). We compare UDC-VIT with six representative UDC still image datasets and two existing UDC video datasets. Using six deep-learning models, we compare UDC-VIT and an existing synthetic UDC video dataset. The results indicate the ineffectiveness of models trained on earlier synthetic UDC video datasets, as they do not reflect the actual characteristics of UDC-degraded videos. We also demonstrate the importance of effective UDC restoration by evaluating face recognition accuracy concerning PSNR, SSIM, and LPIPS scores. UDC-VIT enables further exploration in the UDC video restoration and offers better insights into the challenge. UDC-VIT is available atour project site."
18,679d459debd8ffd557a2ae73,cs.CV,https://arxiv.org/pdf/2501.18543,Learning Priors of Human Motion With Vision Transformers,"Placido Falqueto, Alberto Sanfeliu, Luigi Palopoli, Daniele Fontanelli","Computer Vision and Pattern Recognition, Robotics","A clear understanding of where humans move in a scenario, their usual paths and speeds, and where they stop, is very important for different applications, such as mobility studies in urban areas or robot navigation tasks within human-populated environments.
We propose in this article, a neural architecture based on Vision Transformers (ViTs) to provide this information. This solution can arguably capture spatial correlations more effectively than Convolutional Neural Networks (CNNs). In the paper, we describe the methodology and proposed neural architecture and show the experiments’ results with a standard dataset. We show that the proposed ViT architecture improves the metrics compared to a method based on a CNN."
19,679d459debd8ffd557a2ae74,cs.CV,https://arxiv.org/pdf/2501.18538,Mini-ResEmoteNet: Leveraging Knowledge Distillation for Human-Centered Design,"Amna Murtada, Omnia Abdelrhman, Tahani Abdalla Attia",Computer Vision and Pattern Recognition,
20,679d459debd8ffd557a2ae75,cs.CV,https://arxiv.org/pdf/2501.18533,Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models,"Yi Ding, Lijun Li, Bing Cao, Jing Shao","Computer Vision and Pattern Recognition, Computation and Language, Cryptography and Security","Large Vision-Language Models (VLMs) have achieved remarkable performance across a wide range of tasks. However, their deployment in safety-critical domains poses significant challenges. Existing safety fine-tuning methods, which focus on textual or multimodal content, fall short in addressing challenging cases or disrupt the balance between helpfulness and harmlessness. Our evaluation highlights a safety reasoning gap: these methods lack safety visual reasoning ability, leading to such bottlenecks. To address this limitation and enhance both visual perception and reasoning in safety-critical contexts, we propose a novel dataset that integrates multi-image inputs with safety Chain-of-Thought (CoT) labels as fine-grained reasoning logic to improve model performance. Specifically, we introduce the Multi-Image Safety (MIS) dataset, an instruction-following dataset tailored for multi-image safety scenarios, consisting of training and test splits. Our experiments demonstrate that fine-tuning InternVL2.5-8B with MIS significantly outperforms both powerful open-source models and API-based models in challenging multi-image tasks requiring safety-related visual reasoning. This approach not only delivers exceptional safety performance but also preserves general capabilities without any trade-offs. Specifically, fine-tuning with MIS increases average accuracy by 0.83% across five general benchmarks and reduces the Attack Success Rate (ASR) on multiple safety benchmarks by a large margin. Data and Models are released under:https://dripnowhy.github.io/MIS/NOTE: This paper contains harmful images & text examples."
21,679d459debd8ffd557a2ae76,cs.CV,https://arxiv.org/pdf/2501.18517,Integrating Spatial and Frequency Information for Under-Display Camera Image Restoration,"Kyusu Ahn, Jinpyo Kim, Chanwoo Park, JiSoo Kim, Jaejin Lee",Computer Vision and Pattern Recognition,"Under-Display Camera (UDC) houses a digital camera lens under a display panel. However, UDC introduces complex degradations such as noise, blur, decrease in transmittance, and flare. Despite the remarkable progress, previous research on UDC mainly focuses on eliminating diffraction in the spatial domain and rarely explores its potential in the frequency domain. It is essential to consider both the spatial and frequency domains effectively. For example, degradations, such as noise and blur, can be addressed by local information (e.g., CNN kernels in the spatial domain). At the same time, tackling flares may require leveraging global information (e.g., the frequency domain). In this paper, we revisit the UDC degradations in the Fourier space and figure out intrinsic frequency priors that imply the presence of the flares. Based on this observation, we propose a novel multi-level DNN architecture called SFIM. It efficiently restores UDC-distorted images by integrating local and global (the collective contribution of all points in the image) information. The architecture exploits CNNs to capture local information and FFT-based models to capture global information. SFIM comprises a spatial domain block (SDB), a Frequency Domain Block (FDB), and an Attention-based Multi-level Integration Block (AMIB). Specifically, SDB focuses more on detailed textures such as noise and blur, FDB emphasizes irregular texture loss in extensive areas such as flare, and AMIB enables effective cross-domain interaction. SFIM’s superior performance over state-of-the-art approaches is demonstrated through rigorous quantitative and qualitative assessments across three UDC benchmarks."
22,679d459debd8ffd557a2ae77,cs.CV,https://arxiv.org/pdf/2501.18509,Deconstruct Complexity (DeComplex): A Novel Perspective on Tackling Dense Action Detection,"Faegheh Sardari, Armin Mustafa, Philip J. B. Jackson, Adrian Hilton",Computer Vision and Pattern Recognition,"Dense action detection involves detecting multiple co-occurring actions in an untrimmed video while action classes are often ambiguous and represent overlapping concepts. To address this challenge task, we introduce a novel perspective inspired by how humans tackle complex tasks by breaking them into manageable sub-tasks. Instead of relying on a single network to address the entire problem, as in current approaches, we propose decomposing the problem into detecting key concepts present in action classes—specifically, detecting dense static concepts and detecting dense dynamic concepts—and assigning them to distinct, specialized networks. Furthermore, simultaneous actions in a video often exhibit interrelationships, and exploiting these relationships can improve performance. However, we argue that current networks fail to effectively learn these relationships due to their reliance on binary cross-entropy optimization, which treats each class independently. To address this limitation, we propose providing explicit supervision on co-occurring concepts during network optimization through a novel language-guided contrastive learning loss. Our extensive experiments demonstrate the superiority of our approach over state-of-the-art methods, achieving substantial relative improvements of23.4%and2.5%mAP on the challenging benchmark datasets, Charades and MultiTHUMOS."
23,679d459debd8ffd557a2ae78,cs.CV,https://arxiv.org/pdf/2501.18504,CLEAR: Cue Learning using Evolution for Accurate Recognition Applied to Sustainability Data Extraction,"Peter J. Bentley, Soo Ling Lim, Fuyuki Ishikawa","Computer Vision and Pattern Recognition, Artificial Intelligence, Neural and Evolutionary Computing",
24,679d459debd8ffd557a2ae7d,cs.CV,https://arxiv.org/pdf/2501.18474,Tuning Vision Foundation Model via Test-Time Prompt-Guided Training for VFSS Segmentations,"Chengxi Zeng, David Smithard, Alberto M Gambaruto, Tilo Burghardt",Computer Vision and Pattern Recognition,"Vision foundation models have demonstrated exceptional generalization capabilities in segmentation tasks for both generic and specialized images. However, a performance gap persists between foundation models and task-specific, specialized models. Fine-tuning foundation models on downstream datasets is often necessary to bridge this gap. Unfortunately, obtaining fully annotated ground truth for downstream datasets is both challenging and costly.
To address this limitation, we propose a novel test-time training paradigm that enhances the performance of foundation models on downstream datasets without requiring full annotations. Specifically, our method employs simple point prompts to guide a test-time semi-self-supervised training task. The model learns by resolving the ambiguity of the point prompt through various augmentations. This approach directly tackles challenges in the medical imaging field, where acquiring annotations is both time-intensive and expensive.
We conducted extensive experiments on our new Videofluoroscopy dataset (VFSS-5k) for the instance segmentation task, achieving an average Dice coefficient of 0.868 across 12 anatomies with a single model."
25,679d459debd8ffd557a2ae86,cs.CV,https://arxiv.org/pdf/2501.18328,CodeBrain: Impute Any Brain MRI via Instance-specific Scalar-quantized Codes,"Yicheng Wu, Tao Song, Zhonghua Wu, Zongyuan Ge, Zhaolin Chen, Jianfei Cai","Computer Vision and Pattern Recognition, Artificial Intelligence","MRI imputation aims to synthesize the missing modality from one or more available ones, which is highly desirable since it reduces scanning costs and delivers comprehensive MRI information to enhance clinical diagnosis.
In this paper, we propose a unified model,CodeBrain, designed to adapt to various brain MRI imputation scenarios. The core design lies in casting various inter-modality transformations as a full-modality code prediction task.
To this end, CodeBrain is trained in two stages:ReconstructionandCode Prediction. First, in the Reconstruction stage, we reconstruct each MRI modality, which is mapped into a shared latent space followed by a scalar quantization. Since such quantization is lossy and the code is low dimensional, another MRI modality belonging to the same subject is randomly selected to generate common features to supplement the code and boost the target reconstruction.
In the second stage, we train another encoder by a customized grading loss to predict the full-modality codes from randomly masked MRI samples, supervised by the corresponding quantized codes generated from the first stage.
In this way, the inter-modality transformation is achieved by mapping the instance-specific codes in a finite scalar space.
We evaluated the proposed CodeBrain model on two public brain MRI datasets (i.e., IXI and BraTS 2023). Extensive experiments demonstrate that our CodeBrain model achieves superior imputation performance compared to four existing methods, establishing a new state of the art for unified brain MRI imputation.
Codes will be released."
26,679d459debd8ffd557a2ae87,cs.CV,https://arxiv.org/pdf/2501.18324,A Video-grounded Dialogue Dataset and Metric for Event-driven Activities,"Wiradee Imrattanatrai, Masaki Asada, Kimihiro Hasegawa, Zhi-Qi Cheng, Ken Fukuda, Teruko Mitamura","Computer Vision and Pattern Recognition, Computation and Language","This paper presents VDAct, a dataset for a Video-grounded Dialogue on Event-driven Activities, alongside VDEval, a session-based context evaluation metric specially designed for the task.
Unlike existing datasets, VDAct includes longer and more complex video sequences that depict a variety of event-driven activities that require advanced contextual understanding for accurate response generation.
The dataset comprises 3,000 dialogues with over 30,000 question-and-answer pairs, derived from 1,000 videos with diverse activity scenarios.
VDAct displays a notably challenging characteristic due to its broad spectrum of activity scenarios and wide range of question types.
Empirical studies on state-of-the-art vision foundation models highlight their limitations in addressing certain question types on our dataset.
Furthermore, VDEval, which integrates dialogue session history and video content summaries extracted from our supplementary Knowledge Graphs to evaluate individual responses, demonstrates a significantly higher correlation with human assessments on the VDAct dataset than existing evaluation metrics that rely solely on the context of single dialogue turns."
27,679d459debd8ffd557a2ae88,cs.CV,https://arxiv.org/pdf/2501.18315,Surface Defect Identification using Bayesian Filtering on a 3D Mesh,"Matteo Dalle Vedove, Matteo Bonetto, Edoardo Lamon, Luigi Palopoli, Matteo Saveriano, Daniele Fontanelli","Computer Vision and Pattern Recognition, Robotics","This paper presents a CAD-based approach for automated surface defect
detection. We leverage the a-priori knowledge embedded in a CAD model
and integrate it with point cloud data acquired from commercially
available stereo and depth cameras. The proposed method first
transforms the CAD model into a high-density polygonal mesh, where
each vertex represents a state variable in 3D space. Subsequently, a
weighted least squares algorithm is employed to iteratively estimate
the state of the scanned workpiece based on the captured point cloud
measurements. This framework offers the potential to incorporate
information from diverse sensors into the CAD domain, facilitating a
more comprehensive analysis. Preliminary results demonstrate promising
performance, with the algorithm achieving convergence to a
sub-millimeter standard deviation in the region of interest using only
approximately 50 point cloud samples. This highlights the potential of
utilising commercially available stereo cameras for high-precision
quality control applications."
28,679d459debd8ffd557a2ae89,cs.CV,https://arxiv.org/pdf/2501.18313,Simulation of microstructures and machine learning,"Katja Schladitz, Claudia Redenbach, Tin Barisin, Christian Jung, Natascha Jeziorski, Lovro Bosnar, Juraj Fulir, Petra Gospodnetić",Computer Vision and Pattern Recognition,"Machine learning offers attractive solutions to challenging image processing tasks.
Tedious development and parametrization of algorithmic solutions can be replaced by
training a convolutional neural network or a random forest with a high potential to
generalize. However, machine learning methods rely on huge amounts of representative
image data along with a ground truth, usually obtained by manual annotation.
Thus, limited availability of training data is a critical
bottleneck. We discuss two use cases:
optical quality control in industrial production and segmenting crack structures in 3D images of concrete.
For optical quality control, all defect types have to be trained but
are typically not evenly
represented in the training data. Additionally, manual annotation is
costly and often inconsistent.
It is nearly
impossible in the second case: segmentation of
crack systems in 3D images of concrete.
Synthetic images, generated based on realizations of stochastic geometry models,
offer an elegant way out. A wide variety of structure types can be generated.
The within structure variation is naturally captured by the stochastic nature of
the models and the ground truth is for free. Many new questions
arise. In particular, which characteristics of the real image data have to be
met to which degree of fidelity.111This is a preprint of the following chapter:
Katja Schladitz, Claudia Redenbach, Tin Barisin, Christian Jung,
Natascha Jeziorski, Lovro Bosnar, Juraj Fulir, and Petra
Gospodnetić: Simulation of Microstructures and Machine Learning,
published in Continuum Models and Discrete Systems, edited by François
Willot, Justin Dirrenberger, Samuel Forest, Dominique Jeulin, Andrej
V. Cherkaev, 2024, Springer Cham, reproduced with permission of
Springer Nature Switzerland AG 2024. The final authenticated version
is available online at:
https://doi.org/10.1007/978-3-031-58665-1"
29,679d459debd8ffd557a2ae8a,cs.CV,https://arxiv.org/pdf/2501.18294,A Comprehensive Analysis on Machine Learning based Methods for Lung Cancer Level Classification,"Shayli Farshchiha, Salman Asoudeh, Maryam Shavali Kuhshuri, Mehrshad Eisaeid, Mohamadreza Azadie, Saba Hesaraki","Computer Vision and Pattern Recognition, Artificial Intelligence",
30,679d459debd8ffd557a2ae8b,cs.CV,https://arxiv.org/pdf/2501.18269,MAMS: Model-Agnostic Module Selection Framework for Video Captioning,"Sangho Lee, Il Yong Chun, Hogun Park","Computer Vision and Pattern Recognition, Artificial Intelligence","Multi-modal transformers are rapidly gaining attention in video captioning tasks.
Existing multi-modal video captioning methods typically extract a fixed number of frames, which raises critical challenges.
When a limited number of frames are extracted, important frames with essential information for caption generation may be missed.
Conversely, extracting an excessive number of frames includes consecutive frames, potentially causing redundancy in visual tokens extracted from consecutive video frames.
To extract an appropriate number of frames for each video, this paper proposes the first model-agnostic module selection framework in video captioning that has two main functions:(1)selecting a caption generation module with an appropriate size based on visual tokens extracted from video frames, and(2)constructing subsets of visual tokens for the selected caption generation module.
Furthermore, we propose a new adaptive attention masking scheme that enhances attention on important visual tokens.
Our experiments on three different benchmark datasets demonstrate that the proposed framework significantly improves the performance of three recent video captioning models."
31,679d459debd8ffd557a2ae8c,cs.CV,https://arxiv.org/pdf/2501.18246,Ground Awareness in Deep Learning for Large Outdoor Point Cloud Segmentation,"Kevin Qiu, Dimitri Bulatov, Dorota Iwaszczuk",Computer Vision and Pattern Recognition,"This paper presents an analysis of utilizing elevation data to aid outdoor point cloud semantic segmentation through existing machine-learning networks in remote sensing,specifically in urban, built-up areas.
In dense outdoor point clouds, the receptive field of a machine learning model may be too small to accurately determine the surroundings and context of a point.
By computing Digital Terrain Models (DTMs) from the point clouds, we extract the relative elevation feature, which is the vertical distance from the terrain to a point.
RandLA-Netis employedfor efficient semantic segmentation of large-scale point clouds. We assess its performance across three diverse outdoor datasets captured with varying sensor technologies and sensor locations.
Integration of relative elevation data leads to consistent performance improvements across all three datasets, most notably in the Hessigheim dataset, with an increase of 3.7percentage pointsin average F1 scorefrom 72.35% to 76.01%, by establishing long-range dependencies between ground and objects.We alsoexplore additionallocalfeatures such as planarity, normal vectors, and 2D features, but their efficacy varied based on the characteristics of the point cloud.
Ultimately, this study underscores the important role of thenon-localrelative elevation feature for semantic segmentation of point clouds in remote sensing applications."
32,679d459debd8ffd557a2ae8d,cs.CV,https://arxiv.org/pdf/2501.18237,Arbitrary Data as Images: Fusion of Patient Data Across Modalities and Irregular Intervals with Vision Transformers,"Malte Tölle, Mohamad Scharaf, Samantha Fischer, Christoph Reich, Silav Zeid, Christoph Dieterich, Benjamin Meder, Norbert Frey, Philipp Wild, Sandy Engelhardt","Computer Vision and Pattern Recognition, Artificial Intelligence","A patient undergoes multiple examinations in each hospital stay, where each provides different facets of the health status.
These assessments include temporal data with varying sampling rates, discrete single-point measurements, therapeutic interventions such as medication administration, and images. While physicians are able to process and integrate diverse modalities intuitively, neural networks need specific modeling for each modality complicating the training procedure.
We demonstrate that this complexity can be significantly reduced by visualizing all information as images along with unstructured text and subsequently training a conventional vision-text transformer.
Our approach,VisionTransformer forirregular sampledMulti-modalMeasurements (ViTiMM), not only simplifies data preprocessing and modeling but also outperforms current state-of-the-art methods in predicting in-hospital mortality and phenotyping, as evaluated on 6,175 patients from the MIMIC-IV dataset.
The modalities include patient’s clinical measurements, medications, X-ray images, and electrocardiography scans.
We hope our work inspires advancements in multi-modal medical AI by reducing the training complexity to (visual) prompt engineering, thus lowering entry barriers and enabling no-code solutions for training.
The source code will be made publicly available."
33,679d459debd8ffd557a2ae8e,cs.CV,https://arxiv.org/pdf/2501.18232,Free-T2M: Frequency Enhanced Text-to-Motion Diffusion Model With Consistency Loss,"Wenshuo Chen, Haozhe Jia, Songning Lai, Keming Wu, Hongru Xiao, Lijie Hu, Yutao Yue",Computer Vision and Pattern Recognition,"Rapid progress in text-to-motion generation has been largely driven by diffusion models. However, existing methods focus solely on temporal modeling, thereby overlooking frequency-domain analysis. We identify two key phases in motion denoising: thesemantic planning stageand thefine-grained improving stage. To address these phases effectively, we proposeFrequencyenhancedtext-to-motion diffusion model (Free-T2M), incorporating stage-specific consistency losses that enhance the robustness of static features and improve fine-grained accuracy. Extensive experiments demonstrate the effectiveness of our method. Specifically, on StableMoFusion, our method reduces the FID from0.189to0.051, establishing a new SOTA performance within the diffusion architecture. These findings highlight the importance of incorporating frequency-domain insights into text-to-motion generation for more precise and robust results.
The code and visualization results can be found on thewebsite."
34,679d459debd8ffd557a2ae8f,cs.CV,https://arxiv.org/pdf/2501.18192,Machine Learning Fairness for Depression Detection using EEG Data,"Angus Man Ho Kwok, Jiaee Cheong, Sinan Kalkan, Hatice Gunes","Computer Vision and Pattern Recognition, Machine Learning, Signal Processing",
35,679d459debd8ffd557a2ae90,cs.CV,https://arxiv.org/pdf/2501.18162,IROAM: Improving Roadside Monocular 3D Object Detection Learning from Autonomous Vehicle Data Domain,"Zhe Wang, Xiaoliang Huo, Siqi Fan, Jingjing Liu, Ya-Qin Zhang, Yan Wang","Computer Vision and Pattern Recognition, Robotics","In autonomous driving, The perception capabilities of the ego-vehicle can be improved with roadside sensors, which can provide a holistic view of the environment. However, existing monocular detection methods designed for vehicle cameras are not suitable for roadside cameras due to viewpoint domain gaps. To bridge this gap andImprove ROAdside Monocular 3D object detection, we propose IROAM, a semantic-geometry decoupled contrastive learning framework, which takes vehicle-side and roadside data as input simultaneously. IROAM has two significant modules. In-Domain Query Interaction module utilizes a transformer to learn content and depth information for each domain and outputs object queries. Cross-Domain Query Enhancement To learn better feature representations from two domains, Cross-Domain Query Enhancement decouples queries into semantic and geometry parts and only the former is used for contrastive learning. Experiments demonstrate the effectiveness of IROAM in improving roadside detector’s performance. The results validate that IROAM has the capabilities to learn cross-domain information."
36,679d459debd8ffd557a2ae91,cs.CV,https://arxiv.org/pdf/2501.18124,REMOTE: Real-time Ego-motion Tracking for Various Endoscopes via Multimodal Visual Feature Learning,"Liangjing Shao, Benshuang Chen, Shuting Zhao, Xinrong Chen","Computer Vision and Pattern Recognition, Artificial Intelligence","Real-time ego-motion tracking for endoscope is a significant task for efficient navigation and robotic automation of endoscopy. In this paper, a novel framework is proposed to perform real-time ego-motion tracking for endoscope. Firstly, a multi-modal visual feature learning network is proposed to perform relative pose prediction, in which the motion feature from the optical flow, the scene features and the joint feature from two adjacent observations are all extracted for prediction. Due to more correlation information in the channel dimension of the concatenated image, a novel feature extractor is designed based on an attention mechanism to integrate multi-dimensional information from the concatenation of two continuous frames. To extract more complete feature representation from the fused features, a novel pose decoder is proposed to predict the pose transformation from the concatenated feature map at the end of the framework. At last, the absolute pose of endoscope is calculated based on relative poses. The experiment is conducted on three datasets of various endoscopic scenes and the results demonstrate that the proposed method outperforms state-of-the-art methods. Besides, the inference speed of the proposed method is over 30 frames per second, which meets the real-time requirement. The project page is here:remote-bmxs.netlify.app"
37,679d459debd8ffd557a2ae92,cs.CV,https://arxiv.org/pdf/2501.18116,DeepFRC: An End-to-End Deep Learning Model for Functional Registration and Classification,"Siyuan Jiang, Yihan Hu, Wenjie Li, Pengcheng Zeng","Computer Vision and Pattern Recognition, Machine Learning, Machine Learning","Functional data analysis (FDA) is essential for analyzing continuous, high-dimensional data, yet existing methods often decouple functional registration and classification, limiting their efficiency and performance. We present DeepFRC, an end-to-end deep learning framework that unifies these tasks within a single model. Our approach incorporates an alignment module that learns time warping functions via elastic function registration and a learnable basis representation module for dimensionality reduction on aligned data. This integration enhances both alignment accuracy and predictive performance. Theoretical analysis establishes that DeepFRC achieves low misalignment and generalization error, while simulations elucidate the progression of registration, reconstruction, and classification during training. Experiments on real-world datasets demonstrate that DeepFRC consistently outperforms state-of-the-art methods, particularly in addressing complex registration challenges. Code is available at: https://github.com/Drivergo-93589/DeepFRC."
38,679d459debd8ffd557a2ae93,cs.CV,https://arxiv.org/pdf/2501.18098,Disentangling Safe and Unsafe Corruptions via Anisotropy and Locality,"Ramchandran Muthukumar, Ambar Pal, Jeremias Sulam, Rene Vidal","Computer Vision and Pattern Recognition, Machine Learning","State-of-the-art machine learning systems are vulnerable to small perturbations to their input, where “small” is defined according to a threat model that assigns a positive threat to each perturbation. Most prior works define a task-agnostic, isotropic, and global threat, like theℓpsubscriptℓ𝑝\ell_{p}roman_ℓ start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPTnorm, where the magnitude of the perturbation fully determines the degree of the threat and neither the direction of the attack nor its position in space matter. However, common corruptions in computer vision, such as blur, compression, or occlusions, are not well captured by such threat models. This paper proposes a novel threat model calledProjected Displacement(PD) to study robustness beyond existing isotropic and global threat models. The proposed threat model measures the threat of a perturbation via its alignment withunsafe directions, defined as directions in the input space along which a perturbation of sufficient magnitude changes the ground truth class label. Unsafe directions are identified locally for each input based on observed training data. In this way, the PD threat model exhibits anisotropy and locality. The PD threat model is computationally efficient and can be easily integrated into existing robustness pipelines. Experiments on Imagenet-1k data indicate that, for any input, the set of perturbations with small PD threat includessafeperturbations of largeℓpsubscriptℓ𝑝\ell_{p}roman_ℓ start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPTnorm that preserve the true label, such as noise, blur and compression, while simultaneously excludingunsafeperturbations that alter the true label. Unlike perceptual threat models based on embeddings of large-vision models, the PD threat model can be readily computed for arbitrary classification tasks without pre-training or finetuning. Further additional task information such as sensitivity to image regions or concept hierarchies can be easily integrated into the assessment of threat and thus the PD threat model presents practitioners with a flexible, task-driven threat specification that alleviates the limitations ofℓpsubscriptℓ𝑝\ell_{p}roman_ℓ start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT-threat models."
39,679d459debd8ffd557a2ae94,cs.CV,https://arxiv.org/pdf/2501.18096,LLMs can see and hear without any training,"Kumar Ashutosh, Yossi Gandelsman, Xinlei Chen, Ishan Misra, Rohit Girdhar","Computer Vision and Pattern Recognition, Artificial Intelligence, Computation and Language, Machine Learning",
40,679d459debd8ffd557a2ae95,cs.CV,https://arxiv.org/pdf/2501.18033,Generative AI for Vision: A Comprehensive Study of Frameworks and Applications,Fouad Bousetouane,Computer Vision and Pattern Recognition,"Generative AI is transforming image synthesis, enabling the creation of high-quality, diverse, and photorealistic visuals across industries like design, media, healthcare, and autonomous systems. Advances in techniques such as image-to-image translation, text-to-image generation, domain transfer, and multimodal alignment have broadened the scope of automated visual content creation, supporting a wide spectrum of applications. These advancements are driven by models like Generative Adversarial Networks (GANs), conditional frameworks, and diffusion-based approaches such as Stable Diffusion."
41,679d459debd8ffd557a2ae96,cs.CV,https://arxiv.org/pdf/2501.18011,Anatomy Might Be All You Need: Forecasting What to Do During Surgery,"Gary Sarwin, Alessandro Carretta, Victor Staartjes, Matteo Zoli, Diego Mazzatenta, Luca Regli, Carlo Serra, Ender Konukoglu","Computer Vision and Pattern Recognition, Artificial Intelligence","Surgical guidance can be delivered in various ways. In neurosurgery, spatial guidance and orientation are predominantly achieved through neuronavigation systems that reference pre-operative MRI scans. Recently, there has been growing interest in providingliveguidance by analyzing video feeds from tools such as endoscopes. Existing approaches, including anatomy detection, orientation feedback, phase recognition, and visual question-answering, primarily focus on aiding surgeons in assessing the current surgical scene. This work aims to provide guidance on a finer scale, aiming to provide guidance byforecasting the trajectory of the surgical instrument, essentially addressing the question of what to do next. To address this task, we propose a model that not only leverages the historical locations of surgical instruments but also integrates anatomical features. Importantly, our work does not rely on explicit ground truth labels for instrument trajectories. Instead, the ground truth is generated by a detection model trained to detect both anatomical structures and instruments within surgical videos of a comprehensive dataset containing pituitary surgery videos. By analyzing the interaction between anatomy and instrument movements in these videos and forecasting future instrument movements, we show that anatomical features are a valuable asset in addressing this challenging task. To the best of our knowledge, this work is the first attempt to address this task for manually operated surgeries."
42,679d459debd8ffd557a2ae97,cs.CV,https://arxiv.org/pdf/2501.17987,Pressure Field Reconstruction with SIREN: A Mesh-Free Approach for Image Velocimetry in Complex Noisy Environments,"Renato F. Miotto, William R. Wolf, Fernando Zigunov","Computer Vision and Pattern Recognition, Fluid Dynamics","This work presents a novel approach for pressure field reconstruction from image velocimetry data using SIREN (Sinusoidal Representation Network), emphasizing its effectiveness as an implicit neural representation in noisy environments and its mesh-free nature. While we briefly assess two recently proposed methods — one-shot matrix-omnidirectional integration (OS/MODI) and Green’s function integral (GFI) — the primary focus is on the advantages of the SIREN approach. The OS/MODI technique performs well in noise-free conditions and with structured meshes but struggles when applied to unstructured meshes with high aspect ratio. Similarly, the GFI method encounters difficulties due to singularities inherent from the Newtonian kernel. In contrast, the proposed SIREN approach is a mesh-free method that directly reconstructs the pressure field, bypassing the need for an intrinsic grid connectivity and, hence, avoiding the challenges associated with ill/conditioned cells and unstructured meshes. This provides a distinct advantage over traditional mesh-based methods. Moreover, it is shown that changes in the architecture of the SIREN can be used to filter out inherent noise from velocimetry data. This work positions SIREN as a robust and versatile solution for pressure reconstruction, particularly in noisy environments characterized by the absence of mesh structure, opening new avenues for innovative applications in this field."
43,679d459debd8ffd557a2ae98,cs.CV,https://arxiv.org/pdf/2501.17983,Efficient Feature Fusion for UAV Object Detection,"Xudong Wang, Chaomin Shen, Yaxin Peng",Computer Vision and Pattern Recognition,"Object detection in unmanned aerial vehicle (UAV) remote sensing images poses significant challenges due to unstable image quality, small object sizes, complex backgrounds, and environmental occlusions. Small objects, in particular, occupy minimal portions of images, making their accurate detection highly difficult.
Existing multi-scale feature fusion methods address these challenges to some extent by aggregating features across different resolutions. However, these methods often fail to effectively balance classification and localization performance for small objects, primarily due to insufficient feature representation and imbalanced network information flow.
In this paper, we propose a novel feature fusion framework specifically designed for UAV object detection tasks to enhance both localization accuracy and classification performance. The proposed framework integrates hybrid upsampling and downsampling modules, enabling feature maps from different network depths to be flexibly adjusted to arbitrary resolutions. This design facilitates cross-layer connections and multi-scale feature fusion, ensuring improved representation of small objects.
Our approach leverages hybrid downsampling to enhance fine-grained feature representation, improving spatial localization of small targets, even under complex conditions. Simultaneously, the upsampling module aggregates global contextual information, optimizing feature consistency across scales and enhancing classification robustness in cluttered scenes.
Experimental results on two public UAV datasets demonstrate the effectiveness of the proposed framework. Integrated into the YOLO-V10 model, our method achieves a 2% improvement in average precision (AP) compared to the baseline YOLO-V10 model, while maintaining the same number of parameters. These results highlight the potential of our framework for accurate and efficient UAV object detection."
44,679d459debd8ffd557a2ae99,cs.CV,https://arxiv.org/pdf/2501.17978,VoD-3DGS: View-opacity-Dependent 3D Gaussian Splatting,"Nowak Mateusz, Jarosz Wojciech, Chin Peter","Computer Vision and Pattern Recognition, Graphics, Machine Learning","Reconstructing a 3D scene from images is challenging due to the different ways light interacts with surfaces depending on the viewer’s position and the surface’s material. In classical computer graphics, materials can be classified as diffuse or specular, interacting with light differently. The standard 3D Gaussian Splatting model struggles to represent view-dependent content, since it cannot differentiate an object within the scene from the light interacting with its specular surfaces, which produce highlights or reflections. In this paper, we propose to extend the 3D Gaussian Splatting model by introducing an additional symmetric matrix to enhance the opacity representation of each 3D Gaussian. This improvement allows certain Gaussians to be suppressed based on the viewer’s perspective, resulting in a more accurate representation of view-dependent reflections and specular highlights without compromising the scene’s integrity. By allowing the opacity to be view dependent, our enhanced model achieves state-of-the-art performance on Mip-Nerf, Tanks&Temples, Deep Blending, and Nerf-Synthetic datasets without a significant loss in rendering speed, achieving>60⁢F⁢P⁢Sabsent60𝐹𝑃𝑆>60FPS> 60 italic_F italic_P italic_S, and only incurring a minimal increase in memory used."
45,679d459debd8ffd557a2ae9a,cs.CV,https://arxiv.org/pdf/2501.17977,TransRAD: Retentive Vision Transformer for Enhanced Radar Object Detection,"Lei Cheng, Siyang Cao","Computer Vision and Pattern Recognition, Systems and Control","Despite significant advancements in environment perception capabilities for autonomous driving and intelligent robotics, cameras and LiDARs remain notoriously unreliable in low-light conditions and adverse weather, which limits their effectiveness. Radar serves as a reliable and low-cost sensor that can effectively complement these limitations. However, radar-based object detection has been underexplored due to the inherent weaknesses of radar data, such as low resolution, high noise, and lack of visual information.
In this paper, we present TransRAD, a novel 3D radar object detection model designed to address these challenges by leveraging the Retentive Vision Transformer (RMT) to more effectively learn features from information-dense radar Range-Azimuth-Doppler (RAD) data. Our approach leverages the Retentive Manhattan Self-Attention (MaSA) mechanism provided by RMT to incorporate explicit spatial priors, thereby enabling more accurate alignment with the spatial saliency characteristics of radar targets in RAD data and achieving precise 3D radar detection across Range-Azimuth-Doppler dimensions. Furthermore, we propose Location-Aware NMS to effectively mitigate the common issue of duplicate bounding boxes in deep radar object detection.
The experimental results demonstrate that TransRAD outperforms state-of-the-art methods in both 2D and 3D radar detection tasks, achieving higher accuracy, faster inference speed, and reduced computational complexity. Code is available athttps://github.com/radar-lab/TransRAD"
46,679d459debd8ffd557a2ae9b,cs.CV,https://arxiv.org/pdf/2501.17906,Unsupervised Patch-GAN with Targeted Patch Ranking for Fine-Grained Novelty Detection in Medical Imaging,"Jingkun Chen, Guang Yang, Xiao Zhang, Jingchao Peng, Tianlu Zhang, Jianguo Zhang, Jungong Han, Vicente Grau","Computer Vision and Pattern Recognition, Image and Video Processing","Detecting novel anomalies in medical imaging is challenging due to the limited availability of labeled data for rare abnormalities, which often display high variability and subtlety. This challenge is further compounded when small abnormal regions are embedded within larger normal areas, as whole-image predictions frequently overlook these subtle deviations. To address these issues, we propose an unsupervised Patch-GAN framework designed to detect and localize anomalies by capturing both local detail and global structure. Our framework first reconstructs masked images to learn fine-grained, normal-specific features, allowing for enhanced sensitivity to minor deviations from normality. By dividing these reconstructed images into patches and assessing the authenticity of each patch, our approach identifies anomalies at a more granular level, overcoming the limitations of whole-image evaluation. Additionally, a patch-ranking mechanism prioritizes regions with higher abnormal scores, reinforcing the alignment between local patch discrepancies and the global image context. Experimental results on the ISIC 2016 skin lesion and BraTS 2019 brain tumor datasets validate our framework’s effectiveness, achieving AUCs of 95.79% and 96.05%, respectively, and outperforming three state-of-the-art baselines."
47,679d459debd8ffd557a2ae9c,cs.CV,https://arxiv.org/pdf/2501.17890,VidSole: A Multimodal Dataset for Joint Kinetics Quantification and Disease Detection with Deep Learning,"Archit Kambhamettu, Samantha Snyder, Maliheh Fakhar, Samuel Audia, Ross Miller, Jae Kun Shim, Aniket Bera","Computer Vision and Pattern Recognition, Signal Processing","Understanding internal joint loading is critical for diagnosing gait-related diseases such as knee osteoarthritis; however, current methods of measuring joint risk factors are time-consuming, expensive, and restricted to lab settings. In this paper, we enable the large-scale, cost-effective biomechanical analysis of joint loading via three key contributions: the development and deployment of novel instrumented insoles, the creation of a large multimodal biomechanics dataset (VidSole), and a baseline deep learning pipeline to predict internal joint loading factors. Our novel instrumented insole measures the tri-axial forces and moments across five high-pressure points under the foot. VidSole consists of the forces and moments measured by these insoles along with corresponding RGB video from two viewpoints, 3D body motion capture, and force plate data for over 2,600 trials of 52 diverse participants performing four fundamental activities of daily living (sit-to-stand, stand-to-sit, walking, and running). We feed the insole data and kinematic parameters extractable from video (i.e., pose, knee angle) into a deep learning pipeline consisting of an ensemble Gated Recurrent Unit (GRU) activity classifier followed by activity-specific Long Short Term Memory (LSTM) regression networks to estimate knee adduction moment (KAM), a biomechanical risk factor for knee osteoarthritis. The successful classification of activities at an accuracy of 99.02 percent and KAM estimation with mean absolute error (MAE) less than 0.5 percent*body weight*height, the current threshold for accurately detecting knee osteoarthritis with KAM, illustrates the usefulness of our dataset for future research and clinical settings."
48,679d459debd8ffd557a2ae9d,cs.CV,https://arxiv.org/pdf/2501.18588,Inkspire: Supporting Design Exploration with Generative AI through Analogical Sketching,"David Chuan-En Lin, Hyeonsu B. Kang, Nikolas Martelaro, Aniket Kittur, Yan-Ying Chen, Matthew K. Hong","Human-Computer Interaction, Artificial Intelligence, Computer Vision and Pattern Recognition, Multimedia","With recent advancements in the capabilities of Text-to-Image (T2I) AI models, product designers have begun experimenting with them in their work. However, T2I models struggle to interpret abstract language and the current user experience of T2I tools can induce design fixation rather than a more iterative, exploratory process. To address these challenges, we developed Inkspire, a sketch-driven tool that supports designers in prototyping product design concepts with analogical inspirations and a complete sketch-to-design-to-sketch feedback loop. To inform the design of Inkspire, we conducted an exchange session with designers and distilled design goals for improving T2I interactions. In a within-subjects study comparing Inkspire to ControlNet, we found that Inkspire supported designers with more inspiration and exploration of design ideas, and improved aspects of the co-creative process by allowing designers to effectively grasp the current state of the AI to guide it towards novel design intentions."
49,679d459debd8ffd557a2ae9e,cs.CV,https://arxiv.org/pdf/2501.18418,Task-based Regularization in Penalized Least-Squares for Binary Signal Detection Tasks in Medical Image Denoising,"Wentao Chen, Tianming Xu, Weimin Zhou","Image and Video Processing, Computer Vision and Pattern Recognition","Image denoising algorithms have been extensively investigated for medical imaging. To perform image denoising, penalized least-squares (PLS) problems can be designed and solved, in which the penalty term encodes prior knowledge of the object being imaged. Sparsity-promoting penalties, such as total variation (TV), have been a popular choice for regularizing image denoising problems. However, such hand-crafted penalties may not be able to preserve task-relevant information in measured image data and can lead to oversmoothed image appearances and patchy artifacts that degrade signal detectability. Supervised learning methods that employ convolutional neural networks (CNNs) have emerged as a popular approach to denoising medical images. However, studies have shown that CNNs trained with loss functions based on traditional image quality measures can lead to a loss of task-relevant information in images. Some previous works have investigated task-based loss functions that employ model observers for training the CNN denoising models. However, such training processes typically require a large number of noisy and ground-truth (noise-free or low-noise) image data pairs. In this work, we propose a task-based regularization strategy for use with PLS in medical image denoising. The proposed task-based regularization is associated with the likelihood of linear test statistics of noisy images for Gaussian noise models. The proposed method does not require ground-truth image data and solves an individual optimization problem for denoising each image. Computer-simulation studies are conducted that consider a multivariate-normally distributed (MVN) lumpy background and a binary texture background. It is demonstrated that the proposed regularization strategy can effectively improve signal detectability in denoised images."
50,679d459debd8ffd557a2ae9f,cs.CV,https://arxiv.org/pdf/2501.18412,Real Time Scheduling Framework for Multi Object Detection via Spiking Neural Networks,"Donghwa Kang, Woojin Shin, Cheol-Ho Hong, Minsuk Koo, Brent ByungHoon Kang, Jinkyu Lee, Hyeongboo Baek","Systems and Control, Computer Vision and Pattern Recognition, Neural and Evolutionary Computing","Given the energy constraints in autonomous mobile agents (AMAs), such as unmanned vehicles, spiking neural networks (SNNs) are increasingly favored as a more efficient alternative to traditional artificial neural networks.
AMAs employ multi-object detection (MOD) from multiple cameras to identify nearby objects while ensuring two essential objectives, (R1)timing guaranteeand (R2)high accuracyfor safety.
In this paper, we proposeRT-SNN, thefirstsystem design, aiming at achievingR1andR2in SNN-based MOD systems on AMAs.
Leveraging the characteristic that SNNs gather feature data of input image termed as membrane potential, through iterative computation over multiple timesteps,RT-SNNprovides multiple execution options with adjustable timesteps and a novel method for reusing membrane potential to supportR1.
Then, it captures how these execution strategies influenceR2by introducing a novel notion of mean absolute error and membrane confidence.
Further,RT-SNNdevelops a new scheduling framework consisting of offline schedulability analysis forR1and a run-time scheduling algorithm forR2using the notion of membrane confidence.
We deployedRT-SNNto Spiking-YOLO, the SNN-based MOD model derived from ANN-to-SNN conversion, and our experimental evaluation confirms its effectiveness in meeting theR1andR2requirements while providing significant energy efficiency."
51,679d459debd8ffd557a2aea0,cs.CV,https://arxiv.org/pdf/2501.18362,MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding,"Yuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu, Ermo Hua, Kaiyan Zhang, Ning Ding, Bowen Zhou","Artificial Intelligence, Computation and Language, Computer Vision and Pattern Recognition, Machine Learning",
52,679d459debd8ffd557a2aea1,cs.CV,https://arxiv.org/pdf/2501.18314,AGAV-Rater: Adapting Large Multimodal Model for AI-Generated Audio-Visual Quality Assessment,"Yuqin Cao, Xiongkuo Min, Yixuan Gao, Wei Sun, Guangtao Zhai","Multimedia, Computer Vision and Pattern Recognition, Sound, Audio and Speech Processing","Many video-to-audio (VTA) methods have been proposed for dubbing silent AI-generated videos. An efficient quality assessment method for AI-generated audio-visual content (AGAV) is crucial for ensuring audio-visual quality. Existing audio-visual quality assessment methods struggle with unique distortions in AGAVs, such as unrealistic and inconsistent elements. To address this, we introduceAGAVQA, the first large-scale AGAV quality assessment dataset, comprising3,38233823,3823 , 382AGAVs from16161616VTA methods. AGAVQA includes two subsets: AGAVQA-MOS, which provides multi-dimensional scores for audio quality, content consistency, and overall quality, and AGAVQA-Pair, designed for optimal AGAV pair selection. We further proposeAGAV-Rater, a LMM-based model that can score AGAVs, as well as audio and music generated from text, across multiple dimensions, and selects the best AGAV generated by VTA methods to present to the user. AGAV-Rater achieves state-of-the-art performance on AGAVQA, Text-to-Audio, and Text-to-Music datasets. Subjective tests also confirm that AGAV-Rater enhances VTA performance and user experience. The project page is available athttps://agav-rater.github.io."
53,679d459debd8ffd557a2aea2,cs.CV,https://arxiv.org/pdf/2501.18270,The iToBoS dataset: skin region images extracted from 3D total body photographs for lesion detection,"Anup Saha, Joseph Adeola, Nuria Ferrera, Adam Mothershaw, Gisele Rezze, Séraphin Gaborit, Brian D'Alessandro, James Hudson, Gyula Szabó, Balazs Pataki, Hayat Rajani, Sana Nazari, Hassan Hayat, Clare Primiero, H. Peter Soyer, Josep Malvehy, Rafael Garcia","Image and Video Processing, Artificial Intelligence, Computer Vision and Pattern Recognition","Artificial intelligence has significantly advanced skin cancer diagnosis by enabling rapid and accurate detection of malignant lesions. In this domain, most publicly available image datasets consist of single, isolated skin lesions positioned at the center of the image. While these lesion-centric datasets have been fundamental for developing diagnostic algorithms, they lack the context of the surrounding skin, which is critical for improving lesion detection. The iToBoS dataset was created to address this challenge. It includes 16,954 images of skin regions from 100 participants, captured using 3D total body photography. Each image roughly corresponds to a7×9797\times 97 × 9cm section of skin with all suspicious lesions annotated using bounding boxes. Additionally, the dataset provides metadata such as anatomical location, age group, and sun damage score for each image. This dataset aims to facilitate training and benchmarking of algorithms, with the goal of enabling early detection of skin cancer and deployment of this technology in non-clinical environments."
54,679d459debd8ffd557a2aea3,cs.CV,https://arxiv.org/pdf/2501.18219,Revisiting $\Psi$DONet: microlocally inspired filters for incomplete-data tomographic reconstructions,"Tatiana A. Bubba, Luca Ratti, Andrea Sebastiani","Optimization and Control, Computer Vision and Pattern Recognition, Machine Learning","In this paper, we revisit a supervised learning approach based on unrolling first introduced in[9]and calledΨΨ\Psiroman_ΨDONet, by
providing a deeper microlocal interpretation for its theoretical analysis, and extending its study to the case of sparse-angle tomography. Furthermore, we refine the implementation of the originalΨΨ\Psiroman_ΨDONet considering special filters whose structure is specifically inspired by the streak artifact singularities characterizing tomographic reconstructions from incomplete data. This allows to considerably lower the number of (learnable) parameters while preserving (or even slightly improving) the same quality for the reconstructions from limited-angle data and providing a proof-of-concept for the case of sparse-angle tomographic data."
55,679d459debd8ffd557a2aea4,cs.CV,https://arxiv.org/pdf/2501.18167,Scattering approach to diffusion quantifies axonal damage in brain injury,"Ali Abdollahzadeh, Ricardo Coronado-Leija, Hong-Hsi Lee, Alejandra Sierra, Els Fieremans, Dmitry S. Novikov","Medical Physics, Computer Vision and Pattern Recognition, Biological Physics","Early diagnosis and noninvasive monitoring of neurological disorders require sensitivity to elusive cellular-level alterations that occur much earlier than volumetric changes observable with the millimeter-resolution of medical imaging modalities. Morphological changes in axons, such as axonal varicosities or beadings, are observed in neurological disorders, as well as in development and aging. Here, we reveal the sensitivity of time-dependent diffusion MRI (dMRI) to axonal morphology at the micrometer scale. Scattering theory uncovers the two parameters that determine the diffusive dynamics of water in axons: the average reciprocal cross-section and the variance of long-range cross-sectional fluctuations. This theoretical development allowed us to predict dMRI metrics sensitive to axonal alterations across tens of thousands of axons in seconds rather than months of simulations in a rat model of traumatic brain injury. Our approach bridges the gap between micrometers and millimeters in resolution, offering quantitative, objective biomarkers applicable to a broad spectrum of neurological disorders."
56,679d459debd8ffd557a2aea5,cs.CV,https://arxiv.org/pdf/2501.18161,Using Computer Vision for Skin Disease Diagnosis in Bangladesh Enhancing Interpretability and Transparency in Deep Learning Models for Skin Cancer Classification,"Rafiul Islam, Jihad Khan Dipu, Mehedi Hasan Tusar","Image and Video Processing, Computer Vision and Pattern Recognition",
57,679d459debd8ffd557a2aea6,cs.CV,https://arxiv.org/pdf/2501.18157,Efficient Audiovisual Speech Processing via MUTUD: Multimodal Training and Unimodal Deployment,"Joanna Hong, Sanjeel Parekh, Honglie Chen, Jacob Donley, Ke Tan, Buye Xu, Anurag Kumar","Sound, Computer Vision and Pattern Recognition, Multimedia, Audio and Speech Processing","Building reliable speech systems often requires combining multiple modalities, like audio and visual cues. While such multimodal solutions frequently lead to improvements in performance and may even be critical in certain cases, they come with several constraints such as increased sensory requirements, computational cost, and modality synchronization, to mention a few. These challenges constrain the direct uses of these multimodal solutions in real-world applications. In this work, we develop approaches where the learning happens with all available modalities but the deployment or inference is done with just one or reduced modalities. To do so, we propose a Multimodal Training and Unimodal Deployment (MUTUD) framework which includes a Temporally Aligned Modality feature Estimation (TAME) module that can estimate information from missing modality using modalities present during inference.
This innovative approach facilitates the integration of information across different modalities, enhancing the overall inference process by leveraging the strengths of each modality to compensate for the absence of certain modalities during inference. We apply MUTUD to various audiovisual speech tasks and show that it can reduce the performance gap between the multimodal and corresponding unimodal models to a considerable extent. MUTUD can achieve this while reducing the model size and compute compared to multimodal models, in some cases by almost 80%."
58,679d459debd8ffd557a2aea7,cs.CV,https://arxiv.org/pdf/2501.18110,Lifelong 3D Mapping Framework for Hand-held & Robot-mounted LiDAR Mapping Systems,"Liudi Yang, Sai Manoj Prakhya, Senhua Zhu, Ziyuan Liu","Robotics, Computer Vision and Pattern Recognition","We propose a lifelong 3D mapping framework that is modular, cloud-native by design and more importantly, works for both hand-held and robot-mounted 3DLiDARmapping systems. Our proposed framework comprises of dynamic point removal, multi-session map alignment, map change detection and map version control. First, our sensor-setup agnostic dynamic point removal algorithm works seamlessly with both hand-held and robot-mounted setups to produce clean static 3D maps. Second, the multi-session map alignment aligns these clean static maps automatically, without manual parameter fine-tuning, into a single reference frame, using a two stage approach based on feature descriptor matching and fine registration. Third, our novel map change detection identifies positive and negative changes between two aligned maps. Finally, the map version control maintains a single base map that represents the current state of the environment, and stores the detected positive and negative changes, and boundary information. Our unique map version control system can reconstruct any of the previous clean session maps and allows users to query changes between any two random mapping sessions, all without storing any input raw session maps, making it very unique. Extensive experiments are performed using hand-held commercial LiDAR mapping devices and open-source robot-mounted LiDAR SLAM algorithms to evaluate each module and the whole 3D lifelong mapping framework."
59,679d459debd8ffd557a2aea8,cs.CV,https://arxiv.org/pdf/2501.18109,Influence of High-Performance Image-to-Image Translation Networks on Clinical Visual Assessment and Outcome Prediction: Utilizing Ultrasound to MRI Translation in Prostate Cancer,"Mohammad R. Salmanpour, Amin Mousavi, Yixi Xu, William B Weeks, Ilker Hacihaliloglu","Image and Video Processing, Computer Vision and Pattern Recognition, Biological Physics",
60,679d459debd8ffd557a2aea9,cs.CV,https://arxiv.org/pdf/2501.17897,Visualization of Organ Movements Using Automatic Region Segmentation of Swallowing CT,"Yukihiro Michiwaki, Takahiro Kikuchi, Takashi Ijiri, Yoko Inamoto, Hiroshi Moriya, Takumi Ogawa, Ryota Nakatani, Yuto Masaki, Yoshito Otake, Yoshinobu Sato","Image and Video Processing, Computer Vision and Pattern Recognition, Medical Physics",
61,679d459debd8ffd557a2aeaa,cs.CV,https://arxiv.org/pdf/2501.17887,Docling: An Efficient Open-Source Toolkit for AI-driven Document Conversion,"Nikolaos Livathinos, Christoph Auer, Maksym Lysak, Ahmed Nassar, Michele Dolfi, Panos Vagenas, Cesar Berrospi Ramis, Matteo Omenetti, Kasper Dinkla, Yusik Kim, Shubham Gupta, Rafael Teixeira de Lima, Valery Weber, Lucas Morin, Ingmar Meijer, Viktor Kuropiatnyk, Peter W. J. Staar","Computation and Language, Computer Vision and Pattern Recognition, Software Engineering","We introduceDocling, an easy-to-use, self-contained, MIT-licensed, open-source toolkit for document conversion, that can parse several types of popular document formats into a unified, richly structured representation. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. Docling is released as a Python package and can be used as a Python API or as a CLI tool. Docling’s modular architecture and efficient document representation make it easy to implement extensions, new features, models, and customizations. Docling has been already integrated in other popular open-source frameworks (e.g., LangChain, LlamaIndex, spaCy), making it a natural fit for the processing of documents and the development of high-end applications.
The open-source community has fully engaged in using, promoting, and developing for Docling, which gathered 10k stars on GitHub in less than a month and was reported as the No. 1 trending repository in GitHub worldwide in November 2024."
62,679d459debd8ffd557a2aeab,cs.CV,https://arxiv.org/pdf/2501.17823,U2A: Unified Unimodal Adaptation for Robust and Efficient Multimodal Learning,"Md Kaykobad Reza, Niki Nezakati, Ameya Patil, Mashhour Solh, M. Salman Asif","Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning","Multimodal learning often relies on designing new models and complex training strategies to achieve optimal performance. We present Unified Unimodal Adaptation (U2A), which jointly fine-tunes pretrained unimodal encoders using low-rank adaptation (LoRA) for various multimodal tasks. Our method significantly reduces the number of learnable parameters and eliminates the need for complex training strategies, such as alternating training, gradient modifications, or unimodal fine-tuning. To address missing modalities during both training and testing, we introduce Mask Tokens (MT), which generate missing modality features from available modalities using a single token per modality. This simplifies the process, removing the need for specialized feature estimation or prompt-tuning methods. Our evaluation demonstrates that U2A matches or outperforms state-of-the-art methods in both complete and missing modality settings, showcasing strong performance and robustness across various modalities, tasks, and datasets. We also analyze and report the effectiveness of Mask Tokens in different missing modality scenarios. Overall, our method provides a robust, flexible, and efficient solution for multimodal learning, with minimal computational overhead."
63,679d459debd8ffd557a2aeac,cs.CV,https://arxiv.org/pdf/2501.17821,SSF: Sparse Long-Range Scene Flow for Autonomous Driving,"Ajinkya Khoche, Qingwen Zhang, Laura Pereira Sanchez, Aron Asefaw, Sina Sharif Mansouri, Patric Jensfelt",Computer Vision and Pattern Recognition,"Scene flow enables an understanding of the motion characteristics of the environment in the 3D world.
It gains particular significance in the long-range, where object-based perception methods might fail due to sparse observations far away.
Although significant advancements have been made in scene flow pipelines to handle large-scale point clouds, a gap remains in scalability with respect to long-range.
We attribute this limitation to the common design choice of using dense feature grids, which scale quadratically with range.
In this paper, we propose Sparse Scene Flow (SSF), a general pipeline for long-range scene flow, adopting a sparse convolution based backbone for feature extraction.
This approach introduces a new challenge: a mismatch in size and ordering of sparse feature maps between time-sequential point scans. To address this, we propose a sparse feature fusion scheme, that augments the feature maps with virtual voxels at missing locations.
Additionally, we propose a range-wise metric that implicitly gives greater importance to faraway points.
Our method, SSF, achieves state-of-the-art results on the Argoverse2 dataset, demonstrating strong performance in long-range scene flow estimation. Our code will be released athttps://github.com/KTH-RPL/SSF.git."
64,679d459debd8ffd557a2aead,cs.CV,https://arxiv.org/pdf/2501.17813,P-TAME: Explain Any Image Classifier with Trained Perturbations,"Mariano V. Ntrougkas, Vasileios Mezaris, Ioannis Patras","Computer Vision and Pattern Recognition, Artificial Intelligence","The adoption of Deep Neural Networks (DNNs) in critical fields where predictions need to be accompanied by justifications is hindered by their inherent black-box nature. In this paper, we introduce P-TAME (Perturbation-based Trainable Attention Mechanism for Explanations), a model-agnostic method for explaining DNN-based image classifiers. P-TAME employs an auxiliary image classifier to extract features from the input image, bypassing the need to tailor the explanation method to the internal architecture of the backbone classifier being explained. Unlike traditional perturbation-based methods, which have high computational requirements, P-TAME offers an efficient alternative by generating high-resolution explanations in a single forward pass during inference. We apply P-TAME to explain the decisions of VGG-16, ResNet-50, and ViT-B-16, three distinct and widely used image classifiers. Quantitative and qualitative results show that our method matches or outperforms previous explainability methods, including model-specific approaches. Code and trained models will be released upon acceptance."
65,679d459debd8ffd557a2aeae,cs.CV,https://arxiv.org/pdf/2501.17792,CrowdSplat: Exploring Gaussian Splatting For Crowd Rendering,"Xiaohan Sun, Yinghan Xu, John Dingliana, Carol O'Sullivan",Computer Vision and Pattern Recognition,"We present CrowdSplat, a novel approach
that leverages
3D Gaussian Splatting for real-time, high-quality crowd rendering. Our method utilizes
3D Gaussian functions to represent animated human characters in diverse poses and outfits, which are extracted from monocular videos. We integrate Level of Detail (LoD) rendering to optimize computational efficiency and quality. The CrowdSplat framework consists of two stages: (1) avatar reconstruction and (2) crowd synthesis. The framework is also optimized in GPU memory usage for scalability. Quantitative and qualitative evaluations show that CrowdSplat achieves good levels of rendering quality, memory efficiency, and computational performance. Through these experiments, we show that CrowdSplat is a viable solution for dynamic, realistic crowd simulation in real-time applications."
66,679d459debd8ffd557a2aeaf,cs.CV,https://arxiv.org/pdf/2501.17726,VICCA: Visual Interpretation and Comprehension of Chest X-ray Anomalies in Generated Report Without Human Feedback,"Sayeh Gholipour Picha, Dawood Al Chanti, Alice Caplier","Computer Vision and Pattern Recognition, Computation and Language","As artificial intelligence (AI) becomes increasingly central to healthcare, the demand for explainable and trustworthy models is paramount. Current report generation systems for chest X-rays (CXR) often lack mechanisms for validating outputs without expert oversight, raising concerns about reliability and interpretability. To address these challenges, we propose a novel multimodal framework designed to enhance the semantic alignment and localization accuracy of AI-generated medical reports.
Our framework integrates two key modules: a Phrase Grounding Model, which identifies and localizes pathologies in CXR images based on textual prompts, and a Text-to-Image Diffusion Module, which generates synthetic CXR images from prompts while preserving anatomical fidelity. By comparing features between the original and generated images, we introduce a dual-scoring system: one score quantifies localization accuracy, while the other evaluates semantic consistency.
This approach significantly outperforms existing methods, achieving state-of-the-art results in pathology localization and text-to-image alignment. The integration of phrase grounding with diffusion models, coupled with the dual-scoring evaluation system, provides a robust mechanism for validating report quality, paving the way for more trustworthy and transparent AI in medical imaging."
67,679d459debd8ffd557a2aeb0,cs.CV,https://arxiv.org/pdf/2501.17718,Learning Semantic Facial Descriptors for Accurate Face Animation,"Lei Zhu, Yuanqi Chen, Xiaohang Liu, Thomas H. Li, Ge Li",Computer Vision and Pattern Recognition,
68,679d459debd8ffd557a2aeb1,cs.CV,https://arxiv.org/pdf/2501.17690,Segmentation-Aware Generative Reinforcement Network (GRN) for Tissue Layer Segmentation in 3-D Ultrasound Images for Chronic Low-back Pain (cLBP) Assessment,"Zixue Zeng, Xiaoyan Zhao, Matthew Cartier, Tong Yu, Jing Wang, Xin Meng, Zhiyu Sheng, Maryam Satarpour, John M Cormack, Allison Bean, Ryan Nussbaum, Maya Maurer, Emily Landis-Walkenhorst, Dinesh Kumbhare, Kang Kim, Ajay Wasan, Jiantao Pu","Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
69,679d459debd8ffd557a2aeb2,cs.CV,https://arxiv.org/pdf/2501.17688,ContourFormer:Real-Time Contour-Based End-to-End Instance Segmentation Transformer,"Weiwei Yao, Chen Li, Minjun Xiong, Wenbo Dong, Hao Chen, Xiong Xiao","Computer Vision and Pattern Recognition, Artificial Intelligence","This paper presents Contourformer, a real-time contour-based instance segmentation algorithm. The method is fully based on the DETR paradigm and achieves end-to-end inference through iterative and progressive mechanisms to optimize contours. To improve efficiency and accuracy, we develop two novel techniques: sub-contour decoupling mechanisms and contour fine-grained distribution refinement.In the sub-contour decoupling mechanism, we propose a deformable attention-based module that adaptively selects sampling regions based on the current predicted contour, enabling more effective capturing of object boundary information. Additionally, we design a multi-stage optimization process to enhance segmentation precision by progressively refining sub-contours. The contour fine-grained distribution refinement technique aims to further improve the ability to express fine details of contours.These innovations enable Contourformer to achieve stable and precise segmentation for each instance while maintaining real-time performance. Extensive experiments demonstrate the superior performance of Contourformer on multiple benchmark datasets, including SBD, COCO, and KINS. We conduct comprehensive evaluations and comparisons with existing state-of-the-art methods, showing significant improvements in both accuracy and inference speed.This work provides a new solution for contour-based instance segmentation tasks and lays a foundation for future research, with the potential to become a strong baseline method in this field."
70,679d459debd8ffd557a2aeb3,cs.CV,https://arxiv.org/pdf/2501.17655,FeatureGS: Eigenvalue-Feature Optimization in 3D Gaussian Splatting for Geometrically Accurate and Artifact-Reduced Reconstruction,"Miriam Jäger, Markus Hillemann, Boris Jutzi",Computer Vision and Pattern Recognition,"3D Gaussian Splatting (3DGS) has emerged as a powerful approach for 3D scene reconstruction using 3D Gaussians. However, neither the centers nor surfaces of the Gaussians are accurately aligned to the object surface, complicating their direct use in point cloud and mesh reconstruction. Additionally, 3DGS typically produces floater artifacts, increasing the number of Gaussians and storage requirements.
To address these issues, we present FeatureGS, which incorporates an additional geometric loss term based on an eigenvalue-derived 3D shape feature into the optimization process of 3DGS. The goal is to improve geometric accuracy and enhance properties of planar surfaces with reduced structural entropy in local 3D neighborhoods.
We present four alternative formulations for the geometric loss term based on ’planarity’ of Gaussians, as well as ’planarity’, ’omnivariance’, and ’eigenentropy’ of Gaussian neighborhoods.
We provide quantitative and qualitative evaluations on 15 scenes of the DTU benchmark dataset focusing on following key aspects: Geometric accuracy and artifact-reduction, measured by the Chamfer distance, and memory efficiency, evaluated by the total number of Gaussians. Additionally, rendering quality is monitored by Peak Signal-to-Noise Ratio.
FeatureGS achieves a 30% improvement in geometric accuracy, reduces the number of Gaussians by 90%, and suppresses floater artifacts, while maintaining comparable photometric rendering quality. The geometric loss with ’planarity’ from Gaussians provides the highest geometric accuracy, while ’omnivariance’ in Gaussian neighborhoods reduces floater artifacts and number of Gaussians the most.
This makes FeatureGS a strong method for geometrically accurate, artifact-reduced and memory-efficient 3D scene reconstruction, enabling the direct use of Gaussian centers for geometric representation."
71,679d459debd8ffd557a2aeb4,cs.CV,https://arxiv.org/pdf/2501.17642,Efficient Redundancy Reduction for Open-Vocabulary Semantic Segmentation,"Lin Chen, Qi Yang, Kun Ding, Zhihao Li, Gang Shen, Fei Li, Qiyuan Cao, Shiming Xiang",Computer Vision and Pattern Recognition,"Open-vocabulary semantic segmentation (OVSS) is an open-world task that aims to assign each pixel within an image to a specific class defined by arbitrary text descriptions. Recent advancements in large-scale vision-language models have demonstrated their open-vocabulary understanding capabilities, significantly facilitating the development of OVSS. However, most existing methods suffer from either suboptimal performance or long latency. This study introduces ERR-Seg, a novel framework that effectively reduces redundancy to balance accuracy and efficiency. ERR-Seg incorporates a training-free Channel Reduction Module (CRM) that leverages prior knowledge from vision-language models like CLIP to identify the most relevant classes while discarding others. Moreover, it incorporates Efficient Semantic Context Fusion (ESCF) with spatial-level and class-level sequence reduction strategies. CRM and ESCF result in substantial memory and computational savings without compromising accuracy. Additionally, recognizing the significance of hierarchical semantics extracted from middle-layer features for closed-set semantic segmentation, ERR-Seg introduces the Hierarchical Semantic Module (HSM) to exploit hierarchical semantics in the context of OVSS. Compared to previous state-of-the-art methods under the ADE20K-847 setting, ERR-Seg achieves +5.6%percent5.65.6\%5.6 %mIoU improvement and reduces latency by67.3%percent67.367.3\%67.3 %. The project page is available athttps://lchen1019.github.io/ERR-Seg."
72,679d459debd8ffd557a2aeb5,cs.CV,https://arxiv.org/pdf/2501.17636,Efficient Interactive 3D Multi-Object Removal,"Jingcheng Ni, Weiguang Zhao, Daniel Wang, Ziyao Zeng, Chenyu You, Alex Wong, Kaizhu Huang",Computer Vision and Pattern Recognition,"Object removal is of great significance to 3D scene understanding, essential for applications in content filtering and scene editing. Current mainstream methods primarily focus on removing individual objects, with a few methods dedicated to eliminating an entire area or all objects of a certain category. They however confront the challenge of insufficient granularity and flexibility for real-world applications, where users demand tailored excision and preservation of objects within defined zones. In addition, most of the current methods require kinds of priors when addressing multi-view inpainting, which is time-consuming. To address these limitations, we propose an efficient and user-friendly pipeline for 3D multi-object removal, enabling users to flexibly select areas and define objects for removal or preservation. Concretely, to ensure object consistency and correspondence across multiple views, we propose a novel mask matching and refinement module, which integrates homography-based warping with high-confidence anchor points for segmentation. By leveraging the IoU joint shape context distance loss, we enhance the accuracy of warped masks and improve subsequent inpainting processes. Considering the current immaturity of 3D multi-object removal, we provide a new evaluation dataset to bridge the developmental void. Experimental results demonstrate that our method significantly reduces computational costs, achieving processing speeds more than 80% faster than state-of-the-art methods while maintaining equivalent or higher reconstruction quality."
73,679d459debd8ffd557a2aeb6,cs.CV,https://arxiv.org/pdf/2501.17595,Technical report on label-informed logit redistribution for better domain generalization in low-shot classification with foundation models,"Behraj Khan, Tahir Syed",Computer Vision and Pattern Recognition,"Confidence calibration is an emerging challenge in real-world decision systems based on foundations models when used for downstream vision classification tasks. Due to various reasons exposed, logit scores on the CLIP head remain large irrespective of whether the image-language pairs reconcile. It is difficult to address in data space, given the few-shot regime. We propose a penalty incorporated into loss objective that penalizes incorrect classifications whenever one is made during finetuning, by moving an amount of log-likelihood to the true class commensurate to the relative amplitudes of the two likelihoods. We refer to it asconfidence misalignment penalty (CMP). Extensive experiments on12121212vision datasets and5555domain generalization datasets supports the calibration performance of our method against stat-of-the-art. CMP outperforms the benchmarked prompt learning methods, demonstrating average improvement in Expected Calibration Error (ECE) by average6.016.016.016.01%,4.014.014.014.01% at minimum and9.729.729.729.72% at maximum."
74,679d459debd8ffd557a2aeb7,cs.CV,https://arxiv.org/pdf/2501.17586,Boosting Weak Positives for Text Based Person Search,"Akshay Modi, Ashhar Aziz, Nilanjana Chatterjee, A V Subramanyam","Computer Vision and Pattern Recognition, Machine Learning","Large vision-language models have revolutionized cross-modal object retrieval, but text-based person search (TBPS) remains a challenging task due to limited data and fine-grained nature of the task. Existing methods primarily focus on aligning image-text pairs into a common representation space, often disregarding the fact that real world positive image-text pairs share a varied degree of similarity in between them. This leads models to prioritize easy pairs, and in some recent approaches, challenging samples are discarded as noise during training. In this work, we introduce a boosting technique that dynamically identifies and emphasizes these challenging samples during training. Our approach is motivated from classical boosting technique and dynamically updates the weights of the weak positives, wherein, the rank-1 match does not share the identity of the query. The weight allows these misranked pairs to contribute more towards the loss and the network has to pay more attention towards such samples. Our method achieves improved performance across four pedestrian datasets, demonstrating the effectiveness of our proposed module. Code is availablehttps://anonymous.4open.science/r/TBPS_Boosting."
75,679d459debd8ffd557a2aeb8,cs.CV,https://arxiv.org/pdf/2501.17555,An Exceptional Dataset For Rare Pancreatic Tumor Segmentation,"Wenqi Li, Yingli Chen, Keyang Zhou, Xiaoxiao Hu, Zilu Zheng, Yue Yan, Xinpeng Zhang, Wei Tang, Zhenxing Qian","Computer Vision and Pattern Recognition, Artificial Intelligence","Pancreatic NEuroendocrine Tumors (pNETs) are very rare endocrine neoplasms that account for less than 5% of all pancreatic malignancies, with an incidence of only 1–1.5 cases per 100,000. Early detection of pNETs is critical for improving patient survival, but the rarity of pNETs makes segmenting them from CT a very challenging problem. So far, there has not been a dataset specifically for pNETs available to researchers. To address this issue, we propose a pNETs dataset, a well-annotated Contrast-Enhanced Computed Tomography (CECT) dataset focused exclusively on Pancreatic Neuroendocrine Tumors, containing data from 469 patients. This is the first dataset solely dedicated to pNETs, distinguishing it from previous collections. Additionally, we provide the baseline detection networks with a new slice-wise weight loss function designed for the UNet-based model, improving the overall pNET segmentation performance.
We hope that our dataset can enhance the understanding and diagnosis of pNET Tumors within the medical community, facilitate the development of more accurate diagnostic tools, and ultimately improve patient outcomes and advance the field of oncology."
76,679d459debd8ffd557a2aeb9,cs.CV,https://arxiv.org/pdf/2501.17550,Action Recognition Using Temporal Shift Module and Ensemble Learning,"Anh-Kiet Duong, Petra Gomez-Krämer",Computer Vision and Pattern Recognition,"This paper presents the first-rank solution for the Multi-Modal Action Recognition Challenge, part of the Multi-Modal Visual Pattern Recognition Workshop at the\aclICPR 2024. The competition aimed to recognize human actions using a diverse dataset of 20 action classes, collected from multi-modal sources. The proposed approach is built upon the\aclTSM, a technique aimed at efficiently capturing temporal dynamics in video data, incorporating multiple data input types. Our strategy included transfer learning to leverage pre-trained models, followed by meticulous fine-tuning on the challenge’s specific dataset to optimize performance for the 20 action classes. We carefully selected a backbone network to balance computational efficiency and recognition accuracy and further refined the model using an ensemble technique that integrates outputs from different modalities. This ensemble approach proved crucial in boosting the overall performance. Our solution achieved a perfect top-1 accuracy on the test set, demonstrating the effectiveness of the proposed approach in recognizing human actions across 20 classes. Our code is available online111https://github.com/ffyyytt/TSM-MMVPR."
77,679d459debd8ffd557a2aeba,cs.CV,https://arxiv.org/pdf/2501.17547,Towards Training-Free Open-World Classification with 3D Generative Models,"Xinzhe Xia, Weiguang Zhao, Yuyao Yan, Guanyu Yang, Rui Zhang, Kaizhu Huang, Xi Yang",Computer Vision and Pattern Recognition,"3D open-world classification is a challenging yet essential task in dynamic and unstructured real-world scenarios, requiring both open-category and open-pose recognition. To address these challenges, recent wisdom often takes sophisticated 2D pre-trained models to provide enriched and stable representations. However, these methods largely rely on how 3D objects can be projected into 2D space, which is unfortunately not well solved, and thus significantly limits their performance. Unlike these present efforts, in this paper we make a pioneering exploration of 3D generative models for 3D open-world classification.
Drawing on abundant prior knowledge from 3D generative models, we additionally craft a rotation-invariant feature extractor. This innovative synergy endows our pipeline with the advantages of being training-free, open-category, and pose-invariant, thus well suited to 3D open-world classification."
78,679d459debd8ffd557a2aebb,cs.CV,https://arxiv.org/pdf/2501.17534,3DSES: an indoor Lidar point cloud segmentation dataset with real and pseudo-labels from a 3D model,"Maxime Mérizette, Nicolas Audebert, Pierre Kervella, Jérôme Verdun",Computer Vision and Pattern Recognition,
79,679d459debd8ffd557a2aebc,cs.CV,https://arxiv.org/pdf/2501.17468,Solving Inverse Problems using Diffusion with Fast Iterative Renoising,"Matt C. Bendel, Saurav K. Shastri, Rizwan Ahmad, Philip Schniter",Computer Vision and Pattern Recognition,"Imaging inverse problems can be solved in an unsupervised manner using pre-trained diffusion models.
In most cases, that involves approximating the gradient of the measurement-conditional score function in the reverse process.
Since the approximations produced by existing methods are quite poor, especially early in the reverse process,
we propose a new approach that re-estimates and “renoises” the image several times per diffusion step.
Renoising aims to ensure that the pre-trained diffusion model sees white-Gaussian error, in accordance with how it was trained.
We demonstrate the effectiveness of our “DDfire” method at 20, 100, and 1000 neural function evaluations on linear inverse problems and phase retrieval."
80,679d459debd8ffd557a2aebd,cs.CV,https://arxiv.org/pdf/2501.17441,Towards Making Flowchart Images Machine Interpretable,"Shreya Shukla, Prajwal Gatti, Yogesh Kumar, Vikash Yadav, Anand Mishra","Computer Vision and Pattern Recognition, Artificial Intelligence, Computation and Language, Digital Libraries, Software Engineering","Computer programming textbooks and software documentations often contain flowcharts to illustrate the flow of an algorithm or procedure. Modern OCR engines often tag these flowcharts as graphics and ignore them in further processing. In this paper, we work towards making flowchart images machine-interpretable by converting them to executable Python codes. To this end, inspired by the recent success in natural language to code generation literature, we present a novel transformer-based framework, namelyFloCo-T5. Our model is well-suited for this task, as it can effectively learn semantics, structure, and patterns of programming languages, which it leverages to generate syntactically correct code. We also used a task-specific pre-training objective to pre-trainFloCo-T5using a large number of logic-preserving augmented code samples. Further, to perform a rigorous study of this problem, we introduce theFloCodataset that contains 11,884 flowchart images and their corresponding Python codes. Our experiments show promising results, andFloCo-T5clearly outperforms related competitive baselines on code generation metrics. We make our dataset and implementation publicly available111https://vl2g.github.io/projects/floco."
81,679d459debd8ffd557a2aebe,cs.CV,https://arxiv.org/pdf/2501.17422,SIGN: A Statistically-Informed Gaze Network for Gaze Time Prediction,"Jianping Ye, Michel Wedel","Computer Vision and Pattern Recognition, Applications","This document is a model and instructions forLaTeX.
This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes,
or Math in Paper Title or Abstract."
82,679d459debd8ffd557a2aebf,cs.CV,https://arxiv.org/pdf/2501.17403,General Scene Adaptation for Vision-and-Language Navigation,"Haodong Hong, Yanyuan Qiao, Sen Wang, Jiajun Liu, Qi Wu","Computer Vision and Pattern Recognition, Artificial Intelligence, Computation and Language","Vision-and-Language Navigation (VLN) tasks mainly evaluate agents based on one-time execution of individual instructions across multiple environments, aiming to develop agents capable of functioning in any environment in a zero-shot manner.
However, real-world navigation robots often operate in persistent environments with relatively consistent physical layouts, visual observations, and language styles from instructors. Such a gap in the task setting presents an opportunity to improve VLN agents by incorporating continuous adaptation to specific environments.
To better reflect these real-world conditions, we introduce GSA-VLN (General Scene Adaptation for VLN), a novel task requiring agents to execute navigation instructions within a specific scene and simultaneously adapt to it for improved performance over time.
To evaluate the proposed task, one has to address two challenges in existing VLN datasets: the lack of out-of-distribution (OOD) data, and the limited number and style diversity of instructions for each scene.
Therefore, we propose a new dataset, GSA-R2R, which significantly expands the diversity and quantity of environments and instructions for the Room-to-Room (R2R) dataset to evaluate agent adaptability in both ID and OOD contexts.
Furthermore, we design a three-stage instruction orchestration pipeline that leverages large language models (LLMs) to refine speaker-generated instructions and apply role-playing techniques to rephrase instructions into different speaking styles.
This is motivated by the observation that each individual user often has consistent signatures or preferences in their instructions, taking the use case of home robotic assistants as an example.
We conducted extensive experiments on GSA-R2R to thoroughly evaluate our dataset and benchmark various methods, revealing key factors enabling agents to adapt to specific environments.
Based on our findings, we propose a novel method, Graph-Retained DUET (GR-DUET), which incorporates memory-based navigation graphs with an environment-specific training strategy, achieving state-of-the-art results on all GSA-R2R splits.
The dataset and code are available athttps://github.com/honghd16/GSA-VLN."
83,679d459debd8ffd557a2aec0,cs.CV,https://arxiv.org/pdf/2501.17391,Learning Free Token Reduction for Multi-Modal LLM,"Zihui Zhao, Yingxin Li, Yang Li","Computer Vision and Pattern Recognition, Artificial Intelligence, Computation and Language","Vision-Language Models (VLMs) have achieved remarkable success across a range of multimodal tasks; however, their practical deployment is often constrained by high computational costs and prolonged inference times. Since the vision modality typically carries more information than the text modality, compressing visual prompts offers a promising solution to alleviate these challenges. Existing approaches predominantly focus on refining model architectures or directly reducing the number of visual tokens. However, these methods often compromise inference performance due to a lack of consideration for the unique spatial and temporal characteristics of visual data. In this work, we propose a token compression paradigm that operates on both spatial and temporal dimensions. Our approach includes a learning-free, plug-and-play compression pipeline that can be seamlessly integrated into most Multimodal Large Language Model (MLLM) frameworks. By leveraging this method, we enhance the model’s inference capability while simultaneously reducing its computational cost. Experimental results on the Video-QA task demonstrate the effectiveness of the proposed approach, showcasing significant improvements in efficiency without sacrificing performance."
84,679d459debd8ffd557a2aec1,cs.CV,https://arxiv.org/pdf/2501.17387,Assessing the Capability of YOLO- and Transformer-based Object Detectors for Real-time Weed Detection,"Alicia Allmendinger, Ahmet Oğuz Saltık, Gerassimos G. Peteinatos, Anthony Stein, Roland Gerhards",Computer Vision and Pattern Recognition,"Spot spraying represents an efficient and sustainable method for reducing the amount of pesticides, particularly herbicides, used in agricultural fields. To achieve this, it is of utmost importance to reliably differentiate between crops and weeds, and even between individual weed species in situ and under real-time conditions. To assess suitability for real-time application, different object detection models that are currently state-of-the-art are compared. All available models of YOLOv8, YOLOv9, YOLOv10, and RT-DETR are trained and evaluated with images from a real field situation. The images are separated into two distinct datasets: In the initial data set, each species of plants is trained individually; in the subsequent dataset, a distinction is made between monocotyledonous weeds, dicotyledonous weeds, and three chosen crops. The results demonstrate that while all models perform equally well in the metrics evaluated, the YOLOv9 models, particularly the YOLOv9s and YOLOv9e, stand out in terms of their strong recall scores (66.58 % and 72.36 %), as well as mAP50 (73.52 % and 79.86 %), and mAP50-95 (43.82 % and 47.00 %) in dataset 2. However, the RT-DETR models, especially RT-DETR-l, excel in precision with reaching 82.44 % on dataset 1 and 81.46 % in dataset 2, making them particularly suitable for scenarios where minimizing false positives is critical. In particular, the smallest variants of the YOLO models (YOLOv8n, YOLOv9t, and YOLOv10n) achieve substantially faster inference times down to 7.58 ms for dataset 2 on the NVIDIA GeForce RTX 4090 GPU for analyzing one frame, while maintaining competitive accuracy, highlighting their potential for deployment in resource-constrained embedded computing devices as typically used in productive setups."
85,679d459debd8ffd557a2aec2,cs.CV,https://arxiv.org/pdf/2501.17356,On the Coexistence and Ensembling of Watermarks,"Aleksandar Petrov, Shruti Agarwal, Philip H.S. Torr, Adel Bibi, John Collomosse","Computer Vision and Pattern Recognition, Artificial Intelligence, Computers and Society","Watermarking, the practice of embedding imperceptible information into media such as images, videos, audio, and text, is essential for intellectual property protection, content provenance and attribution.
The growing complexity of digital ecosystems necessitates watermarks for different uses to be embedded in the same media.
However, to detect and decode all watermarks, they need to coexist well with one another.
We perform the first study of coexistence of deep image watermarking methods and, contrary to intuition, we find that various open-source watermarks can coexist with only minor impacts on image quality and decoding robustness.
The coexistence of watermarks also opens the avenue for ensembling watermarking methods.
We show how ensembling can increase the overall message capacity and enable new trade-offs between capacity, accuracy, robustness and image quality, without needing to retrain the base models."
86,679d459debd8ffd557a2aec3,cs.CV,https://arxiv.org/pdf/2501.17343,Post-Training Quantization for 3D Medical Image Segmentation: A Practical Study on Real Inference Engines,"Chongyu Qu, Ritchie Zhao, Ye Yu, Bin Liu, Tianyuan Yao, Junchao Zhu, Bennett A. Landman, Yucheng Tang, Yuankai Huo","Computer Vision and Pattern Recognition, Artificial Intelligence","Quantizing deep neural networks ,reducing the precision (bit-width) of their computations, can remarkably decrease memory usage and accelerate processing, making these models more suitable for large-scale medical imaging applications with limited computational resources. However, many existing methods studied “fake quantization”, which simulates lower precision operations during inference, but does not actually reduce model size or improve real-world inference speed. Moreover, the potential of deploying real 3D low-bit quantization on modern GPUs is still unexplored. In this study, we introduce a real post-training quantization (PTQ) framework that successfully implements true 8-bit quantization on state-of-the-art (SOTA) 3D medical segmentation models, i.e., U-Net, SegResNet, SwinUNETR, nnU-Net, UNesT, TransUNet, ST-UNet,and VISTA3D. Our approach involves two main steps. First, we use TensorRT to perform fake quantization for both weights and activations with unlabeled calibration dataset. Second, we convert this fake quantization into real quantization via TensorRT engine on real GPUs, resulting in real-world reductions in model size and inference latency. Extensive experiments demonstrate that our framework effectively performs 8-bit quantization on GPUs without sacrificing model performance. This advancement enables the deployment of efficient deep learning models in medical imaging applications where computational resources are constrained.
The code and models have been released, including U-Net, TransUNet pretrained on the BTCV dataset for abdominal (13-label) segmentation, UNesT pretrained on the Whole Brain Dataset for whole brain (133-label) segmentation, and nnU-Net, SegResNet, SwinUNETR and VISTA3D pretrained on TotalSegmentator V2 for full body (104-label) segmentation.https://github.com/hrlblab/PTQ."
87,679d459debd8ffd557a2aec4,cs.CV,https://arxiv.org/pdf/2501.17328,WASUP: Interpretable Classification with Weight-Input Alignment and Class-Discriminative SUPports Vectors,"Tom Nuno Wolf, Christian Wachinger","Computer Vision and Pattern Recognition, Machine Learning","The deployment of deep learning models in critical domains necessitates a balance between high accuracy and interpretability.
We introduce WASUP, an inherently interpretable neural network that provides local and global explanations of its decision-making process.
We prove that these explanations are faithful by fulfilling established axioms for explanations.
Leveraging the concept of case-based reasoning, WASUP extracts class-representative support vectors from training images, ensuring they capture relevant features while suppressing irrelevant ones.
Classification decisions are made by calculating and aggregating similarity scores between these support vectors and the input’s latent feature vector.
We employ B-Cos transformations, which align model weights with inputs to enable faithful mappings of latent features back to the input space, facilitating local explanations in addition to global explanations of case-based reasoning.
We evaluate WASUP on three tasks: fine-grained classification on Stanford Dogs, multi-label classification on Pascal VOC, and pathology detection on the RSNA dataset.
Results indicate that WASUP not only achieves competitive accuracy compared to state-of-the-art black-box models but also offers insightful explanations verified through theoretical analysis.
Our findings underscore WASUP’s potential for applications where understanding model decisions is as critical as the decisions themselves."
88,679d459debd8ffd557a2aec5,cs.CV,https://arxiv.org/pdf/2501.17289,A Contrastive Teacher-Student Framework for Novelty Detection under Style Shifts,"Hossein Mirzaei, Mojtaba Nafez, Moein Madadi, Arad Maleki, Mahdi Hajialilue, Zeinab Sadat Taghavi, Sepehr Rezaee, Ali Ansari, Bahar Dibaei Nia, Kian Shamsaie, Mohammadreza Salehi, Mackenzie W. Mathis, Mahdieh Soleymani Baghshah, Mohammad Sabokrou, Mohammad Hossein Rohban",Computer Vision and Pattern Recognition,"There have been several efforts to improve Novelty Detection (ND) performance. However, ND methods often suffer significant performance drops under minor distribution shifts caused by changes in the environment, known as style shifts. This challenge arises from the ND setup, where the absence of out-of-distribution (OOD) samples during training causes the detector to be biased toward the dominant style features in the in-distribution (ID) data. As a result, the model mistakenly learns to correlate style with core features, using this shortcut for detection. Robust ND is crucial for real-world applications like autonomous driving and medical imaging, where test samples may have different styles than the training data. Motivated by this, we propose a robust ND method that crafts an auxiliary OOD set with style features similar to the ID set but with different core features. Then, a task-based knowledge distillation strategy is utilized to distinguish core features from style features and help our model rely on core features for discriminating crafted OOD and ID sets. We verified the effectiveness of our method through extensive experimental evaluations on several datasets, including synthetic and real-world benchmarks, against nine different ND methods. The code repository is available at:https://github.com/rohban-lab/CTS."
89,679d459debd8ffd557a2aec6,cs.CV,https://arxiv.org/pdf/2501.17260,ViT-2SPN: Vision Transformer-based Dual-Stream Self-Supervised Pretraining Networks for Retinal OCT Classification,"Mohammadreza Saraei, Igor Kozak, Eung-Joo Lee","Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning","Optical Coherence Tomography (OCT) is a non-invasive imaging modality essential for diagnosing various eye diseases. Despite its clinical significance, developing OCT-based diagnostic tools faces challenges, such as limited public datasets, sparse annotations, and privacy concerns. Although deep learning has made progress in automating OCT analysis, these challenges remain unresolved. To address these limitations, we introduce the Vision Transformer-based Dual-Stream Self-Supervised Pretraining Network (ViT-2SPN), a novel framework designed to enhance feature extraction and improve diagnostic accuracy. ViT-2SPN employs a three-stage workflow: Supervised Pretraining, Self-Supervised Pretraining (SSP), and Supervised Fine-Tuning. The pretraining phase leverages the OCTMNIST dataset (97,477 unlabeled images across four disease classes) with data augmentation to create dual-augmented views. A Vision Transformer (ViT-Base) backbone extracts features, while a negative cosine similarity loss aligns feature representations. Pretraining is conducted over 50 epochs with a learning rate of 0.0001 and momentum of 0.999. Fine-tuning is performed on a stratified 5.129% subset of OCTMNIST using 10-fold cross-validation. ViT-2SPN achieves a mean AUC of 0.93, accuracy of 0.77, precision of 0.81, recall of 0.75, and an F1 score of 0.76, outperforming existing SSP-based methods. These results underscore the robustness and clinical potential of ViT-2SPN in retinal OCT classification. The code is available in\hrefhttps://github.com/mrsaraei/ViT-2SPN.githttps://github.com/mrsaraei/ViT-2SPN.git."
90,679d459debd8ffd557a2aec7,cs.CV,https://arxiv.org/pdf/2501.17171,Separated Inter/Intra-Modal Fusion Prompts for Compositional Zero-Shot Learning,Sua Jung,"Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning, Image and Video Processing",
91,679d459debd8ffd557a2aec8,cs.CV,https://arxiv.org/pdf/2501.17822,Aggregation Schemes for Single-Vector WSI Representation Learning in Digital Pathology,"Sobhan Hemati, Ghazal Alabtah, Saghir Alfasly, H.R. Tizhoosh","Image and Video Processing, Artificial Intelligence, Computer Vision and Pattern Recognition, Information Retrieval, Quantitative Methods","A crucial step to efficiently integrate Whole Slide Images (WSIs) in computational pathology is assigning a single high-quality feature vector, i.e., one embedding, to each WSI. With the existence of many pre-trained deep neural networks and the emergence of foundation models, extracting embeddings for sub-images (i.e., tiles or patches) is straightforward. However, for WSIs, given their high resolution and gigapixel nature, inputting them into existing GPUs as a single image is not feasible. As a result, WSIs are usually split into many patches. Feeding each patch to a pre-trained model, each WSI can then be represented by a set of patches, hence, a set of embeddings. Hence, in such a setup, WSI representation learning reduces to set representation learning where for each WSI we have access to a set of patch embeddings. To obtain a single embedding from a set of patch embeddings for each WSI, multiple set-based learning schemes have been proposed in the literature. In this paper, we evaluate the WSI search performance of multiple recently developed aggregation techniques (mainly set representation learning techniques) including simple average or max pooling operations, Deep Sets, Memory networks, Focal attention, Gaussian Mixture Model (GMM) Fisher Vector, and deep sparse and binary Fisher Vector on four different primary sites including bladder, breast, kidney, and Colon from TCGA. Further, we benchmark the search performance of these methods against the median of minimum distances of patch embeddings, a non-aggregating approach used for WSI retrieval."
92,679d459debd8ffd557a2aec9,cs.CV,https://arxiv.org/pdf/2501.17811,Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling,"Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan","Artificial Intelligence, Computation and Language, Computer Vision and Pattern Recognition","In this work, we introduceJanus-Pro, an advanced version of the previous work Janus. Specifically, Janus-Pro incorporates (1) an optimized training strategy, (2) expanded training data, and (3) scaling to larger model size.
With these improvements, Janus-Pro achieves significant advancements in both multimodal understanding and text-to-image instruction-following capabilities, while also enhancing the stability of text-to-image generation. We hope this work will inspire further exploration in the field. Code and models are publicly available."
93,679d459debd8ffd557a2aeca,cs.CV,https://arxiv.org/pdf/2501.17758,Glioma Multimodal MRI Analysis System for Tumor Layered Diagnosis via Multi-task Semi-supervised Learning,"Yihao Liu, Zhihao Cui, Liming Li, Junjie You, Xinle Feng, Jianxin Wang, Xiangyu Wang, Qing Liu, Minghua Wu","Image and Video Processing, Computer Vision and Pattern Recognition","Gliomas are the most common primary tumors of the central nervous system. Multimodal MRI is widely used for the preliminary screening of gliomas and plays a crucial role in auxiliary diagnosis, therapeutic efficacy, and prognostic evaluation. Currently, the computer-aided diagnostic studies of gliomas using MRI have focused on independent analysis events such as tumor segmentation, grading, and radiogenomic classification, without studying inter-dependencies among these events. In this study, we propose a Glioma Multimodal MRI Analysis System (GMMAS) that utilizes a deep learning network for processing multiple events simultaneously, leveraging their inter-dependencies through an uncertainty-based multi-task learning architecture and synchronously outputting tumor region segmentation, glioma histological subtype, IDH mutation genotype, and 1p/19q chromosome disorder status. Compared with the reported single-task analysis models, GMMAS improves the precision across tumor layered diagnostic tasks. Additionally, we have employed a two-stage semi-supervised learning method, enhancing model performance by fully exploiting both labeled and unlabeled MRI samples. Further, by utilizing an adaptation module based on knowledge self-distillation and contrastive learning for cross-modal feature extraction, GMMAS exhibited robustness in situations of modality absence and revealed the differing significance of each MRI modal. Finally, based on the analysis outputs of the GMMAS, we created a visual and user-friendly platform for doctors and patients, introducing GMMAS-GPT to generate personalized prognosis evaluations and suggestions."
94,679d459debd8ffd557a2aecb,cs.CV,https://arxiv.org/pdf/2501.17699,PulmoFusion: Advancing Pulmonary Health with Efficient Multi-Modal Fusion,"Ahmed Sharshar, Yasser Attia, Mohammad Yaqub, Mohsen Guizani","Image and Video Processing, Artificial Intelligence, Computer Vision and Pattern Recognition","Traditional remote spirometry lacks the precision required for effective pulmonary monitoring. We present a novel, non-invasive approach using multimodal predictive models that integrate RGB or thermal video data with patient metadata. Our method leverages energy-efficient Spiking Neural Networks (SNNs) for the regression of Peak Expiratory Flow (PEF) and classification of Forced Expiratory Volume (FEV1) and Forced Vital Capacity (FVC), using lightweight CNNs to overcome SNN limitations in regression tasks. Multimodal data integration is improved with a Multi-Head Attention Layer, and we employ K-Fold validation and ensemble learning to boost robustness. Using thermal data, our SNN models achieve 92%±plus-or-minus\pm±2% accuracy on a breathing-cycle basis and 99.5%±plus-or-minus\pm±0.5% patient-wise. PEF regression models attain Relative RMSEs of 0.11±plus-or-minus\pm±0.05 (thermal) and 0.26±plus-or-minus\pm±0.07 (RGB), with an MAE of 4.52% for FEV1/FVC predictions, establishing state-of-the-art performance.111Code and dataset can be found onhttps://github.com/ahmed-sharshar/RespiroDynamics.git"
95,679d459debd8ffd557a2aecc,cs.CV,https://arxiv.org/pdf/2501.17635,In-Context Meta LoRA Generation,"Yihua Shao, Minxi Yan, Yang Liu, Siyu Chen, Wenjie Chen, Xinwei Long, Ziyang Yan, Lei Li, Chenyu Zhang, Nicu Sebe, Hao Tang, Yan Wang, Hao Zhao, Mengzhu Wang, Jingcai Guo","Computation and Language, Artificial Intelligence, Computer Vision and Pattern Recognition","Low-rank Adaptation (LoRA) has demonstrated remarkable capabilities for task specific fine-tuning. However, in scenarios that involve multiple tasks, training a separate LoRA model for each one results in considerable inefficiency in terms of storage and inference. Moreover, existing parameter generation methods fail to capture the correlations among these tasks, making multi-task LoRA parameter generation challenging. To address these limitations, we propose In-Context Meta LoRA (ICM-LoRA), a novel approach that efficiently achieves task-specific customization of large language models (LLMs). Specifically, we use training data from all tasks to train a tailored generator, Conditional Variational Autoencoder (CVAE). CVAE takes task descriptions as inputs and produces task-aware LoRA weights as outputs. These LoRA weights are then merged with LLMs to create task-specialized models without the need for additional fine-tuning. Furthermore, we utilize in-context meta-learning for knowledge enhancement and task mapping, to capture the relationship between tasks and parameter distributions. As a result, our method achieves more accurate LoRA parameter generation for diverse tasks using CVAE. ICM-LoRA enables more accurate LoRA parameter reconstruction than current parameter reconstruction methods and is useful for implementing task-specific enhancements of LoRA parameters. At the same time, our method occupies 283MB, only 1% storage compared with the original LoRA."
96,679d459debd8ffd557a2aecd,cs.CV,https://arxiv.org/pdf/2501.17634,Federated Learning With Individualized Privacy Through Client Sampling,"Lucas Lange, Ole Borchardt, Erhard Rahm","Machine Learning, Cryptography and Security, Computer Vision and Pattern Recognition","With growing concerns about user data collection, individualized privacy has emerged as a promising solution to balance protection and utility by accounting for diverse user privacy preferences. Instead of enforcing a uniform level of anonymization for all users, this approach allows individuals to choose privacy settings that align with their comfort levels. Building on this idea, we propose an adapted method for enabling Individualized Differential Privacy (IDP) in Federated Learning (FL) by handling clients according to their personal privacy preferences. By extending the SAMPLE algorithm from centralized settings to FL, we calculate client-specific sampling rates based on their heterogeneous privacy budgets and integrate them into a modified IDP-FedAvg algorithm. We test this method under realistic privacy distributions and multiple datasets. The experimental results demonstrate that our approach achieves clear improvements over uniform DP baselines, reducing the trade-off between privacy and utility. Compared to the alternative SCALE method in related work, which assigns differing noise scales to clients, our method performs notably better. However, challenges remain for complex tasks with non-i.i.d. data, primarily stemming from the constraints of the decentralized setting."
97,679d459debd8ffd557a2aece,cs.CV,https://arxiv.org/pdf/2501.17628,Dual Invariance Self-training for Reliable Semi-supervised Surgical Phase Recognition,"Sahar Nasirihaghighi, Negin Ghamsarian, Raphael Sznitman, Klaus Schoeffmann","Image and Video Processing, Computer Vision and Pattern Recognition","Accurate surgical phase recognition is crucial for advancing computer-assisted interventions, yet the scarcity of labeled data hinders training reliable deep learning models. Semi-supervised learning (SSL), particularly with pseudo-labeling, shows promise over fully supervised methods but often lacks reliable pseudo-label assessment mechanisms. To address this gap, we propose a novel SSL framework,Dual Invariance Self-Training (DIST), that incorporates bothTemporalandTransformation Invarianceto enhance surgical phase recognition. Our two-step self-training process dynamically selects reliable pseudo-labels, ensuring robust pseudo-supervision. Our approach mitigates the risk of noisy pseudo-labels, steering decision boundaries toward true data distribution and improving generalization to unseen data. Evaluations on Cataract and Cholec80 datasets show our method outperforms state-of-the-art SSL approaches, consistently surpassing both supervised and SSL baselines across various network architectures. Code is available athttps://github.com/Sahar-Nasiri/DIST."
98,679d459debd8ffd557a2aecf,cs.CV,https://arxiv.org/pdf/2501.17594,Watch Your STEPP: Semantic Traversability Estimation using Pose Projected Features,"Sebastian Ægidius, Dennis Hadjivelichkov, Jianhao Jiao, Jonathan Embley-Riches, Dimitrios Kanoulas","Robotics, Computer Vision and Pattern Recognition","Understanding the traversability of terrain is essential for autonomous robot navigation, particularly in unstructured environments such as natural landscapes. Although traditional methods, such as occupancy mapping, provide a basic framework, they often fail to account for the complex mobility capabilities of some platforms such as legged robots. In this work, we propose a method for estimating terrain traversability by learning from demonstrations of human walking. Our approach leverages dense, pixel-wise feature embeddings generated using the DINOv2 vision Transformer model, which are processed through an encoder-decoder MLP architecture to analyze terrain segments. The averaged feature vectors, extracted from the masked regions of interest, are used to train the model in a reconstruction-based framework. By minimizing reconstruction loss, the network distinguishes between familiar terrain with a low reconstruction error and unfamiliar or hazardous terrain with a higher reconstruction error. This approach facilitates the detection of anomalies, allowing a legged robot to navigate more effectively through challenging terrain. We run real-world experiments on the ANYmal legged robot both indoor and outdoor to prove our proposed method. The code is open-source, while video demonstrations can be found on our website:https://rpl-cs-ucl.github.io/STEPP/"
99,679d459debd8ffd557a2aed0,cs.CV,https://arxiv.org/pdf/2501.17570,Trustworthy image-to-image translation: evaluating uncertainty calibration in unpaired training scenarios,"Ciaran Bench, Emir Ahmed, Spencer A. Thomas","Image and Video Processing, Computer Vision and Pattern Recognition, Medical Physics","Mammographic screening is an effective method for detecting breast cancer, facilitating early diagnosis. However, the current need to manually inspect images places a heavy burden on healthcare systems, spurring a desire for automated diagnostic protocols. Techniques based on deep neural networks have been shown effective in some studies, but their tendency to overfit leaves considerable risk for poor generalisation and misdiagnosis, preventing their widespread adoption in clinical settings. Data augmentation schemes based on unpaired neural style transfer models have been proposed that improve generalisability by diversifying the representations of training image features in the absence of paired training data (images of the same tissue in either image style). But these models are similarly prone to various pathologies, and evaluating their performance is challenging without ground truths/large datasets (as is often the case in medical imaging). Here, we consider two frameworks/architectures: a GAN-based cycleGAN, and the more recently developed diffusion-based SynDiff. We evaluate their performance when trained on image patches parsed from three open access mammography datasets and one non-medical image dataset. We consider the use of uncertainty quantification to assess model trustworthiness, and propose a scheme to evaluate calibration quality in unpaired training scenarios. This ultimately helps facilitate the trustworthy use of image-to-image translation models in domains where ground truths are not typically available."
100,679d459debd8ffd557a2aed1,cs.CV,https://arxiv.org/pdf/2501.17322,Influence of field of view in visual prostheses design: Analysis with a VR system,"Melani Sanchez-Garcia, Ruben Martinez-Cantin, Jesus Bermudez-Cameo, Jose J. Guerrero","Human-Computer Interaction, Computer Vision and Pattern Recognition",
101,679d459debd8ffd557a2aed2,cs.CV,https://arxiv.org/pdf/2501.17266,Advancing the Biological Plausibility and Efficacy of Hebbian Convolutional Neural Networks,"Julian Jimenez Nimmo, Esther Mondragon","Neural and Evolutionary Computing, Computer Vision and Pattern Recognition","The research presented in this paper advances the integration of Hebbian learning into Convolutional Neural Networks (CNNs) for image processing, systematically exploring different architectures to build an optimal configuration, adhering to biological tenability. Hebbian learning operates on local unsupervised neural information to form feature representations, providing an alternative to the popular but arguably biologically implausible and computationally intensive backpropagation learning algorithm. The suggested optimal architecture significantly enhances recent research aimed at integrating Hebbian learning with competition mechanisms and CNNs, expanding their representational capabilities by incorporating hard Winner-Takes-All (WTA) competition, Gaussian lateral inhibition mechanisms and Bienenstock–Cooper–Munro (BCM) learning rule in a single model. The resulting model achieved 76% classification accuracy on CIFAR-10, rivalling its end-to-end backpropagation variant (77%) and critically surpassing the state-of-the-art hard-WTA performance in CNNs of the same network depth (64.6%) by 11.4%. Moreover, results showed clear indications of sparse hierarchical learning through increasingly complex and abstract receptive fields. In summary, our implementation enhances both the performance and the generalisability of the learnt representations and constitutes a crucial step towards more biologically realistic artificial neural networks"
102,679d459debd8ffd557a2aed3,cs.CV,https://arxiv.org/pdf/2501.17162,CubeDiff: Repurposing Diffusion-Based Image Models for Panorama Generation,"Nikolai Kalischek, Michael Oechsle, Fabian Manhardt, Philipp Henzler, Konrad Schindler, Federico Tombari","Computer Vision and Pattern Recognition, Machine Learning","We introduce a novel method for generating 360° panoramas from text prompts or images. Our approach leverages recent advances in 3D generation by employing multi-view diffusion models to jointly synthesize the six faces of a cubemap. Unlike previous methods that rely on processing equirectangular projections or autoregressive generation, our method treats each face as a standard perspective image, simplifying the generation process and enabling the use of existing multi-view diffusion models. We demonstrate that these models can be adapted to produce high-quality cubemaps without requiring correspondence-aware attention layers. Our model allows for fine-grained text control, generates high resolution panorama images and generalizes well beyond its training set, whilst achieving state-of-the-art results, both qualitatively and quantitatively. Project page:https://cubediff.github.io/"
103,679d459debd8ffd557a2aed4,cs.CV,https://arxiv.org/pdf/2501.17159,IC-Portrait: In-Context Matching for View-Consistent Personalized Portrait,"Han Yang, Enis Simsar, Sotiris Anagnostidi, Yanlong Zang, Thomas Hofmann, Ziwei Liu",Computer Vision and Pattern Recognition,"Existing diffusion models show great potential for identity-preserving generation. However, personalized portrait generation remains challenging due to the diversity in user profiles, including variations in appearance and lighting conditions. To address these challenges, we proposeIC-Portrait, a novel framework designed to accurately encode individual identities for personalized portrait generation. Our key insight is that pre-trained diffusion models are fast learners (e.g.,100∼200similar-to100200100\sim 200100 ∼ 200steps) for in-context dense correspondence matching, which motivates the two major designs of our IC-Portrait framework. Specifically, we reformulate portrait generation into two sub-tasks:1) Lighting-Aware Stitching: we find that masking a high proportion of the input image,e.g.,80%, yields a highly effective self-supervisory representation learning of reference image lighting.2) View-Consistent Adaptation: we leverage a synthetic view-consistent profile dataset to learn the in-context correspondence. The reference profile can then be warped into arbitrary poses for strong spatial-aligned view conditioning. Coupling these two designs by simply concatenating latents to form ControlNet-like supervision and modeling, enables us to significantly enhance the identity preservation fidelity and stability.
Extensive evaluations demonstrate that IC-Portrait consistently outperforms existing state-of-the-art methods both quantitatively and qualitatively, with particularly notable improvements in visual qualities. Furthermore, IC-Portrait even demonstrates 3D-aware relighting capabilities."
104,679d459debd8ffd557a2aed5,cs.CV,https://arxiv.org/pdf/2501.17131,Scenario Understanding of Traffic Scenes Through Large Visual Language Models,"Rivera Esteban, Lübberstedt Jannik, Nico Uhlemann, Markus Lienkamp",Computer Vision and Pattern Recognition,"Deep learning models for autonomous driving, encompassing perception, planning, and control, depend on vast datasets to achieve their high performance. However, their generalization often suffers due to domain-specific data distributions, making an effective scene-based categorization of samples necessary to improve their reliability across diverse domains. Manual captioning, though valuable, is both labor-intensive and time-consuming, creating a bottleneck in the data annotation process. Large Visual Language Models (LVLMs) present a compelling solution by automating image analysis and categorization through contextual queries, often without requiring retraining for new categories. In this study, we evaluate the capabilities of LVLMs, including GPT-4 and LLaVA, to understand and classify urban traffic scenes on both an in-house dataset and the BDD100K. We propose a scalable captioning pipeline that integrates state-of-the-art models, enabling a flexible deployment on new datasets. Our analysis, combining quantitative metrics with qualitative insights, demonstrates the effectiveness of LVLMs to understand urban traffic scenarios and highlights their potential as an efficient tool for data-driven advancements in autonomous driving."
105,679d459debd8ffd557a2aed6,cs.CV,https://arxiv.org/pdf/2501.17085,Evaluating CrowdSplat: Perceived Level of Detail for Gaussian Crowds,"Xiaohan Sun, Yinghan Xu, John Dingliana, Carol O'Sullivan",Computer Vision and Pattern Recognition,"Efficient and realistic crowd rendering is an important element of many real-time graphics applications such as Virtual Reality (VR) and games. To this end, Levels of Detail (LOD) avatar representations such as polygonal meshes, image-based impostors, and point clouds have been proposed and evaluated. More recently, 3D Gaussian Splatting has been explored as a potential method for real-time crowd rendering. In this paper, we present a two-alternative forced choice (2AFC) experiment that aims to determine the perceived quality of 3D Gaussian avatars. Three factors were explored:Motion,LOD(i.e., #Gaussians), and the avatar height inPixels(corresponding to the viewing distance). Participants viewed pairs of animated 3D Gaussian avatars and were tasked with choosing the most detailed one. Our findings can inform the optimization of LOD strategies in Gaussian-based crowd rendering, thereby helping to achieve efficient rendering while maintaining visual quality in real-time applications."
106,679d459debd8ffd557a2aed7,cs.CV,https://arxiv.org/pdf/2501.17076,DINOSTAR: Deep Iterative Neural Object Detector Self-Supervised Training for Roadside LiDAR Applications,"Muhammad Shahbaz, Shaurya Agarwal",Computer Vision and Pattern Recognition,"Recent advancements in deep-learning methods for object detection in point-cloud data have enabled numerous roadside applications, fostering improvements in transportation safety and management. However, the intricate nature of point-cloud data poses significant challenges for human-supervised labeling, resulting in substantial expenditures of time and capital. This paper addresses the issue by developing an end-to-end, scalable, and self-supervised framework for training deep object detectors tailored for roadside point-cloud data. The proposed framework leverages self-supervised, statistically modeled teachers to train off-the-shelf deep object detectors, thus circumventing the need for human supervision. The teacher models follow fine-tuned set standard practices of background filtering, object clustering, bounding-box fitting, and classification to generate noisy labels. It is presented that by training the student model over the combined noisy annotations from multitude of teachers enhances its capacity to discern background/foreground more effectively and forces it to learn diverse point-cloud-representations for object categories of interest. The evaluations, involving publicly available roadside datasets and state-of-art deep object detectors, demonstrate that the proposed framework achieves comparable performance to deep object detectors trained on human-annotated labels, despite not utilizing such human-annotations in its training process."
107,679d459debd8ffd557a2aed8,cs.CV,https://arxiv.org/pdf/2501.17053,Contextual Self-paced Learning for Weakly Supervised Spatio-Temporal Video Grounding,"Akash Kumar, Zsolt Kira, Yogesh Singh Rawat",Computer Vision and Pattern Recognition,"In this work, we focus on Weakly Supervised Spatio-Temporal Video Grounding (WSTVG). It is a multimodal task aimed at localizing specific subjects spatio-temporally based on textual queries without bounding box supervision. Motivated by recent advancements in multi-modal foundation models for grounding tasks, we first explore the potential of state-of-the-art object detection models for WSTVG. Despite their robust zero-shot capabilities, our adaptation reveals significant limitations, including inconsistent temporal predictions, inadequate understanding of complex queries, and challenges in adapting to difficult scenarios.
We proposeCoSPaL(Contextual Self-Paced Learning), a novel approach which is designed to overcome these limitations. CoSPaL integrates three core components: (1)Tubelet Phrase Grounding (TPG), which introduces spatio-temporal prediction by linking textual queries to tubelets; (2)Contextual Referral Grounding (CRG), which improves comprehension of complex queries by extracting contextual information to refine object identification over time; and (3)Self-Paced Scene Understanding (SPS), a training paradigm that progressively increases task difficulty, enabling the model to adapt to complex scenarios by transitioning from coarse to fine-grained understanding.
We demonstrate the effectiveness of CoSPaL on three benchmark WSTVG datasets, achieving a 3.9% absolute improvement on VidSTG and a 7.9% improvement on HCSTVG-v1."
108,679d459debd8ffd557a2aed9,cs.CV,https://arxiv.org/pdf/2501.17044,Synthesizing 3D Abstractions by Inverting Procedural Buildings with Transformers,"Maximilian Dax, Jordi Berbel, Jan Stria, Leonidas Guibas, Urs Bergmann","Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning","We generate abstractions of buildings, reflecting the essential aspects of their geometry and structure, by learning to invert procedural models. We first build a dataset of abstract procedural building models paired with simulated point clouds and then learn the inverse mapping through a transformer. Given a point cloud, the trained transformer then infers the corresponding abstracted building in terms of a programmatic language description. This approach leverages expressive procedural models developed for gaming and animation, and thereby retains desirable properties such as efficient rendering of the inferred abstractions and strong priors for regularity and symmetry. Our approach achieves good reconstruction accuracy in terms of geometry and structure, as well as structurally consistent inpainting."
109,679d459debd8ffd557a2aeda,cs.CV,https://arxiv.org/pdf/2501.16997,MAUCell: An Adaptive Multi-Attention Framework for Video Frame Prediction,"Shreyam Gupta, P. Agrawal, Priyam Gupta","Computer Vision and Pattern Recognition, Machine Learning, Robotics","Temporal sequence modeling stands as the fundamental foundation for video prediction systems and real-time forecasting operations as well as anomaly detection applications. The achievement of accurate predictions through efficient resource consumption remains an ongoing issue in contemporary temporal sequence modeling. We introduce the Multi-Attention Unit (MAUCell) which combines Generative Adversarial Networks (GANs) and spatio-temporal attention mechanisms to improve video frame prediction capabilities. Our approach implements three types of attention models to capture intricate motion sequences. A dynamic combination of these attention outputs allows the model to reach both advanced decision accuracy along with superior quality while remaining computationally efficient. The integration of GAN elements makes generated frames appear more true to life therefore the framework creates output sequences which mimic real-world footage. The new design system maintains equilibrium between temporal continuity and spatial accuracy to deliver reliable video prediction. Through a comprehensive evaluation methodology which merged the perceptual LPIPS measurement together with classic tests MSE, MAE, SSIM and PSNR exhibited enhancing capabilities than contemporary approaches based on direct benchmark tests of Moving MNIST, KTH Action, and CASIA-B (Preprocessed) datasets. Our examination indicates that MAUCell shows promise for operational time requirements. The research findings demonstrate how GANs work best with attention mechanisms to create better applications for predicting video sequences."
110,679d459debd8ffd557a2aedb,cs.CV,https://arxiv.org/pdf/2501.16992,FedEFM: Federated Endovascular Foundation Model with Unseen Data,"Tuong Do, Nghia Vu, Tudor Jianu, Baoru Huang, Minh Vu, Jionglong Su, Erman Tjiputra, Quang D. Tran, Te-Chuan Chiu, Anh Nguyen",Computer Vision and Pattern Recognition,"In endovascular surgery, the precise identification of catheters and guidewires in X-ray images is essential for reducing intervention risks. However, accurately segmenting catheter and guidewire structures is challenging due to the limited availability of labeled data. Foundation models offer a promising solution by enabling the collection of similar-domain data to train models whose weights can be fine-tuned for downstream tasks. Nonetheless, large-scale data collection for training is constrained by the necessity of maintaining patient privacy. This paper proposes a new method to train a foundation model in a decentralized federated learning setting for endovascular intervention. To ensure the feasibility of the training, we tackle the unseen data issue using differentiable Earth Mover’s Distance within a knowledge distillation framework. Once trained, our foundation model’s weights provide valuable initialization for downstream tasks, thereby enhancing task-specific performance. Intensive experiments show that our approach achieves new state-of-the-art results, contributing to advancements in endovascular intervention and robotic-assisted endovascular surgery, while addressing the critical issue of data sharing in the medical domain."
111,679d459debd8ffd557a2aedc,cs.CV,https://arxiv.org/pdf/2501.16981,Modulating CNN Features with Pre-Trained ViT Representations for Open-Vocabulary Object Detection,"Xiangyu Gao, Yu Dai, Benliu Qiu, Hongliang Li",Computer Vision and Pattern Recognition,"Owing to large-scale image-text contrastive training, pre-trained vision language model (VLM) like CLIP shows superior open-vocabulary recognition ability.
Most existing open-vocabulary object detectors attempt to utilize the pre-trained VLM to attain generative representation. F-ViT uses the pre-trained visual encoder as the backbone network and freezes it during training.
However, the frozen backbone doesn’t benefit from the labeled data to strengthen the representation.
Therefore, we propose a novel two-branch backbone network design, named as ViT-Feature-Modulated Multi-Scale Convolutional network (VMCNet).
VMCNet consists of a trainable convolutional branch, a frozen pre-trained ViT branch and a feature modulation module. The trainable CNN branch could be optimized with labeled data while the frozen pre-trained ViT branch could keep the representation ability derived from large-scale pre-training. Then, the proposed feature modulation module could modulate the multi-scale CNN features with the representations from ViT branch. With the proposed mixed structure, detector is more likely to discover novel categories.
Evaluated on two popular benchmarks, our method boosts the detection performance on novel category and outperforms the baseline. On OV-COCO, the proposed method achieves 44.3 APnovel50superscriptsubscriptabsent50novel{}_{50}^{\mathrm{novel}}start_FLOATSUBSCRIPT 50 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT roman_novel end_POSTSUPERSCRIPTwith ViT-B/16 and 48.5 APnovel50superscriptsubscriptabsent50novel{}_{50}^{\mathrm{novel}}start_FLOATSUBSCRIPT 50 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT roman_novel end_POSTSUPERSCRIPTwith ViT-L/14. On OV-LVIS, VMCNet with ViT-B/16 and ViT-L/14 reaches 27.8 and 38.4 mAPr."
112,679d459debd8ffd557a2aedd,cs.CV,https://arxiv.org/pdf/2501.16971,RODEO: Robust Outlier Detection via Exposing Adaptive Out-of-Distribution Samples,"Hossein Mirzaei, Mohammad Jafari, Hamid Reza Dehbashi, Ali Ansari, Sepehr Ghobadi, Masoud Hadi, Arshia Soltani Moakhar, Mohammad Azizmalayeri, Mahdieh Soleymani Baghshah, Mohammad Hossein Rohban","Computer Vision and Pattern Recognition, Machine Learning","In recent years, there have been significant improvements in various forms of image outlier detection. However, outlier detection performance under adversarial settings lags far behind that in standard settings. This is due to the lack of effective exposure to adversarial scenarios during training, especially on unseen outliers, leading to detection models failing to learn robust features. To bridge this gap, we introduce RODEO, a data-centric approach that generates effective outliers for robust outlier detection. More specifically, we show that incorporating outlier exposure (OE) and adversarial training can be an effective strategy for this purpose, as long as the exposed training outliers meet certain characteristics, including diversity, and both conceptual differentiability and analogy to the inlier samples. We leverage a text-to-image model to achieve this goal. We demonstrate both quantitatively and qualitatively that our adaptive OE method effectively generates “diverse” and “near-distribution” outliers, leveraging information from both text and image domains. Moreover, our experimental results show that utilizing our synthesized outliers significantly enhances the performance of the outlier detector, particularly in adversarial settings. The implementation of our work is available at:https://github.com/rohban-lab/RODEO."
113,679d459debd8ffd557a2aede,cs.CV,https://arxiv.org/pdf/2501.16969,What Really Matters for Learning-based LiDAR-Camera Calibration,"Shujuan Huang, Chunyu Lin, Yao Zhao",Computer Vision and Pattern Recognition,"Calibration is an essential prerequisite for the accurate data fusion of LiDAR and camera sensors. Traditional calibration techniques often require specific targets or suitable scenes to obtain reliable 2D-3D correspondences. To tackle the challenge of target-less and online calibration, deep neural networks have been introduced to solve the problem in a data-driven manner. While previous learning-based methods have achieved impressive performance on specific datasets, they still struggle in complex real-world scenarios. Most existing works focus on improving calibration accuracy but overlook the underlying mechanisms. In this paper, we revisit the development of learning-based LiDAR-Camera calibration and encourage the community to pay more attention to the underlying principles to advance practical applications. We systematically analyze the paradigm of mainstream learning-based methods, and identify the critical limitations of regression-based methods with the widely used data generation pipeline. Our findings reveal that most learning-based methods inadvertently operate as retrieval networks, focusing more on single-modality distributions rather than cross-modality correspondences. We also investigate how the input data format and preprocessing operations impact network performance and summarize the regression clues to inform further improvements."
114,679d459debd8ffd557a2aedf,cs.CV,https://arxiv.org/pdf/2501.16947,Image-based Geo-localization for Robotics: Are Black-box Vision-Language Models there yet?,"Sania Waheed, Bruno Ferrarini, Michael Milford, Sarvapali D. Ramchurn, Shoaib Ehsan","Computer Vision and Pattern Recognition, Robotics","The advances in Vision-Language models (VLMs) offer exciting opportunities for robotic applications involving image geo-localization – the problem of identifying the geo-coordinates of a place based on visual data only. Recent research works have focused on using a VLM as embeddings extractor for geo-localization, however, the most sophisticated VLMs may only be available as black boxes that are accessible through an API, and come with a number of limitations: there is no access to training data, model features and gradients; retraining is not possible; the number of predictions may be limited by the API; training on model outputs is often prohibited; and queries are open-ended. The utilization of a VLM as a stand-alone, zero-shot geo-localization system using a single text-based prompt is largely unexplored. To bridge this gap, this paper undertakes the first systematic study, to the best of our knowledge, to investigate the potential of some of the state-of-the-art VLMs as stand-alone, zero-shot geo-localization systems in a black-box setting with realistic constraints. We consider three main scenarios for this thorough investigation: a) fixed text-based prompt; b) semantically-equivalent text-based prompts; and c) semantically-equivalent query images. We also take into account the auto-regressive and probabilistic generation process of the VLMs when investigating their utility for geo-localization task by using model consistency as a metric in addition to traditional accuracy. Our work provides new insights in the capabilities of different VLMs for the above-mentioned scenarios."
115,679d459debd8ffd557a2aee0,cs.CV,https://arxiv.org/pdf/2501.16917,B-FPGM: Lightweight Face Detection via Bayesian-Optimized Soft FPGM Pruning,"Nikolaos Kaparinos, Vasileios Mezaris",Computer Vision and Pattern Recognition,"Face detection is a computer vision application that increasingly demands lightweight models to facilitate deployment on devices with limited computational resources. Neural network pruning is a promising technique that can effectively reduce network size without significantly affecting performance. In this work, we propose a novel face detection pruning pipeline that leverages Filter Pruning via Geometric Median (FPGM) pruning, Soft Filter Pruning (SFP) and Bayesian optimization in order to achieve a superior trade-off between size and performance compared to existing approaches. FPGM pruning is a structured pruning technique that allows pruning the least significant filters in each layer, while SFP iteratively prunes the filters and allows them to be updated in any subsequent training step. Bayesian optimization is employed in order to optimize the pruning rates of each layer, rather than relying on engineering expertise to determine the optimal pruning rates for each layer. In our experiments across all three subsets of the WIDER FACE dataset, our proposed approach B-FPGM consistently outperforms existing ones in balancing model size and performance. All our experiments were applied to EResFD, the currently smallest (in number of parameters) well-performing face detector of the literature; a small ablation study with a second small face detector, EXTD, is also reported. The source code and trained pruned face detection models can be found at:https://github.com/IDTITI/B-FPGM."
116,679d459debd8ffd557a2aee1,cs.CV,https://arxiv.org/pdf/2501.16904,Adversarial Masked Autoencoder Purifier with Defense Transferability,"Yuan-Chih Chen, Chun-Shien Lu",Computer Vision and Pattern Recognition,"The study of adversarial defense still struggles to combat with advanced adversarial attacks.
In contrast to most prior studies that rely on the diffusion model for test-time defense to remarkably increase the inference time, we propose Masked AutoEncoder Purifier (MAEP), which integrates Masked AutoEncoder (MAE) into an adversarial purifier
framework for test-time purification.
While MAEP achieves promising adversarial robustness, it particularly features model defense transferability and attack generalization without relying on using additional data that is different from the training dataset.
To our knowledge, MAEP is the first study of adversarial purifier based on MAE.
Extensive experimental results demonstrate that our method can not only maintain clear accuracy
with only a slight drop but also exhibit a close gap between the clean and robust accuracy.
Notably, MAEP trained on CIFAR10 achieves state-of-the-art performance even when tested directly on ImageNet, outperforming existing diffusion-based models trained specifically on ImageNet."
117,679d459debd8ffd557a2aee2,cs.CV,https://arxiv.org/pdf/2501.16896,Frequency Matters: Explaining Biases of Face Recognition in the Frequency Domain,"Marco Huber, Fadi Boutros, Naser Damer",Computer Vision and Pattern Recognition,"Face recognition (FR) models are vulnerable to performance variations across demographic groups. The causes for these performance differences are unclear due to the highly complex deep learning-based structure of face recognition models. Several works aimed at exploring possible roots of gender and ethnicity bias, identifying semantic reasons such as hairstyle, make-up, or facial hair as possible sources. Motivated by recent discoveries of the importance of frequency patterns in convolutional neural networks, we explain bias in face recognition using state-of-the-art frequency-based explanations. Our extensive results show that different frequencies are important to FR models depending on the ethnicity of the samples."
118,679d459debd8ffd557a2aee3,cs.CV,https://arxiv.org/pdf/2501.16889,Extending Information Bottleneck Attribution to Video Sequences,"Veronika Solopova, Lucas Schmidt, Dorothea Kolossa","Computer Vision and Pattern Recognition, Artificial Intelligence","We introduce VIBA, a novel approach for explainable video classification by adapting Information Bottlenecks for Attribution (IBA) to video sequences. While most traditional explainability methods are designed for image models, our IBA framework addresses the need for explainability in temporal models used for video analysis. To demonstrate its effectiveness, we apply VIBA to video deepfake detection, testing it on two architectures: the Xception model for spatial features and a VGG11-based model for capturing motion dynamics through optical flow. Using a custom dataset that reflects recent deepfake generation techniques, we adapt IBA to create relevance and optical flow maps, visually highlighting manipulated regions and motion inconsistencies. Our results show that VIBA generates temporally and spatially consistent explanations, which align closely with human annotations, thus providing interpretability for video classification and particularly for deepfake detection."
119,679d459debd8ffd557a2aee4,cs.CV,https://arxiv.org/pdf/2501.16870,Experimenting with Affective Computing Models in Video Interviews with Spanish-speaking Older Adults,"Josep Lopez Camunas, Cristina Bustos, Yanjun Zhu, Raquel Ros, Agata Lapedriza",Computer Vision and Pattern Recognition,"Understanding emotional signals in older adults is crucial for designing virtual assistants that support their well-being. However, existing affective computing models often face significant limitations: (1) limited availability of datasets representing older adults, especially in non-English-speaking populations, and (2) poor generalization of models trained on younger or homogeneous demographics. To address these gaps, this study evaluates state-of-the-art affective computing models—including facial expression recognition, text sentiment analysis, and smile detection—using videos of older adults interacting with either a person or a virtual avatar. As part of this effort, we introduce a novel dataset featuring Spanish-speaking older adults engaged in human-to-human video interviews. Through three comprehensive analyses, we investigate (1) the alignment between human-annotated labels and automatic model outputs, (2) the relationships between model outputs across different modalities, and (3) individual variations in emotional signals. Using both the Wizard of Oz (WoZ) dataset and our newly collected dataset, we uncover limited agreement between human annotations and model predictions, weak consistency across modalities, and significant variability among individuals. These findings highlight the shortcomings of generalized emotion perception models and emphasize the need of incorporating personal variability and cultural nuances into future systems."
120,679d459debd8ffd557a2aee5,cs.CV,https://arxiv.org/pdf/2501.16811,Not Every Patch is Needed: Towards a More Efficient and Effective Backbone for Video-based Person Re-identification,"Lanyun Zhu, Tianrun Chen, Deyi Ji, Jieping Ye, Jun Liu",Computer Vision and Pattern Recognition,"This paper proposes a new effective and efficient plug-and-play backbone for video-based person re-identification (ReID). Conventional video-based ReID methods typically use CNN or transformer backbones to extract deep features for every position in every sampled video frame. Here, we argue that this exhaustive feature extraction could be unnecessary, since we find that different frames in a ReID video often exhibit small differences and contain many similar regions due to the relatively slight movements of human beings. Inspired by this, a more selective, efficient paradigm is explored in this paper. Specifically, we introduce a patch selection mechanism to reduce computational cost by choosing only the crucial and non-repetitive patches for feature extraction. Additionally, we present a novel network structure that generates and utilizes pseudo frame global context to address the issue of incomplete views resulting from sparse inputs. By incorporating these new designs, our backbone can achieve both high performance and low computational cost. Extensive experiments on multiple datasets show that our approach reduces the computational cost by 74% compared to ViT-B and 28% compared to ResNet50, while the accuracy is on par with ViT-B and outperforms ResNet50 significantly."
121,679d459debd8ffd557a2aee6,cs.CV,https://arxiv.org/pdf/2501.16787,Dynamic Hypergraph Representation for Bone Metastasis Cancer Analysis,"Yuxuan Chen, Jiawen Li, Huijuan Shi, Yang Xu, Tian Guan, Lianghui Zhu, Yonghong He, Anjia Han",Computer Vision and Pattern Recognition,"Bone metastasis analysis is a significant challenge in pathology and plays a critical role in determining patient quality of life and treatment strategies. The microenvironment and specific tissue structures are essential for pathologists to predict the primary bone cancer origins and primary bone cancer subtyping. By digitizing bone tissue sections into whole slide images (WSIs) and leveraging deep learning to model slide embeddings, this analysis can be enhanced. However, tumor metastasis involves complex multivariate interactions with diverse bone tissue structures, which traditional WSI analysis methods such as multiple instance learning (MIL) fail to capture. Moreover, graph neural networks (GNNs), limited to modeling pairwise relationships, are hard to represent high-order biological associations. To address these challenges, we propose a dynamic hypergraph neural network (DyHG) that overcomes the edge construction limitations of traditional graph representations by connecting multiple nodes via hyperedges. A low-rank strategy is used to reduce the complexity of parameters in learning hypergraph structures, while a Gumbel-Softmax-based sampling strategy optimizes the patch distribution across hyperedges. An MIL aggregator is then used to derive a graph-level embedding for comprehensive WSI analysis. To evaluate the effectiveness of DyHG, we construct two large-scale datasets for primary bone cancer origins and subtyping classification based on real-world bone metastasis scenarios. Extensive experiments demonstrate that DyHG significantly outperforms state-of-the-art (SOTA) baselines, showcasing its ability to model complex biological interactions and improve the accuracy of bone metastasis analysis."
122,679d459debd8ffd557a2aee7,cs.CV,https://arxiv.org/pdf/2501.16786,Exploring the Role of Explicit Temporal Modeling in Multimodal Large Language Models for Video Understanding,"Yun Li, Zhe Liu, Yajing Kong, Guangrui Li, Jiyuan Zhang, Chao Bian, Feng Liu, Lina Yao, Zhenbang Sun","Computer Vision and Pattern Recognition, Computation and Language","Applying Multimodal Large Language Models (MLLMs) to video understanding presents significant challenges due to the need to model temporal relations across frames. Existing approaches adopt eitherimplicit temporal modeling, relying solely on the LLM decoder, orexplicit temporal modeling, employing auxiliary temporal encoders. To investigate this debate between the two paradigms, we propose the Stackable Temporal Encoder (STE). STE enables flexible explicit temporal modeling with adjustable temporal receptive fields and token compression ratios. Using STE, we systematically compare implicit and explicit temporal modeling across dimensions such as overall performance, token compression effectiveness, and temporal-specific understanding. We also explore STE’s design considerations and broader impacts as a plug-in module and in image modalities. Our findings emphasize the critical role of explicit temporal modeling, providing actionable insights to advance video MLLMs."
123,679d459debd8ffd557a2aee8,cs.CV,https://arxiv.org/pdf/2501.16778,"FlexMotion: Lightweight, Physics-Aware, and Controllable Human Motion Generation","Arvin Tashakori, Arash Tashakori, Gongbo Yang, Z. Jane Wang, Peyman Servati","Computer Vision and Pattern Recognition, Artificial Intelligence, Graphics, Machine Learning","Lightweight, controllable, and physically plausible human motion synthesis is crucial for animation, virtual reality, robotics, and human-computer interaction applications. Existing methods often compromise between computational efficiency, physical realism, or spatial controllability. We propose FlexMotion, a novel framework that leverages a computationally lightweight diffusion model operating in the latent space, eliminating the need for physics simulators and enabling fast and efficient training. FlexMotion employs a multimodal pre-trained Transformer encoder-decoder, integrating joint locations, contact forces, joint actuations and muscle activations to ensure the physical plausibility of the generated motions. FlexMotion also introduces a plug-and-play module, which adds spatial controllability over a range of motion parameters (e.g., joint locations, joint actuations, contact forces, and muscle activations). Our framework achieves realistic motion generation with improved efficiency and control, setting a new benchmark for human motion synthesis. We evaluate FlexMotion on extended datasets and demonstrate its superior performance in terms of realism, physical plausibility, and controllability."
124,679d459debd8ffd557a2aee9,cs.CV,https://arxiv.org/pdf/2501.16769,Beyond-Labels: Advancing Open-Vocabulary Segmentation With Vision-Language Models,Muhammad Atta ur Rahman,Computer Vision and Pattern Recognition,"Self-supervised learning can resolve numerous image or linguistic processing problems when effectively trained. This study investigated simple yet efficient methods for adapting previously learned foundation models for open-vocabulary semantic segmentation tasks. Our research proposed ”BeyondLabels,” a lightweight transformer-based fusion module that uses a handful of image segmentation data to fuse frozen image representations with language concepts. Furthermore, we efficiently captured positional information in images using Fourier embeddings, thus improving the generalization across various image sizes. Extensive ablation tests were performed to investigate the important components of our proposed method; when tested against the common benchmark PASCAL-5i, it demonstrated superior performance despite being trained on frozen image and language characteristics."
125,679d459debd8ffd557a2aeea,cs.CV,https://arxiv.org/pdf/2501.16767,Target-driven Self-Distillation for Partial Observed Trajectories Forecasting,"Pengfei Zhu, Peng Shu, Mengshi Qi, Liang Liu, Huadong Ma",Computer Vision and Pattern Recognition,"Accurate prediction of future trajectories of traffic agents is essential for ensuring safe autonomous driving. However, partially observed trajectories can significantly degrade the performance of even state-of-the-art models. Previous approaches often rely on knowledge distillation to transfer features from fully observed trajectories to partially observed ones. This involves firstly training a fully observed model and then using a distillation process to create the final model. While effective, they require multi-stage training, making the training process very expensive. Moreover, knowledge distillation can lead to a performance degradation of the model. In this paper, we introduce aTarget-drivenSelf-Distillation method (TSD) for motion forecasting. Our method leverages predicted accurate targets to guide the model in making predictions under partial observation conditions. By employing self-distillation, the model learns from the feature distributions of both fully observed and partially observed trajectories during a single end-to-end training process. This enhances the model’s ability to predict motion accurately in both fully observed and partially observed scenarios. We evaluate our method on multiple datasets and state-of-the-art motion forecasting models. Extensive experimental results demonstrate that our approach achieves significant performance improvements in both settings. To facilitate further research, we will release our code and model checkpoints."
126,679d459debd8ffd557a2aeeb,cs.CV,https://arxiv.org/pdf/2501.16764,DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian Splat Generation,"Chenguo Lin, Panwang Pan, Bangbang Yang, Zeming Li, Yadong Mu",Computer Vision and Pattern Recognition,"Recent advancements in 3D content generation from text or a single image struggle with limited high-quality 3D datasets and inconsistency from 2D multi-view generation.
We introduceDiffSplat, a novel 3D generative framework that natively generates 3D Gaussian splats by taming large-scale text-to-image diffusion models.
It differs from previous 3D generative models by effectively utilizing web-scale 2D priors while maintaining 3D consistency in a unified model.
To bootstrap the training, a lightweight reconstruction model is proposed to instantly produce multi-view Gaussian splat grids for scalable dataset curation.
In conjunction with the regular diffusion loss on these grids, a 3D rendering loss is introduced to facilitate 3D coherence across arbitrary views.
The compatibility with image diffusion models enables seamless adaptions of numerous techniques for image generation to the 3D realm.
Extensive experiments reveal the superiority ofDiffSplatin text- and image-conditioned generation tasks and downstream applications.
Thorough ablation studies validate the efficacy of each critical design choice and provide insights into the underlying mechanism."
127,679d459debd8ffd557a2aeec,cs.CV,https://arxiv.org/pdf/2501.16760,AdaSemSeg: An Adaptive Few-shot Semantic Segmentation of Seismic Facies,"Surojit Saha, Ross Whitaker","Computer Vision and Pattern Recognition, Machine Learning","Automated interpretation of seismic images using deep learning methods is challenging because of the limited availability of training data. Few-shot learning is a suitable learning paradigm in such scenarios due to its ability to adapt to a new task with limited supervision (small training budget). Existing few-shot semantic segmentation (FSSS) methods fix the number of target classes. Therefore, they do not support joint training on multiple datasets varying in the number of classes. In the context of the interpretation of seismic facies, fixing the number of target classes inhibits the generalization capability of a model trained on one facies dataset to another, which is likely to have a different number of facies. To address this shortcoming, we propose a few-shot semantic segmentation method for interpreting seismic facies that can adapt to the varying number of facies across the dataset, dubbed theAdaSemSeg. In general, the backbone network of FSSS methods is initialized with the statistics learned from the ImageNet dataset for better performance. The lack of such a huge annotated dataset for seismic images motivates using a self-supervised algorithm on seismic datasets to initialize the backbone network. We have trained the AdaSemSeg on three public seismic facies datasets with different numbers of facies and evaluated the proposed method on multiple metrics. The performance of the AdaSemSeg onunseendatasets (not used in training) is better than the prototype-based few-shot method and baselines."
128,679d459debd8ffd557a2aeed,cs.CV,https://arxiv.org/pdf/2501.16757,ITVTON:Virtual Try-On Diffusion Transformer Model Based on Integrated Image and Text,Haifeng Ni,Computer Vision and Pattern Recognition,"Recent advancements in virtual fitting for characters and clothing have leveraged diffusion models to improve the realism of garment fitting. However, challenges remain in handling complex scenes and poses, which can result in unnatural garment fitting and poorly rendered intricate patterns. In this work, we introduce ITVTON, a novel method that enhances clothing-character interactions by combining clothing and character images along spatial channels as inputs, thereby improving fitting accuracy for the inpainting model. Additionally, we incorporate integrated textual descriptions from multiple images to boost the realism of the generated visual effects. To optimize computational efficiency, we limit training to the attention parameters within a single diffusion transformer (Single-DiT) block. To more rigorously address the complexities of real-world scenarios, we curated training samples from the IGPair dataset, thereby enhancing ITVTON’s performance across diverse environments. Extensive experiments demonstrate that ITVTON outperforms baseline methods both qualitatively and quantitatively, setting a new standard for virtual fitting tasks."
129,679d459debd8ffd557a2aeee,cs.CV,https://arxiv.org/pdf/2501.16753,Overcoming Semantic Dilution in Transformer-Based Next Frame Prediction,"Hy Nguyen, Srikanth Thudumu, Hung Du, Rajesh Vasa, Kon Mouzakis","Computer Vision and Pattern Recognition, Artificial Intelligence","Next-frame prediction in videos is crucial for applications such as autonomous driving, object tracking, and motion prediction. The primary challenge in next-frame prediction lies in effectively capturing and processing both spatial and temporal information from previous video sequences. The transformer architecture, known for its prowess in handling sequence data, has made remarkable progress in this domain. However, transformer-based next-frame prediction models face notable issues: (a) The multi-head self-attention (MHSA) mechanism requires the input embedding to be split intoN𝑁Nitalic_Nchunks, whereN𝑁Nitalic_Nis the number of heads. Each segment captures only a fraction of the original embedding’s information, which distorts the representation of the embedding in the latent space, resulting in a semantic dilution problem; (b) These models predict the embeddings of the next frames rather than the frames themselves, but the loss function based on the errors of the reconstructed frames, not the predicted embeddings – this creates a discrepancy between the training objective and the model output. We propose a Semantic Concentration Multi-Head Self-Attention (SCMHSA) architecture, which effectively mitigates semantic dilution in transformer-based next-frame prediction. Additionally, we introduce a loss function that optimizes SCMHSA in the latent space, aligning the training objective more closely with the model output. Our method demonstrates superior performance compared to the original transformer-based predictors."
130,679d459debd8ffd557a2aeef,cs.CV,https://arxiv.org/pdf/2501.16751,DebugAgent: Efficient and Interpretable Error Slice Discovery for Comprehensive Model Debugging,"Muxi Chen, Chenchen Zhao, Qiang Xu","Computer Vision and Pattern Recognition, Artificial Intelligence","Despite the significant success of deep learning models in computer vision, they often exhibit systematic failures on specific data subsets, known as error slices. Identifying and mitigating these error slices is crucial to enhancing model robustness and reliability in real-world scenarios. In this paper, we introduceDebugAgent, an automated framework for error slice discovery and model repair. DebugAgent first generates task-specific visual attributes to highlight instances prone to errors through an interpretable and structured process. It then employs an efficient slice enumeration algorithm to systematically identify error slices, overcoming the combinatorial challenges that arise during slice exploration. Additionally, DebugAgent extends its capabilities by predicting error slices beyond the validation set, addressing a key limitation of prior approaches. Extensive experiments across multiple domains — including image classification, pose estimation, and object detection — show that DebugAgent not only improves the coherence and precision of identified error slices but also significantly enhances the model repair capabilities.https://github.com/cure-lab/DebugAgent."
131,679d459debd8ffd557a2aef0,cs.CV,https://arxiv.org/pdf/2501.16737,Consistency Diffusion Models for Single-Image 3D Reconstruction with Priors,"Chenru Jiang, Chengrui Zhang, Xi Yang, Jie Sun, Kaizhu Huang",Computer Vision and Pattern Recognition,"This paper delves into the study of 3D point cloud reconstruction from a single image. Our objective is to develop the Consistency Diffusion Model, exploring synergistic 2D and 3D priors in the Bayesian framework to ensure superior consistency in the reconstruction process, a challenging yet critical requirement in this field. Specifically, we introduce a pioneering training framework under diffusion models that brings two key innovations. First, we convert 3D structural priors derived from the initial 3D point cloud as a bound term to increase evidence in the variational Bayesian framework, leveraging these robust intrinsic priors to tightly govern the diffusion training process and bolster consistency in reconstruction. Second, we extract and incorporate 2D priors from the single input image, projecting them onto the 3D point cloud to enrich the guidance for diffusion training. Our framework not only sidesteps potential model learning shifts that may arise from directly imposing additional constraints during training but also precisely transposes the 2D priors into the 3D domain. Extensive experimental evaluations reveal that our approach sets new benchmarks in both synthetic and real-world datasets. The code is included with the submission."
132,679d459debd8ffd557a2aef1,cs.CV,https://arxiv.org/pdf/2501.16724,B-RIGHT: Benchmark Re-evaluation for Integrity in Generalized Human-Object Interaction Testing,"Yoojin Jang, Junsu Kim, Hayeon Kim, Eun-ki Lee, Eun-sol Kim, Seungryul Baek, Jaejun Yoo",Computer Vision and Pattern Recognition,"Human-object interaction (HOI) is an essential problem in artificial intelligence (AI) which aims to understand the visual world that involves complex relationships between humans and objects. However, current benchmarks such as HICO-DET face the following limitations: (1) severe class imbalance and (2) varying number of train and test sets for certain classes. These issues can potentially lead to either inflation or deflation of model performance during evaluation, ultimately undermining the reliability of evaluation scores. In this paper, we propose a systematic approach to develop a new class-balanced dataset,BenchmarkRe-evaluation forIntegrity inGeneralizedHuman-objectInteractionTesting (B-RIGHT), that addresses these imbalanced problems. B-RIGHT achieves class balance by leveraging balancing algorithm and automated generation-and-filtering processes, ensuring an equal number of instances for each HOI class. Furthermore, we design a balanced zero-shot test set to systematically evaluate models on unseen scenario. Re-evaluating existing models using B-RIGHT reveals substantial the reduction of score variance and changes in performance rankings compared to conventional HICO-DET. Our experiments demonstrate that evaluation under balanced conditions ensure more reliable and fair model comparisons."
133,679d459debd8ffd557a2aef2,cs.CV,https://arxiv.org/pdf/2501.16720,One Head Eight Arms: Block Matrix based Low Rank Adaptation for CLIP-based Few-Shot Learning,"Chunpeng Zhou, Qianqian Shen, Zhi Yu, Jiajun Bu, Haishuai Wang","Computer Vision and Pattern Recognition, Artificial Intelligence","Recent advancements in fine-tuning Vision-Language Foundation Models (VLMs) have garnered significant attention for their effectiveness in downstream few-shot learning tasks.While these recent approaches exhibits some performance improvements, they often suffer from excessive training parameters and high computational costs. To address these challenges, we propose a novel Block matrix-based low-rank adaptation framework, called Block-LoRA, for fine-tuning VLMs on downstream few-shot tasks. Inspired by recent work on Low-Rank Adaptation (LoRA), Block-LoRA partitions the original low-rank decomposition matrix of LoRA into a series of sub-matrices while sharing all down-projection sub-matrices. This structure not only reduces the number of training parameters, but also transforms certain complex matrix multiplication operations into simpler matrix addition, significantly lowering the computational cost of fine-tuning.
Notably, Block-LoRA enables fine-tuning CLIP on the ImageNet few-shot benchmark using a single 24GB GPU.
We also show that Block-LoRA has the more
tighter bound of generalization error than vanilla LoRA.
Without bells and whistles, extensive experiments demonstrate that Block-LoRA achieves competitive performance compared to state-of-the-art CLIP-based few-shot methods, while maintaining a low training parameters count and reduced computational overhead.111Under Review"
134,679d459debd8ffd557a2aef3,cs.CV,https://arxiv.org/pdf/2501.16716,Point Cloud Upsampling as Statistical Shape Model for Pelvic,"Tongxu Zhang, Bei Wang",Computer Vision and Pattern Recognition,"We propose a novel framework that integrates medical image segmentation and point cloud upsampling for accurate shape reconstruction of pelvic models. Using the SAM-Med3D model for segmentation and a point cloud upsampling network trained on the MedShapeNet dataset, our method transforms sparse medical imaging data into high-resolution 3D bone models. This framework leverages prior knowledge of anatomical shapes, achieving smoother and more complete reconstructions. Quantitative evaluations using metrics such as Chamfer Distance etc, demonstrate the effectiveness of the point cloud upsampling in pelvic model. Our approach offers potential applications in reconstructing other skeletal structures, providing a robust solution for medical image analysis and statistical shape modeling."
135,679d459debd8ffd557a2aef4,cs.CV,https://arxiv.org/pdf/2501.16714,Separate Motion from Appearance: Customizing Motion via Customizing Text-to-Video Diffusion Models,"Huijie Liu, Jingyun Wang, Shuai Ma, Jie Hu, Xiaoming Wei, Guoliang Kang","Computer Vision and Pattern Recognition, Artificial Intelligence","Motion customization aims to adapt the diffusion model (DM) to generate videos with the motion specified by a set of video clips with the same motion concept.
To realize this goal, the adaptation of DM should be possible to model the specified motion concept, without compromising the ability to generate diverse appearances.
Thus, the key to solving this problem lies in how to separate the motion concept from the appearance in the adaptation process of DM.
Typical previous works explore different ways to represent and insert a motion concept into large-scale pretrained text-to-video diffusion models, e.g., learning a motion LoRA, using latent noise residuals, etc.
While those methods can encode the motion concept, they also inevitably encode the appearance in the reference videos, resulting in weakened appearance generation capability.
In this paper, we follow the typical way to learn a motion LoRA to encode the motion concept, but propose two novel strategies to enhance motion-appearance separation, including temporal attention purification (TAP) and appearance highway (AH).
Specifically, we assume that in the temporal attention module, the pretrained Value embeddings are sufficient to serve as basic components needed by producing a new motion.
Thus, in TAP, we choose only to reshape the temporal attention with motion LoRAs so that Value embeddings can be reorganized to produce a new motion.
Further, in AH, we alter the starting point of each skip connection in U-Net from the output of each temporal attention module to the output of each spatial attention module.
Extensive experiments demonstrate that compared to previous works, our method can generate videos with appearance more aligned with the text descriptions and motion more consistent with the reference videos."
136,679d459debd8ffd557a2aef5,cs.CV,https://arxiv.org/pdf/2501.16704,DFCon: Attention-Driven Supervised Contrastive Learning for Robust Deepfake Detection,"MD Sadik Hossain Shanto, Mahir Labib Dihan, Souvik Ghosh, Riad Ahmed Anonto, Hafijul Hoque Chowdhury, Abir Muhtasim, Rakib Ahsan, MD Tanvir Hassan, MD Roqunuzzaman Sojib, Sheikh Azizul Hakim, M. Saifur Rahman","Computer Vision and Pattern Recognition, Cryptography and Security, Machine Learning, Image and Video Processing, Signal Processing","This report presents our approach for the IEEE SP Cup 2025: Deepfake Face Detection in the Wild (DFWild-Cup), focusing on detecting deepfakes across diverse datasets. Our methodology employs advanced backbone models, including MaxViT, CoAtNet, and EVA-02, fine-tuned using supervised contrastive loss to enhance feature separation. These models were specifically chosen for their complementary strengths. Integration of convolution layers and strided attention in MaxViT is well-suited for detecting local features. In contrast, hybrid use of convolution and attention mechanisms in CoAtNet effectively captures multi-scale features. Robust pretraining with masked image modeling of EVA-02 excels at capturing global features. After training, we freeze the parameters of these models and train the classification heads. Finally, a majority voting ensemble is employed to combine the predictions from these models, improving robustness and generalization to unseen scenarios. The proposed system addresses the challenges of detecting deepfakes in real-world conditions and achieves a commendable accuracy of 95.83% on the validation dataset."
137,679d459debd8ffd557a2aef6,cs.CV,https://arxiv.org/pdf/2501.16700,Determining Mosaic Resilience in Sugarcane Plants using Hyperspectral Images,"Ali Zia, Jun Zhou, Muyiwa Olayemi","Computer Vision and Pattern Recognition, Artificial Intelligence",
138,679d459debd8ffd557a2aef7,cs.CV,https://arxiv.org/pdf/2501.16684,SliceOcc: Indoor 3D Semantic Occupancy Prediction with Vertical Slice Representation,"Jianing Li, Ming Lu, Hao Wang, Chenyang Gu, Wenzhao Zheng, Li Du, Shanghang Zhang",Computer Vision and Pattern Recognition,"3D semantic occupancy prediction is a crucial task in visual perception, as it requires the simultaneous comprehension of both scene geometry and semantics. It plays a crucial role in understanding 3D scenes and has great potential for various applications, such as robotic vision perception and autonomous driving. Many existing works utilize planar-based representations such as Bird’s Eye View (BEV) and Tri-Perspective View (TPV). These representations aim to simplify the complexity of 3D scenes while preserving essential object information, thereby facilitating efficient scene representation. However, in dense indoor environments with prevalent occlusions, directly applying these planar-based methods often leads to difficulties in capturing global semantic occupancy, ultimately degrading model performance. In this paper, we present a new vertical slice representation that divides the scene along the vertical axis and projects spatial point features onto the nearest pair of parallel planes. To utilize these slice features, we propose SliceOcc, an RGB camera-based model specifically tailored for indoor 3D semantic occupancy prediction. SliceOcc utilizes pairs of slice queries and cross-attention mechanisms to extract planar features from input images. These local planar features are then fused to form a global scene representation, which is employed for indoor occupancy prediction. Experimental results on the EmbodiedScan dataset demonstrate that SliceOcc achieves a mIoU of 15.45% across 81 indoor categories, setting a new state-of-the-art performance among RGB camera-based models for indoor 3D semantic occupancy prediction. Code is available at:https://github.com/NorthSummer/SliceOcc."
139,679d459debd8ffd557a2aef8,cs.CV,https://arxiv.org/pdf/2501.16679,Polyp-Gen: Realistic and Diverse Polyp Image Generation for Endoscopic Dataset Expansion,"Shengyuan Liu, Zhen Chen, Qiushi Yang, Weihao Yu, Di Dong, Jiancong Hu, Yixuan Yuan",Computer Vision and Pattern Recognition,"Automated diagnostic systems (ADS) have shown significant potential in the early detection of polyps during endoscopic examinations, thereby reducing the incidence of colorectal cancer. However, due to high annotation costs and strict privacy concerns, acquiring high-quality endoscopic images poses a considerable challenge in the development of ADS. Despite recent advancements in generating synthetic images for dataset expansion, existing endoscopic image generation algorithms failed to accurately generate the details of polyp boundary regions and typically required medical priors to specify plausible locations and shapes of polyps, which limited the realism and diversity of the generated images. To address these limitations, we present Polyp-Gen, the first full-automatic diffusion-based endoscopic image generation framework. Specifically, we devise a spatial-aware diffusion training scheme with a lesion-guided loss to enhance the structural context of polyp boundary regions. Moreover, to capture medical priors for the localization of potential polyp areas, we introduce a hierarchical retrieval-based sampling strategy to match similar fine-grained spatial features. In this way, our Polyp-Gen can generate realistic and diverse endoscopic images for building reliable ADS. Extensive experiments demonstrate the state-of-the-art generation quality, and the synthetic images can improve the downstream polyp detection task. Additionally, our Polyp-Gen has shown remarkable zero-shot generalizability on other datasets. The source code is available athttps://github.com/CUHK-AIM-Group/Polyp-Gen."
140,679d459debd8ffd557a2aef9,cs.CV,https://arxiv.org/pdf/2501.16677,Improving Interpretability and Accuracy in Neuro-Symbolic Rule Extraction Using Class-Specific Sparse Filters,"Parth Padalkar, Jaeseong Lee, Shiyi Wei, Gopal Gupta","Computer Vision and Pattern Recognition, Artificial Intelligence","There has been significant focus on creating neuro-symbolic models for interpretable image classification using Convolutional Neural Networks (CNNs). These methods aim to replace the CNN with a neuro-symbolic model consisting of the CNN, which is used as a feature extractor, and an interpretable rule-set extracted from the CNN itself. While these approaches provide interpretability through the extracted rule-set, they often compromise accuracy compared to the original CNN model.
In this paper, we identify the root cause of this accuracy loss as the post-training binarization of filter activations to extract the rule-set. To address this, we propose a novel sparsity loss function that enables class-specific filter binarization during CNN training, thus minimizing information loss when extracting the rule-set. We evaluate several training strategies with our novel sparsity loss, analyzing their effectiveness and providing guidance on their appropriate use. Notably, we set a new benchmark, achieving a9%improvement in accuracy and a53%reduction in rule-set size on average, compared to the previous SOTA, while coming within3%of the original CNN’s accuracy. This highlights the significant potential of interpretable neuro-symbolic models as viable alternatives to black-box CNNs."
141,679d459debd8ffd557a2aefa,cs.CV,https://arxiv.org/pdf/2501.16665,CSPCL: Category Semantic Prior Contrastive Learning for Deformable DETR-Based Prohibited Item Detectors,"Mingyuan Li, Tong Jia, Hui Lu, Bowen Ma, Hao Wang, Dongyue Chen",Computer Vision and Pattern Recognition,"Prohibited item detection based on X-ray images is one of the most effective security inspection methods. However, the foreground-background feature coupling caused by the overlapping phenomenon specific to X-ray images makes general detectors designed for natural images perform poorly. To address this issue, we propose a Category Semantic Prior Contrastive Learning (CSPCL) mechanism, which aligns the class prototypes perceived by the classifier with the content queries to correct and supplement the missing semantic information responsible for classification, thereby enhancing the model sensitivity to foreground features.
To achieve this alignment, we design a specific contrastive loss, CSP loss, which includes Intra-Class Truncated Attraction (ITA) loss and Inter-Class Adaptive Repulsion (IAR) loss, and outperforms classic N-pair loss and InfoNCE loss. Specifically, ITA loss leverages class prototypes to attract intra-class category-specific content queries while preserving necessary distinctiveness. IAR loss utilizes class prototypes to adaptively repel inter-class category-specific content queries based on the similarity between class prototypes, helping disentangle features of similar categories.
CSPCL is general and can be easily integrated into Deformable DETR-based models. Extensive experiments on the PIXray and OPIXray datasets demonstrate that CSPCL significantly enhances the performance of various state-of-the-art models without increasing complexity.
The code will be open source once the paper is accepted."
142,679d459debd8ffd557a2aefb,cs.CV,https://arxiv.org/pdf/2501.16662,Vision-based autonomous structural damage detection using data-driven methods,"Seyyed Taghi Ataei, Parviz Mohammad Zadeh, Saeid Ataei","Computer Vision and Pattern Recognition, Artificial Intelligence, Image and Video Processing","This study addresses the critical need for efficient and accurate damage detection in wind turbine structures, an essential component of renewable energy infrastructure. Traditional inspection methods, such as manual visual assessments and non-destructive testing (NDT), are often costly, time-intensive, and prone to human error. To overcome these limitations, this research explores the application of advanced deep learning algorithms for vision-based structural health monitoring (SHM). A dataset of wind turbine surface images, featuring categories of damage and pollution, was prepared and augmented to enhance model training. Three state-of-the-art algorithms—YOLOv7, its lightweight variant, and Faster R-CNN—were employed to detect and classify surface damage. The models were trained and tested on a dataset divided into training, testing, and evaluation subsets (80%-10%-10%)."
143,679d459debd8ffd557a2aefc,cs.CV,https://arxiv.org/pdf/2501.16652,Molecular-driven Foundation Model for Oncologic Pathology,"Anurag Vaidya, Andrew Zhang, Guillaume Jaume, Andrew H. Song, Tong Ding, Sophia J. Wagner, Ming Y. Lu, Paul Doucet, Harry Robertson, Cristina Almagro-Perez, Richard J. Chen, Dina ElHarouni, Georges Ayoub, Connor Bossi, Keith L. Ligon, Georg Gerber, Long Phi Le, Faisal Mahmood","Computer Vision and Pattern Recognition, Artificial Intelligence",
144,679d459debd8ffd557a2aefd,cs.CV,https://arxiv.org/pdf/2501.16617,Predicting 3D representations for Dynamic Scenes,"Di Qi, Tong Yang, Beining Wang, Xiangyu Zhang, Wenqiang Zhang",Computer Vision and Pattern Recognition,"We present a novel framework for dynamic radiance field prediction given monocular video streams. Unlike previous methods that primarily focus on predicting future frames, our method goes a step further by generating explicit 3D representations of the dynamic scene. The framework builds on two core designs. First, we adopt an ego-centric unbounded triplane to explicitly represent the dynamic physical world. Second, we develop a 4D-aware transformer to aggregate features from monocular videos to update the triplane. Coupling these two designs enables us to train the proposed model with large-scale monocular videos in a self-supervised manner. Our model achieves top results in dynamic radiance field prediction on NVIDIA dynamic scenes, demonstrating its strong performance on 4D physical world modeling. Besides, our model shows a superior generalizability to unseen scenarios. Notably, we find that our approach emerges capabilities for geometry and semantic learning."
145,679d459debd8ffd557a2aefe,cs.CV,https://arxiv.org/pdf/2501.16612,CascadeV: An Implementation of Wurstchen Architecture for Video Generation,"Wenfeng Lin, Jiangchuan Wei, Boyuan Liu, Yichen Zhang, Shiyue Yan, Mingyu Guo",Computer Vision and Pattern Recognition,"Recently, with the tremendous success of diffusion models in the field of text-to-image (T2I) generation, increasing attention has been directed toward their potential in text-to-video (T2V) applications. However, the computational demands of diffusion models pose significant challenges, particularly in generating high-resolution videos with high frame rates. In this paper, we propose CascadeV, a cascaded latent diffusion model (LDM), that is capable of producing state-of-the-art 2K resolution videos.
Experiments demonstrate that our cascaded model achieves a higher compression ratio, substantially reducing the computational challenges associated with high-quality video generation.
We also implement a spatiotemporal alternating grid 3D attention mechanism, which effectively integrates spatial and temporal information, ensuring superior consistency across the generated video frames.
Furthermore, our model can be cascaded with existing T2V models, theoretically enabling a 4×\times×increase in resolution or frames per second without any fine-tuning.
Our code is available athttps://github.com/bytedance/CascadeV."
146,679d459debd8ffd557a2aeff,cs.CV,https://arxiv.org/pdf/2501.16608,Unsupervised Domain Adaptation with Dynamic Clustering and Contrastive Refinement for Gait Recognition,"Xiaolei Liu, Yan Sun, Mark Nixon",Computer Vision and Pattern Recognition,"Gait recognition is an emerging identification technology that distinguishes individuals at long distances by analyzing individual walking patterns. Traditional techniques rely heavily on large-scale labeled datasets, which incurs high costs and significant labeling challenges. Recently, researchers have explored unsupervised gait recognition with clustering-based unsupervised domain adaptation methods and achieved notable success. However, these methods directly use pseudo-label generated by clustering and neglect pseudo-label noise caused by domain differences, which affects the effect of the model training process. To mitigate these issues, we proposed a novel model called GaitDCCR, which aims to reduce the influence of noisy pseudo labels on clustering and model training. Our approach can be divided into two main stages: clustering and training stage. In the clustering stage, we propose Dynamic Cluster Parameters (DCP) and Dynamic Weight Centroids (DWC) to improve the efficiency of clustering and obtain reliable cluster centroids. In the training stage, we employ the classical teacher-student structure and propose Confidence-based Pseudo-label Refinement (CPR) and Contrastive Teacher Module (CTM) to encourage noisy samples to converge towards clusters containing their true identities. Extensive experiments on public gait datasets have demonstrated that our simple and effective method significantly enhances the performance of unsupervised gait recognition, laying the foundation for its application in the real-world. The code is available athttps://github.com/YanSun-github/GaitDCCR"
147,679d459debd8ffd557a2af00,cs.CV,https://arxiv.org/pdf/2501.16583,Directing Mamba to Complex Textures: An Efficient Texture-Aware State Space Model for Image Restoration,"Long Peng, Xin Di, Zhanfeng Feng, Wenbo Li, Renjing Pei, Yang Wang, Xueyang Fu, Yang Cao, Zheng-Jun Zha",Computer Vision and Pattern Recognition,"Image restoration aims to recover details and enhance contrast in degraded images. With the growing demand for high-quality imaging (e.g., 4K and 8K), achieving a balance between restoration quality and computational efficiency has become increasingly critical. Existing methods, primarily based on CNNs, Transformers, or their hybrid approaches, apply uniform deep representation extraction across the image. However, these methods often struggle to effectively model long-range dependencies and largely overlook the spatial characteristics of image degradation (regions with richer textures tend to suffer more severe damage), making it hard to achieve the best trade-off between restoration quality and efficiency. To address these issues, we propose a novel texture-aware image restoration method, TAMambaIR, which simultaneously perceives image textures and achieves a trade-off between performance and efficiency. Specifically, we introduce a novel Texture-Aware State Space Model, which enhances texture awareness and improves efficiency by modulating the transition matrix of the state-space equation and focusing on regions with complex textures. Additionally, we design a Multi-Directional Perception Block to improve multi-directional receptive fields while maintaining low computational overhead. Extensive experiments on benchmarks for image super-resolution, deraining, and low-light image enhancement demonstrate that TAMambaIR achieves state-of-the-art performance with significantly improved efficiency, establishing it as a robust and efficient framework for image restoration."
148,679d459debd8ffd557a2af01,cs.CV,https://arxiv.org/pdf/2501.16571,Efficient Object Detection of Marine Debris using Pruned YOLO Model,"Abi Aryaza, Novanto Yudistira, Tibyani","Computer Vision and Pattern Recognition, Artificial Intelligence",
149,679d459debd8ffd557a2af02,cs.CV,https://arxiv.org/pdf/2501.16559,LoRA-X: Bridging Foundation Models with Training-Free Cross-Model Adaptation,"Farzad Farhadzadeh, Debasmit Das, Shubhankar Borse, Fatih Porikli",Computer Vision and Pattern Recognition,
150,679d459debd8ffd557a2af03,cs.CV,https://arxiv.org/pdf/2501.16551,PackDiT: Joint Human Motion and Text Generation via Mutual Prompting,"Zhongyu Jiang, Wenhao Chai, Zhuoran Zhou, Cheng-Yen Yang, Hsiang-Wei Huang, Jenq-Neng Hwang","Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning","Human motion generation has advanced markedly with the advent of diffusion models. Most recent studies have concentrated on generating motion sequences based on text prompts, commonly referred to as text-to-motion generation. However, the bidirectional generation of motion and text, enabling tasks such as motion-to-text alongside text-to-motion, has been largely unexplored. This capability is essential for aligning diverse modalities and supports unconditional generation. In this paper, we introducePackDiT, the first diffusion-based generative model capable of performing various tasks simultaneously, including motion generation, motion prediction, text generation, text-to-motion, motion-to-text, and joint motion-text generation. Our core innovation leverages mutual blocks to integrate multiple diffusion transformers (DiTs) across different modalities seamlessly.
We train PackDiT on the HumanML3D dataset, achieving state-of-the-art text-to-motion performance with an FID score of0.1060.1060.1060.106, along with superior results in motion prediction and in-between tasks. Our experiments further demonstrate that diffusion models are effective for motion-to-text generation, achieving performance comparable to that of autoregressive models."
151,679d459debd8ffd557a2af04,cs.CV,https://arxiv.org/pdf/2501.16525,Multi-Objective Deep-Learning-based Biomechanical Deformable Image Registration with MOREA,"Georgios Andreadis, Eduard Ruiz Munné, Thomas H. W. Bäck, Peter A. N. Bosman, Tanja Alderliesten","Computer Vision and Pattern Recognition, Artificial Intelligence, Neural and Evolutionary Computing","When choosing a deformable image registration (DIR) approach for images with large deformations and content mismatch, the realism of found transformations often needs to be traded off against the required runtime.
DIR approaches using deep learning (DL) techniques have shown remarkable promise in instantly predicting a transformation.
However, on difficult registration problems, the realism of these transformations can fall short.
DIR approaches using biomechanical, finite element modeling (FEM) techniques can find more realistic transformations, but tend to require much longer runtimes.
This work proposes the first hybrid approach to combine them, with the aim of getting the best of both worlds.
This hybrid approach, called DL-MOREA, combines a recently introduced multi-objective DL-based DIR approach which leverages the VoxelMorph framework, called DL-MODIR, with MOREA, an evolutionary algorithm-based, multi-objective DIR approach in which a FEM-like biomechanical mesh transformation model is used.
In our proposed hybrid approach, the DL results are used to smartly initialize MOREA, with the aim of more efficiently optimizing its mesh transformation model.
We empirically compare DL-MOREA against its components, DL-MODIR and MOREA, on CT scan pairs capturing large bladder filling differences of 15 cervical cancer patients.
While MOREA requires a median runtime of 45 minutes, DL-MOREA can already find high-quality transformations after 5 minutes.
Compared to the DL-MODIR transformations, the transformations found by DL-MOREA exhibit far less folding and improve or preserve the bladder contour distance error."
152,679d459debd8ffd557a2af05,cs.CV,https://arxiv.org/pdf/2501.16481,Generating customized prompts for Zero-Shot Rare Event Medical Image Classification using LLM,"Payal Kamboj, Ayan Banerjee, Bin Xu, Sandeep Gupta",Computer Vision and Pattern Recognition,"Rare events, due to their infrequent occurrences, do not have much data, and hence deep learning techniques fail in estimating the distribution for such data. Open-vocabulary models represent an innovative approach to image classification. Unlike traditional models, these models classify images into any set of categories specified with natural language prompts during inference. These prompts usually comprise manually crafted templates (e.g., ‘a photo of a {}’) that are filled in with the names of each category. This paper introduces a simple yet effective method for generating highly accurate and contextually descriptive prompts containing discriminative characteristics. Rare event detection, especially in medicine, is more challenging due to low inter-class and high intra-class variability. To address these, we propose a novel approach that uses domain-specific expert knowledge on rare events to generate customized and contextually relevant prompts, which are then used by large language models for image classification. Our zero-shot, privacy-preserving method enhances rare event classification without additional training, outperforming state-of-the-art techniques. Code available athttps://github.com/payalkamboj/CuPKL."
153,679d459debd8ffd557a2af06,cs.CV,https://arxiv.org/pdf/2501.16469,Object Detection for Medical Image Analysis: Insights from the RT-DETR Model,"Weijie He, Yuwei Zhang, Ting Xu, Tai An, Yingbin Liang, Bo Zhang","Computer Vision and Pattern Recognition, Machine Learning",
154,679d459debd8ffd557a2af07,cs.CV,https://arxiv.org/pdf/2501.16467,Cross-Domain Semantic Segmentation with Large Language Model-Assisted Descriptor Generation,"Philip Hughes, Larry Burns, Luke Adams",Computer Vision and Pattern Recognition,"Semantic segmentation plays a crucial role in enabling machines to understand and interpret visual scenes at a pixel level. While traditional segmentation methods have achieved remarkable success, their generalization to diverse scenes and unseen object categories remains limited. Recent advancements in large language models (LLMs) offer a promising avenue for bridging visual and textual modalities, providing a deeper understanding of semantic relationships. In this paper, we propose LangSeg, a novel LLM-guided semantic segmentation method that leverages context-sensitive, fine-grained subclass descriptors generated by LLMs. Our framework integrates these descriptors with a pre-trained Vision Transformer (ViT) to achieve superior segmentation performance without extensive model retraining.
We evaluate LangSeg on two challenging datasets, ADE20K and COCO-Stuff, where it outperforms state-of-the-art models, achieving up to a 6.1% improvement in mean Intersection over Union (mIoU). Additionally, we conduct a comprehensive ablation study and human evaluation to validate the effectiveness of our method in real-world scenarios. The results demonstrate that LangSeg not only excels in semantic understanding and contextual alignment but also provides a flexible and efficient framework for language-guided segmentation tasks. This approach opens up new possibilities for interactive and domain-specific segmentation applications."
155,679d459debd8ffd557a2af08,cs.CV,https://arxiv.org/pdf/2501.16411,PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding,"Wei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Guizilini, Yue Wang","Computer Vision and Pattern Recognition, Artificial Intelligence, Computation and Language, Machine Learning, Robotics",
156,679d459debd8ffd557a2af09,cs.CV,https://arxiv.org/pdf/2501.16410,DynAlign: Unsupervised Dynamic Taxonomy Alignment for Cross-Domain Segmentation,"Han Sun, Rui Gong, Ismail Nejjar, Olga Fink",Computer Vision and Pattern Recognition,"Current unsupervised domain adaptation (UDA) methods for semantic segmentation typically assume identical class labels between the source and target domains. This assumption ignores the label-level domain gap, which is common in real-world scenarios, thus limiting their ability to identify finer-grained or novel categories without requiring extensive manual annotation.
A promising direction to address this limitation lies in recent advancements in foundation models, which exhibit strong generalization abilities due to their rich prior knowledge. However, these models often struggle with domain-specific nuances and underrepresented fine-grained categories.
To address these challenges, we introduceDynAlign, a framework that integrates UDA with foundation models to bridge both the image-level and label-level domain gaps. Our approach leverages prior semantic knowledge to align source categories with target categories that can be novel, more fine-grained, or named differently (e.g.,‘vehicle’to {‘car’,‘truck’,‘bus’}). Foundation models are then employed for precise segmentation and category reassignment. To further enhance accuracy, we propose a knowledge fusion approach that dynamically adapts to varying scene contexts.DynAligngenerates accurate predictions in a new target label space without requiring any manual annotations, allowing seamless adaptation to new taxonomies through either model retraining or direct inference.
Experiments on the street scene semantic segmentation benchmarks GTA→→\rightarrow→Mapillary Vistas and GTA→→\rightarrow→IDD validate the effectiveness of our approach, achieving a significant improvement over existing methods.
Our code will be publically available."
157,679d459debd8ffd557a2af0a,cs.CV,https://arxiv.org/pdf/2501.17161,"SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training","Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, Yi Ma","Artificial Intelligence, Computer Vision and Pattern Recognition, Machine Learning","Supervised fine-tuning (SFT) and reinforcement learning (RL) are widely used post-training techniques for foundation models.
However, their respective role in enhancing model generalization remains unclear.
This paper studies the comparative effect of SFT and RL on generalization and memorization, focusing on text-based and visual environments.
We introduceGeneralPoints, an arithmetic reasoning card game, and also considerV-IRL, a real-world navigation environment, to assess how models trained with SFT and RL generalize to unseen variants in both textual and visual domains.
We show that RL, especially when trained with an outcome-based reward, generalizes in both the rule-based textual and visual environments.
SFT, in contrast, tends to memorize the training data and struggles to generalize out-of-distribution in either scenario.
Further analysis reveals that RL improves the model’s underlying visual recognition capabilities, contributing to its enhanced generalization in visual domains.
Despite RL’s superior generalization, we show that SFT is still helpful for effective RL training: SFT stabilizes the model’s output format, enabling subsequent RL to achieve its performance gains.
These findings demonstrate the advantage of RL for acquiring generalizable knowledge in complex, multi-modal tasks."
158,679d459debd8ffd557a2af0b,cs.CV,https://arxiv.org/pdf/2501.17160,A Hybrid Deep Learning CNN Model for Enhanced COVID-19 Detection from Computed Tomography (CT) Scan Images,"Suresh Babu Nettur, Shanthi Karpurapu, Unnati Nettur, Likhit Sagar Gajja, Sravanthy Myneni, Akhil Dusi, Lalithya Posham","Image and Video Processing, Artificial Intelligence, Computer Vision and Pattern Recognition",
159,679d459debd8ffd557a2af0c,cs.CV,https://arxiv.org/pdf/2501.17099,Text-to-Image Generation for Vocabulary Learning Using the Keyword Method,"Nuwan T. Attygalle, Matjaž Kljun, Aaron Quigley, Klen čOpič Pucihar, Jens Grubert, Verena Biener, Luis A. Leiva, Juri Yoneyama, Alice Toniolo, Angela Miguel, Hirokazu Kato, Maheshya Weerasinghe","Human-Computer Interaction, Computer Vision and Pattern Recognition, Graphics, Machine Learning",
160,679d459debd8ffd557a2af0d,cs.CV,https://arxiv.org/pdf/2501.17062,EdgeMLOps: Operationalizing ML models with Cumulocity IoT and thin-edge.io for Visual quality Inspection,"Kanishk Chaturvedi, Johannes Gasthuber, Mohamed Abdelaal","Machine Learning, Artificial Intelligence, Computer Vision and Pattern Recognition",
161,679d459debd8ffd557a2af0e,cs.CV,https://arxiv.org/pdf/2501.16879,Ultra-high resolution multimodal MRI dense labelled holistic brain atlas,"José V. Manjón, Sergio Morell-Ortega, Marina Ruiz-Perez, Boris Mansencal, Edern Le Bot, Marien Gadea, Enrique Lanuza, Gwenaelle Catheline, Thomas Tourdias, Vincent Planche, Rémi Giraud, Denis Rivière, Jean-François Mangin, Nicole Labra-Avila, Roberto Vivo-Hernando, Gregorio Rubio, Fernando Aparici, Maria de la Iglesia-Vaya, Pierrick Coupé","Image and Video Processing, Computer Vision and Pattern Recognition","In this paper, we introduceholiAtlas, a holistic, multimodal and high-resolution human
brain atlas. This atlas covers different levels of details of the human brain anatomy, from
the organ to the substructure level, using a new dense labelled protocol generated from
the fusion of multiple local protocols at different scales. This atlas has been constructed
averaging images and segmentations of 75 healthy subjects from the Human
Connectome Project database. Specifically, MR images of T1, T2 and WMn (White
Matter nulled) contrasts at 0.125 mm3resolution that were nonlinearly registered
and averaged using symmetric group-wise
normalisation to construct the atlas. At the finest level, theholiAtlasprotocol has 350
different labels derived from 10 different delineation protocols. These labels were
grouped at different scales to provide a holistic view of the brain at different levels in a
coherent and consistent manner. This multiscale and multimodal atlas can be used for
the development of new ultra-high resolution segmentation methods that can potentially
leverage the early detection of neurological disorders."
162,679d459debd8ffd557a2af0f,cs.CV,https://arxiv.org/pdf/2501.16803,RG-Attn: Radian Glue Attention for Multi-modality Multi-agent Cooperative Perception,"Lantao Li, Kang Yang, Wenqi Zhang, Xiaoxue Wang, Chen Sun","Robotics, Computer Vision and Pattern Recognition, Networking and Internet Architecture, Image and Video Processing","Cooperative perception offers an optimal solution to overcome the perception limitations of single-agent systems by leveraging Vehicle-to-Everything (V2X) communication for data sharing and fusion across multiple agents. However, most existing approaches focus on single-modality data exchange, limiting the potential of both homogeneous and heterogeneous fusion across agents. This overlooks the opportunity to utilize multi-modality data per agent, restricting the system’s performance. In the automotive industry, manufacturers adopt diverse sensor configurations, resulting in heterogeneous combinations of sensor modalities across agents. To harness the potential of every possible data source for optimal performance, we design a robust LiDAR and camera cross-modality fusion module, Radian-Glue-Attention (RG-Attn), applicable to both intra-agent cross-modality fusion and inter-agent cross-modality fusion scenarios, owing to the convenient coordinate conversion by transformation matrix and the unified sampling/inversion mechanism. We also propose two different architectures, named Paint-To-Puzzle (PTP) and Co-Sketching-Co-Coloring (CoS-CoCo), for conducting cooperative perception. PTP aims for maximum precision performance and achieves smaller data packet size by limiting cross-agent fusion to a single instance, but requiring all participants to be equipped with LiDAR. In contrast, CoS-CoCo supports agents with any configuration—LiDAR-only, camera-only, or LiDAR-camera-both, presenting more generalization ability. Our approach achieves state-of-the-art (SOTA) performance on both real and simulated cooperative perception datasets. The code will be released at GitHub in early 2025."
163,679d459debd8ffd557a2af10,cs.CV,https://arxiv.org/pdf/2501.16740,Efficient Knowledge Distillation of SAM for Medical Image Segmentation,"Kunal Dasharath Patil, Gowthamaan Palani, Ganapathy Krishnamurthi","Image and Video Processing, Artificial Intelligence, Computer Vision and Pattern Recognition","The Segment Anything Model (SAM) has set a new standard in interactive image segmentation, offering robust performance across various tasks. However, its significant computational requirements limit its deployment in real-time or resource-constrained environments. To address these challenges, we propose a novel knowledge distillation approach, KD SAM, which incorporates both encoder and decoder optimization through a combination of Mean Squared Error (MSE) and Perceptual Loss. This dual-loss framework captures structural and semantic features, enabling the student model to maintain high segmentation accuracy while reducing computational complexity. Based on the model evaluation on datasets, including Kvasir-SEG, ISIC 2017, Fetal Head Ultrasound, and Breast Ultrasound, we demonstrate that KD SAM achieves comparable or superior performance to the baseline models, with significantly fewer parameters. KD SAM effectively balances segmentation accuracy and computational efficiency, making it well-suited for real-time medical image segmentation applications in resource-constrained environments."
164,679d459debd8ffd557a2af11,cs.CV,https://arxiv.org/pdf/2501.16733,Dream to Drive with Predictive Individual World Model,"Yinfeng Gao, Qichao Zhang, Da-wei Ding, Dongbin Zhao","Robotics, Computer Vision and Pattern Recognition, Machine Learning","It is still a challenging topic to make reactive driving behaviors in complex urban environments as road users’ intentions are unknown.
Model-based reinforcement learning (MBRL) offers great potential to learn a reactive policy by constructing a world model that can provide informative states and imagination training.
However, a critical limitation in relevant research lies in the scene-level reconstruction representation learning, which may overlook key interactive vehicles and hardly model the interactive features among vehicles and their long-term intentions.
Therefore, this paper presents a novel MBRL method with a predictive individual world model (PIWM) for autonomous driving. PIWM describes the driving environment from an individual-level perspective and captures vehicles’ interactive relations and their intentions via trajectory prediction task.
Meanwhile, a behavior policy is learned jointly with PIWM. It is trained in PIWM’s imagination and effectively navigates in the urban driving scenes leveraging intention-aware latent states.
The proposed method is trained and evaluated on simulation environments built upon real-world challenging interactive scenarios. Compared with popular model-free and state-of-the-art model-based reinforcement learning methods, experimental results show that the proposed method achieves the best performance in terms of safety and efficiency."
165,679d459debd8ffd557a2af12,cs.CV,https://arxiv.org/pdf/2501.16698,3D-MoE: A Mixture-of-Experts Multi-modal LLM for 3D Vision and Pose Diffusion via Rectified Flow,"Yueen Ma, Yuzheng Zhuang, Jianye Hao, Irwin King","Computation and Language, Computer Vision and Pattern Recognition, Robotics","3D vision and spatial reasoning have long been recognized as preferable for accurately perceiving our three-dimensional world, especially when compared with traditional visual reasoning based on 2D images. Due to the difficulties in collecting high-quality 3D data, research in this area has only recently gained momentum. With the advent of powerful large language models (LLMs), multi-modal LLMs for 3D vision have been developed over the past few years. However, most of these models focus primarily on the vision encoder for 3D data. In this paper, we propose converting existing densely activated LLMs into mixture-of-experts (MoE) models, which have proven effective for multi-modal data processing. In addition to leveraging these models’ instruction-following capabilities, we further enable embodied task planning by attaching a diffusion head, Pose-DiT, that employs a novel rectified flow diffusion scheduler. Experimental results on 3D question answering and task-planning tasks demonstrate that our 3D-MoE framework achieves improved performance with fewer activated parameters."
166,679d459debd8ffd557a2af13,cs.CV,https://arxiv.org/pdf/2501.16664,Improving Vision-Language-Action Model with Online Reinforcement Learning,"Yanjiang Guo, Jianke Zhang, Xiaoyu Chen, Xiang Ji, Yen-Jen Wang, Yucheng Hu, Jianyu Chen","Robotics, Computer Vision and Pattern Recognition, Machine Learning","Recent studies have successfully integrated large vision-language models (VLMs) into low-level robotic control by supervised fine-tuning (SFT) with expert robotic datasets, resulting in what we term vision-language-action (VLA) models. Although the VLA models are powerful, how to improve these large models during interaction with environments remains an open question.
In this paper, we explore how to further improve these VLA models via Reinforcement Learning (RL), a commonly used fine-tuning technique for large models.
However, we find that directly applying online RL to large VLA models presents significant challenges, including training instability that severely impacts the performance of large models, and computing burdens that exceed the capabilities of most local machines. To address these challenges, we propose iRe-VLA framework, which iterates between Reinforcement Learning and Supervised Learning to effectively improve VLA models, leveraging the exploratory benefits of RL while maintaining the stability of supervised learning. Experiments in two simulated benchmarks and a real-world manipulation suite validate the effectiveness of our method."
167,679d459debd8ffd557a2af14,cs.CV,https://arxiv.org/pdf/2501.16629,CHiP: Cross-modal Hierarchical Direct Preference Optimization for Multimodal LLMs,"Jinlan Fu, Shenzhen Huangfu, Hao Fei, Xiaoyu Shen, Bryan Hooi, Xipeng Qiu, See-Kiong Ng","Computation and Language, Computer Vision and Pattern Recognition","Multimodal Large Language Models (MLLMs) still struggle with hallucinations despite their impressive capabilities. Recent studies have attempted to mitigate this by applying Direct Preference Optimization (DPO) to multimodal scenarios using preference pairs from text-based responses.
However, our analysis of representation distributions reveals that multimodal DPO struggles to align image and text representations and to distinguish between hallucinated and non-hallucinated descriptions. To address these challenges,
In this work, we propose aCross-modalHierarchical DirectPreference Optimization (CHiP) to address these limitations.
We introduce a visual preference optimization module within the DPO framework, enabling MLLMs to learn from both textual and visual preferences simultaneously. Furthermore, we propose a hierarchical textual preference optimization module that allows the model to capture preferences at multiple granular levels, including response, segment, and token levels.
We evaluate CHiP through both quantitative and qualitative analyses, with results across multiple benchmarks demonstrating its effectiveness in reducing hallucinations. On the Object HalBench dataset, CHiP outperforms DPO in hallucination reduction, achieving improvements of 52.7% and 55.5% relative points based on the base model Muffin and LLaVA models, respectively. We make all our datasets and code publicly available.111https://github.com/LVUGAI/CHiP"
168,679d459debd8ffd557a2af15,cs.CV,https://arxiv.org/pdf/2501.16550,PhysAnimator: Physics-Guided Generative Cartoon Animation,"Tianyi Xie, Yiwei Zhao, Ying Jiang, Chenfanfu Jiang","Graphics, Computer Vision and Pattern Recognition","Creating hand-drawn animation sequences is labor-intensive and demands professional expertise. We introduce PhysAnimator, a novel approach for generating physically plausible meanwhile anime-stylized animation from static anime illustrations. Our method seamlessly integrates physics-based simulations with data-driven generative models to produce dynamic and visually compelling animations. To capture the fluidity and exaggeration characteristic of anime, we perform image-space deformable body simulations on extracted mesh geometries. We enhance artistic control by introducing customizable energy strokes and incorporating rigging point support, enabling the creation of tailored animation effects such as wind interactions. Finally, we extract and warp sketches from the simulation sequence, generating a texture-agnostic representation, and employ a sketch-guided video diffusion model to synthesize high-quality animation frames. The resulting animations exhibit temporal consistency and visual plausibility, demonstrating the effectiveness of our method in creating dynamic anime-style animations."
169,679d459debd8ffd557a2af16,cs.CV,https://arxiv.org/pdf/2501.16458,BiFold: Bimanual Cloth Folding with Language Guidance,"Oriol Barbany, Adrià Colomé, Carme Torras","Robotics, Computer Vision and Pattern Recognition","Cloth folding is a complex task due to the inevitable self-occlusions of clothes, their complicated dynamics, and the disparate materials, geometries, and textures that garments can have.
In this work, we learn folding actions conditioned on text commands. Translating high-level, abstract instructions into precise robotic actions requires sophisticated language understanding and manipulation capabilities.
To do that, we leverage a pre-trained vision-language model and repurpose it to predict manipulation actions. Our model, BiFold, can take context into account and achieves state-of-the-art performance on an existing language-conditioned folding benchmark.
Given the lack of annotated bimanual folding data, we devise a procedure to automatically parse actions of a simulated dataset and tag them with aligned text instructions. BiFold attains the best performance on our dataset and can transfer to new instructions, garments, and environments."
170,679d459debd8ffd557a2af17,cs.CV,https://arxiv.org/pdf/2501.16389,Bridging the Sim2Real Gap: Vision Encoder Pre-Training for Visuomotor Policy Transfer,"Samuel Biruduganti, Yash Yardi, Lars Ankile","Robotics, Computer Vision and Pattern Recognition","Simulation offers a scalable and efficient alternative to real-world data collection for learning visuomotor robotic policies. However, the simulation-to-reality, or “Sim2Real” distribution shift—introduced by employing simulation-trained policies in real-world environments—frequently prevents successful policy transfer. This study explores the potential of using large-scale pre-training of vision encoders to address the Sim2Real gap. We examine a diverse collection of encoders, evaluating their ability to (1) extract features necessary for robot control while (2) remaining invariant to task-irrelevant environmental variations. We quantitatively measure the encoder’s feature extraction capabilities through linear probing and its domain invariance by computing distances between simulation and real-world embedding centroids. Additional qualitative insights are provided through t-SNE plots and GradCAM saliency maps. Findings suggest that encoders pre-trained on manipulation-specific datasets generally outperform those trained on generic datasets in bridging the Sim2Real gap.https://github.com/yyardi/Bridging-the-Sim2Real-Gap"
171,679d459debd8ffd557a2af18,cs.CV,https://arxiv.org/pdf/2501.16378,Internal Activation Revision: Safeguarding Vision Language Models Without Parameter Update,"Qing Li, Jiahui Geng, Zongxiong Chen, Kun Song, Lei Ma, Fakhri Karray","Machine Learning, Artificial Intelligence, Computation and Language, Computer Vision and Pattern Recognition","Warning: This paper contains offensive content that may disturb some readers.Vision-language models (VLMs) demonstrate strong multimodal capabilities but have been found to be more susceptible to generating harmful content compared to their backbone large language models (LLMs). Our investigation reveals that the integration of images significantly shifts the model’s internal activations during the forward pass, diverging from those triggered by textual input. Moreover, the safety alignments of LLMs embedded within VLMs are not sufficiently robust to handle the activations discrepancies, making the models vulnerable to even the simplest jailbreaking attacks. To address this issue, we propose aninternal activation revisionapproach that efficiently revises activations during generation, steering the model toward safer outputs. Our framework incorporates revisions at both the layer and head levels, offering control over the model’s generation at varying levels of granularity. In addition, we explore three strategies for constructing positive and negative samples and two approaches for extracting revision vectors, resulting in different variants of our method. Comprehensive experiments demonstrate that the internal activation revision method significantly improves the safety of widely used VLMs, reducing attack success rates by an average of 48.94%, 34.34%, 43.92%, and 52.98% on SafeBench, Safe-Unsafe, Unsafe, and MM-SafetyBench, respectively, while minimally impacting model helpfulness."
172,679d459debd8ffd557a2af19,cs.CV,https://arxiv.org/pdf/2501.16330,RelightVid: Temporal-Consistent Diffusion Model for Video Relighting,"Ye Fang, Zeyi Sun, Shangzhan Zhang, Tong Wu, Yinghao Xu, Pan Zhang, Jiaqi Wang, Gordon Wetzstein, Dahua Lin","Computer Vision and Pattern Recognition, Artificial Intelligence","Diffusion models have demonstrated remarkable success in image generation and editing, with recent advancements enabling albedo-preserving image relighting. However, applying these models to video relighting remains challenging due to the lack of paired video relighting datasets and the high demands for output fidelity and temporal consistency, further complicated by the inherent randomness of diffusion models. To address these challenges, we introduceRelightVid, a flexible framework for video relighting that can accept background video, text prompts, or environment maps as relighting conditions. Trained on in-the-wild videos with carefully designed illumination augmentations and rendered videos under extreme dynamic lighting,RelightVidachieves arbitrary video relighting with high temporal consistency without intrinsic decomposition while preserving the illumination priors of its image backbone."
173,679d459debd8ffd557a2af1a,cs.CV,https://arxiv.org/pdf/2501.16319,Adaptive Iterative Compression for High-Resolution Files: an Approach Focused on Preserving Visual Quality in Cinematic Workflows,"Leonardo Melo, Filipe Litaiff","Computer Vision and Pattern Recognition, Emerging Technologies, Machine Learning, Performance","This study presents an iterative adaptive compression model for high-resolution DPX-derived TIFF files used in cinematographic workflows and digital preservation. The model employs SSIM and PSNR metrics to dynamically adjust compression parameters across three configurations (C0, C1, C2), achieving storage reductions up to 83.4% while maintaining high visual fidelity (SSIM > 0.95)."
174,679d459debd8ffd557a2af1b,cs.CV,https://arxiv.org/pdf/2501.16312,LinPrim: Linear Primitives for Differentiable Volumetric Rendering,"Nicolas von Lützow, Matthias Nießner",Computer Vision and Pattern Recognition,"Volumetric rendering has become central to modern novel view synthesis methods, which use differentiable rendering to optimize 3D scene representations directly from observed views.
While many recent works build on NeRF(Mildenhall et al.,2022)or 3D Gaussians(Kerbl et al.,2023), we explore an alternative volumetric scene representation.
More specifically, we introduce two new scene representations based onlinearprimitives—octahedra and tetrahedra—both of which define homogeneous volumes bounded by triangular faces.
This formulation aligns naturally with standard mesh-based tools, minimizing overhead for downstream applications.
To optimize these primitives, we present a differentiable rasterizer that runs efficiently on GPUs, allowing end-to-end gradient-based optimization while maintaining real-time rendering capabilities.
Through experiments on real-world datasets, we demonstrate comparable performance to state-of-the-art volumetric methods while requiring fewer primitives to achieve similar reconstruction fidelity.
Our findings provide insights into the geometry of volumetric rendering and suggest that adopting explicit polyhedra can expand the design space of scene representations."
175,679d459debd8ffd557a2af1c,cs.CV,https://arxiv.org/pdf/2501.16300,Large Models in Dialogue for Active Perception and Anomaly Detection,"Tzoulio Chamiti, Nikolaos Passalis, Anastasios Tefas","Computer Vision and Pattern Recognition, Artificial Intelligence","Autonomous aerial monitoring is an important task aimed at gathering information from areas that may not be easily accessible by humans. At the same time, this task often requires recognizing anomalies from a significant distance and/or not previously encountered in the past. In this paper, we propose a novel framework that leverages the advanced capabilities provided by Large Language Models (LLMs) to actively collect information and perform anomaly detection in novel scenes. To this end, we propose an LLM-based model dialogue approach, in which two deep learning models engage in a dialogue to actively control a drone to increase perception and anomaly detection accuracy.
We conduct our experiments in a high fidelity simulation environment where an LLM is provided with a predetermined set of natural language movement commands mapped into executable code functions. Additionally, we deploy a multimodal Visual Question Answering (VQA) model charged with the task of visual question answering and captioning. By engaging the two models in conversation, the LLM asks exploratory questions while simultaneously flying a drone into different parts of the scene, providing a novel way to implement active perception. By leveraging LLM’s reasoning ability, we output an improved detailed description of the scene going beyond existing static perception approaches. In addition to information gathering, our approach is utilized for anomaly detection and our results demonstrate the proposed method’s effectiveness in informing and alerting about potential hazards."
176,679d459debd8ffd557a2af1d,cs.CV,https://arxiv.org/pdf/2501.16297,FALCON: Resolving Visual Redundancy and Fragmentation in High-resolution Multimodal Large Language Models via Visual Registers,"Renshan Zhang, Rui Shao, Gongwei Chen, Kaiwen Zhou, Weili Guan, Liqiang Nie",Computer Vision and Pattern Recognition,"The incorporation of high-resolution visual input equips multimodal large language models (MLLMs) with enhanced visual perception capabilities for real-world tasks. However, most existing high-resolution MLLMs rely on a cropping-based approach to process images, which leads to fragmented visual encoding and a sharp increase in redundant tokens. To tackle these issues, we propose theFALCONmodel. FALCON introduces a novelvisual registertechnique to simultaneously:1)Eliminate redundant tokens at the stage of visual encoding.To directly address the visual redundancy present in the output of vision encoder, we propose a Register-based Representation Compacting (ReCompact) mechanism. This mechanism introduces a set of learnable visual registers designed to adaptively aggregate essential information while discarding redundancy. It enables the encoder to produce a more compact visual representation with a minimal number of output tokens, thus eliminating the need for an additional compression module.2)Ensure continuity in visual encoding.To address the potential encoding errors caused by fragmented visual inputs, we develop a Register Interactive Attention (ReAtten) module. This module facilitates effective and efficient information exchange across sub-images by enabling interactions between visual registers. It ensures the continuity of visual semantics throughout the encoding.
We conduct comprehensive experiments with FALCON on high-resolution benchmarks across a wide range of scenarios. FALCON demonstrates superior performance with a remarkable9-foldand16-foldreduction in visual tokens."
177,679d459debd8ffd557a2af1e,cs.CV,https://arxiv.org/pdf/2501.16289,Multi-view Structural Convolution Network for Domain-Invariant Point Cloud Recognition of Autonomous Vehicles,"Younggun Kim, Beomsik Cho, Seonghoon Ryoo, Soomok Lee",Computer Vision and Pattern Recognition,"Point cloud representation has recently become a research hotspot in the field of computer vision and has been utilized for autonomous vehicles. However, adapting deep learning networks for point cloud data recognition is challenging due to the variability in datasets and sensor technologies. This variability underscores the necessity for adaptive techniques to maintain accuracy under different conditions. In this paper, we present the Multi-View Structural Convolution Network (MSCN) designed for domain-invariant point cloud recognition. MSCN comprises Structural Convolution Layers (SCL) that extract local context geometric features from point clouds and Structural Aggregation Layers (SAL) that extract and aggregate both local and overall context features from point clouds. Additionally, our MSCN enhances feature representation robustness by training with unseen domain point clouds derived from source domain point clouds. This method acquires domain-invariant features and exhibits robust, consistent performance across various point cloud datasets, ensuring compatibility with diverse sensor configurations without the need for parameter adjustments. This highlights MSCN’s potential to significantly improve the reliability and domain invariant features in different environments. Our code is available athttps://github.com/MLMLab/MSCN."
178,679d459debd8ffd557a2af1f,cs.CV,https://arxiv.org/pdf/2501.16246,CLISC: Bridging clip and sam by enhanced cam for unsupervised brain tumor segmentation,"Xiaochuan Ma, Jia Fu, Wenjun Liao, Shichuan Zhang, Guotai Wang",Computer Vision and Pattern Recognition,"Brain tumor segmentation is important for diagnosis of the tumor, and current deep-learning methods rely on a large set of annotated images for training, with high annotation costs. Unsupervised segmentation is promising to avoid human annotations while the performance is often limited. In this study, we present a novel unsupervised segmentation approach that leverages the capabilities of foundation models, and it consists of three main steps:(1)A vision-language model (i.e., CLIP) is employed to obtain image-level pseudo-labels for training a classification network. Class Activation Mapping (CAM) is then employed to extract Regions of Interest (ROIs), where an adaptive masking-based data augmentation is used to enhance ROI identification.(2)The ROIs are used to generate bounding box and point prompts for the Segment Anything Model (SAM) to obtain segmentation pseudo-labels.(3)A 3D segmentation network is trained with the SAM-derived pseudo-labels, where low-quality pseudo-labels are filtered out in a self-learning process based on the similarity between the SAM’s output and the network’s prediction. Evaluation on the BraTS2020 dataset demonstrates that our approach obtained an average Dice Similarity Score (DSC) of 85.60%, outperforming five state-of-the-art unsupervised segmentation methods by more than 10 percentage points. Besides, our approach outperforms directly using SAM for zero-shot inference, and its performance is close to fully supervised learning."
179,679d459debd8ffd557a2af20,cs.CV,https://arxiv.org/pdf/2501.16239,Distilling foundation models for robust and efficient models in digital pathology,"Alexandre Filiot, Nicolas Dop, Oussama Tchita, Auriane Riou, Rémy Dubois, Thomas Peeters, Daria Valter, Marin Scalbert, Charlie Saillard, Geneviève Robin, Antoine Olivier",Computer Vision and Pattern Recognition,"In recent years, the advent of foundation models (FM) for digital pathology has relied heavily on scaling the pre-training datasets and the model size, yielding large and powerful models. While it resulted in improving the performance on diverse downstream tasks, it also introduced increased computational cost and inference time. In this work, we explore the distillation of a large foundation model into a smaller one, reducing the number of parameters by several orders of magnitude. Leveraging distillation techniques, our distilled model, H0-mini, achieves nearly comparable performance to large FMs at a significantly reduced inference cost. It is evaluated on several public benchmarks, achieving 3rd place on the HEST benchmark and 5th place on the EVA benchmark. Additionally, a robustness analysis conducted on the PLISM dataset demonstrates that our distilled model reaches excellent robustness to variations in staining and scanning conditions, significantly outperforming other state-of-the art models. This opens new perspectives to design lightweight and robust models for digital pathology, without compromising on performance."
180,679d459debd8ffd557a2af21,cs.CV,https://arxiv.org/pdf/2501.16227,PDC-ViT : Source Camera Identification using Pixel Difference Convolution and Vision Transformer,"Omar Elharrouss, Younes Akbari, Noor Almaadeed, Somaya Al-Maadeed, Fouad Khelifi, Ahmed Bouridane",Computer Vision and Pattern Recognition,
181,679d459debd8ffd557a2af22,cs.CV,https://arxiv.org/pdf/2501.16222,SPECIAL: Zero-shot Hyperspectral Image Classification With CLIP,"Li Pang, Jing Yao, Kaiyu Li, Xiangyong Cao",Computer Vision and Pattern Recognition,"Hyperspectral image (HSI) classification aims at categorizing each pixel in an HSI into a specific land cover class, which is crucial for applications like remote sensing, environmental monitoring, and agriculture. Although deep learning-based HSI classification methods have achieved significant advancements, existing methods still rely on manually labeled data for training, which is both time-consuming and labor-intensive.
To address this limitation, we introduce a novel zero-Shot hyperspectralimage classification framework based on CLIP (SPECIAL), aiming to eliminate the need for manual annotations. TheSPECIALframework consists of two main stages: (1) CLIP-based pseudo-label generation, and (2) noisy label learning. In the first stage, HSI is spectrally interpolated to produce RGB bands. These bands are subsequently classified using CLIP, resulting in noisy pseudo-labels that are accompanied by confidence scores.
To improve the quality of these labels, we propose a scaling strategy that fuses predictions from multiple spatial scales.
In the second stage, spectral information and a label refinement technique are incorporated to mitigate label noise and further enhance classification accuracy.
Experimental results on three benchmark datasets demonstrate that ourSPECIALoutperforms existing methods in zero-shot HSI classification, showing its potential for more practical applications. The code is available athttps://github.com/LiPang/SPECIAL."
182,679d459debd8ffd557a2af23,cs.CV,https://arxiv.org/pdf/2501.16221,Automatic Calibration of a Multi-Camera System with Limited Overlapping Fields of View for 3D Surgical Scene Reconstruction,"Tim Flückiger, Jonas Hein, Valery Fischer, Philipp Fürnstahl, Lilian Calvet",Computer Vision and Pattern Recognition,"Purpose:The purpose of this study is to develop an automated and accurate external camera calibration method for multi-camera systems used in 3D surgical scene reconstruction (3D-SSR), eliminating the need for operator intervention or specialized expertise. The method specifically addresses the problem of limited overlapping fields of view caused by significant variations in optical zoom levels and camera locations."
183,679d459debd8ffd557a2af24,cs.CV,https://arxiv.org/pdf/2501.16211,UDBE: Unsupervised Diffusion-based Brightness Enhancement in Underwater Images,"Tatiana Taís Schein, Gustavo Pereira de Almeira, Stephanie Loi Brião, Rodrigo Andrade de Bem, Felipe Gomes de Oliveira, Paulo L. J. Drews-Jr","Computer Vision and Pattern Recognition, Artificial Intelligence, Image and Video Processing","Activities in underwater environments are paramount in several scenarios, which drives the continuous development of underwater image enhancement techniques. A major challenge in this domain is the depth at which images are captured, with increasing depth resulting in a darker environment. Most existing methods for underwater image enhancement focus on noise removal and color adjustment, with few works dedicated to brightness enhancement. This work introduces a novel unsupervised learning approach to underwater image enhancement using a diffusion model. Our method, called UDBE, is based on conditional diffusion to maintain the brightness details of the unpaired input images. The input image is combined with a color map and a Signal-Noise Relation map (SNR) to ensure stable training and prevent color distortion in the output images.
The results demonstrate that our approach achieves an impressive accuracy rate in the datasets UIEB, SUIM and RUIE, well-established underwater image benchmarks. Additionally, the experiments validate the robustness of our approach, regarding the image quality metrics PSNR, SSIM, UIQM, and UISM, indicating the good performance of the brightness enhancement process. The source code is availablehere."
184,679d459debd8ffd557a2af25,cs.CV,https://arxiv.org/pdf/2501.16182,The Linear Attention Resurrection in Vision Transformer,Chuanyang Zheng,"Computer Vision and Pattern Recognition, Artificial Intelligence","Vision Transformers (ViTs) have recently taken computer vision by storm. However, the softmax attention underlying ViTs comes with a quadratic complexity in time and memory, hindering the application of ViTs to high-resolution images. We revisit the attention design and propose a linear attention method to address the limitation, which doesn’t sacrifice ViT’s core advantage of capturing global representation like existing methods (e.g.local window attention of Swin). We further investigate the key difference between linear attention and softmax attention. Our empirical results suggest that linear attention lacks a fundamental property of concentrating the distribution of the attention matrix. Inspired by this observation, we introduce a local concentration module to enhance linear attention. By incorporating enhanced linear global attention and local window attention, we propose a new ViT architecture, dubbed L2ViT. Notably, L2ViT can effectively capture both global interactions and local representations while enjoying linear computational complexity. Extensive experiments demonstrate the strong performance of L2ViT. On image classification, L2ViT achieves 84.4% Top-1 accuracy on ImageNet-1K without any extra training data or label. By further pre-training on ImageNet-22k, it attains 87.0% when fine-tuned with resolution 3842. For downstream tasks, L2ViT delivers favorable performance as a backbone on object detection as well as semantic segmentation."
185,679d459debd8ffd557a2af26,cs.CV,https://arxiv.org/pdf/2501.16177,BAG: Body-Aligned 3D Wearable Asset Generation,"Zhongjin Luo, Yang Li, Mingrui Zhang, Senbo Wang, Han Yan, Xibin Song, Taizhang Shang, Wei Mao, Hongdong Li, Xiaoguang Han, Pan Ji","Computer Vision and Pattern Recognition, Artificial Intelligence, Graphics","While recent advancements have shown remarkable progress in general 3D shape generation models,
the challenge of leveraging these approaches to automatically generate wearable 3D assets remains unexplored.
To this end, we present BAG, aBody-alignedAssetGeneration method to output 3D wearable asset that can be automatically dressed on given 3D human bodies.
This is achived by controlling the 3D generation process using human body shape and pose information.
Specifically, we first build a general single-image to consistent multiview image diffusion model,
and train it on the large Objaverse dataset to achieve diversity and generalizability.
Then we train a Controlnet to guide the multiview generator to produce body-aligned multiview images.
The control signal utilizes the multiview 2D projections of the target human body, where pixel values represent the XYZ coordinates of the body surface in a canonical space.
The body-conditioned multiview diffusion generates body-aligned multiview images, which are then fed into a native 3D diffusion model to produce the 3D shape of the asset.
Finally, by recovering the similarity transformation using multiview silhouette supervision and addressing asset-body penetration with physics simulators, the 3D asset can be accurately fitted onto the target human body.
Experimental results demonstrate significant advantages over existing methods in terms of image prompt-following capability, shape diversity, and shape quality. Our project page is available athttps://bag-3d.github.io/."
186,679d459debd8ffd557a2af27,cs.CV,https://arxiv.org/pdf/2501.16147,Efficient Portrait Matte Creation With Layer Diffusion and Connectivity Priors,"Zhiyuan Lu, Hao Lu, Hua Huang",Computer Vision and Pattern Recognition,"Learning effective deep portrait matting models requires training data of both high quality and large quantity. Neither quality nor quantity can be easily met for portrait matting, however. Since the most accurate ground-truth portrait mattes are acquired in front of the green screen, it is almost impossible to harvest a large-scale portrait matting dataset in reality. This work shows that one can leverage text prompts and the recent Layer Diffusion model to generate high-quality portrait foregrounds and extract latent portrait mattes. However, the portrait mattes cannot be readily in use due to significant generation artifacts. Inspired by the connectivity priors observed in portrait images, that is, the border of portrait foregrounds always appears connected, a connectivity-aware approach is introduced to refine portrait mattes. Building on this, a large-scale portrait matting dataset is created, termed LD-Portrait-20K, with20,0512005120,05120 , 051portrait foregrounds and high-quality alpha mattes. Extensive experiments demonstrated the value of the LD-Portrait-20K dataset, with models trained on it significantly outperforming those trained on other datasets. In addition, comparisons with the chroma keying algorithm and an ablation study on dataset capacity further confirmed the effectiveness of the proposed matte creation approach. Further, the dataset also contributes to state-of-the-art video portrait matting, implemented by simple video segmentation and a trimap-based image matting model trained on this dataset."
187,679d459debd8ffd557a2af28,cs.CV,https://arxiv.org/pdf/2501.16146,Toward Efficient Generalization in 3D Human Pose Estimation via a Canonical Domain Approach,"Hoosang Lee, Jeha Ryu","Computer Vision and Pattern Recognition, Artificial Intelligence","Recent advancements in deep learning methods have significantly improved the performance of 3D Human Pose Estimation (HPE). However, performance degradation caused by domain gaps between source and target domains remains a major challenge to generalization, necessitating extensive data augmentation and/or fine-tuning for each specific target domain. To address this issue more efficiently, we propose a novel canonical domain approach that maps both the source and target domains into a unified canonical domain, alleviating the need for additional fine-tuning in the target domain. To construct the canonical domain, we introduce a canonicalization process to generate a novel canonical 2D-3D pose mapping that ensures 2D-3D pose consistency and simplifies 2D-3D pose patterns, enabling more efficient training of lifting networks. The canonicalization of both domains is achieved through the following steps: (1) in the source domain, the lifting network is trained within the canonical domain; (2) in the target domain, input 2D poses are canonicalized prior to inference by leveraging the properties of perspective projection and known camera intrinsics. Consequently, the trained network can be directly applied to the target domain without requiring additional fine-tuning. Experiments conducted with various lifting networks and publicly available datasets (e.g., Human3.6M, Fit3D, MPI-INF-3DHP) demonstrate that the proposed method substantially improves generalization capability across datasets while using the same data volume."
188,679d459debd8ffd557a2af29,cs.CV,https://arxiv.org/pdf/2501.16100,Automated Detection of Sport Highlights from Audio and Video Sources,"Francesco Della Santa, Morgana Lalli","Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning","This study presents a novel Deep Learning-based and lightweight approach for the automated detection of sports highlights (HLs) from audio and video sources. HL detection is a key task in sports video analysis, traditionally requiring significant human effort. Our solution leverages Deep Learning (DL) models trained on relatively small datasets of audio Mel-spectrograms and grayscale video frames, achieving promising accuracy rates of 89% and 83% for audio and video detection, respectively. The use of small datasets, combined with simple architectures, demonstrates the practicality of our method for fast and cost-effective deployment. Furthermore, an ensemble model combining both modalities shows improved robustness against false positives and false negatives. The proposed methodology offers a scalable solution for automated HL detection across various types of sports video content, reducing the need for manual intervention. Future work will focus on enhancing model architectures and extending this approach to broader scene-detection tasks in media analysis."
189,679d459debd8ffd557a2af2a,cs.CV,https://arxiv.org/pdf/2501.16085,ARFlow: Autogressive Flow with Hybrid Linear Attention,"Mude Hui, Rui-Jie Zhu, Songlin Yang, Yu Zhang, Zirui Wang, Yuyin Zhou, Jason Eshraghian, Cihang Xie",Computer Vision and Pattern Recognition,"Flow models are effective at progressively generating realistic images, but they generally struggle to capture long-range dependencies during the generation process as they compress all the information from previous time steps into a single corrupted image.
To address this limitation, we propose integrating autoregressive modeling—known for its excellence in modeling complex, high-dimensional joint probability distributions—into flow models. During training, at each step, we construct causally-ordered sequences by sampling multiple images from the same semantic category and applying different levels of noise, where images with higher noise levels serve as causal predecessors to those with lower noise levels. This design enables the model to learn broader category-level variations while maintaining proper causal relationships in the flow process. During generation, the model autoregressively conditions the previously generated images from earlier denoising steps, forming a contextual and coherent generation trajectory.
Additionally, we design a customized hybrid linear attention mechanism tailored to our modeling approach to enhance computational efficiency.
Our approach, termed ARFlow, under 400k training steps, achieves 14.08 FID scores on ImageNet at 128 × 128 without classifier-free guidance, reaching 4.34 FID with classifier-free guidance 1.5, significantly outperforming the previous flow-based model SiT’s 9.17 FID. Extensive ablation studies demonstrate the effectiveness of our modeling strategy and chunk-wise attention design."
190,679d459debd8ffd557a2af2b,cs.CV,https://arxiv.org/pdf/2501.16065,CILP-FGDI: Exploiting Vision-Language Model for Generalizable Person Re-Identification,"Huazhong Zhao, Lei Qi, Xin Geng",Computer Vision and Pattern Recognition,"The Visual Language Model, known for its robust cross-modal capabilities, has been extensively applied in various computer vision tasks.
In this paper, we explore the use of CLIP (Contrastive Language-Image Pretraining), a vision-language model pretrained on large-scale image-text pairs to align visual and textual features, for acquiring fine-grained and domain-invariant representations in generalizable person re-identification.
The adaptation of CLIP to the task presents two primary challenges: learning more fine-grained features to enhance discriminative ability, and learning more domain-invariant features to improve the model’s generalization capabilities.
To mitigate the first challenge thereby enhance the ability to learn fine-grained features, a three-stage strategy is proposed to boost the accuracy of text descriptions.
Initially, the image encoder is trained to effectively adapt to person re-identification tasks.
In the second stage, the features extracted by the image encoder are used to generate textual descriptions (i.e., prompts) for each image.
Finally, the text encoder with the learned prompts is employed to guide the training of the final image encoder.
To enhance the model’s generalization capabilities to unseen domains, a bidirectional guiding method is introduced to learn domain-invariant image features.
Specifically, domain-invariant and domain-relevant prompts are generated, and both positive (i.e., pulling together image features and domain-invariant prompts) and negative (i.e., pushing apart image features and domain-relevant prompts) views are used to train the image encoder.
Collectively, these strategies contribute to the development of an innovative CLIP-based framework for learning fine-grained generalized features in person re-identification.
The effectiveness of the proposed method is validated through a comprehensive series of experiments conducted on multiple benchmarks.
Our code is available athttps://github.com/Qi5Lei/CLIP-FGDI."
191,679d459debd8ffd557a2af2c,cs.CV,https://arxiv.org/pdf/2501.16037,Addressing Out-of-Label Hazard Detection in Dashcam Videos: Insights from the COOOL Challenge,"Anh-Kiet Duong, Petra Gomez-Krämer",Computer Vision and Pattern Recognition,"This paper presents a novel approach for hazard analysis in dashcam footage, addressing the detection of driver reactions to hazards, the identification of hazardous objects, and the generation of descriptive captions. We first introduce a method for detecting driver reactions through speed and sound anomaly detection, leveraging unsupervised learning techniques. For hazard detection, we employ a set of heuristic rules as weak classifiers, which are combined using an ensemble method. This ensemble approach is further refined with differential privacy to mitigate overconfidence, ensuring robustness despite the lack of labeled data. Lastly, we use state-of-the-art vision-language models for hazard captioning, generating descriptive labels for the detected hazards. Our method achieved the highest scores in the Challenge on Out-of-Label in Autonomous Driving, demonstrating its effectiveness across all three tasks. Source codes are publicly available athttps://github.com/ffyyytt/COOOL_2025."
192,679d459debd8ffd557a2af2d,cs.CV,https://arxiv.org/pdf/2501.16022,Freestyle Sketch-in-the-Loop Image Segmentation,"Subhadeep Koley, Viswanatha Reddy Gajjala, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Ayan Kumar Bhunia, Yi-Zhe Song",Computer Vision and Pattern Recognition,"In this paper, we expand the domain of sketch research into the field of image segmentation, aiming to establish freehand sketches as a query modality for subjective image segmentation. Our innovative approach introduces a “sketch-in-the-loop” image segmentation framework, enabling the segmentation of visual concepts partially, completely, or in groupings – a truly “freestyle” approach – without the need for a purpose-made dataset (i.e., mask-free). This framework capitalises on the synergy between sketch-based image retrieval (SBIR) models and large-scale pre-trained models (CLIP or DINOv2). The former provides an effective training signal, while fine-tuned versions of the latter execute the subjective segmentation. Additionally, our purpose-made augmentation strategy enhances the versatility of our sketch-guided mask generation, allowing segmentation at multiple granularity levels. Extensive evaluations across diverse benchmark datasets underscore the superior performance of our method in comparison to existing approaches across various evaluation scenarios."
193,679d459debd8ffd557a2af2e,cs.CV,https://arxiv.org/pdf/2501.16003,Improving Tropical Cyclone Forecasting With Video Diffusion Models,"Zhibo Ren, Pritthijit Nath, Pancham Shukla","Computer Vision and Pattern Recognition, Atmospheric and Oceanic Physics","Tropical cyclone (TC) forecasting is crucial for disaster preparedness and mitigation. While recent deep learning approaches have shown promise, existing methods often treat TC evolution as a series of independent frame-to-frame predictions, limiting their ability to capture long-term dynamics. We present a novel application of video diffusion models for TC forecasting that explicitly models temporal dependencies through additional temporal layers. Our approach enables the model to generate multiple frames simultaneously, better capturing cyclone evolution patterns. We introduce a two-stage training strategy that significantly improves individual-frame quality and performance in low-data regimes. Experimental results show our method outperforms the previous approach of Nath et al. by 19.3% in MAE, 16.2% in PSNR, and 36.1% in SSIM. Most notably, we extend the reliable forecasting horizon from 36 to 50 hours. Through comprehensive evaluation using both traditional metrics and Fréchet Video Distance (FVD), we demonstrate that our approach produces more temporally coherent forecasts while maintaining competitive single-frame quality. Code accessible athttps://github.com/Ren-creater/forecast-video-diffmodels."
194,679d459debd8ffd557a2af2f,cs.CV,https://arxiv.org/pdf/2501.15998,Controllable Forgetting Mechanism for Few-Shot Class-Incremental Learning,"Kirill Paramonov, Mete Ozay, Eunju Yang, Jijoong Moon, Umberto Michieli","Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning","Class-incremental learning in the context of limited personal labeled samples (few-shot) is critical for numerous real-world applications, such as smart home devices. A key challenge in these scenarios is balancing the trade-off between adapting to new, personalized classes and maintaining the performance of the model on the original, base classes. Fine-tuning the model on novel classes often leads to the phenomenon of catastrophic forgetting, where the accuracy of base classes declines unpredictably and significantly.
In this paper, we propose a simple yet effective mechanism to address this challenge by controlling the trade-off between novel and base class accuracy. We specifically target the ultra-low-shot scenario, where only a single example is available per novel class. Our approach introduces a Novel Class Detection (NCD) rule, which adjusts the degree of forgettinga prioriwhile simultaneously enhancing performance on novel classes.
We demonstrate the versatility of our solution by applying it to state-of-the-art Few-Shot Class-Incremental Learning (FSCIL) methods, showing consistent improvements across different settings. To better quantify the trade-off between novel and base class performance, we introduce new metrics: NCR@2FOR and NCR@5FOR. Our approach achieves up to a 30% improvement in novel class accuracy on the CIFAR100 dataset (1-shot, 1 novel class) while maintaining a controlled base class forgetting rate of 2%."
195,679d459debd8ffd557a2af30,cs.CV,https://arxiv.org/pdf/2501.15981,MatCLIP: Light- and Shape-Insensitive Assignment of PBR Material Models,"Michael Birsak, John Femiani, Biao Zhang, Peter Wonka","Computer Vision and Pattern Recognition, Graphics, Machine Learning","Assigning realistic materials to 3D models remains a significant challenge in computer graphics.
We propose MatCLIP, a novel method that extracts shape- and lighting-insensitive descriptors of Physically Based Rendering (PBR) materials to assign plausible textures to 3D objects based on images, such as the output of Latent Diffusion Models (LDMs) or photographs.
Matching PBR materials to static images is challenging because the PBR representation captures the dynamic appearance of materials under varying viewing angles, shapes, and lighting conditions.
By extending an Alpha-CLIP-based model on material renderings across diverse shapes and lighting, and encoding multiple viewing conditions for PBR materials, our approach generates descriptors that bridge the domains of PBR representations with photographs or renderings, including LDM outputs. This enables consistent material assignments without requiring explicit knowledge of material relationships between different parts of an object.
MatCLIP achieves a top-1 classification accuracy of 76.6%, outperforming state-of-the-art methods such as PhotoShape and MatAtlas by over 15 percentage points on publicly available datasets. Our method can be used to construct material assignments for 3D shape datasets such as ShapeNet, 3DCoMPaT++, and Objaverse. All code and data will be released."
196,679d459debd8ffd557a2af31,cs.CV,https://arxiv.org/pdf/2501.15891,Any2AnyTryon: Leveraging Adaptive Position Embeddings for Versatile Virtual Clothing Tasks,"Hailong Guo, Bohan Zeng, Yiren Song, Wentao Zhang, Chuang Zhang, Jiaming Liu",Computer Vision and Pattern Recognition,"Image-based virtual try-on (VTON) aims to generate a virtual try-on result by transferring an input garment onto a target person’s image. However, the scarcity of paired garment-model data makes it challenging for existing methods to achieve high generalization and quality in VTON. Also, it limits the ability to generate mask-free try-ons. To tackle the data scarcity problem, approaches such as Stable Garment and MMTryon use a synthetic data strategy, effectively increasing the amount of paired data on the model side. However, existing methods are typically limited to performing specific try-on tasks and lack user-friendliness."
197,679d459debd8ffd557a2af32,cs.CV,https://arxiv.org/pdf/2501.15890,A Data-Centric Approach: Dimensions of Visual Complexity and How to find Them,"Karahan Sarıtaş, Tingke Shen, Surabhi S Nath, Peter Dayan","Computer Vision and Pattern Recognition, Artificial Intelligence","Understanding how humans perceive visual complexity is a key area of study in visual cognition. Previous approaches to modeling visual complexity have often resulted in intricate, difficult-to-interpret solutions that employ numerous features or sophisticated deep learning architectures. While these complex models achieve high performance on specific datasets, they often sacrifice interpretability, making it challenging to understand the factors driving human perception of complexity. A recent model based on image segmentations showed promise in addressing this challenge; however, it presented limitations in capturing structural and semantic aspects of visual complexity. In this paper, we propose viable and effective features to overcome these shortcomings. Specifically, we develop multiscale features for the structural aspect of complexity, including the Multiscale Sobel Gradient (MSG), which captures spatial intensity variations across scales, and Multiscale Unique Colors (MUC), which quantifies image colorfulness by indexing quantized RGB values. We also introduce a new dataset SVG based on Visual Genome to explore the semantic aspect of visual complexity, obtaining surprise scores based on the element of surprise in images, which we demonstrate significantly contributes to perceived complexity. Overall, we suggest that the nature of the data is fundamental to understanding and modeling visual complexity, highlighting the importance of both structural and semantic dimensions in providing a comprehensive, interpretable assessment. The code for our analysis, experimental setup, and dataset will be made publicly available upon acceptance."
198,679d459debd8ffd557a2af33,cs.CV,https://arxiv.org/pdf/2501.15878,Slot-Guided Adaptation of Pre-trained Diffusion Models for Object-Centric Learning and Compositional Generation,"Adil Kaan Akan, Yucel Yemez","Computer Vision and Pattern Recognition, Machine Learning","We present SlotAdapt, an object-centric learning method that combines slot attention with pretrained diffusion models by introducing adapters for slot-based conditioning. Our method preserves the generative power of pretrained diffusion models, while avoiding their text-centric conditioning bias. We also incorporate an additional guidance loss into our architecture to align cross-attention from adapter layers with slot attention. This enhances the alignment of our model with the objects in the input image without using external supervision. Experimental results show that our method outperforms state-of-the-art techniques in object discovery and image generation tasks across multiple datasets, including those with real images. Furthermore, we demonstrate through experiments that our method performs remarkably well on complex real-world images for compositional generation, in contrast to other slot-based generative methods in the literature. The project page can be found athttps://kaanakan.github.io/SlotAdapt/."
199,679d459debd8ffd557a2af34,cs.CV,https://arxiv.org/pdf/2501.15870,D-PLS: Decoupled Semantic Segmentation for 4D-Panoptic-LiDAR-Segmentation,"Maik Steinhauser, Laurenz Reichardt, Nikolas Ebert, Oliver Wasenmüller","Computer Vision and Pattern Recognition, Artificial Intelligence","This paper introduces a novel approach to 4D Panoptic LiDAR Segmentation that decouples semantic and instance segmentation, leveraging single-scan semantic predictions as prior information for instance segmentation. Our method D-PLS first performs single-scan semantic segmentation and aggregates the results over time, using them to guide instance segmentation. The modular design of D-PLS allows for seamless integration on top of any semantic segmentation architecture, without requiring architectural changes or retraining. We evaluate our approach on the SemanticKITTI dataset, where it demonstrates significant improvements over the baseline in both classification and association tasks, as measured by the LiDAR Segmentation and Tracking Quality (LSTQ) metric. Furthermore, we show that our decoupled architecture not only enhances instance prediction but also surpasses the baseline due to advancements in single-scan semantic segmentation."
200,679d459debd8ffd557a2af35,cs.CV,https://arxiv.org/pdf/2501.15860,The Components of Collaborative Joint Perception and Prediction -- A Conceptual Framework,"Lei Wan, Hannan Ejaz Keen, Alexey Vinel",Computer Vision and Pattern Recognition,"Connected Autonomous Vehicles(CAVs)benefit fromVehicle-to-Everything(V2X)communication, which enables the exchange of sensor data to achieveCollaborative Perception(CP). To reduce cumulative errors in perception modules and mitigate the visual occlusion, this paper introduces a new task,Collaborative Joint Perception and Prediction(Co-P&P), and provides a conceptual framework for its implementation to improve motion prediction of surrounding objects, thereby enhancing vehicle awareness in complex traffic scenarios. The framework consists of two decoupled core modules,Collaborative Scene Completion(CSC)andJoint Perception and Prediction(P&P)module, which simplify practical deployment and enhance scalability. Additionally, we outline the challenges in Co-P&P and discuss future directions for this research area."
201,679d459debd8ffd557a2af36,cs.CV,https://arxiv.org/pdf/2501.15852,CausalSR: Structural Causal Model-Driven Super-Resolution with Counterfactual Inference,"Zhengyang Lu, Bingjie Lu, Feng Wang",Computer Vision and Pattern Recognition,"Physical and optical factors interacting with sensor characteristics create complex image degradation patterns. Despite advances in deep learning-based super-resolution, existing methods overlook the causal nature of degradation by adopting simplistic black-box mappings. This paper formulates super-resolution using structural causal models to reason about image degradation processes. We establish a mathematical foundation that unifies principles from causal inference, deriving necessary conditions for identifying latent degradation mechanisms and corresponding propagation. We propose a novel counterfactual learning strategy that leverages semantic guidance to reason about hypothetical degradation scenarios, leading to theoretically-grounded representations that capture invariant features across different degradation conditions. The framework incorporates an adaptive intervention mechanism with provable bounds on treatment effects, allowing precise manipulation of degradation factors while maintaining semantic consistency. Through extensive empirical validation, we demonstrate that our approach achieves significant improvements over state-of-the-art methods, particularly in challenging scenarios with compound degradations. On standard benchmarks, our method consistently outperforms existing approaches by significant margins (0.86-1.21dB PSNR), while providing interpretable insights into the restoration process. The theoretical framework and empirical results demonstrate the fundamental importance of causal reasoning in understanding image restoration systems."
202,679d459debd8ffd557a2af37,cs.CV,https://arxiv.org/pdf/2501.15847,Can Location Embeddings Enhance Super-Resolution of Satellite Imagery?,"Daniel Panangian, Ksenia Bittner",Computer Vision and Pattern Recognition,"Publicly available satellite imagery, such as Sentinel-2, often lacks the spatial resolution required for accurate analysis of remote sensing tasks including urban planning and disaster response. Current super-resolution techniques are typically trained on limited datasets, leading to poor generalization across diverse geographic regions. In this work, we propose a novel super-resolution framework that enhances generalization by incorporating geographic context through location embeddings. Our framework employsGenerative Adversarial Networks (GANs)and incorporates techniques from diffusion models to enhance image quality. Furthermore, we address tiling artifacts by integrating information from neighboring images, enabling the generation of seamless, high-resolution outputs. We demonstrate the effectiveness of our method on the building segmentation task, showing significant improvements over state-of-the-art methods and highlighting its potential for real-world applications."
203,679d459debd8ffd557a2af38,cs.CV,https://arxiv.org/pdf/2501.15839,Controllable Hand Grasp Generation for HOI and Efficient Evaluation Methods,"Ishant, Rongliang Wu, Joo Hwee Lim",Computer Vision and Pattern Recognition,"Controllable affordance Hand-Object Interaction (HOI) generation has become an increasingly important area of research in computer vision. In HOI generation, the hand grasp generation is a crucial step for effectively controlling the geometry of the hand. Current hand grasp generation methods rely on 3D information for both the hand and the object. In addition, these methods lack controllability concerning the hand’s location and orientation. We treat the hand pose as the discrete graph structure and exploit the geometric priors. It is well established that higher order contextual dependency among the points improves the quality of the results in general. We propose a framework of higher order geometric representations (HOR’s) inspired by spectral graph theory and vector algebra to improve the quality of generated hand poses. We demonstrate the effectiveness of our proposed HOR’s in devising a controllable novel diffusion method (based on 2D information) for hand grasp generation that outperforms the state-of-the-art (SOTA). Overcoming the limitations of existing methods: like lacking of controllability and dependency on 3D information. Once we have the generated pose, it is very natural to evaluate them using a metric. Popular metrics like FID and MMD are biased and inefficient for evaluating the generated hand poses. Using our proposed HOR’s, we introduce an efficient and stable framework of evaluation metrics for grasp generation methods, addressing inefficiencies and biases in FID and MMD."
204,679d459debd8ffd557a2af39,cs.CV,https://arxiv.org/pdf/2501.15808,ClearSight: Human Vision-Inspired Solutions for Event-Based Motion Deblurring,"Xiaopeng Lin, Yulong Huang, Hongwei Ren, Zunchang Liu, Yue Zhou, Haotian Fu, Bojun Cheng",Computer Vision and Pattern Recognition,"Motion deblurring addresses the challenge of image blur caused by camera or scene movement. Event cameras provide motion information that is encoded in the asynchronous event streams. To efficiently leverage the temporal information of event streams, we employ Spiking Neural Networks (SNNs) for motion feature extraction and Artificial Neural Networks (ANNs) for color information processing. Due to the non-uniform distribution and inherent redundancy of event data, existing cross-modal feature fusion methods exhibit certain limitations. Inspired by the visual attention mechanism in the human visual system, this study introduces a bioinspired dual-drive hybrid network (BDHNet). Specifically, the Neuron Configurator Module (NCM) is designed to dynamically adjusts neuron configurations based on cross-modal features, thereby focusing the spikes in blurry regions and adapting to varying blurry scenarios dynamically. Additionally, the Region of Blurry Attention Module (RBAM) is introduced to generate a blurry mask in an unsupervised manner, effectively extracting motion clues from the event features and guiding more accurate cross-modal feature fusion. Extensive subjective and objective evaluations demonstrate that our method outperforms current state-of-the-art methods on both synthetic and real-world datasets."
205,679d459debd8ffd557a2af3a,cs.CV,https://arxiv.org/pdf/2501.15798,MM-Retinal V2: Transfer an Elite Knowledge Spark into Fundus Vision-Language Pretraining,"Ruiqi Wu, Na Su, Chenran Zhang, Tengfei Ma, Tao Zhou, Zhiting Cui, Nianfeng Tang, Tianyu Mao, Yi Zhou, Wen Fan, Tianxing Wu, Shenqi Jing, Huazhu Fu",Computer Vision and Pattern Recognition,"Vision-language pretraining (VLP) has been investigated to generalize across diverse downstream tasks for fundus image analysis. Although recent methods showcase promising achievements, they significantly rely on large-scale private image-text data but pay less attention to the pretraining manner, which limits their further advancements. In this work, we introduce MM-Retinal V2, a high-quality image-text paired dataset comprising CFP, FFA, and OCT image modalities. Then, we propose a novel fundus vision-language pretraining model, namely KeepFIT V2, which is pretrained by integrating knowledge from the elite data spark into categorical public datasets. Specifically, a preliminary textual pretraining is adopted to equip the text encoder with primarily ophthalmic textual knowledge. Moreover, a hybrid image-text knowledge injection module is designed for knowledge transfer, which is essentially based on a combination of global semantic concepts from contrastive learning and local appearance details from generative learning. Extensive experiments across zero-shot, few-shot, and linear probing settings highlight the generalization and transferability of KeepFIT V2, delivering performance competitive to state-of-the-art fundus VLP models trained on large-scale private image-text datasets. Our dataset and model are publicly available via https://github.com/lxirich/MM-Retinal."
206,679d459debd8ffd557a2af3b,cs.CV,https://arxiv.org/pdf/2501.15795,Can Multimodal Large Language Models be Guided to Improve Industrial Anomaly Detection?,"Zhiling Chen, Hanning Chen, Mohsen Imani, Farhad Imani",Computer Vision and Pattern Recognition,"In industrial settings, the accurate detection of anomalies is essential for maintaining product quality and ensuring operational safety. Traditional industrial anomaly detection (IAD) models often struggle with flexibility and adaptability, especially in dynamic production environments where new defect types and operational changes frequently arise. Recent advancements in Multimodal Large Language Models (MLLMs) hold promise for overcoming these limitations by combining visual and textual information processing capabilities. MLLMs excel in general visual understanding due to their training on large, diverse datasets, but they lack domain-specific knowledge, such as industry-specific defect tolerance levels, which limits their effectiveness in IAD tasks. To address these challenges, we propose Echo, a novel multi-expert framework designed to enhance MLLM performance for IAD. Echo integrates four expert modules: Reference Extractor which provides a contextual baseline by retrieving similar normal images, Knowledge Guide which supplies domain-specific insights, Reasoning Expert which enables structured, stepwise reasoning for complex queries, and Decision Maker which synthesizes information from all modules to deliver precise, context-aware responses. Evaluated on the MMAD benchmark, Echo demonstrates significant improvements in adaptability, precision, and robustness, moving closer to meeting the demands of real-world industrial anomaly detection."
207,679d459debd8ffd557a2af3c,cs.CV,https://arxiv.org/pdf/2501.15775,Do Existing Testing Tools Really Uncover Gender Bias in Text-to-Image Models?,"Yunbo Lyu, Zhou Yang, Yuqing Niu, Jing Jiang, David Lo","Computer Vision and Pattern Recognition, Software Engineering","Text-to-Image (T2I) models have recently gained significant attention due to their ability to generate high-quality images and are consequently used in a wide range of applications.
However, there are concerns about the gender bias of these models.
Previous studies have shown that T2I modelscan perpetuate or even amplify gender stereotypes when provided with neutral text prompts (e.g., ‘a photo of a CEO’ is often associates with male images, while ‘a photo of nurse’ is often associates with female images).Researchers have proposed automated gender bias uncovering detectors for T2I models, but a crucial gap exists:no existing work comprehensively compares the various detectors and understands how the gender bias detected by them deviates from the actual situation.This study addresses this gap byvalidatingprevious gender bias detectors using amanually labeleddataset and comparing how the bias identified by various detectors deviates from the actual bias in T2I models, as verified by manual confirmation.
We create a dataset consisting of 6,000 images generated from three cutting-edge T2I models, Stable Diffusion XL, Stable Diffusion 3, and Dreamlike Photoreal 2.0.
During the human-labeling process, we find that all three T2I models generate a portion (12.48% on average) of low-quality images (e.g., generate images with no face present), where human annotators cannot determine the gender of the person.
Our analysis reveals that all three T2I models show a preference for generating male images, with SDXL being the most biased.
Additionally, images generated using prompts containing professional descriptions (e.g., lawyer or doctor) show the most bias.
We evaluate seven gender bias detectors and find that none fully capture the actual level of bias in T2I models, with some detectors overestimating bias by up to 26.95%.
We further investigate the causes of inaccurate estimations, highlighting the limitations of detectors in dealing with low-quality images.
Based on our findings, we propose an enhanced detector called CLIP-Enhance, which most accurately measures the gender bias in T2I models, with a difference of only 0.47%-1.23%, and most effectively filters out 82.91% of low-quality images.111This paper potentially contains offensive information for some groups.We have made our dataset and code publicly available.222https://doi.org/10.6084/m9.figshare.27377649.v1"
208,679d459debd8ffd557a2af3d,cs.CV,https://arxiv.org/pdf/2501.15774,Efficient Attention-Sharing Information Distillation Transformer for Lightweight Single Image Super-Resolution,"Karam Park, Jae Woong Soh, Nam Ik Cho",Computer Vision and Pattern Recognition,"Transformer-based Super-Resolution (SR) methods have demonstrated superior performance compared to convolutional neural network (CNN)-based SR approaches due to their capability to capture long-range dependencies. However, their high computational complexity necessitates the development of lightweight approaches for practical use. To address this challenge, we propose the Attention-Sharing Information Distillation (ASID) network, a lightweight SR network that integrates attention-sharing and an information distillation structure specifically designed for Transformer-based SR methods. We modify the information distillation scheme, originally designed for efficient CNN operations, to reduce the computational load of stacked self-attention layers, effectively addressing the efficiency bottleneck. Additionally, we introduce attention-sharing across blocks to further minimize the computational cost of self-attention operations. By combining these strategies, ASID achieves competitive performance with existing SR methods while requiring only around 300 K parameters – significantly fewer than existing CNN-based and Transformer-based SR models. Furthermore, ASID outperforms state-of-the-art SR methods when the number of parameters is matched, demonstrating its efficiency and effectiveness. The code and supplementary material are available on the project page."
209,679d459debd8ffd557a2af3e,cs.CV,https://arxiv.org/pdf/2501.15763,NanoHTNet: Nano Human Topology Network for Efficient 3D Human Pose Estimation,"Jialun Cai, Mengyuan Liu, Hong Liu, Wenhao Li, Shuheng Zhou",Computer Vision and Pattern Recognition,"The widespread application of 3D human pose estimation (HPE) is limited by resource-constrained edge devices like Jetson Nano, requiring more efficient models.
A key approach to enhancing efficiency involves designing networks based on the structural characteristics of input data. However, effectively utilizing the structural priors in human skeletal inputs remains challenging.
To address this, we leverage both explicit and implicit spatio-temporal priors of the human body through innovative model design and a pre-training proxy task.
First, we propose a Nano Human Topology Network (NanoHTNet), a tiny 3D HPE network with stacked Hierarchical Mixers to capture explicit features.
Specifically, the spatial Hierarchical Mixer efficiently learns the human physical topology across multiple semantic levels, while the temporal Hierarchical Mixer with discrete cosine transform and low-pass filtering captures local instantaneous movements and global action coherence.
Moreover, Efficient Temporal-Spatial Tokenization (ETST) is introduced to enhance spatio-temporal interaction and reduce computational complexity significantly.
Second, PoseCLR is proposed as a general pre-training method based on contrastive learning for 3D HPE, aimed at extracting implicit representations of human topology.
By aligning 2D poses from diverse viewpoints in the proxy task, PoseCLR aids 3D HPE encoders like NanoHTNet in more effectively capturing the high-dimensional features of the human body, leading to further performance improvements.
Extensive experiments verify that NanoHTNet with PoseCLR outperforms other state-of-the-art methods in efficiency, making it ideal for deployment on edge devices like the Jetson Nano.
Code and models are available athttps://github.com/vefalun/NanoHTNet."
210,679d459debd8ffd557a2af3f,cs.CV,https://arxiv.org/pdf/2501.15757,"Efficiency Bottlenecks of Convolutional Kolmogorov-Arnold Networks: A Comprehensive Scrutiny with ImageNet, AlexNet, LeNet and Tabular Classification","Ashim Dahal, Saydul Akbar Murad, Nick Rahimi","Computer Vision and Pattern Recognition, Artificial Intelligence","Algorithmic level developments like Convolutional Neural Networks, transformers, attention mechanism, Retrieval Augmented Generation and so on have changed Artificial Intelligence. Recent such development was observed by Kolmogorov-Arnold Networks that suggested to challenge the fundamental concept of a Neural Network, thus change Multilayer Perceptron, and Convolutional Neural Networks. They received a good reception in terms of scientific modeling, yet had some drawbacks in terms of efficiency. In this paper, we train Convolutional Kolmogorov Arnold Networks (CKANs) with the ImageNet-1k dataset with 1.3 million images, MNIST dataset with 60k images and a tabular biological science related MoA dataset and test the promise of CKANs in terms of FLOPS, Inference Time, number of trainable parameters and training time against the accuracy, precision, recall and f-1 score they produce against the standard industry practice on CNN models. We show that the CKANs perform fair yet slower than CNNs in small size dataset like MoA and MNIST but are not nearly comparable as the dataset gets larger and more complex like the ImageNet. The code implementation of this paper can be found on the link:https://github.com/ashimdahal/Study-of-Convolutional-Kolmogorov-Arnold-networks"
211,679d459debd8ffd557a2af40,cs.CV,https://arxiv.org/pdf/2501.15724,"A Survey on Computational Pathology Foundation Models: Datasets, Adaptation Strategies, and Evaluation Tasks","Dong Li, Guihong Wan, Xintao Wu, Xinyu Wu, Ajit J. Nirmal, Christine G. Lian, Peter K. Sorger, Yevgeniy R. Semenov, Chen Zhao","Computer Vision and Pattern Recognition, Artificial Intelligence","Computational pathology foundation models (CPathFMs) have emerged as a powerful approach for analyzing histopathological data, leveraging self-supervised learning
to extract robust feature representations from unlabeled whole-slide images.
These models, categorized into uni-modal and multi-modal frameworks, have demonstrated promise in automating complex pathology tasks such as segmentation, classification,
and biomarker discovery.
However,
the development of CPathFMs presents significant challenges, such as limited data accessibility, high variability across datasets, the necessity for domain-specific adaptation, and the lack of standardized evaluation benchmarks. This survey provides a comprehensive review of CPathFMs in computational pathology, focusing on datasets, adaptation strategies, and evaluation tasks. We analyze key techniques, such as contrastive learning and multi-modal integration, and highlight existing gaps in current research. Finally, we explore future directions from four perspectives for advancing CPathFMs. This survey serves as a valuable resource for researchers, clinicians, and AI practitioners, guiding the advancement of CPathFMs toward robust and clinically applicable AI-driven pathology solutions."
212,679d459debd8ffd557a2af41,cs.CV,https://arxiv.org/pdf/2501.15666,MimicGait: A Model Agnostic approach for Occluded Gait Recognition using Correlational Knowledge Distillation,"Ayush Gupta, Rama Chellappa",Computer Vision and Pattern Recognition,"Gait recognition is an important biometric technique over large distances. State-of-the-art gait recognition systems perform very well in controlled environments at close range. Recently, there has been an increased interest in gait recognition in the wild prompted by the collection of outdoor, more challenging datasets containing variations in terms of illumination, pitch angles, and distances. An important problem in these environments is that of occlusion, where the subject is partially blocked from camera view.
While important, this problem has received little attention. Thus, we proposeMimicGait, amodel-agnosticapproach for gait recognition in the presence of occlusions. We train the network using amulti-instance correlational distillation lossto capture both inter-sequence and intra-sequence correlations in the occluded gait patterns of a subject, utilizing an auxiliary Visibility Estimation Network to guide the training of the proposed mimic network. We demonstrate the effectiveness of our approach on challenging real-world datasets like GREW, Gait3D and BRIAR.
We release the code inhttps://github.com/Ayush-00/mimicgait."
213,679d459debd8ffd557a2af42,cs.CV,https://arxiv.org/pdf/2501.15660,Marker Track: Accurate Fiducial Marker Tracking for Evaluation of Residual Motions During Breath-Hold Radiotherapy,"Aimee Guo, Weihua Mao","Computer Vision and Pattern Recognition, Artificial Intelligence, Image and Video Processing, Medical Physics",
214,679d459debd8ffd557a2af43,cs.CV,https://arxiv.org/pdf/2501.15656,Classifying Deepfakes Using Swin Transformers,"Aprille J. Xi, Eason Chen",Computer Vision and Pattern Recognition,"The proliferation of deepfake technology poses significant challenges to the authenticity and trustworthiness of digital media, necessitating the development of robust detection methods. This study explores the application of Swin Transformers, a state-of-the-art architecture leveraging shifted windows for self-attention, in detecting and classifying deepfake images. Using the Real and Fake Face Detection dataset by Yonsei University’s Computational Intelligence Photography Lab, we evaluate the Swin Transformer and hybrid models such as Swin-ResNet and Swin-KNN, focusing on their ability to identify subtle manipulation artifacts. Our results demonstrate that the Swin Transformer outperforms conventional CNN-based architectures, including VGG16, ResNet18, and AlexNet, achieving a test accuracy of 71.29%. Additionally, we present insights into hybrid model design, highlighting the complementary strengths of transformer and CNN-based approaches in deepfake detection. This study underscores the potential of transformer-based architectures for improving accuracy and generalizability in image-based manipulation detection, paving the way for more effective countermeasures against deepfake threats."
215,679d459debd8ffd557a2af44,cs.CV,https://arxiv.org/pdf/2501.15653,A Privacy Enhancing Technique to Evade Detection by Street Video Cameras Without Using Adversarial Accessories,"Jacob Shams, Ben Nassi, Satoru Koda, Asaf Shabtai, Yuval Elovici",Computer Vision and Pattern Recognition,"In this paper, we propose a privacy-enhancing technique leveraging an inherent property of automatic pedestrian detection algorithms, namely, that the training of deep neural network (DNN) based methods is generally performed using curated datasets and laboratory settings, while the operational areas of these methods are dynamic real-world environments. In particular, we leverage a novel side effect of this gap between the laboratory and the real world: location-based weakness in pedestrian detection. We demonstrate that the position (distance, angle, height) of a person, and ambient light level, directly impact the confidence of a pedestrian detector when detecting the person. We then demonstrate that this phenomenon is present in pedestrian detectors observing a stationary scene of pedestrian traffic, with blind spot areas of weak detection of pedestrians with low confidence. We show how privacy-concerned pedestrians can leverage these blind spots to evade detection by constructing a minimum confidence path between two points in a scene, reducing the maximum confidence and average confidence of the path by up to 0.09 and 0.13, respectively, over direct and random paths through the scene. To counter this phenomenon, and force the use of more costly and sophisticated methods to leverage this vulnerability, we propose a novel countermeasure to improve the confidence of pedestrian detectors in blind spots, raising the max/average confidence of paths generated by our technique by 0.09 and 0.05, respectively. In addition, we demonstrate that our countermeasure improves a Faster R-CNN-based pedestrian detector’s TPR and average true positive confidence by 0.03 and 0.15, respectively."
216,679d459debd8ffd557a2af45,cs.CV,https://arxiv.org/pdf/2501.15648,Can Pose Transfer Models Generate Realistic Human Motion?,"Vaclav Knapp, Matyas Bohacek","Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning","Recent pose-transfer methods aim to generate temporally consistent and fully controllable videos of human action where the motion from a reference video is reenacted by a new identity. We evaluate three state-of-the-art pose-transfer methods—AnimateAnyone,MagicAnimate, andExAvatar—by generating videos with actions and identities outside the training distribution and conducting a participant study about the quality of these videos. In a controlled environment of20202020distinct human actions, we find that participants, presented with the pose-transferred videos, correctly identify the desired action only42.92%percent42.9242.92\%42.92 %of the time. Moreover, the participants find the actions in the generated videos consistent with the reference (source) videos only36.46%percent36.4636.46\%36.46 %of the time. These results vary by method: participants find the splatting-basedExAvatarmore consistent and photorealistic than the diffusion-basedAnimateAnyoneandMagicAnimate."
217,679d459debd8ffd557a2af46,cs.CV,https://arxiv.org/pdf/2501.15641,Bringing Characters to New Stories: Training-Free Theme-Specific Image Generation via Dynamic Visual Prompting,"Yuxin Zhang, Minyan Luo, Weiming Dong, Xiao Yang, Haibin Huang, Chongyang Ma, Oliver Deussen, Tong-Yee Lee, Changsheng Xu",Computer Vision and Pattern Recognition,"The stories and characters that captivate us as we grow up shape unique fantasy worlds,
with images serving as the primary medium for visually experiencing these realms. Personalizing generative models through fine-tuning with theme-specific data has become a prevalent approach in text-to-image generation.
However, unlike object customization, which focuses on learning specific objects, theme-specific generation encompasses diverse elements such as characters, scenes, and objects. Such diversity also introduces a key challenge: how to adaptively generate multi-character, multi-concept, and continuous theme-specific images (TSI). Moreover, fine-tuning approaches often come with significant computational overhead, time costs, and risks of overfitting.
This paper explores a fundamental question: Can image generation models directly leverage images as contextual input, similarly to how large language models use text as context?
To address this, we presentT-Prompter, a novel training-free TSI method for generation.T-Prompterintroduces visual prompting, a mechanism that integrates reference images into generative models, allowing users to seamlessly specify the target theme without requiring additional training. To further enhance this process, we propose a Dynamic Visual Prompting (DVP) mechanism, which iteratively optimizes visual prompts to improve the accuracy and quality of generated images.
Our approach enables diverse applications, including consistent story generation, character design, realistic character generation, and style-guided image generation.
Comparative evaluations against state-of-the-art personalization methods demonstrate thatT-Prompterachieves significantly better results and excels in maintaining character identity preserving, style consistency and text alignment, offering a robust and flexible solution for theme-specific image generation."
218,679d459debd8ffd557a2af47,cs.CV,https://arxiv.org/pdf/2501.15619,GaussianToken: An Effective Image Tokenizer with 2D Gaussian Splatting,"Jiajun Dong, Chengkun Wang, Wenzhao Zheng, Lei Chen, Jiwen Lu, Yansong Tang","Computer Vision and Pattern Recognition, Artificial Intelligence","Effective image tokenization is crucial for both multi-modal understanding and generation tasks due to the necessity of the alignment with discrete text data.
To this end, existing approaches utilize vector quantization (VQ) to project pixels onto a discrete codebook and reconstruct images from the discrete representation.
However, compared with the continuous latent space, the limited discrete codebook space significantly restrict the representational ability of these image tokenizers.
In this paper, we propose GaussianToken: An Effective Image Tokenizer with 2D Gaussian Splatting as a solution.
We first represent the encoded samples as multiple flexible featured 2D Gaussians characterized by positions, rotation angles, scaling factors, and feature coefficients.
We adopt the standard quantization for the Gaussian features and then concatenate the quantization results with the other intrinsic Gaussian parameters before the corresponding splatting operation and the subsequent decoding module.
In general, GaussianToken integrates the local influence of 2D Gaussian distribution into the discrete space and thus enhances the representation capability of the image tokenizer.
Competitive reconstruction performances on CIFAR, Mini-ImageNet, and ImageNet-1K demonstrate the effectiveness of our framework.
Our code is available at:https://github.com/ChrisDong-THU/GaussianToken."
219,679d459debd8ffd557a2af48,cs.CV,https://arxiv.org/pdf/2501.15616,IPVTON: Image-based 3D Virtual Try-on with Image Prompt Adapter,"Xiaojing Zhong, Zhonghua Wu, Xiaofeng Yang, Guosheng Lin, Qingyao Wu",Computer Vision and Pattern Recognition,"Given a pair of images depicting a person and a garment separately, image-based 3D virtual try-on methods aim to reconstruct a 3D human model that realistically portrays the person wearing the desired garment. In this paper, we present IPVTON, a novel image-based 3D virtual try-on framework. IPVTON employs score distillation sampling with image prompts to optimize a hybrid 3D human representation, integrating target garment features into diffusion priors through an image prompt adapter. To avoid interference with non-target areas, we leverage mask-guided image prompt embeddings to focus the image features on the try-on regions. Moreover, we impose geometric constraints on the 3D model with a pseudo silhouette generated by ControlNet, ensuring that the clothed 3D human model retains the shape of the source identity while accurately wearing the target garments. Extensive qualitative and quantitative experiments demonstrate that IPVTON outperforms previous methods in image-based 3D virtual try-on tasks, excelling in both geometry and texture."
220,679d459debd8ffd557a2af49,cs.CV,https://arxiv.org/pdf/2501.15603,Advancing TDFN: Precise Fixation Point Generation Using Reconstruction Differences,"Shuguang Wang, Yuanjing Wang",Computer Vision and Pattern Recognition,"Wang and Wang (2025)proposed the Task-Driven
Fixation Network (TDFN) based on the fixation mechanism, which leverages
low-resolution information along with high-resolution details near
fixation points to accomplish specific visual tasks. The model employs
reinforcement learning to generate fixation points. However, training
reinforcement learning models is challenging, particularly when aiming
to generate pixel-level accurate fixation points on high-resolution
images. This paper introduces an improved fixation point generation
method by leveraging the difference between the reconstructed image
and the input image to train the fixation point generator. This approach
directs fixation points to areas with significant differences between
the reconstructed and input images. Experimental results demonstrate
that this method achieves highly accurate fixation points, significantly
enhances the network’s classification accuracy, and reduces the average
number of required fixations to achieve a predefined accuracy level."
221,679d459debd8ffd557a2af4a,cs.CV,https://arxiv.org/pdf/2501.15595,SedarEval: Automated Evaluation using Self-Adaptive Rubrics,"Zhiyuan Fan, Weinong Wang, Xing Wu, Debing Zhang",Computer Vision and Pattern Recognition,
222,679d459debd8ffd557a2af4b,cs.CV,https://arxiv.org/pdf/2501.15579,ConceptCLIP: Towards Trustworthy Medical AI via Concept-Enhanced Contrastive Langauge-Image Pre-training,"Yuxiang Nie, Sunan He, Yequan Bie, Yihui Wang, Zhixuan Chen, Shu Yang, Hao Chen","Computer Vision and Pattern Recognition, Computation and Language","Trustworthiness is essential for the precise and interpretable application of artificial intelligence (AI) in medical imaging.
Traditionally, precision and interpretability have been addressed as separate tasks, namely medical image analysis and explainable AI, each developing its own models independently.
In this study, for the first time, we investigate the development of a unified medical vision-language pre-training model that can achieve both accurate analysis and interpretable understanding of medical images across various modalities.
To build the model, we constructMedConcept-23M, a large-scale dataset comprising 23 million medical image-text pairs extracted from 6.2 million scientific articles, enriched with concepts from the Unified Medical Language System (UMLS). Based on MedConcept-23M, we introduceConceptCLIP, a medical AI model utilizing concept-enhanced contrastive language-image pre-training.
The pre-training of ConceptCLIP involves two primary components: image-text alignment learning (IT-Align) and patch-concept alignment learning (PC-Align).
Specifically, IT-Align facilitates the global alignment of medical image and text representations,
while PC-Align uses UMLS knowledge for detailed alignment between image patches and their corresponding conceptual representations.
This dual alignment strategy enhances the model’s capability to associate specific image regions with relevant concepts, thereby improving both the precision of analysis and the interpretability of the AI system.
We conducted extensive experiments on 5 diverse types of medical image analysis tasks, spanning 51 subtasks across 10 image modalities, with the broadest range of downstream tasks.
The results
demonstrate the effectiveness of the proposed vision-language pre-training model.
Further explainability analysis across 6 modalities reveals that ConceptCLIP achieves superior performance, underscoring its robust ability to advance explainable AI in medical imaging. These findings highlight ConceptCLIP’s capability in promoting trustworthy AI in the field of medicine."
223,679d459debd8ffd557a2af4c,cs.CV,https://arxiv.org/pdf/2501.15562,CE-SDWV: Effective and Efficient Concept Erasure for Text-to-Image Diffusion Models via a Semantic-Driven Word Vocabulary,"Jiahang Tu, Qian Feng, Chufan Chen, Jiahua Dong, Hanbin Zhao, Chao Zhang, Hui Qian","Computer Vision and Pattern Recognition, Artificial Intelligence","Large-scale text-to-image (T2I) diffusion models have achieved remarkable generative performance about various concepts. With the limitation of privacy and safety in practice, the generative capability concerning NSFW (Not Safe For Work) concepts is undesirable, e.g., producing sexually explicit photos, and licensed images. The concept erasure task for T2I diffusion models has attracted considerable attention and requires an effective and efficient method. To achieve this goal, we propose a CE-SDWV framework, which removes the target concepts (e.g., NSFW concepts) of T2I diffusion models in the text semantic space by only adjusting the text condition tokens and does not need to re-train the original T2I diffusion model’s weights. Specifically, our framework first builds a target concept-related word vocabulary to enhance the representation of the target concepts within the text semantic space, and then utilizes an adaptive semantic component suppression strategy to ablate the target concept-related semantic information in the text condition tokens. To further adapt the above text condition tokens to the original image semantic space, we propose an end-to-end gradient-orthogonal token optimization strategy. Extensive experiments on I2P and UnlearnCanvas benchmarks demonstrate the effectiveness and efficiency of our method."
224,679d459debd8ffd557a2af4d,cs.CV,https://arxiv.org/pdf/2501.15558,Ocean-OCR: Towards General OCR Application via a Vision-Language Model,"Song Chen, Xinyu Guo, Yadong Li, Tao Zhang, Mingan Lin, Dongdong Kuang, Youwei Zhang, Lingfeng Ming, Fengyu Zhang, Yuran Wang, Jianhua Xu, Zenan Zhou, Weipeng Chen",Computer Vision and Pattern Recognition,"Multimodal large language models (MLLMs) have shown impressive capabilities across various domains, excelling in processing and understanding information from multiple modalities. Despite the rapid progress made previously, insufficient OCR ability hinders MLLMs from excelling in text-related tasks. In this paper, we presentOcean-OCR, a 3B MLLM with state-of-the-art performance on various OCR scenarios and comparable understanding ability on general tasks. We employ Native Resolution ViT to enable variable resolution input and utilize a substantial collection of high-quality OCR datasets to enhance the model performance.
We demonstrate the superiority of Ocean-OCR through comprehensive experiments on open-source OCR benchmarks and across various OCR scenarios. These scenarios encompass document understanding, scene text recognition, and handwritten recognition, highlighting the robust OCR capabilities of Ocean-OCR.
Note that Ocean-OCR is the first MLLM to outperform professional OCR models such as TextIn and PaddleOCR."
225,679d459debd8ffd557a2af4e,cs.CV,https://arxiv.org/pdf/2501.15547,Building Efficient Lightweight CNN Models,Nathan Isong,"Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning","Convolutional Neural Networks (CNNs) are pivotal in image classification tasks due to their robust feature extraction capabilities. However, their high computational and memory requirements pose challenges for deployment in resource-constrained environments. This paper introduces a methodology to construct lightweight CNNs while maintaining competitive accuracy. The approach integrates two stages of training; dual-input-output model and transfer learning with progressive unfreezing. The dual-input-output model train on original and augmented datasets, enhancing robustness. Progressive unfreezing is applied to the unified model to optimize pre-learned features during fine-tuning, enabling faster convergence and improved model accuracy."
226,679d459debd8ffd557a2af4f,cs.CV,https://arxiv.org/pdf/2501.15520,Efficient Self-Supervised Grading of Prostate Cancer Pathology,"Riddhasree Bhattacharyya, Surochita Pal Das, Sushmita Mitra",Computer Vision and Pattern Recognition,"Prostate cancer grading using the ISUP system (International Society of Urological Pathology) for treatment decisions is highly subjective and requires considerable expertise. Despite advances in computer-aided diagnosis systems, few have handled efficient ISUP grading on Whole Slide Images (WSIs) of prostate biopsies based only on slide-level labels. Some of the general challenges include managing gigapixel WSIs, obtaining patch-level annotations, and dealing with stain variability across centers. One of the main task-specific challenges faced by deep learning in ISUP grading, is the learning of patch-level features of Gleason patterns (GPs) based only on their slide labels. In this scenario, an efficient framework for ISUP grading is developed."
227,679d459debd8ffd557a2af50,cs.CV,https://arxiv.org/pdf/2501.15519,Fuzzy-aware Loss for Source-free Domain Adaptation in Visual Emotion Recognition,"Ying Zheng, Yiyi Zhang, Yi Wang, Lap-Pui Chau","Computer Vision and Pattern Recognition, Machine Learning","Source-free domain adaptation in visual emotion recognition (SFDA-VER) is a highly challenging task that requires adapting VER models to the target domain without relying on source data, which is of great significance for data privacy protection. However, due to the unignorable disparities between visual emotion data and traditional image classification data, existing SFDA methods perform poorly on this task. In this paper, we investigate the SFDA-VER task from a fuzzy perspective and identify two key issues: fuzzy emotion labels and fuzzy pseudo-labels. These issues arise from the inherent uncertainty of emotion annotations and the potential mispredictions in pseudo-labels. To address these issues, we propose a novel fuzzy-aware loss (FAL) to enable the VER model to better learn and adapt to new domains under fuzzy labels. Specifically, FAL modifies the standard cross entropy loss and focuses on adjusting the losses of non-predicted categories, which prevents a large number of uncertain or incorrect predictions from overwhelming the VER model during adaptation. In addition, we provide a theoretical analysis of FAL and prove its robustness in handling the noise in generated pseudo-labels. Extensive experiments on 26 domain adaptation sub-tasks across three benchmark datasets demonstrate the effectiveness of our method."
228,679d459debd8ffd557a2af51,cs.CV,https://arxiv.org/pdf/2501.15513,TinyLLaVA-Video: A Simple Framework of Small-scale Large Multimodal Models for Video Understanding,"Xingjian Zhang, Xi Weng, Yihao Yue, Zhaoxin Fan, Wenjun Wu, Lei Huang",Computer Vision and Pattern Recognition,"We present the TinyLLaVA-Video, a video understanding model with parameters not exceeding 4B that processes video sequences in a simple manner, without the need for complex architectures, supporting both fps sampling and uniform frame sampling. Our model is characterized by modularity and scalability, allowing training and inference with limited computational resources and enabling users to replace components based on their needs. We validate the effectiveness of this framework through experiments, the best model achieving performance comparable to certain existing 7B models on multiple video understanding benchmarks. The code and training recipes are fully open source, with all components and training data publicly available. We hope this work can serve as a baseline for practitioners exploring small-scale multimodal models for video understanding. It is available athttps://github.com/ZhangXJ199/TinyLLaVA-Video."
229,679d459debd8ffd557a2af52,cs.CV,https://arxiv.org/pdf/2501.15510,Universal Image Restoration Pre-training via Degradation Classification,"JiaKui Hu, Lujia Jin, Zhengjian Yao, Yanye Lu",Computer Vision and Pattern Recognition,"This paper proposes the Degradation Classification Pre-Training (DCPT), which enables models to learn how to classify the degradation type of input images for universal image restoration pre-training. Unlike the existing self-supervised pre-training methods, DCPT utilizes the degradation type of the input image as an extremely weak supervision, which can be effortlessly obtained, even intrinsic in all image restoration datasets. DCPT comprises two primary stages. Initially, image features are extracted from the encoder. Subsequently, a lightweight decoder, such as ResNet18, is leveraged to classify the degradation type of the input image solely based on the features extracted in the first stage, without utilizing the input image. The encoder is pre-trained with a straightforward yet potent DCPT, which is used to address universal image restoration and achieve outstanding performance. Following DCPT, both convolutional neural networks (CNNs) and transformers demonstrate performance improvements, with gains of up to 2.55 dB in the 10D all-in-one restoration task and 6.53 dB in the mixed degradation scenarios. Moreover, previous self-supervised pretraining methods, such as masked image modeling, discard the decoder after pre-training, while our DCPT utilizes the pre-trained parameters more effectively. This superiority arises from the degradation classifier acquired during DCPT, which facilitates transfer learning between models of identical architecture trained on diverse degradation types. Source code and models are available athttps://github.com/MILab-PKU/dcpt."
230,679d459debd8ffd557a2af53,cs.CV,https://arxiv.org/pdf/2501.15503,Domain Adaptation from Generated Multi-Weather Images for Unsupervised Maritime Object Classification,"Dan Song, Shumeng Huo, Wenhui Li, Lanjun Wang, Chao Xue, An-An Liu",Computer Vision and Pattern Recognition,"The classification and recognition of maritime objects are crucial for enhancing maritime safety, monitoring, and intelligent sea environment prediction.
However, existing unsupervised methods for maritime object classification often struggle with the long-tail data distributions in both object categories and weather conditions.
In this paper, we construct a dataset named AIMO produced by large-scale generative models with diverse weather conditions and balanced object categories, and collect a dataset named RMO with real-world images where long-tail issue exists.
We propose a novel domain adaptation approach that leverages AIMO (source domain) to address the problem of limited labeled data, unbalanced distribution and domain shift in RMO (target domain), and enhance the generalization of source features with the Vision-Language Models such as CLIP.
Experimental results shows that the proposed method significantly improves the classification accuracy, particularly for samples within rare object categories and weather conditions. Datasets and codes will be publicly available athttps://github.com/honoria0204/AIMO."
231,679d459debd8ffd557a2af54,cs.CV,https://arxiv.org/pdf/2501.15492,Color Flow Imaging Microscopy Improves Identification of Stress Sources of Protein Aggregates in Biopharmaceuticals,"Michaela Cohrs, Shiwoo Koak, Yejin Lee, Yu Jin Sung, Wesley De Neve, Hristo L. Svilenov, Utku Ozbulak","Computer Vision and Pattern Recognition, Artificial Intelligence","Protein-based therapeutics play a pivotal role in modern medicine targeting various diseases. Despite their therapeutic importance, these products can aggregate and form subvisible particles (SvPs), which can compromise their efficacy and trigger immunological responses, emphasizing the critical need for robust monitoring techniques. Flow Imaging Microscopy (FIM) has been a significant advancement in detecting SvPs, evolving from monochrome to more recently incorporating color imaging. Complementing SvP images obtained via FIM, deep learning techniques have recently been employed successfully for stress source identification of monochrome SvPs. In this study, we explore the potential of color FIM to enhance the characterization of stress sources in SvPs. To achieve this, we curate a new dataset comprising 16,000 SvPs from eight commercial monoclonal antibodies subjected to heat and mechanical stress. Using both supervised and self-supervised convolutional neural networks, as well as vision transformers in large-scale experiments, we demonstrate that deep learning with color FIM images consistently outperforms monochrome images, thus highlighting the potential of color FIM in stress source classification compared to its monochrome counterparts."
232,679d459debd8ffd557a2af55,cs.CV,https://arxiv.org/pdf/2501.15469,CISOL: An Open and Extensible Dataset for Table Structure Recognition in the Construction Industry,"David Tschirschwitz, Volker Rodehorst",Computer Vision and Pattern Recognition,"Reproducibility and replicability are critical pillars of empirical research, particularly in machine learning, where they depend not only on the availability of models, but also on the datasets used to train and evaluate those models. In this paper, we introduce the Construction Industry Steel Ordering List (CISOL) dataset, which was developed with a focus on transparency to ensure reproducibility, replicability, and extensibility. CISOL provides a valuable new research resource and highlights the importance of having diverse datasets, even in niche application domains such as table extraction in civil engineering."
233,679d459debd8ffd557a2af56,cs.CV,https://arxiv.org/pdf/2501.15464,TractoGPT: A GPT architecture for White Matter Segmentation,"Anoushkrit Goel, Simroop Singh, Ankita Joshi, Ranjeet Ranjan Jha, Chirag Ahuja, Aditya Nigam, Arnav Bhavsar","Computer Vision and Pattern Recognition, Artificial Intelligence","White matter bundle segmentation is crucial for studying brain structural connectivity, neurosurgical planning, and neurological disorders. White Matter Segmentation remains challenging due to structural similarity in streamlines, subject variability, symmetry in 2 hemispheres, etc. To address these challenges, we proposeTractoGPT, a GPT-based architecture trained on streamline, cluster, and fusion data representations separately. TractoGPT is a fully-automatic method that generalizes across datasets and retains shape information of the white matter bundles.
Experiments also show thatTractoGPToutperforms state-of-the-art methods on average DICE, Overlap and Overreach scores. We use TractoInferno and 105HCP datasets and validate generalization across datasets."
234,679d459debd8ffd557a2af57,cs.CV,https://arxiv.org/pdf/2501.15455,CD-Lamba: Boosting Remote Sensing Change Detection via a Cross-Temporal Locally Adaptive State Space Model,"Zhenkai Wu, Xiaowen Ma, Rongrong Lian, Kai Zheng, Mengting Ma, Wei Zhang, Siyang Song",Computer Vision and Pattern Recognition,"Mamba, with its advantages of global perception and linear complexity, has been widely applied to identify changes of the target regions within the remote sensing (RS) images captured under complex scenarios and varied conditions. However, existing remote sensing change detection (RSCD) approaches based on Mamba frequently struggle to effectively perceive the inherent locality of change regions as they direct flatten and scan RS images (i.e., the features of the same region of changes are not distributed continuously within the sequence but are mixed with features from other regions throughout the sequence). In this paper, we propose a novel locally adaptive SSM-based approach, termed CD-Lamba, which effectively enhances the locality of change detection while maintaining global perception. Specifically, our CD-Lamba includes a Locally Adaptive State-Space Scan (LASS) strategy for locality enhancement, a Cross-Temporal State-Space Scan (CTSS) strategy for bi-temporal feature fusion, and a Window Shifting and Perception (WSP) mechanism to enhance interactions across segmented windows.
These strategies are integrated into a multi-scale Cross-Temporal Locally Adaptive State-Space Scan (CT-LASS) module to effectively highlight changes and refine changes’ representations feature generation. CD-Lamba significantly enhances local-global spatio-temporal interactions in bi-temporal images, offering improved performance in RSCD tasks.
Extensive experimental results show that CD-Lamba achieves state-of-the-art performance on four benchmark datasets with a satisfactory efficiency-accuracy trade-off. Our code is publicly available athttps://github.com/xwmaxwma/rschange."
235,679d459debd8ffd557a2af58,cs.CV,https://arxiv.org/pdf/2501.15454,On the Discrimination and Consistency for Exemplar-Free Class Incremental Learning,"Tianqi Wang, Jingcai Guo, Depeng Li, Zhi Chen",Computer Vision and Pattern Recognition,"Exemplar-free class incremental learning (EF-CIL) is a nontrivial task that requires continuously enriching model capability with new classes while maintaining previously learned knowledge without storing and replaying any old class exemplars.
An emerging theory-guided framework for CIL trains task-specific models for a shared network, shifting the pressure of forgetting to task-id prediction.
In EF-CIL,
task-id prediction is more challenging due to the lack of inter-task interaction (e.g., replays of exemplars). To address this issue, we conduct a theoretical analysis of the importance and feasibility of preserving a discriminative and consistent feature space, upon which we propose a novel method termed DCNet.
Concretely, it progressively maps class representations into a hyperspherical space, in which different classes are orthogonally distributed to achieve ample inter-class separation. Meanwhile, it also introduces compensatory training to adaptively adjust supervision intensity, thereby aligning the degree of intra-class aggregation.
Extensive experiments and theoretical analysis verified the superiority of the proposed DCNet111Code:https://anonymous.4open.science/r/DCNet-70E9.."
236,679d459debd8ffd557a2af59,cs.CV,https://arxiv.org/pdf/2501.15452,Identifying Critical Tokens for Accurate Predictions in Transformer-based Medical Imaging Models,"Solha Kang, Joris Vankerschaver, Utku Ozbulak","Computer Vision and Pattern Recognition, Artificial Intelligence",
237,679d459debd8ffd557a2af5a,cs.CV,https://arxiv.org/pdf/2501.15449,Breaking the SSL-AL Barrier: A Synergistic Semi-Supervised Active Learning Framework for 3D Object Detection,"Zengran Wang, Yanan Zhang, Jiaxin Chen, Di Huang",Computer Vision and Pattern Recognition,"To address the annotation burden in LiDAR-based 3D object detection, active learning (AL) methods offer a promising solution. However, traditional active learning approaches solely rely on a small amount of labeled data to train an initial model for data selection, overlooking the potential of leveraging the abundance of unlabeled data. Recently, attempts to integrate semi-supervised learning (SSL) into AL with the goal of leveraging unlabeled data have faced challenges in effectively resolving the conflict between the two paradigms, resulting in less satisfactory performance. To tackle this conflict, we propose aSynergisticSemi-SupervisedActiveLearning framework, dubbed as S-SSAL. Specifically, from the perspective of SSL, we propose a Collaborative PseudoScene Pre-training (CPSP) method that effectively learns from unlabeled data without introducing adverse effects. From the perspective of AL, we design a Collaborative Active Learning (CAL) method, which complements the uncertainty and diversity methods by model cascading. This allows us to fully exploit the potential of the CPSP pre-trained model. Extensive experiments conducted on KITTI and Waymo demonstrate the effectiveness of our S-SSAL framework. Notably, on the KITTI dataset, utilizing only 2% labeled data, S-SSAL can achieve performance comparable to models trained on the full dataset."
238,679d459debd8ffd557a2af5b,cs.CV,https://arxiv.org/pdf/2501.15448,SQ-DM: Accelerating Diffusion Models with Aggressive Quantization and Temporal Sparsity,"Zichen Fan, Steve Dai, Rangharajan Venkatesan, Dennis Sylvester, Brucek Khailany","Computer Vision and Pattern Recognition, Artificial Intelligence, Hardware Architecture, Machine Learning",
239,679d459debd8ffd557a2af5c,cs.CV,https://arxiv.org/pdf/2501.15445,StochSync: Stochastic Diffusion Synchronization for Image Generation in Arbitrary Spaces,"Kyeongmin Yeo, Jaihoon Kim, Minhyuk Sung","Computer Vision and Pattern Recognition, Artificial Intelligence","We propose a zero-shot method for generating images in arbitrary spaces (e.g., a sphere for360∘superscript360360^{\circ}360 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPTpanoramas and a mesh surface for texture) using a pretrained image diffusion model. The zero-shot generation of various visual content using a pretrained image diffusion model has been explored mainly in two directions. First, Diffusion Synchronization–performing reverse diffusion processes jointly across different projected spaces while synchronizing them in the target space–generates high-quality outputs when enough conditioning is provided, but it struggles in its absence. Second, Score Distillation Sampling–gradually updating the target space data through gradient descent–results in better coherence but often lacks detail. In this paper, we reveal for the first time the interconnection between these two methods while highlighting their differences. To this end, we proposeStochSync, a novel approach that combines the strengths of both, enabling effective performance with weak conditioning. Our experiments demonstrate thatStochSyncprovides the best performance in360∘superscript360360^{\circ}360 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPTpanorama generation (where image conditioning is not given), outperforming previous finetuning-based methods, and also delivers comparable results in 3D mesh texturing (where depth conditioning is provided) with previous methods. Project page is athttps://stochsync.github.io/."
240,679d459debd8ffd557a2af5d,cs.CV,https://arxiv.org/pdf/2501.15443,InfoBFR: Real-World Blind Face Restoration via Information Bottleneck,"Nan Gao, Jia Li, Huaibo Huang, Ke Shang, Ran He",Computer Vision and Pattern Recognition,"Blind face restoration (BFR) is a highly challenging problem due to the uncertainty of data degradation patterns. Current BFR methods have realized certain restored productions but with inherent neural degradations that limit real-world generalization in complicated scenarios. In this paper, we propose a plug-and-play frameworkInfoBFRto tackle neural degradations, e.g., prior bias, topological distortion, textural distortion, and artifact residues, which achieves high-generalization face restoration in diverse wild and heterogeneous scenes. Specifically, based on the results from pre-trained BFR models, InfoBFR considers information compression using manifold information bottleneck (MIB) and information compensation with efficient diffusion LoRA to conduct information optimization. InfoBFR effectively synthesizes high-fidelity faces without attribute and identity distortions. Comprehensive experimental results demonstrate the superiority of InfoBFR over state-of-the-art GAN-based and diffusion-based BFR methods, with around 70ms consumption, 16M trainable parameters, and nearly85%BFR-boosting. It is promising that InfoBFR will be the first plug-and-play restorer universally employed by diverse BFR models to conquer neural degradations."
241,679d459debd8ffd557a2af5e,cs.CV,https://arxiv.org/pdf/2501.15440,Dfilled: Repurposing Edge-Enhancing Diffusion for Guided DSM Void Filling,"Daniel Panangian, Ksenia Bittner",Computer Vision and Pattern Recognition,"Digital Surface Models (DSMs)are essential for accurately representing Earth’s topography in geospatial analyses.DSMscapture detailed elevations of natural and man-made features, crucial for applications like urban planning, vegetation studies, and 3D reconstruction. However,DSMsderived from stereo satellite imagery often contain voids or missing data due to occlusions, shadows, and low-signal areas. Previous studies have primarily focused on void filling fordigital elevation models (DEMs)andDigital Terrain Models (DTMs), employing methods such asinverse distance weighting (IDW), kriging, and spline interpolation. While effective for simpler terrains, these approaches often fail to handle the intricate structures present inDSMs. To overcome these limitations, we introduceDfilled, a guidedDSMvoid filling method that leverages optical remote sensing images through edge-enhancing diffusion. Dfilled repurposes deep anisotropic diffusion models, which originally designed for super-resolution tasks, to inpaintDSMs. Additionally, we utilize Perlin noise to create inpainting masks that mimic natural void patterns inDSMs.
Experimental evaluations demonstrate that Dfilled surpasses traditional interpolation methods and deep learning approaches inDSMvoid filling tasks. Both quantitative and qualitative assessments highlight the method’s ability to manage complex features and deliver accurate, visually coherent results."
242,679d459debd8ffd557a2af5f,cs.CV,https://arxiv.org/pdf/2501.15438,Cross-Modal Transfer from Memes to Videos: Addressing Data Scarcity in Hateful Video Detection,"Han Wang, Rui Yang Tan, Roy Ka-Wei Lee","Computer Vision and Pattern Recognition, Multimedia","Detecting hate speech in online content is essential to ensuring safer digital spaces. While significant progress has been made in text and meme modalities, video-based hate speech detection remains under-explored, hindered by a lack of annotated datasets and the high cost of video annotation. This gap is particularly problematic given the growing reliance on large models, which demand substantial amounts of training data. To address this challenge, we leverage meme datasets as both a substitution and an augmentation strategy for training hateful video detection models. Our approach introduces a human-assisted reannotation pipeline to align meme dataset labels with video datasets, ensuring consistency with minimal labeling effort. Using two state-of-the-art vision-language models, we demonstrate that meme data can substitute for video data in resource-scarce scenarios and augment video datasets to achieve further performance gains. Our results consistently outperform state-of-the-art benchmarks, showcasing the potential of cross-modal transfer learning for advancing hateful video detection111Dataset and code available athttps://github.com/Social-AI-Studio/CrossModalTransferLearning.."
243,679d459debd8ffd557a2af60,cs.CV,https://arxiv.org/pdf/2501.15434,Mitigating Spurious Negative Pairs for Robust Industrial Anomaly Detection,"Hossein Mirzaei, Mojtaba Nafez, Jafar Habibi, Mohammad Sabokrou, Mohammad Hossein Rohban",Computer Vision and Pattern Recognition,"Despite significant progress in Anomaly Detection (AD), the robustness of existing detection methods against adversarial attacks remains a challenge, compromising their reliability in critical real-world applications such as autonomous driving. This issue primarily arises from the AD setup, which assumes that training data is limited to a group of unlabeled normal samples, making the detectors vulnerable to adversarial anomaly samples during testing. Additionally, implementing adversarial training as a safeguard encounters difficulties, such as formulating an effective objective function without access to labels. An ideal objective function for adversarial training in AD should promote strong perturbations both within and between the normal and anomaly groups to maximize margin between normal and anomaly distribution. To address these issues, we first propose crafting a pseudo-anomaly group derived from normal group samples. Then, we demonstrate that adversarial training with contrastive loss could serve as an ideal objective function, as it creates both inter- and intra-group perturbations. However, we notice that spurious negative pairs compromise the conventional contrastive loss to achieve robust AD. Spurious negative pairs are those that should be closely mapped but are erroneously separated. These pairs introduce noise and misguide the direction of inter-group adversarial perturbations. To overcome the effect of spurious negative pairs, we define opposite pairs and adversarially pull them apart to strengthen inter-group perturbations. Experimental results demonstrate our superior performance in both clean and adversarial scenarios, with a26.1%improvement in robust detection across various challenging benchmark datasets. The implementation of our work is available at:https://github.com/rohban-lab/COBRA."
244,679d459debd8ffd557a2af61,cs.CV,https://arxiv.org/pdf/2501.15431,Self-supervised Benchmark Lottery on ImageNet: Do Marginal Improvements Translate to Improvements on Similar Datasets?,"Utku Ozbulak, Esla Timothy Anzaku, Solha Kang, Wesley De Neve, Joris Vankerschaver","Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning","Machine learning (ML) research strongly relies on benchmarks in order to determine the relative effectiveness of newly proposed models. Recently, a number of prominent research effort argued that a number of models that improve the state-of-the-art by a small margin tend to do so by winning what they call a\saybenchmark lottery. An important benchmark in the field of machine learning and computer vision is the ImageNet where newly proposed models are often showcased based on their performance on this dataset. Given the large number of self-supervised learning (SSL) frameworks that has been proposed in the past couple of years each coming with marginal improvements on the ImageNet dataset, in this work, we evaluate whether those marginal improvements on ImageNet translate to improvements on similar datasets or not. To do so, we investigate twelve popular SSL frameworks on five ImageNet variants and discover that models that seem to perform well on ImageNet may experience significant performance declines on similar datasets. Specifically, state-of-the-art frameworks such asDINOandSwav, which are praised for their performance, exhibit substantial drops in performance whileMoCoandBarlow Twinsdisplays comparatively good results. As a result, we argue that otherwise good and desirable properties of models remain hidden when benchmarking is only performed on the ImageNet validation set, making us call for more adequate benchmarking. To avoid the\saybenchmark lottery on ImageNet and to ensure a fair benchmarking process, we investigate the usage of a unified metric that takes into account the performance of models on other ImageNet variant datasets."
245,679d459debd8ffd557a2af62,cs.CV,https://arxiv.org/pdf/2501.15420,Visual Generation Without Guidance,"Huayu Chen, Kai Jiang, Kaiwen Zheng, Jianfei Chen, Hang Su, Jun Zhu","Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning","Classifier-Free Guidance (CFG) has been a default technique in various visual generative models, yet it requires inference from both conditional and unconditional models during sampling.
We propose to build visual models that are free from guided sampling. The resulting algorithm, Guidance-Free Training (GFT), matches the performance of CFG while reducing sampling to a single model, halving the computational cost.
Unlike previous distillation-based approaches that rely on pretrained CFG networks, GFT enables training directly from scratch.
GFT is simple to implement. It retains the same maximum likelihood objective as CFG and differs mainly in the parameterization of conditional models.
Implementing GFT requires only minimal modifications to existing codebases, as most design choices and hyperparameters are directly inherited from CFG.
Our extensive experiments across five distinct visual models demonstrate the effectiveness and versatility of GFT. Across domains of diffusion, autoregressive, and masked-prediction modeling, GFT consistently achieves comparable or even lower FID scores, with similar diversity-fidelity trade-offs compared with CFG baselines, all while being guidance-free. Code will be available athttps://github.com/thu-ml/GFT."
246,679d459debd8ffd557a2af63,cs.CV,https://arxiv.org/pdf/2501.15415,OCSU: Optical Chemical Structure Understanding for Molecule-centric Scientific Discovery,"Siqi Fan, Yuguang Xie, Bowen Cai, Ailin Xie, Gaochao Liu, Mu Qiao, Jie Xing, Zaiqing Nie",Computer Vision and Pattern Recognition,"Understanding the chemical structure from a graphical representation of a molecule is a challenging image caption task that would greatly benefit molecule-centric scientific discovery. Variations in molecular images and caption subtasks pose a significant challenge in both image representation learning and task modeling. Yet, existing methods only focus on a specific caption task that translates a molecular image into its graph structure, i.e. OCSR. In this paper, we propose theOptical Chemical Structure Understanding (OCSU)task, which extends OCSR to molecular image caption from motif level to molecule level and abstract level. We present two approaches for that, including an OCSR-based method and an end-to-end OCSR-free method. The proposedDouble-Checkachieves SOTA OCSR performance on real-world patent and journal article scenarios via attentive feature enhancement for local ambiguous atoms. Cascading with SMILES-based molecule understanding methods, it can leverage the power of existing task-specific models for OCSU. WhileMol-VLis an end-to-end optimized VLM-based model. An OCSU dataset,Vis-CheBI20, is built based on the widely used CheBI20 dataset for training and evaluation. Extensive experimental results on Vis-CheBI20 demonstrate the effectiveness of the proposed approaches. Improving OCSR capability can lead to a better OCSU performance for OCSR-based approach, and the SOTA performance of Mol-VL demonstrates the great potential of end-to-end approach."
247,679d459debd8ffd557a2af64,cs.CV,https://arxiv.org/pdf/2501.15409,TdAttenMix: Top-Down Attention Guided Mixup,"Zhiming Wang, Lin Gu, Feng Lu","Computer Vision and Pattern Recognition, Artificial Intelligence","CutMix is a data augmentation strategy that cuts and pastes image patches to mixup training data. Existing methods pick either random or salient areas which are often inconsistent to labels, thus misguiding the training model. By our knowledge, we integrate human gaze to guide cutmix for the first time. Since human attention is driven by both high-level recognition and low-level clues, we propose a controllable Top-down Attention Guided Module to obtain a general artificial attention which balances top-down and bottom-up attention. The proposed TdATttenMix then picks the patches and adjust the label mixing ratio that focuses on regions relevant to the current label. Experimental results demonstrate that our TdAttenMix outperforms existing state-of-the-art mixup methods across eight different benchmarks. Additionally, we introduce a new metric based on the human gaze and use this metric to investigate the issue of image-label inconsistency."
248,679d459debd8ffd557a2af65,cs.CV,https://arxiv.org/pdf/2501.15407,Turn That Frown Upside Down: FaceID Customization via Cross-Training Data,"Shuhe Wang, Xiaoya Li, Xiaofei Sun, Guoyin Wang, Tianwei Zhang, Jiwei Li, Eduard Hovy","Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning","Existing face identity (FaceID) customization methods perform well but are limited to generating identical faces as the input, while in real-world applications, users often desire images of the same person but with variations, such as different expressions (e.g., smiling, angry) or angles (e.g., side profile). This limitation arises from the lack of datasets with controlled input-output facial variations, restricting models’ ability to learn effective modifications."
249,679d459debd8ffd557a2af66,cs.CV,https://arxiv.org/pdf/2501.15394,Doracamom: Joint 3D Detection and Occupancy Prediction with Multi-view 4D Radars and Cameras for Omnidirectional Perception,"Lianqing Zheng, Jianan Liu, Runwei Guan, Long Yang, Shouyi Lu, Yuanzhe Li, Xiaokai Bai, Jie Bai, Zhixiong Ma, Hui-Liang Shen, Xichan Zhu",Computer Vision and Pattern Recognition,"3D object detection and occupancy prediction are critical tasks in autonomous driving, attracting significant attention. Despite the potential of recent vision-based methods, they encounter challenges under adverse conditions. Thus, integrating cameras with next-generation 4D imaging radar to achieve unified multi-task perception is highly significant, though research in this domain remains limited. In this paper, we propose Doracamom, the first framework that fuses multi-view cameras and 4D radar for joint 3D object detection and semantic occupancy prediction, enabling comprehensive environmental perception. Specifically, we introduce a novelCoarse Voxel Queries Generatorthat integrates geometric priors from 4D radar with semantic features from images to initialize voxel queries, establishing a robust foundation for subsequent Transformer-based refinement. To leverage temporal information, we design aDual-Branch Temporal Encoderthat processes multi-modal temporal features in parallel across BEV and voxel spaces, enabling comprehensive spatio-temporal representation learning. Furthermore, we propose aCross-Modal BEV-Voxel Fusionmodule that adaptively fuses complementary features through attention mechanisms while employing auxiliary tasks to enhance feature quality. Extensive experiments on the OmniHD-Scenes, View-of-Delft (VoD), and TJ4DRadSet datasets demonstrate that Doracamom achieves state-of-the-art performance in both tasks, establishing new benchmarks for multi-modal 3D perception. Code and models will be publicly available."
250,679d459debd8ffd557a2af67,cs.CV,https://arxiv.org/pdf/2501.15389,CP2M: Clustered-Patch-Mixed Mosaic Augmentation for Aerial Image Segmentation,"Yijie Li, Hewei Wang, Jinfeng Xu, Zixiao Ma, Puzhen Wu, Shaofan Wang, Soumyabrata Dev",Computer Vision and Pattern Recognition,"Remote sensing image segmentation is pivotal for earth observation, underpinning applications such as environmental monitoring and urban planning. Due to the limited annotation data available in remote sensing images, numerous studies have focused on data augmentation as a means to alleviate overfitting in deep learning networks. However, some existing data augmentation strategies rely on simple transformations that may not sufficiently enhance data diversity or model generalization capabilities. This paper proposes a novel augmentation strategy, Clustered-Patch-Mixed Mosaic (CP2M), designed to address these limitations. CP2M integrates a Mosaic augmentation phase with a clustered patch mix phase. The former stage constructs a new sample from four random samples, while the latter phase uses the connected component labeling algorithm to ensure the augmented data maintains spatial coherence and avoids introducing irrelevant semantics when pasting random patches. Our experiments on the ISPRS Potsdam dataset demonstrate that CP2M substantially mitigates overfitting, setting new benchmarks for segmentation accuracy and model robustness in remote sensing tasks. In the spirit of reproducible research, the code, dataset, and experimental results are publicly available at:https://github.com/Att100/CP2M."
251,679d459debd8ffd557a2af68,cs.CV,https://arxiv.org/pdf/2501.15385,DDUNet: Dual Dynamic U-Net for Highly-Efficient Cloud Segmentation,"Yijie Li, Hewei Wang, Jinfeng Xu, Puzhen Wu, Yunzhong Xiao, Shaofan Wang, Soumyabrata Dev","Computer Vision and Pattern Recognition, Image and Video Processing","Cloud segmentation amounts to separating cloud pixels from non-cloud pixels in an image. Current deep learning methods for cloud segmentation suffer from three issues. (a) Constrain on their receptive field due to the fixed size of the convolution kernel. (b) Lack of robustness towards different scenarios. (c) Requirement of a large number of parameters and limitations for real-time implementation. To address these issues, we propose a Dual Dynamic U-Net (DDUNet) for supervised cloud segmentation. The DDUNet adheres to a U-Net architecture and integrates two crucial modules: the dynamic multi-scale convolution (DMSC), improving merging features under different reception fields, and the dynamic weights and bias generator (DWBG) in classification layers to enhance generalization ability. More importantly, owing to the use of depth-wise convolution, the DDUNet is a lightweight network that can achieve 95.3% accuracy on the SWINySEG dataset with only 0.33M parameters, and achieve superior performance over three different configurations of the SWINySEg dataset in both accuracy and efficiency. Our code is publicly available at:https://github.com/Att100/DDUNet."
252,679d459debd8ffd557a2af69,cs.CV,https://arxiv.org/pdf/2501.15384,MetaOcc: Surround-View 4D Radar and Camera Fusion Framework for 3D Occupancy Prediction with Dual Training Strategies,"Long Yang, Lianqing Zheng, Wenjin Ai, Minghao Liu, Sen Li, Qunshu Lin, Shengyu Yan, Jie Bai, Zhixiong Ma, Xichan Zhu","Computer Vision and Pattern Recognition, Artificial Intelligence","3D occupancy prediction is crucial for autonomous driving perception. Fusion of 4D radar and camera provides a potential solution of robust occupancy prediction on serve weather with least cost. How to achieve effective multi-modal feature fusion and reduce annotation costs remains significant challenges. In this work, we propose MetaOcc, a novel multi-modal occupancy prediction framework that fuses surround-view cameras and 4D radar for comprehensive environmental perception. We first design a height self-attention module for effective 3D feature extraction from sparse radar points. Then, a local-global fusion mechanism is proposed to adaptively capture modality contributions while handling spatio-temporal misalignments. Temporal alignment and fusion module is employed to further aggregate historical feature. Furthermore, we develop a semi-supervised training procedure leveraging open-set segmentor and geometric constraints for pseudo-label generation, enabling robust perception with limited annotations. Extensive experiments on OmniHD-Scenes dataset demonstrate that MetaOcc achieves state-of-the-art performance, surpassing previous methods by significant margins. Notably, as the first semi-supervised 4D radar and camera fusion-based occupancy prediction approach, MetaOcc maintains 92.5% of the fully-supervised performance while using only 50% of ground truth annotations, establishing a new benchmark for multi-modal 3D occupancy prediction. Code and data are available athttps://github.com/LucasYang567/MetaOcc."
253,679d459debd8ffd557a2af6a,cs.CV,https://arxiv.org/pdf/2501.15377,Fine Tuning without Catastrophic Forgetting via Selective Low Rank Adaptation,"Reza Akbarian Bafghi, Carden Bagwell, Avinash Ravichandran, Ashish Shrivastava, Maziar Raissi",Computer Vision and Pattern Recognition,"Adapting deep learning models to new domains often requires computationally intensive retraining and risks catastrophic forgetting. While fine-tuning enables domain-specific adaptation, it can reduce robustness to distribution shifts, impacting out-of-distribution (OOD) performance. Pre-trained zero-shot models like CLIP offer strong generalization but may suffer degraded robustness after fine-tuning. Building on Task Adaptive Parameter Sharing (TAPS), we propose a simple yet effective extension as a parameter-efficient fine-tuning (PEFT) method, using an indicator function to selectively activate Low-Rank Adaptation (LoRA) blocks. Our approach minimizes knowledge loss, retains its generalization strengths under domain shifts, and significantly reduces computational costs compared to traditional fine-tuning. We demonstrate that effective fine-tuning can be achieved with as few as 5% of active blocks, substantially improving efficiency. Evaluations on pre-trained models such as CLIP and DINO-ViT demonstrate our method’s broad applicability and effectiveness in maintaining performance and knowledge retention."
254,679d459debd8ffd557a2af6b,cs.CV,https://arxiv.org/pdf/2501.15371,Acquiring Submillimeter-Accurate Multi-Task Vision Datasets for Computer-Assisted Orthopedic Surgery,"Emma Most, Jonas Hein, Frédéric Giraud, Nicola A. Cavalcanti, Lukas Zingg, Baptiste Brument, Nino Louman, Fabio Carrillo, Philipp Fürnstahl, Lilian Calvet",Computer Vision and Pattern Recognition,"Purpose:Advances in computer vision, particularly inoptical image-based3D reconstruction and feature matching, enable applications like marker-less surgical navigation and digitization of surgery. However, their development is hindered by a lack of suitable datasetswith 3D ground truth. This work explores an approach to generating realistic and accurateex vivodatasets tailored for 3D reconstruction and feature matching in open orthopedic surgery.Methods:A set of posed images and an accurately registered ground truthsurface meshof the scene are required to develop vision-based 3D reconstruction and matching methods suitable for surgery. We propose a framework consisting of three core steps and compare different methods for each step: 3D scanning, calibration of viewpoints for a set of high-resolution RGB images, and an optical-based method for scene registration.Results:We evaluate each step of this framework on anex vivoscoliosis surgery using a pig spine, conducted under real operating room conditions.A mean 3D Euclidean error of 0.35 mm is achieved with respect to the 3D ground truth.Conclusion:The proposed method results in submillimeter accurate 3D ground truths and surgical images with a spatial resolution of 0.1 mm. This opens the door to acquiring future surgical datasets for high-precision applications."
255,679d459debd8ffd557a2af6c,cs.CV,https://arxiv.org/pdf/2501.15370,Scaling Large Vision-Language Models for Enhanced Multimodal Comprehension In Biomedical Image Analysis,"Robinson Umeike, Neil Getty, Fangfang Xia, Rick Stevens","Computer Vision and Pattern Recognition, Artificial Intelligence",
256,679d459debd8ffd557a2af6d,cs.CV,https://arxiv.org/pdf/2501.15369,iFormer: Integrating ConvNet and Transformer for Mobile Application,Chuanyang Zheng,"Computer Vision and Pattern Recognition, Artificial Intelligence","We present a new family of mobile hybrid vision networks, called iFormer,
with a focus on optimizing latency and accuracy on mobile applications. iFormer effectively integrates the fast local representation capacity of convolution with the efficient global modeling ability of self-attention. The local interactions are derived from transforming a standard convolutional network,i.e., ConvNeXt, to design a more lightweight mobile network. Our newly introduced mobile modulation attention removes memory-intensive operations in MHA and employs an efficient modulation mechanism to boost dynamic global representational capacity. We conduct comprehensive experiments demonstrating that iFormer outperforms existing lightweight networks across various tasks.
Notably, iFormer achieves an impressive Top-1 accuracy of 80.4% on ImageNet-1k with a latency of only 1.10 ms on an iPhone 13,
surpassing the recently proposed MobileNetV4 under similar latency constraints. Additionally, our method shows significant improvements in downstream tasks, including COCO object detection, instance segmentation, and ADE20k semantic segmentation, while still maintaining low latency on mobile devices for high-resolution inputs in these scenarios."
257,679d459debd8ffd557a2af6e,cs.CV,https://arxiv.org/pdf/2501.15326,Recognize Any Surgical Object: Unleashing the Power of Weakly-Supervised Data,"Jiajie Li, Brian R Quaranto, Chenhui Xu, Ishan Mishra, Ruiyang Qin, Dancheng Liu, Peter C W Kim, Jinjun Xiong",Computer Vision and Pattern Recognition,"We present RASO, a foundation model designed toRecognizeAnySurgicalObject, offering robust open-set recognition capabilities across a broad range of surgical procedures and object classes, in both surgical images and videos. RASO leverages a novel weakly-supervised learning framework that generates tag-image-text pairs automatically from large-scale unannotated surgical lecture videos, significantly reducing the need for manual annotations. Our scalable data generation pipeline gatherers to 2,200 surgical procedures and produces 3.6 million tag annotations across 2,066 unique surgical tags. Our experiments show that RASO achieves improvements of 2.9 mAP, 4.5 mAP, 10.6 mAP, and 7.2 mAP on four standard surgical benchmarks respectively in zero-shot settings, and surpasses state-of-the-art models in supervised surgical action recognition tasks. We will open-source our code, model, and dataset to facilitate further research."
258,679d459debd8ffd557a2af6f,cs.CV,https://arxiv.org/pdf/2501.15286,Efficient Point Clouds Upsampling via Flow Matching,"Zhi-Song Liu, Chenhang He, Lei Li","Computer Vision and Pattern Recognition, Signal Processing","Diffusion models are a powerful framework for tackling ill-posed problems, with recent advancements extending their use to point cloud upsampling. Despite their potential, existing diffusion models struggle with inefficiencies as they map Gaussian noise to real point clouds, overlooking the geometric information inherent in sparse point clouds. To address these inefficiencies, we propose PUFM, a flow matching approach to directly map sparse point clouds to their high-fidelity dense counterparts. Our method first employs midpoint interpolation to sparse point clouds, resolving the density mismatch between sparse and dense point clouds. Since point clouds are unordered representations, we introduce a pre-alignment method based on Earth Mover’s Distance (EMD) optimization to ensure coherent interpolation between sparse and dense point clouds, which enables a more stable learning path in flow matching. Experiments on synthetic datasets demonstrate that our method delivers superior upsampling quality but with fewer sampling steps. Further experiments on ScanNet and KITTI also show that our approach generalizes well on RGB-D point clouds and LiDAR point clouds, making it more practical for real-world applications."
259,679d459debd8ffd557a2af70,cs.CV,https://arxiv.org/pdf/2501.15263,Explainable YOLO-Based Dyslexia Detection in Synthetic Handwriting Data,Nora Fink,"Computer Vision and Pattern Recognition, Machine Learning",
260,679d459debd8ffd557a2af71,cs.CV,https://arxiv.org/pdf/2501.15262,Dynamic Estimation of Tea Flowering Based on an Improved YOLOv5 and ANN Model,"Qianxi Mi, Pengcheng Yuan, Chunlei Ma, Jiedan Chen, Mingzhe Yao","Computer Vision and Pattern Recognition, Quantitative Methods",
261,679d459debd8ffd557a2af72,cs.CV,https://arxiv.org/pdf/2501.15257,Pre-trained Model Guided Mixture Knowledge Distillation for Adversarial Federated Learning,"Yu Qiao, Huy Q. Le, Apurba Adhikary, Choong Seon Hong",Computer Vision and Pattern Recognition,"This paper aims to improve the robustness of a small global model while maintaining clean accuracy under adversarial attacks and non-IID challenges in federated learning. By leveraging the concise knowledge embedded in the class probabilities from a pre-trained model for both clean and adversarial image classification, we propose a Pre-trained Model-guided Adversarial Federated Learning (PM-AFL) training paradigm. This paradigm integrates vanilla mixture and adversarial mixture knowledge distillation to effectively balance accuracy and robustness while promoting local models to learn from diverse data. Specifically, for clean accuracy, we adopt a dual distillation strategy where the class probabilities of randomly paired images and their blended versions are aligned between the teacher model and the local models. For adversarial robustness, we use a similar distillation approach but replace clean samples on the local side with adversarial examples. Moreover, considering the bias between local and global models, we also incorporate a consistency regularization term to ensure that local adversarial predictions stay aligned with their corresponding global clean ones. These strategies collectively enable local models to absorb diverse knowledge from the teacher model while maintaining close alignment with the global model, thereby mitigating overfitting to local optima and enhancing the generalization of the global model. Experiments demonstrate that the PM-AFL-based paradigm outperforms other methods that integrate defense strategies by a notable margin."
262,679d459debd8ffd557a2af73,cs.CV,https://arxiv.org/pdf/2501.15253,Generalizable Deepfake Detection via Effective Local-Global Feature Extraction,"Jiazhen Yan, Ziqiang Li, Ziwen He, Zhangjie Fu",Computer Vision and Pattern Recognition,"The rapid advancement of GANs and diffusion models has led to the generation of increasingly realistic fake images, posing significant hidden dangers and threats to society. Consequently, deepfake detection has become a pressing issue in today’s world. While some existing methods focus on forgery features from either a local or global perspective, they often overlook the complementary nature of these features. Other approaches attempt to incorporate both local and global features but rely on simplistic strategies, such as cropping, which fail to capture the intricate relationships between local features. To address these limitations, we propose a novel method that effectively combines local spatial-frequency domain features with global frequency domain information, capturing detailed and holistic forgery traces. Specifically, our method uses Discrete Wavelet Transform (DWT) and sliding windows to tile forged features and leverages attention mechanisms to extract local spatial-frequency domain information. Simultaneously, the phase component of the Fast Fourier Transform (FFT) is integrated with attention mechanisms to extract global frequency domain information, complementing the local features and ensuring the integrity of forgery detection. Comprehensive evaluations on open-world datasets generated by34 distinct generative modelsdemonstrate a significant improvement of2.9%over existing state-of-the-art methods."
263,679d459debd8ffd557a2af74,cs.CV,https://arxiv.org/pdf/2501.15248,Enhancing Fetal Plane Classification Accuracy with Data Augmentation Using Diffusion Models,"Yueying Tian, Elif Ucurum, Xudong Han, Rupert Young, Chris Chatwin, Philip Birch",Computer Vision and Pattern Recognition,"Ultrasound imaging is widely used in medical diagnosis, especially for fetal health assessment. However, the availability of high-quality annotated ultrasound images is limited, which restricts the training of machine learning models. In this paper, we investigate the use of diffusion models to generate synthetic ultrasound images to improve the performance on fetal plane classification. We train different classifiers first on synthetic images and then fine-tune them with real images. Extensive experimental results demonstrate that incorporating generated images into training pipelines leads to better classification accuracy than training with real images alone. The findings suggest that generating synthetic data using diffusion models can be a valuable tool in overcoming the challenges of data scarcity in ultrasound medical imaging."
264,679d459debd8ffd557a2af75,cs.CV,https://arxiv.org/pdf/2501.15211,"""Stones from Other Hills can Polish Jade"": Zero-shot Anomaly Image Synthesis via Cross-domain Anomaly Injection","Siqi Wang, Yuanze Hu, Xinwang Liu, Siwei Wang, Guangpu Wang, Chuanfu Xu, Jie Liu, Ping Chen",Computer Vision and Pattern Recognition,"Industrial image anomaly detection (IAD) is a pivotal topic with huge value. Due to anomaly’s nature, real anomalies in a specific modern industrial domain (i.e. domain-specific anomalies) are usually too rare to collect, which severely hinders IAD. Thus, zero-shot anomaly synthesis (ZSAS), which synthesizes pseudo anomaly images without any domain-specific anomaly, emerges as a vital technique for IAD. However, existing solutions are either unable to synthesize authentic pseudo anomalies, or require cumbersome training. Thus, we focus on ZSAS and propose a brand-new paradigm that can realize both authentic and training-free ZSAS. It is based on a chronically-ignored fact:Although domain-specific anomalies are rare, real anomalies from other domains (i.e. cross-domain anomalies) are actually abundant and directly applicable to ZSAS.Specifically, our new ZSAS paradigm makes three-fold contributions: First, we propose a novel method named Cross-domain Anomaly Injection (CAI), which directly exploits cross-domain anomalies to enable highly authentic ZSAS in a training-free manner. Second, to supply CAI with sufficient cross-domain anomalies, we build the first domain-agnostic anomaly dataset within our best knowledge, which provides ZSAS with abundant real anomaly patterns. Third, we propose a CAI-guided Diffusion Mechanism, which further breaks the quantity limit of real anomalies and enable unlimited anomaly synthesis. Our head-to-head comparison with existing ZSAS solutions justifies our paradigm’s superior performance for IAD and demonstrates it as an effective and pragmatic ZSAS solution."
265,679d459debd8ffd557a2af76,cs.CV,https://arxiv.org/pdf/2501.15201,A Training-free Synthetic Data Selection Method for Semantic Segmentation,"Hao Tang, Siyue Yu, Jian Pang, Bingfeng Zhang",Computer Vision and Pattern Recognition,"Training semantic segmenter with synthetic data has been attracting great attention due to its easy accessibility and huge quantities. Most previous methods focused on producing large-scale synthetic image-annotation samples and then training the segmenter with all of them. However, such a solution remains a main challenge in that the poor-quality samples are unavoidable, and using them to train the model will damage the training process. In this paper, we propose a training-free Synthetic Data Selection (SDS) strategy with CLIP to select high-quality samples for building a reliable synthetic dataset. Specifically, given massive synthetic image-annotation pairs, we first design a Perturbation-based CLIP Similarity (PCS) to measure the reliability of synthetic image, thus removing samples with low-quality images. Then we propose a class-balance Annotation Similarity Filter (ASF) by comparing the synthetic annotation with the response of CLIP to remove the samples related to low-quality annotations. The experimental results show that using our method significantly reduces the data size by half, while the trained segmenter achieves higher performance."
266,679d459debd8ffd557a2af77,cs.CV,https://arxiv.org/pdf/2501.15187,Uni-Sign: Toward Unified Sign Language Understanding at Scale,"Zecheng Li, Wengang Zhou, Weichao Zhao, Kepeng Wu, Hezhen Hu, Houqiang Li",Computer Vision and Pattern Recognition,
267,679d459debd8ffd557a2af78,cs.CV,https://arxiv.org/pdf/2501.15167,Enhancing Intent Understanding for Ambiguous Prompts through Human-Machine Co-Adaptation,"Yangfan He, Jianhui Wang, Kun Li, Yijin Wang, Li Sun, Jun Yin, Miao Zhang, Xueqian Wang",Computer Vision and Pattern Recognition,"Modern image generation systems have demonstrated the ability to produce realistic and high-quality visuals. However, user prompts often contain ambiguities, making it challenging for these systems to accurately interpret user intent. As a result, users frequently need to revise their prompts multiple times to achieve the desired output. While existing approaches aim to improve prompt quality, they often fail to address the nuanced requirements of non-expert users effectively.
We introduce Visual Co-Adaptation (VCA), a novel framework designed to iteratively refine user prompts and align generated images with user preferences. VCA leverages a pre-trained language model fine-tuned via reinforcement learning and integrates multi-turn dialogues for prompt disambiguation. At the core of the framework, the Incremental Context-Enhanced Dialogue Block enables dynamic clarification through user feedback and targeted questions, while the Semantic Exploration and Disambiguation Module (SESD) incorporates Retrieval-Augmented Generation (RAG) and CLIP-based scoring to resolve ambiguities in complex prompts. For pixel-level precision and global consistency, the Pixel Precision and Consistency Optimization Module (PPCO) employs Proximal Policy Optimization (PPO) and advanced attention mechanisms to refine image details while maintaining visual harmony. A human-in-the-loop feedback mechanism further enhances model performance by integrating user insights into the training pipeline.
Extensive experiments demonstrate that VCA significantly improves user satisfaction, image-text alignment, and aesthetic quality. Compared to state-of-the-art models such as DALL-E 3, Stable Diffusion, and Imagen, VCA reduces the average number of dialogue rounds to 4.3, achieves a CLIP score of 0.92, and increases user satisfaction to 4.73/5. Additionally, we contribute a novel multi-round dialogue dataset comprising prompt-image pairs and user intent annotations, enabling further experimentation and showcasing the efficacy of our approach.
The project page is available athttps://tathataai.github.io/FIS-Diffusion/."
268,679d459debd8ffd557a2af79,cs.CV,https://arxiv.org/pdf/2501.15151,SpikSSD: Better Extraction and Fusion for Object Detection with Spiking Neuron Networks,"Yimeng Fan, Changsong Liu, Mingyang Li, Wei Zhang",Computer Vision and Pattern Recognition,"As the third generation of neural networks, Spiking Neural Networks (SNNs) have gained widespread attention due to their low energy consumption and biological interpretability. Recently, SNNs have made considerable advancements in computer vision. However, efficiently conducting feature extraction and fusion under the spiking characteristics of SNNs for object detection remains a pressing challenge. To address this problem, we propose the SpikSSD, a novel Spiking Single Shot Multibox Detector. Specifically, we design a full-spiking backbone network, MDS-ResNet, which effectively adjusts the membrane synaptic input distribution at each layer, achieving better spiking feature extraction. Additionally, for spiking feature fusion, we introduce the Spiking Bi-direction Fusion Module (SBFM), which for the first time realizes bi-direction fusion of spiking features, enhancing the multi-scale detection capability of the model.
Experimental results show that SpikSSD achieves 40.8% mAP on the GEN1 dataset, 76.3% and 52.4% mAP@0.5 on VOC 2007 and COCO 2017 datasets respectively with the lowest firing rate, outperforming existing SNN-based approaches at ultralow energy consumption. This work sets a new benchmark for future research in SNN-based object detection. Our code is publicly available inhttps://github.com/yimeng-fan/SpikSSD."
269,679d459debd8ffd557a2af7a,cs.CV,https://arxiv.org/pdf/2501.15144,Exploring Primitive Visual Measurement Understanding and the Role of Output Format in Learning in Vision-Language Models,"Ankit Yadav, Lingqiao Liu, Yuankai Qi",Computer Vision and Pattern Recognition,"TheIJCAI–25 Proceedingswill be printed from electronic
manuscripts submitted by the authors. The electronic manuscript will
also be included in the online version of the proceedings. This paper
provides the style instructions."
270,679d459debd8ffd557a2af7b,cs.CV,https://arxiv.org/pdf/2501.15140,Analyzing and Boosting the Power of Fine-Grained Visual Recognition for Multi-modal Large Language Models,"Hulingxiao He, Geng Li, Zijun Geng, Jinglin Xu, Yuxin Peng","Computer Vision and Pattern Recognition, Artificial Intelligence, Computation and Language, Machine Learning","Multi-modal large language models (MLLMs) have shown remarkable abilities in various visual understanding tasks. However, MLLMs still struggle with fine-grained visual recognition (FGVR), which aims to identify subordinate-level categories from images. This can negatively impact more advanced capabilities of MLLMs, such as object-centric visual question answering and reasoning. In our study, we revisit three quintessential capabilities of MLLMs for FGVR, including object information extraction, category knowledge reserve, object-category alignment, and position of the root cause as a misalignment problem. To address this issue, we presentFinedefics, an MLLM that enhances the model’s FGVR capability by incorporating informative attribute descriptions of objects into the training phase. We employ contrastive learning on object-attribute pairs and attribute-category pairs simultaneously and use examples from similar but incorrect categories as hard negatives, naturally bringing representations of visual objects and category names closer. Extensive evaluations across multiple popular FGVR datasets demonstrate that Finedefics outperforms existing MLLMs of comparable parameter sizes, showcasing its remarkable efficacy. The code is available athttps://github.com/PKU-ICST-MIPL/Finedefics_ICLR2025."
271,679d459debd8ffd557a2af7c,cs.CV,https://arxiv.org/pdf/2501.15138,TranStable: Towards Robust Pixel-level Online Video Stabilization by Jointing Transformer and CNN,"zhizhen li, tianyi zhuo, Yifei Cao, Jizhe Yu, Yu Liu",Computer Vision and Pattern Recognition,
272,679d459debd8ffd557a2af7d,cs.CV,https://arxiv.org/pdf/2501.15122,Snapshot Compressed Imaging Based Single-Measurement Computer Vision for Videos,"Fengpu Pan, Jiangtao Wen, Yuxing Han","Computer Vision and Pattern Recognition, Artificial Intelligence","Snapshot compressive imaging (SCI) is a promising technique for capturing high-speed video at low bandwidth and low power, typically by compressing multiple frames into a single measurement. However, similar to traditional CMOS image sensor based imaging systems, SCI also faces challenges in low-lighting photon-limited and low-signal-to-noise-ratio image conditions. In this paper, we propose a novel Compressive Denoising Autoencoder (CompDAE) using the STFormer architecture as the backbone, to explicitly model noise characteristics and provide computer vision functionalities such as edge detection and depth estimation directly from compressed sensing measurements, while accounting for realistic low-photon conditions. We evaluate the effectiveness of CompDAE across various datasets and demonstrated significant improvements in task performance compared to conventional RGB-based methods.
In the case of ultra-low-lighting (APC≤\leq≤20) while conventional methods failed, the proposed algorithm can still maintain competitive performance."
273,679d459debd8ffd557a2af7e,cs.CV,https://arxiv.org/pdf/2501.15119,Efficient Video Neural Network Processing Based on Motion Estimation,"Haichao Wang, Jiangtao Wen, Yuxing Han","Computer Vision and Pattern Recognition, Image and Video Processing","Video neural network (VNN) processing using the conventional pipeline first converts Bayer video information into human understandable RGB videos using image signal processing (ISP) on a pixel by pixel basis. Then, VNN processing is performed on a frame by frame basis. Both ISP and VNN are computationally expensive with high power consumption and latency. In this paper, we propose an efficient VNN processing framework. Instead of using ISP, computer vision tasks are directly accomplished using Bayer pattern information. To accelerate VNN processing, motion estimation is introduced to find temporal redundancies in input video data so as to avoid repeated and unnecessary computations. Experiments show greater than 67% computation reduction, while maintaining computer vision task accuracy for typical computer vision tasks and data sets."
274,679d459debd8ffd557a2af7f,cs.CV,https://arxiv.org/pdf/2501.15111,HumanOmni: A Large Vision-Speech Language Model for Human-Centric Video Understanding,"Jiaxing Zhao, Qize Yang, Yixing Peng, Detao Bai, Shimin Yao, Boyuan Sun, Xiang Chen, Shenghao Fu, Weixuan chen, Xihan Wei, Liefeng Bo",Computer Vision and Pattern Recognition,"In human-centric scenes, the ability to simultaneously understand visual and auditory information is crucial.
While recent omni models can process multiple modalities, they generally lack effectiveness in human-centric scenes due to the absence of large-scale, specialized datasets and non-targeted architectures.
In this work, we developed HumanOmni, the industry’s first human-centric Omni-multimodal large language model.
We constructed a dataset containing over 2.4 million human-centric video clips with detailed captions and more than 14 million instructions, facilitating the understanding of diverse human-centric scenes.
HumanOmni includes three specialized branches for understanding different types of scenes.
It adaptively fuses features from these branches based on user instructions, significantly enhancing visual understanding in scenes centered around individuals.
Moreover, HumanOmni integrates audio features to ensure a comprehensive understanding of environments and individuals.
Our experiments validate HumanOmni’s advanced capabilities in handling human-centric scenes across a variety of tasks, including emotion recognition, facial expression description, and action understanding.
Our model will be open-sourced to facilitate further development and collaboration within both academia and industry."
275,679d459debd8ffd557a2af80,cs.CV,https://arxiv.org/pdf/2501.15099,Bringing RGB and IR Together: Hierarchical Multi-Modal Enhancement for Robust Transmission Line Detection,"Shengdong Zhang, Xiaoqin Zhang, Wenqi Ren, Linlin Shen, Shaohua Wan, Jun Zhang, Yujing M Jiang","Computer Vision and Pattern Recognition, Machine Learning","Ensuring a stable power supply in rural areas relies heavily on effective inspection of power equipment, particularly transmission lines (TLs). However, detecting TLs from aerial imagery can be challenging when dealing with misalignments between visible light (RGB) and infrared (IR) images, as well as mismatched high- and low-level features in convolutional networks. To address these limitations, we propose a novel Hierarchical Multi-Modal Enhancement Network (HMMEN) that integrates RGB and IR data for robust and accurate TL detection. Our method introduces two key components: (1) a Mutual Multi-Modal Enhanced Block (MMEB), which fuses and enhances hierarchical RGB and IR feature maps in a coarse-to-fine manner, and (2) a Feature Alignment Block (FAB) that corrects misalignments between decoder outputs and IR feature maps by leveraging deformable convolutions. We employ MobileNet-based encoders for both RGB and IR inputs to accommodate edge-computing constraints and reduce computational overhead. Experimental results on diverse weather and lighting conditions—fog, night, snow, and daytime—demonstrate the superiority and robustness of our approach compared to state-of-the-art methods, resulting in fewer false positives, enhanced boundary delineation, and better overall detection performance. This framework thus shows promise for practical large-scale power line inspections with unmanned aerial vehicles."
276,679d459debd8ffd557a2af81,cs.CV,https://arxiv.org/pdf/2501.15096,Towards Better Robustness: Progressively Joint Pose-3DGS Learning for Arbitrarily Long Videos,"Zhen-Hui Dong, Sheng Ye, Yu-Hui Wen, Nannan Li, Yong-Jin Liu",Computer Vision and Pattern Recognition,"3D Gaussian Splatting (3DGS) has emerged as a powerful representation due to its efficiency and high-fidelity rendering. However, 3DGS training requires a known camera pose for each input view, typically obtained by Structure-from-Motion (SfM) pipelines. Pioneering works have attempted to relax this restriction but still face difficulties when handling long sequences with complex camera trajectories. In this work, we propose Rob-GS, a robust framework to progressively estimate camera poses and optimize 3DGS for arbitrarily long video sequences. Leveraging the inherent continuity of videos, we design an adjacent pose tracking method to ensure stable pose estimation between consecutive frames. To handle arbitrarily long inputs, we adopt a “divide and conquer” scheme that adaptively splits the video sequence into several segments and optimizes them separately. Extensive experiments on theTanks and Templesdataset and our collected real-world dataset show that our Rob-GS outperforms the state-of-the-arts."
277,679d459debd8ffd557a2af82,cs.CV,https://arxiv.org/pdf/2501.15074,PatentLMM: Large Multimodal Model for Generating Descriptions for Patent Figures,"Shreya Shukla, Nakul Sharma, Manish Gupta, Anand Mishra","Computer Vision and Pattern Recognition, Artificial Intelligence","Writing comprehensive and accurate descriptions of technical drawings in patent documents is crucial to effective knowledge sharing and enabling the replication and protection of intellectual property.
However, automation of this task has been largely overlooked by the research community. To this end, we introducePatentDesc-355K, a novel large-scale dataset containing∼similar-to\sim∼355K patent figures along with their brief and detailed textual descriptions extracted from 60K+ US patent documents. In addition, we proposePatentLMM– a novel multimodal large language model specifically tailored to generate high-quality descriptions of patent figures. Our proposedPatentLMMcomprises two key components: (i)PatentMME, a specialized multimodal vision encoder that captures the unique structural elements of patent figures, and (ii)PatentLLaMA, a domain-adapted version of LLaMA fine-tuned on a large collection of patents. Extensive experiments demonstrate that training a vision encoder specifically designed for patent figures significantly boosts the performance, generating coherent descriptions compared to fine-tuning similar-sized off-the-shelf multimodal models.PatentDesc-355KandPatentLMMpave the way for automating the understanding of patent figures, enabling efficient knowledge sharing and faster drafting of patent documents. We make the code and data publicly available111https://vl2g.github.io/projects/PatentLMM/."
278,679d459debd8ffd557a2af83,cs.CV,https://arxiv.org/pdf/2501.15073,SpatioTemporal Learning for Human Pose Estimation in Sparsely-Labeled Videos,"Yingying Jiao, Zhigang Wang, Sifan Wu, Shaojing Fan, Zhenguang Liu, Zhuoyue Xu, Zheqi Wu",Computer Vision and Pattern Recognition,"Human pose estimation in videos remains a challenge, largely due to the reliance on extensive manual annotation of large datasets, which is expensive and labor-intensive. Furthermore, existing approaches often struggle to capture long-range temporal dependencies and overlook the complementary relationship between temporal pose heatmaps and visual features. To address these limitations, we introduce STDPose, a novel framework that enhances human pose estimation by learning spatiotemporal dynamics in sparsely-labeled videos. STDPose incorporates two key innovations: 1) A novel Dynamic-Aware Mask to capture long-range motion context, allowing for a nuanced understanding of pose changes. 2) A system for encoding and aggregating spatiotemporal representations and motion dynamics to effectively model spatiotemporal relationships, improving the accuracy and robustness of pose estimation. STDPose establishes a new performance benchmark for both video pose propagation (i.e., propagating pose annotations from labeled frames to unlabeled frames) and pose estimation tasks, across three large-scale evaluation datasets. Additionally, utilizing pseudo-labels generated by pose propagation, STDPose achieves competitive performance with only 26.7% labeled data."
279,679d459debd8ffd557a2af84,cs.CV,https://arxiv.org/pdf/2501.15061,PolaFormer: Polarity-aware Linear Attention for Vision Transformers,"Weikang Meng, Yadan Luo, Xin Li, Dongmei Jiang, Zheng Zhang","Computer Vision and Pattern Recognition, Artificial Intelligence","Linear attention has emerged as a promising alternative to softmax-based attention, leveraging kernelized feature maps to reduce complexity from quadratic to linear in sequence length. However, the non-negative constraint on feature maps and the relaxed exponential function used in approximation lead to significant information loss compared to the original query-key dot products, resulting in less discriminative attention maps with higher entropy. To address the missing interactions driven by negative values in query-key pairs, we propose a polarity-aware linear attention mechanism that explicitly models both same-signed and opposite-signed query-key interactions, ensuring comprehensive coverage of relational information. Furthermore, to restore the spiky properties of attention maps, we provide a theoretical analysis proving the existence of a class of element-wise functions (with positive first and second derivatives) that can reduce entropy in the attention distribution. For simplicity, and recognizing the distinct contributions of each dimension, we employ a learnable power function for rescaling, allowing strong and weak attention signals to be effectively separated. Extensive experiments demonstrate that the proposed PolaFormer improves performance on various vision tasks, enhancing both expressiveness and efficiency by up to 4.6%."
280,679d459debd8ffd557a2af85,cs.CV,https://arxiv.org/pdf/2501.15058,KETA: Kinematic-Phrases-Enhanced Text-to-Motion Generation via Fine-grained Alignment,"Yu Jiang, Yixing Chen, Xingyang Li",Computer Vision and Pattern Recognition,"Motion synthesis plays a vital role in various fields of artificial intelligence.
Among the various conditions of motion generation, text can describe motion details elaborately and is easy to acquire, making text-to-motion(T2M) generation important. State-of-the-art T2M techniques mainly leverage diffusion models to generate motions with text prompts as guidance, tackling the many-to-many nature of T2M tasks.
However, existing T2M approaches face challenges, given the gap between the natural language domain and the physical domain, making it difficult to generate motions fully consistent with the texts."
281,679d459debd8ffd557a2af86,cs.CV,https://arxiv.org/pdf/2501.15052,Graph-Based Cross-Domain Knowledge Distillation for Cross-Dataset Text-to-Image Person Retrieval,"Bingjun Luo, Jinpeng Wang, Wang Zewen, Junjie Zhu, Xibin Zhao","Computer Vision and Pattern Recognition, Artificial Intelligence, Multimedia","Video surveillance systems are crucial components for ensuring public safety and management in smart city.
As a fundamental task in video surveillance, text-to-image person retrieval aims to retrieve the target person from an image gallery that best matches the given text description.
Most existing text-to-image person retrieval methods are trained in a supervised manner that requires sufficient labeled data in the target domain.
However, it is common in practice that only unlabeled data is available in the target domain due to the difficulty and cost of data annotation, which limits the generalization of existing methods in practical application scenarios.
To address this issue, we propose a novel unsupervised domain adaptation method, termed Graph-Based Cross-Domain Knowledge Distillation (GCKD), to learn the cross-modal feature representation for text-to-image person retrieval in a cross-dataset scenario.
The proposed GCKD method consists of two main components.
Firstly, a graph-based multi-modal propagation module is designed to bridge the cross-domain correlation among the visual and textual samples.
Secondly, a contrastive momentum knowledge distillation module is proposed to learn the cross-modal feature representation using the online knowledge distillation strategy.
By jointly optimizing the two modules, the proposed method is able to achieve efficient performance for cross-dataset text-to-image person retrieval.
Extensive experiments on three publicly available text-to-image person retrieval datasets demonstrate the effectiveness of the proposed GCKD method, which consistently outperforms the state-of-the-art baselines."
282,679d459debd8ffd557a2af87,cs.CV,https://arxiv.org/pdf/2501.15046,Evaluating Hallucination in Large Vision-Language Models based on Context-Aware Object Similarities,"Shounak Datta, Dhanasekar Sundararaman","Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning","Despite their impressive performance on multi-modal tasks, large vision-language models (LVLMs) tend to suffer from hallucinations. An important type is object hallucination, where LVLMs generate objects that are inconsistent with the images shown to the model. Existing works typically attempt to quantify object hallucinations by detecting and measuring the fraction of hallucinated objects in generated captions. Additionally, more recent work also measures object hallucinations by directly querying the LVLM with binary questions about the presence of likely hallucinated objects based on object statistics like top-k𝑘kitalic_kfrequent objects and top-k𝑘kitalic_kco-occurring objects. In this paper, we present Context-Aware Object Similarities (CAOS), a novel approach for evaluating object hallucination in LVLMs using object statistics as well as the generated captions. CAOS uniquely integrates object statistics with semantic relationships between objects in captions and ground-truth data. Moreover, existing approaches usually only detect and measure hallucinations belonging to a predetermined set of in-domain objects (typically the set of all ground-truth objects for the training dataset) and ignore generated objects that are not part of this set, leading to under-evaluation. To address this, we further employ language model–based object recognition to detect potentially out-of-domain hallucinated objects and use an ensemble of LVLMs for verifying the presence of such objects in the query image. CAOS also examines the sequential dynamics of object generation, shedding light on how the order of object appearance influences hallucinations, and employs word embedding models to analyze the semantic reasons behind hallucinations. By providing a systematic framework to identify and interpret object hallucinations, CAOS aims to offer a nuanced understanding of both the hallucination tendencies of LVLMs and the factors contributing to object hallucinations."
283,679d459debd8ffd557a2af88,cs.CV,https://arxiv.org/pdf/2501.15045,Towards Robust Unsupervised Attention Prediction in Autonomous Driving,"Mengshi Qi, Xiaoyang Bi, Pengfei Zhu, Huadong Ma","Computer Vision and Pattern Recognition, Artificial Intelligence","Robustly predicting attention regions of interest for self-driving systems is crucial for driving safety but presents significant challenges due to the labor-intensive nature of obtaining large-scale attention labels and the domain gap between self-driving scenarios and natural scenes. These challenges are further exacerbated by complex traffic environments, including camera corruption under adverse weather, noise interferences, and central bias from long-tail distributions. To address these issues, we propose a robust unsupervised attention prediction method. An Uncertainty Mining Branch refines predictions by analyzing commonalities and differences across multiple pre-trained models on natural scenes, while a Knowledge Embedding Block bridges the domain gap by incorporating driving knowledge to adaptively enhance pseudo-labels. Additionally, we introduce RoboMixup, a novel data augmentation method that improves robustness against corruption through soft attention and dynamic augmentation, and mitigates central bias by integrating random cropping into Mixup as a regularizer. To systematically evaluate robustness in self-driving attention prediction, we introduce theDriverAttention-Cbenchmark, comprising over 100k frames across three subsets: BDD-A-C, DR(eye)VE-C, and DADA-2000-C. Our method achieves performance equivalent to or surpassing fully supervised state-of-the-art approaches on three public datasets and the proposed robustness benchmark, reducing relative corruption degradation by 58.8% and 52.8%, and improving central bias robustness by 12.4% and 11.4% in KLD and CC metrics, respectively. Code and data are available athttps://github.com/zaplm/DriverAttention."
284,679d459debd8ffd557a2af89,cs.CV,https://arxiv.org/pdf/2501.15043,Prompt-Aware Controllable Shadow Removal,"Kerui Chen, Zhiliang Wu, Wenjin Hou, Kun Li, Hehe Fan, Yi Yang",Computer Vision and Pattern Recognition,"Shadow removal aims to restore the image content in shadowed regions.
While deep learning-based methods have shown promising results, they still face key challenges: 1) uncontrolled removal of all shadows, or 2) controllable removal but heavily relies on precise shadow region masks.
To address these issues, we introduce a novel paradigm: prompt-aware controllable shadow removal. Unlike existing approaches, our paradigm allows for targeted shadow removal from specific subjects based on user prompts (e.g., dots, lines, or subject masks).
This approach eliminates the need for shadow annotations and offers flexible, user-controlled shadow removal.
Specifically, we propose an end-to-end learnable model, thePrompt-AwareCntrollableShadowRemovalNetwork (PACSRNet).
PACSRNet consists of two key modules: a prompt-aware module that generates shadow masks for the specified subject based on the user prompt, and a shadow removal module that uses the shadow prior from the first module to restore the content in the shadowed regions.
Additionally, we enhance the shadow removal module by incorporating feature information from the prompt-aware module through a linear operation, providing prompt-guided support for shadow removal.
Recognizing that existing shadow removal datasets lack diverse user prompts, we contribute a new dataset specifically designed for prompt-based controllable shadow removal.
Extensive experimental results demonstrate the effectiveness and superiority of PACSRNet."
285,679d459debd8ffd557a2af8a,cs.CV,https://arxiv.org/pdf/2501.15040,Complementary Subspace Low-Rank Adaptation of Vision-Language Models for Few-Shot Classification,"Zhongqi Wang, Jia Dai, Kai Li, Xu Li, Yanmeng Guo, Maosheng Xiang",Computer Vision and Pattern Recognition,"Vision language model (VLM) has been designed for large scale image-text alignment as a pretrained foundation model. For downstream few shot classification tasks, parameter efficient fine-tuning (PEFT) VLM has gained much popularity in the computer vision community. PEFT methods like prompt tuning and linear adapter have been studied for fine-tuning VLM while low rank adaptation (LoRA) algorithm has rarely been considered for few shot fine-tuning VLM. The main obstacle to use LoRA for few shot fine-tuning is the catastrophic forgetting problem. Because the visual language alignment knowledge is important for the generality in few shot learning, whereas low rank adaptation interferes with the most informative direction of the pretrained weight matrix. We propose the complementary subspace low rank adaptation (Comp-LoRA) method to regularize the catastrophic forgetting problem in few shot VLM finetuning. In detail, we optimize the low rank matrix in the complementary subspace, thus preserving the general vision language alignment ability of VLM when learning the novel few shot information.
We conduct comparison experiments of the proposed Comp-LoRA method and other PEFT methods on fine-tuning VLM for few shot classification. And we also present the suppression on the catastrophic forgetting problem of our proposed method against directly applying LoRA to VLM. The results show that the proposed method surpasses the baseline method by about +1.0% Top-1 accuracy and preserves the VLM zero-shot performance over the baseline method by about +1.3% Top-1 accuracy. The code will be released on github."
286,679d459debd8ffd557a2af8b,cs.CV,https://arxiv.org/pdf/2501.15008,HuGDiffusion: Generalizable Single-Image Human Rendering via 3D Gaussian Diffusion,"Yingzhi Tang, Qijian Zhang, Junhui Hou",Computer Vision and Pattern Recognition,"We present HuGDiffusion, a generalizable 3D Gaussian splatting (3DGS) learning pipeline to achieve novel view synthesis (NVS) of human characters from single-view input images. Existing approaches typically require monocular videos or calibrated multi-view images as inputs, whose applicability could be weakened in real-world scenarios with arbitrary and/or unknown camera poses. In this paper, we aim to generate the set of 3DGS attributes via a diffusion-based framework conditioned on human priors extracted from a single image. Specifically, we begin with carefully integrated human-centric feature extraction procedures to deduce informative conditioning signals. Based on our empirical observations that jointly learning the whole 3DGS attributes is challenging to optimize, we design a multi-stage generation strategy to obtain different types of 3DGS attributes. To facilitate the training process, we investigate constructing proxy ground-truth 3D Gaussian attributes as high-quality attribute-level supervision signals. Through extensive experiments, our HuGDiffusion shows significant performance improvements over the state-of-the-art methods.Our code will be made publicly available."
287,679d459debd8ffd557a2af8c,cs.CV,https://arxiv.org/pdf/2501.14999,VideoPure: Diffusion-based Adversarial Purification for Video Recognition,"Kaixun Jiang, Zhaoyu Chen, Jiyuan Fu, Lingyi Hong, Jinglun Li, Wenqiang Zhang",Computer Vision and Pattern Recognition,"Recent work indicates that video recognition models are vulnerable to adversarial examples, posing a serious security risk to downstream applications. However, current research has primarily focused on adversarial attacks, with limited work exploring defense mechanisms. Furthermore, due to the spatial-temporal complexity of videos, existing video defense methods face issues of high cost, overfitting, and limited defense performance. Recently, diffusion-based adversarial purification methods have achieved robust defense performance in the image domain. However, due to the additional temporal dimension in videos, directly applying these diffusion-based adversarial purification methods to the video domain suffers performance and efficiency degradation. To achieve an efficient and effective video adversarial defense method, we propose the first diffusion-based video purification framework to improve video recognition models’ adversarial robustness: VideoPure. Given an adversarial example, we first employ temporal DDIM inversion to transform the input distribution into a temporally consistent and trajectory-defined distribution, covering adversarial noise while preserving more video structure. Then, during DDIM denoising, we leverage intermediate results at each denoising step and conduct guided spatial-temporal optimization, removing adversarial noise while maintaining temporal consistency. Finally, we input the list of optimized intermediate results into the video recognition model for multi-step voting to obtain the predicted class. We investigate the defense performance of our method against state-of-the-art black-box, gray-box, and adaptive attacks on benchmark datasets and models. Compared with other adversarial purification methods, our method overall demonstrates better defense performance against different attacks. Moreover, our method can be applied as a flexible defense plugin for video recognition models. Our code is available at https://github.com/deep-kaixun/VideoPure"
288,679d459debd8ffd557a2af8d,cs.CV,https://arxiv.org/pdf/2501.14945,MATCHA:Towards Matching Anything,"Fei Xue, Sven Elflein, Laura Leal-Taixé, Qunjie Zhou",Computer Vision and Pattern Recognition,"Establishing correspondences across images is a fundamental challenge in computer vision, underpinning tasks like Structure-from-Motion, image editing, and point tracking. Traditional methods are often specialized for specific correspondence types, geometric, semantic, or temporal, whereas humans naturally identify alignments across these domains. Inspired by this flexibility, we propose MATCHA, a unified feature model designed to “rule them all”, establishing robust correspondences across diverse matching tasks.
Building on insights that diffusion model features can encode multiple correspondence types, MATCHA augments this capacity by dynamically fusing high-level semantic and low-level geometric features through an attention-based module, creating expressive, versatile, and robust features.
Additionally, MATCHA integrates object-level features from DINOv2 to further boost generalization, enabling a single feature capable of matching anything.
Extensive experiments validate that MATCHA consistently surpasses state-of-the-art methods across geometric, semantic, and temporal matching tasks, setting a new foundation for a unified approach for the fundamental correspondence problem in computer vision. To the best of our knowledge, MATCHA is the first approach that is able to effectively tackle diverse matching tasks with a single unified feature."
289,679d459debd8ffd557a2af8e,cs.CV,https://arxiv.org/pdf/2501.14929,"Motion-enhancement to Echocardiography Segmentation via Inserting a Temporal Attention Module: An Efficient, Adaptable, and Scalable Approach","Md. Kamrul Hasan, Guang Yang, Choon Hwai Yap","Computer Vision and Pattern Recognition, Artificial Intelligence","Cardiac anatomy segmentation is essential for clinical assessment of cardiac function and disease diagnosis to inform treatment and intervention. In performing segmentation, deep learning (DL) algorithms improved accuracy significantly compared to traditional image processing approaches. More recently, studies showed that enhancing DL segmentation with motion information can further improve it. A range of methods for injecting motion information has been proposed, but many of them increase the dimensionality of input images (which is computationally expensive) or have not used an optimal method to insert motion information, such as non-DL registration, non-attention-based networks, or single-headed attention. Here, we present a novel, computation-efficient alternative where a novel, scalable temporal attention module (TAM) extracts temporal feature interactions multiple times and where TAM has a multi-headed, KQV projection cross-attention architecture. The module can be seamlessly integrated into a wide range of existing CNN- or Transformer-based networks, providing novel flexibility for inclusion in future implementations. Extensive evaluations on different cardiac datasets—2D echocardiography (CAMUS) and 3D echocardiography (MITEA)—demonstrate the model’s effectiveness when integrated into well-established backbone networks like UNet, FCN8s, UNetR, SwinUNetR, and the recent I2UNet. We further find that the optimized TAM-enhanced FCN8s network performs well compared to contemporary alternatives. Our results confirm TAM’s robustness, scalability, and generalizability across diverse datasets and backbones."
290,679d459debd8ffd557a2af8f,cs.CV,https://arxiv.org/pdf/2501.14918,3D/2D Registration of Angiograms using Silhouette-based Differentiable Rendering,"Taewoong Lee, Sarah Frisken, Nazim Haouchine",Computer Vision and Pattern Recognition,"We present a method for 3D/2D registration of Digital Subtraction Angiography (DSA) images to provide valuable insight into brain hemodynamics and angioarchitecture. Our approach formulates the registration as a pose estimation problem, leveraging both anteroposterior and lateral DSA views and employing differentiable rendering. Preliminary experiments on real and synthetic datasets demonstrate the effectiveness of our method, with both qualitative and quantitative evaluations highlighting its potential for clinical applications. The code is available athttps://github.com/taewoonglee17/TwoViewsDSAReg."
291,679d459debd8ffd557a2af90,cs.CV,https://arxiv.org/pdf/2501.14914,Light3R-SfM: Towards Feed-forward Structure-from-Motion,"Sven Elflein, Qunjie Zhou, Sérgio Agostinho, Laura Leal-Taixé","Computer Vision and Pattern Recognition, Machine Learning","We present Light3R-SfM, a feed-forward, end-to-end learnable framework for efficient large-scale Structure-from-Motion (SfM) from unconstrained image collections. Unlike existing SfM solutions that rely on costly matching and global optimization to achieve accurate 3D reconstructions, Light3R-SfM addresses this limitation through a novel latent global alignment module. This module replaces traditional global optimization with a learnable attention mechanism, effectively capturing multi-view constraints across images for robust and precise camera pose estimation. Light3R-SfM constructs a sparse scene graph via retrieval-score-guided shortest path tree to dramatically reduce memory usage and computational overhead compared to the naive approach. Extensive experiments demonstrate that Light3R-SfM achieves competitive accuracy while significantly reducing runtime, making it ideal for 3D reconstruction tasks in real-world applications with a runtime constraint. This work pioneers a data-driven, feed-forward SfM approach, paving the way toward scalable, accurate, and efficient 3D reconstruction in the wild."
292,679d459debd8ffd557a2af91,cs.CV,https://arxiv.org/pdf/2501.14905,Measuring and Mitigating Hallucinations in Vision-Language Dataset Generation for Remote Sensing,"Madeline Anderson, Miriam Cha, William T. Freeman, J. Taylor Perron, Nathaniel Maidel, Kerri Cahoy",Computer Vision and Pattern Recognition,"Vision language models have achieved impressive results across various fields. However, adoption in remote sensing remains limited, largely due to the scarcity of paired image-text data. To bridge this gap, synthetic caption generation has gained interest, traditionally relying on rule-based methods that use metadata or bounding boxes. While these approaches provide some description, they often lack the depth needed to capture complex wide-area scenes. Large language models (LLMs) offer a promising alternative for generating more descriptive captions, yet they can produce generic outputs and are prone to hallucination. In this paper, we propose a new method to enhance vision-language datasets for remote sensing by integrating maps as external data sources, enabling the generation of detailed, context-rich captions. Additionally, we present methods to measure and mitigate hallucinations in LLM-generated text. We introduce fMoW-mm, a multimodal dataset incorporating satellite imagery, maps, metadata, and text annotations. We demonstrate its effectiveness for automatic target recognition in few-shot settings, achieving superior performance compared to other vision-language remote sensing datasets."
293,679d459debd8ffd557a2af92,cs.CV,https://arxiv.org/pdf/2501.14896,Glissando-Net: Deep sinGLe vIew category level poSe eStimation ANd 3D recOnstruction,"Bo Sun, Hao Kang, Li Guan, Haoxiang Li, Philippos Mordohai, Gang Hua",Computer Vision and Pattern Recognition,"We present a deep learning model, dubbed Glissando-Net, to simultaneously estimate the pose and reconstruct the 3D shape of objects at the category level from a single RGB image. Previous works predominantly focused on either estimating poses (often at the instance level), or reconstructing shapes, but not both. Glissando-Net is composed of two auto-encoders that are jointly trained, one for RGB images and the other for point clouds. We embrace two key design choices in Glissando-Net to achieve a more accurate prediction of the 3D shape and pose of the object given a single RGB image as input. First, we augment the feature maps of the point cloud encoder and decoder with transformed feature maps from the image decoder, enabling effective 2D-3D interaction in both training and prediction. Second, we predict both the 3D shape and pose of the object in the decoder stage. This way, we better utilize the information in the 3D point clouds presented only in the training stage to train the network for more accurate prediction. We jointly train the two encoder-decoders for RGB and point cloud data to learn how to pass latent features to the point cloud decoder during inference. In testing, the encoder of the 3D point cloud is discarded. The design of Glissando-Net is inspired by codeSLAM. Unlike codeSLAM, which targets 3D reconstruction of scenes, we focus on pose estimation and shape reconstruction of objects, and directly predict the object pose and a pose invariant 3D reconstruction without the need of the code optimization step. Extensive experiments, involving both ablation studies and comparison with competing methods, demonstrate the efficacy of our proposed method, and compare favorably with the state-of-the-art."
294,679d459debd8ffd557a2af93,cs.CV,https://arxiv.org/pdf/2501.14894,Improving reliability of uncertainty-aware gaze estimation with probability calibration,"Qiaojie Zheng, Jiucai Zhang, Xiaoli Zhang",Computer Vision and Pattern Recognition,"Current deep learning powered appearance based uncertainty-aware gaze estimation models produce inconsistent and unreliable uncertainty estimation that limits their adoptions in downstream applications. In this study, we propose a workflow to improve the accuracy of uncertainty estimation using probability calibration with a few post hoc samples. The probability calibration process employs a simple secondary regression model to compensate for inaccuracies in estimated uncertainties from the deep learning model. Training of the secondary model is detached from the main deep learning model and thus no expensive weight tuning is required. The added calibration process is lightweight and relatively independent from the deep learning process, making it fast to run and easy to implement. We evaluated the effectiveness of the calibration process under four potential application scenarios with two datasets that have distinctive image characteristics due to the data collection setups. The calibration process is most effective when the calibration and testing data share similar characteristics. Even under suboptimal circumstances that calibration and testing data differ, the calibration process can still make corrections to reduce prediction errors in uncertainty estimates made by uncalibrated models."
295,679d459debd8ffd557a2af94,cs.CV,https://arxiv.org/pdf/2501.14885,Hybrid Interpretable Deep Learning Framework for Skin Cancer Diagnosis: Integrating Radial Basis Function Networks with Explainable AI,"Mirza Ahsan Ullah, Tehseen Zia",Computer Vision and Pattern Recognition,"Skin cancer is one of the most prevalent and potentially life-threatening diseases worldwide, necessitating early and accurate diagnosis to improve patient outcomes. Conventional diagnostic methods, reliant on clinical expertise and histopathological analysis, are often time-intensive, subjective, and prone to variability. To address these limitations, we propose a novel hybrid deep learning framework that integrates convolutional neural networks (CNNs) with Radial Basis Function (RBF) Networks to achieve high classification accuracy and enhanced interpretability. The motivation for incorporating RBF Networks lies in their intrinsic interpretability and localized response to input features, which make them well-suited for tasks requiring transparency and fine-grained decision-making. Unlike traditional deep learning models that rely on global feature representations, RBF Networks allow for mapping segments of images to chosen prototypes, exploiting salient features within a single image. This enables clinicians to trace predictions to specific, interpretable patterns. The framework incorporates segmentation-based feature extraction, active learning for prototype selection, and K-Medoids clustering to focus on these salient features. Evaluations on the ISIC 2016 and ISIC 2017 datasets demonstrate the model’s effectiveness, achieving classification accuracies of 83.02% and 72.15% using ResNet50, respectively, and outperforming VGG16-based configurations. By generating interpretable explanations for predictions, the framework aligns with clinical workflows, bridging the gap between predictive performance and trustworthiness. This study highlights the potential of hybrid models to deliver actionable insights, advancing the development of reliable AI-assisted diagnostic tools for high-stakes medical applications."
296,679d459debd8ffd557a2af95,cs.CV,https://arxiv.org/pdf/2501.14828,An Ensemble Model with Attention Based Mechanism for Image Captioning,"Israa Al Badarneh, Bassam Hammo, Omar Al-Kadi","Computer Vision and Pattern Recognition, Artificial Intelligence","Image captioning creates informative text from an input image by creating a relationship between the words and the actual content of an image. Recently, deep learning models that utilize transformers have been the most successful in automatically generating image captions. The capabilities of transformer networks have led to notable progress in several activities related to vision. In this paper, we thoroughly examine transformer models, emphasizing the critical role that attention mechanisms play. The proposed model uses a transformer encoder-decoder architecture to create textual captions and a deep learning convolutional neural network to extract features from the images. To create the captions, we present a novel ensemble learning framework that improves the richness of the generated captions by utilizing several deep neural network architectures based on a voting mechanism that chooses the caption with the highest bilingual evaluation understudy (BLEU) score. The proposed model was evaluated using publicly available datasets. Using the Flickr8K dataset, the proposed model achieved the highest BLEU-[1-3] scores with rates of 0.728, 0.495, and 0.323, respectively. The suggested model outperformed the latest methods in Flickr30k datasets, determined by BLEU-[1-4] scores with rates of 0.798, 0.561, 0.387, and 0.269, respectively. The model efficacy was also obtained by the Semantic propositional image caption evaluation (SPICE) metric with a scoring rate of 0.164 for the Flicker8k dataset and 0.387 for the Flicker30k. Finally, ensemble learning significantly advances the process of image captioning and, hence, can be leveraged in various applications across different domains."
297,679d459debd8ffd557a2af96,cs.CV,https://arxiv.org/pdf/2501.14818,Eagle 2: Building Post-Training Data Strategies from Scratch for Frontier Vision-Language Models,"Zhiqi Li, Guo Chen, Shilong Liu, Shihao Wang, Vibashan VS, Yishen Ji, Shiyi Lan, Hao Zhang, Yilin Zhao, Subhashree Radhakrishnan, Nadine Chang, Karan Sapra, Amala Sanjay Deshmukh, Tuomas Rintamaki, Matthieu Le, Ilia Karmanov, Lukas Voegtle, Philipp Fischer, De-An Huang, Timo Roman, Tong Lu, Jose M. Alvarez, Bryan Catanzaro, Jan Kautz, Andrew Tao, Guilin Liu, Zhiding Yu","Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning","Abstract:Recently, promising progress has been made by open-source vision-language models (VLMs) in bringing their capabilities closer to those of proprietary frontier models. However, most open-source models only publish their final model weights, leaving the critical details of data strategies and implementation largely opaque. In this work, we address VLM post-training from a data-centric perspective, showing the key role of data strategy in developing frontier VLMs. By studying and building our post-training data strategy from scratch, we share detailed insights into the development processes, aiming to benefit the development of competitive models for the open-source community. Our introduced data strategy, together with training recipes and model design, leads to a family of performant VLMs namedEagle 2. Specifically, Eagle2-9B achieves state-of-the-art results across various multimodal benchmarks, matching certain competitive models with up to 70B parameters.Links:Github Code|HF Models|Demo"
298,679d459debd8ffd557a2af97,cs.CV,https://arxiv.org/pdf/2501.16295,Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity,"Weixin Liang, Junhong Shen, Genghan Zhang, Ning Dong, Luke Zettlemoyer, Lili Yu","Machine Learning, Artificial Intelligence, Computation and Language, Computer Vision and Pattern Recognition",
299,679d459debd8ffd557a2af98,cs.CV,https://arxiv.org/pdf/2501.16282,Brain-Adapter: Enhancing Neurological Disorder Analysis with Adapter-Tuning Multimodal Large Language Models,"Jing Zhang, Xiaowei Yu, Yanjun Lyu, Lu Zhang, Tong Chen, Chao Cao, Yan Zhuang, Minheng Chen, Tianming Liu, Dajiang Zhu","Image and Video Processing, Artificial Intelligence, Computer Vision and Pattern Recognition",
300,679d459debd8ffd557a2af99,cs.CV,https://arxiv.org/pdf/2501.16273,Return of the Encoder: Maximizing Parameter Efficiency for SLMs,"Mohamed Elfeki, Rui Liu, Chad Voegele","Computation and Language, Artificial Intelligence, Computer Vision and Pattern Recognition","The dominance of large decoder-only language models has overshadowed encoder-decoder architectures, despite their fundamental efficiency advantages in sequence processing. For small language models (SLMs) - those with 1 billion parameters or fewer - our systematic analysis across GPU, CPU, and NPU platforms reveals that encoder-decoder architectures achieve 47% lower first-token latency and 4.7x higher throughput compared to decoder-only models on edge devices. These gains may be attributed to encoder-decoder’s one-time input processing and efficient separation of understanding and generation phases."
301,679d459debd8ffd557a2af9a,cs.CV,https://arxiv.org/pdf/2501.16249,Lightweight Weighted Average Ensemble Model for Pneumonia Detection in Chest X-Ray Images,"Suresh Babu Nettur, Shanthi Karpurapu, Unnati Nettur, Likhit Sagar Gajja, Sravanthy Myneni, Akhil Dusi, Lalithya Posham","Image and Video Processing, Artificial Intelligence, Computer Vision and Pattern Recognition",
302,679d459debd8ffd557a2af9b,cs.CV,https://arxiv.org/pdf/2501.16101,3D Reconstruction of non-visible surfaces of objects from a Single Depth View -- Comparative Study,"Rafał Staszak, Piotr Michałek, Jakub Chudziński, Marek Kopicki, Dominik Belter","Robotics, Computer Vision and Pattern Recognition","Scene and object reconstruction is an important problem in robotics, in particular in planning collision-free trajectories or in object manipulation. This paper compares two strategies for the reconstruction of non-visible parts of the object surface from a single RGB-D camera view. The first method, named DeepSDF predicts the Signed Distance Transform to the object surface for a given point in 3D space. The second method, named MirrorNet reconstructs the occluded objects’ parts by generating images from the other side of the observed object. Experiments performed with objects from the ShapeNet dataset, show that the view-dependent MirrorNet is faster and has smaller reconstruction errors in most categories."
303,679d459debd8ffd557a2af9c,cs.CV,https://arxiv.org/pdf/2501.15994,Real-Time Brain Tumor Detection in Intraoperative Ultrasound Using YOLO11: From Model Training to Deployment in the Operating Room,"Santiago Cepeda, Olga Esteban-Sinovas, Roberto Romero, Vikas Singh, Prakash Shetty, Aliasgar Moiyadi, Ilyess Zemmoura, Giuseppe Roberto Giammalva, Massimiliano Del Bene, Arianna Barbotti, Francesco DiMeco, Timothy R. West, Brian V. Nahed, Ignacio Arrese, Roberto Hornero, Rosario Sarabia","Image and Video Processing, Computer Vision and Pattern Recognition","Intraoperative ultrasound (ioUS) is a valuable tool in brain tumor surgery due to its versatility, affordability, and seamless integration into the surgical workflow. However, its adoption remains limited, primarily because of the challenges associated with image interpretation and the steep learning curve required for effective use. This study aimed to enhance the interpretability of ioUS images by developing a real-time brain tumor detection system deployable in the operating room.
We collected 2D ioUS images from the Brain Tumor Intraoperative Ultrasound Database (BraTioUS) and the public ReMIND dataset, annotated with expert-refined tumor labels. Using the YOLO11 architecture and its variants, we trained object detection models to identify brain tumors. The dataset included 1,732 images from 192 patients, divided into training, validation, and test sets. Data augmentation expanded the training set to 11,570 images. In the test dataset, YOLO11s achieved the best balance of precision and computational efficiency, with a mAP@50 of 0.95, mAP@50-95 of 0.65, and a processing speed of 34.16 frames per second. The proposed solution was prospectively validated in a cohort of 15 consecutively operated patients diagnosed with brain tumors. Neurosurgeons confirmed its seamless integration into the surgical workflow, with real-time predictions accurately delineating tumor regions.
These findings highlight the potential of real-time object detection algorithms to enhance ioUS-guided brain tumor surgery, addressing key challenges in interpretation and providing a foundation for future development of computer vision-based tools for neuro-oncological surgery."
304,679d459debd8ffd557a2af9d,cs.CV,https://arxiv.org/pdf/2501.15963,Evaluating Data Influence in Meta Learning,"Chenyang Ren, Huanyi Xie, Shu Yang, Meng Ding, Lijie Hu, Di Wang","Machine Learning, Artificial Intelligence, Computer Vision and Pattern Recognition","As one of the most fundamental models, meta learning aims to effectively address few-shot learning challenges. However, it still faces significant issues related to the training data, such as training inefficiencies due to numerous low-contribution tasks in large datasets and substantial noise from incorrect labels.
Thus, training data attribution methods are needed for meta learning. However, the dual-layer structure of mata learning complicates the modeling of training data contributions because of the interdependent influence between meta-parameters and task-specific parameters, making existing data influence evaluation tools inapplicable or inaccurate. To address these challenges, based on the influence function, we propose a general data attribution evaluation framework for meta-learning within the bilevel optimization framework.
Our approach introduces task influence functions (task-IF) and instance influence functions (instance-IF) to accurately assess the impact of specific tasks and individual data points in closed forms. This framework comprehensively models data contributions across both the inner and outer training processes, capturing the direct effects of data points on meta-parameters as well as their indirect influence through task-specific parameters. We also provide several strategies to enhance computational efficiency and scalability. Experimental results demonstrate the framework’s effectiveness in training data evaluation via several downstream tasks."
305,679d459debd8ffd557a2af9e,cs.CV,https://arxiv.org/pdf/2501.15955,Rethinking the Bias of Foundation Model under Long-tailed Distribution,"Jiahao Chen, Bin Qin, Jiangmeng Li, Hao Chen, Bing Su","Machine Learning, Computer Vision and Pattern Recognition, Machine Learning","Long-tailed learning has garnered increasing attention due to its practical significance. Among the various approaches, the fine-tuning paradigm has gained considerable interest with the advent of foundation models. However, most existing methods primarily focus on leveraging knowledge from these models, overlooking the inherent biases introduced by the imbalanced training data they rely on. In this paper, we examine how such imbalances from pre-training affect long-tailed downstream tasks. Specifically, we find the imbalance biases inherited in foundation models on downstream task as parameter imbalance and data imbalance. During fine-tuning, we observe that parameter imbalance plays a more critical role, while data imbalance can be mitigated using existing re-balancing strategies. Moreover, we find that parameter imbalance cannot be effectively addressed by current re-balancing techniques, such as adjusting the logits, during training, unlike data imbalance. To tackle both imbalances simultaneously, we build our method on causal learning and view the incomplete semantic factor as the confounder, which brings spurious correlations between input samples and labels. To resolve the negative effects of this, we propose a novel backdoor adjustment method that learns the true causal effect between input samples and labels, rather than merely fitting the correlations in the data.
Notably, we achieve an average performance increase of about1.67%percent1.671.67\%1.67 %on each dataset."
306,679d459debd8ffd557a2af9f,cs.CV,https://arxiv.org/pdf/2501.15953,Understanding Long Videos via LLM-Powered Entity Relation Graphs,"Meng Chu, Yicong Li, Tat-Seng Chua","Information Retrieval, Computer Vision and Pattern Recognition","The analysis of extended video content poses unique challenges in artificial intelligence, particularly when dealing with the complexity of tracking and understanding visual elements across time. Current methodologies that process video frames sequentially struggle to maintain coherent tracking of objects, especially when these objects temporarily vanish and later reappear in the footage. A critical limitation of these approaches is their inability to effectively identify crucial moments in the video, largely due to their limited grasp of temporal relationships.
To overcome these obstacles, we present GraphVideoAgent, a cutting-edge system that leverages the power of graph-based object tracking in conjunction with large language model capabilities. At its core, our framework employs a dynamic graph structure that maps and monitors the evolving relationships between visual entities throughout the video sequence. This innovative approach enables more nuanced understanding of how objects interact and transform over time, facilitating improved frame selection through comprehensive contextual awareness.
Our approach demonstrates remarkable effectiveness when tested against industry benchmarks. In evaluations on the EgoSchema dataset, GraphVideoAgent achieved a 2.2% improvement over existing methods while requiring analysis of only 8.2 frames on average. Similarly, testing on the NExT-QA benchmark yielded a 2.0% performance increase with an average frame requirement of 8.1. These results underscore the efficiency of our graph-guided methodology in enhancing both accuracy and computational performance in long-form video understanding tasks."
307,679d459debd8ffd557a2afa0,cs.CV,https://arxiv.org/pdf/2501.15831,Pfungst and Clever Hans: Identifying the unintended cues in a widely used Alzheimer's disease MRI dataset using explainable deep learning,"Christian Tinauer, Maximilian Sackl, Rudolf Stollberger, Stefan Ropele, Christian Langkammer","Image and Video Processing, Computer Vision and Pattern Recognition, Machine Learning","Backgrounds.Deep neural networks have demonstrated high accuracy in classifying Alzheimer’s disease (AD). This study aims to enlighten the underlying black-box nature and reveal individual contributions of T1-weighted (T1w) gray-white matter texture, volumetric information and preprocessing on classification performance.Methods.We utilized T1w MRI data from the Alzheimer’s Disease Neuroimaging Initiative to distinguish matched AD patients (990 MRIs) from healthy controls (990 MRIs). Preprocessing included skull stripping and binarization at varying thresholds to systematically eliminate texture information. A deep neural network was trained on these configurations, and the model performance was compared using McNemar tests with discrete Bonferroni-Holm correction. Layer-wise Relevance Propagation (LRP) and structural similarity metrics between heatmaps were applied to analyze learned features.Results.Classification performance metrics (accuracy, sensitivity, and specificity) were comparable across all configurations, indicating a negligible influence of T1w gray- and white signal texture. Models trained on binarized images demonstrated similar feature performance and relevance distributions, with volumetric features such as atrophy and skull-stripping features emerging as primary contributors.Conclusions.We revealed a previously undiscovered Clever Hans effect in a widely used AD MRI dataset. Deep neural networks classification predominantly rely on volumetric features, while eliminating gray-white matter T1w texture did not decrease the performance. This study clearly demonstrates an overestimation of the importance of gray-white matter contrasts, at least for widely used structural T1w images, and highlights potential misinterpretation of performance metrics."
308,679d459debd8ffd557a2afa1,cs.CV,https://arxiv.org/pdf/2501.15743,Z-Stack Scanning can Improve AI Detection of Mitosis: A Case Study of Meningiomas,"Hongyan Gu, Ellie Onstott, Wenzhong Yan, Tengyou Xu, Ruolin Wang, Zida Wu, Xiang 'Anthony' Chen, Mohammad Haeri","Image and Video Processing, Computer Vision and Pattern Recognition","Z-stack scanning is an emerging whole slide imaging technology that captures multiple focal planes alongside the z-axis of a glass slide. Because z-stacking can offer enhanced depth information compared to the single-layer whole slide imaging, this technology can be particularly useful in analyzing small-scaled histopathological patterns. However, its actual clinical impact remains debated with mixed results. To clarify this, we investigate the effect of z-stack scanning on artificial intelligence (AI) mitosis detection of meningiomas. With the same set of 22 Hematoxylin and Eosin meningioma glass slides scanned by three different digital pathology scanners, we tested the performance of three AI pipelines on both single-layer and z-stacked whole slide images (WSIs). Results showed that in all scanner-AI combinations, z-stacked WSIs significantly increased AI’s sensitivity (+17.14%) on the mitosis detection with only a marginal impact on precision. Our findings provide quantitative evidence that highlights z-stack scanning as a promising technique for AI mitosis detection, paving the way for more reliable AI-assisted pathology workflows, which can ultimately benefit patient management."
309,679d459debd8ffd557a2afa2,cs.CV,https://arxiv.org/pdf/2501.15733,Leveraging Video Vision Transformer for Alzheimer's Disease Diagnosis from 3D Brain MRI,"Taymaz Akan, Sait Alp, Md. Shenuarin Bhuiyan, Elizabeth A. Disbrow, Steven A. Conrad, John A. Vanchiere, Christopher G. Kevil, Mohammad A. N. Bhuiyan","Image and Video Processing, Artificial Intelligence, Computer Vision and Pattern Recognition",
310,679d459debd8ffd557a2afa3,cs.CV,https://arxiv.org/pdf/2501.15712,SeqSeg: Learning Local Segments for Automatic Vascular Model Construction,"Numi Sveinsson Cepero, Shawn C. Shadden","Image and Video Processing, Computer Vision and Pattern Recognition, Tissues and Organs","Computational modeling of cardiovascular function has become a critical part of diagnosing, treating and understanding cardiovascular disease. Most strategies involve constructing anatomically accurate computer models of cardiovascular structures, which is a multistep, time-consuming process. To improve the model generation process, we herein present SeqSeg (sequential segmentation): a novel deep learning based automatic tracing and segmentation algorithm for constructing image-based vascular models. SeqSeg leverages local U-Net-based inference to sequentially segment vascular structures from medical image volumes. We tested SeqSeg on CT and MR images of aortic and aortofemoral models and compared the predictions to those of benchmark 2D and 3D global nnU-Net models, which have previously shown excellent accuracy for medical image segmentation. We demonstrate that SeqSeg is able to segment more complete vasculature and is able to generalize to vascular structures not annotated in the training data."
311,679d459debd8ffd557a2afa4,cs.CV,https://arxiv.org/pdf/2501.15659,AirIO: Learning Inertial Odometry with Enhanced IMU Feature Observability,"Yuheng Qiu, Can Xu, Yutian Chen, Shibo Zhao, Junyi Geng, Sebastian Scherer","Robotics, Computer Vision and Pattern Recognition, Machine Learning","Inertial odometry (IO) using only Inertial Measurement Units (IMUs) offers a lightweight and cost-effective solution for Unmanned Aerial Vehicle (UAV) applications, yet existing learning-based IO models often fail to generalize to UAVs due to the highly dynamic and non-linear-flight patterns that differ from pedestrian motion.
In this work, we identify that the conventional practice of transforming raw IMU data to global coordinates undermines the observability of critical kinematic information in UAVs. By preserving the body-frame representation, our method achieves substantial performance improvements, with a 66.7% average increase in accuracy across three datasets. Furthermore, explicitly encoding attitude information into the motion network results in an additional 23.8% improvement over prior results. Combined with a data-driven IMU correction model (AirIMU) and an uncertainty-aware Extended Kalman Filter (EKF), our approach ensures robust state estimation under aggressive UAV maneuvers without relying on external sensors or control inputs. Notably, our method also demonstrates strong generalizability to unseen data not included in the training set, underscoring its potential for real-world UAV applications."
312,679d459debd8ffd557a2afa5,cs.CV,https://arxiv.org/pdf/2501.15610,Radiologist-in-the-Loop Self-Training for Generalizable CT Metal Artifact Reduction,"Chenglong Ma, Zilong Li, Yuanlin Li, Jing Han, Junping Zhang, Yi Zhang, Jiannan Liu, Hongming Shan","Image and Video Processing, Computer Vision and Pattern Recognition","Metal artifacts in computed tomography (CT) images can significantly degrade image quality and impede accurate diagnosis.
Supervised metal artifact reduction (MAR) methods, trained using simulated datasets, often struggle to perform well on real clinical CT images due to a substantial domain gap.
Although state-of-the-art semi-supervised methods use pseudo ground-truths generated by a prior network to mitigate this issue, their reliance on a fixed prior limits both the quality and quantity of these pseudo ground-truths, introducing confirmation bias and reducing clinical applicability.
To address these limitations, we propose a novelradiologist-in-the-loopself-training framework for MAR, termed RISE-MAR, which can integrate radiologists’ feedback into the semi-supervised learning process, progressively improving the quality and quantity of pseudo ground-truths for enhanced generalization on real clinical CT images.
For quality assurance, we introduce a clinical quality assessor model that emulates radiologist evaluations, effectively selecting high-quality pseudo ground-truths for semi-supervised training.
For quantity assurance, our self-training framework iteratively generates additional high-quality pseudo ground-truths, expanding the clinical dataset and further improving model generalization.
Extensive experimental results on multiple clinical datasets demonstrate the superior generalization performance of our RISE-MAR over state-of-the-art methods, advancing the development of MAR models for practical application. Code is available athttps://github.com/Masaaki-75/rise-mar."
313,679d459debd8ffd557a2afa6,cs.CV,https://arxiv.org/pdf/2501.15598,Diffusion Generative Modeling for Spatially Resolved Gene Expression Inference from Histology Images,"Sichen Zhu, Yuchen Zhu, Molei Tao, Peng Qiu","Quantitative Methods, Artificial Intelligence, Computer Vision and Pattern Recognition, Machine Learning, Machine Learning","Spatial Transcriptomics (ST) allows a high-resolution measurement of RNA sequence abundance by systematically connecting cell morphology depicted in Hematoxylin and Eosin (H&E) stained histology images to spatially resolved gene expressions. ST is a time-consuming, expensive yet powerful experimental technique that provides new opportunities to understand cancer mechanisms at a fine-grained molecular level, which is critical for uncovering new approaches for disease diagnosis and treatments. Here, we presentStem(SpaTially resolved geneExpression inference with diffusionModel), a novel computational tool that leverages a conditional diffusion generative model to enable in silico gene expression inference from H&E stained images. Through better capturing the inherent stochasticity and heterogeneity in ST data,Stemachieves state-of-the-art performance on spatial gene expression prediction and generates biologically meaningful gene profiles for new H&E stained images at test time. We evaluate the proposed algorithm on datasets with various tissue sources and sequencing platforms, where it demonstrates clear improvement over existing approaches.Stemgenerates high-fidelity gene expression predictions that share similar gene variation levels as ground truth data, suggesting that our method preserves the underlying biological heterogeneity. Our proposed pipeline opens up the possibility of analyzing existing, easily accessible H&E stained histology images from a genomics point of view without physically performing gene expression profiling and empowers potential biological discovery from H&E stained histology images.Code is available at:https://github.com/SichenZhu/Stem."
314,679d459debd8ffd557a2afa7,cs.CV,https://arxiv.org/pdf/2501.15588,"Tumor Detection, Segmentation and Classification Challenge on Automated 3D Breast Ultrasound: The TDSC-ABUS Challenge","Gongning Luo, Mingwang Xu, Hongyu Chen, Xinjie Liang, Xing Tao, Dong Ni, Hyunsu Jeong, Chulhong Kim, Raphael Stock, Michael Baumgartner, Yannick Kirchhoff, Maximilian Rokuss, Klaus Maier-Hein, Zhikai Yang, Tianyu Fan, Nicolas Boutry, Dmitry Tereshchenko, Arthur Moine, Maximilien Charmetant, Jan Sauer, Hao Du, Xiang-Hui Bai, Vipul Pai Raikar, Ricardo Montoya-del-Angel, Robert Marti, Miguel Luna, Dongmin Lee, Abdul Qayyum, Moona Mazher, Qihui Guo, Changyan Wang, Navchetan Awasthi, Qiaochu Zhao, Wei Wang, Kuanquan Wang, Qiucheng Wang, Suyu Dong","Image and Video Processing, Computer Vision and Pattern Recognition","Breast cancer is one of the most common causes of death among women worldwide. Early detection helps in reducing the number of deaths. Automated 3D Breast Ultrasound (ABUS) is a newer approach for breast screening, which has many advantages over handheld mammography such as safety, speed, and higher detection rate of breast cancer.
Tumor detection, segmentation, and classification are key components in the analysis of medical images, especially challenging in the context of 3D ABUS due to the significant variability in tumor size and shape, unclear tumor boundaries, and a low signal-to-noise ratio. The lack of publicly accessible, well-labeled ABUS datasets further hinders the advancement of systems for breast tumor analysis.
Addressing this gap, we have organized the inauguralTumorDetection,Segmentation, andClassification Challenge onAutomated 3DBreastUltrasound 2023 (TDSC-ABUS2023). This initiative aims to spearhead research in this field and create a definitive benchmark for tasks associated with 3D ABUS image analysis.
In this paper, we summarize the top-performing algorithms from the challenge and provide critical analysis for ABUS image examination. We offer the TDSC-ABUS challenge as an open-access platform athttps://tdsc-abus2023.grand-challenge.org/to benchmark and inspire future developments in algorithmic research."
315,679d459debd8ffd557a2afa8,cs.CV,https://arxiv.org/pdf/2501.15573,Approximate Message Passing for Bayesian Neural Networks,"Romeo Sommerfeld, Christian Helms, Ralf Herbrich","Machine Learning, Computer Vision and Pattern Recognition",
316,679d459debd8ffd557a2afa9,cs.CV,https://arxiv.org/pdf/2501.15572,"Comparative clinical evaluation of ""memory-efficient"" synthetic 3d generative adversarial networks (gan) head-to-head to state of art: results on computed tomography of the chest","Mahshid shiri, Chandra Bortolotto, Alessandro Bruno, Alessio Consonni, Daniela Maria Grasso, Leonardo Brizzi, Daniele Loiacono, Lorenzo Preda","Image and Video Processing, Artificial Intelligence, Computer Vision and Pattern Recognition",
317,679d459debd8ffd557a2afaa,cs.CV,https://arxiv.org/pdf/2501.15505,Unveiling the Potential of iMarkers: Invisible Fiducial Markers for Advanced Robotics,"Ali Tourani, Deniz Isinsu Avsar, Hriday Bavle, Jose Luis Sanchez-Lopez, Jan Lagerwall, Holger Voos","Robotics, Computer Vision and Pattern Recognition","Fiducial markers are widely used in various robotics tasks, facilitating enhanced navigation, object recognition, and scene understanding.
Despite their advantages for robots and Augmented Reality (AR) applications, they often disrupt the visual aesthetics of environments because they are visible to humans, making them unsuitable for non-intrusive use cases.
To address this gap, this paper presents“iMarkers”—innovative, unobtrusive fiducial markers detectable exclusively by robots equipped with specialized sensors.
These markers offer high flexibility in production, allowing customization of their visibility range and encoding algorithms to suit various demands.
The paper also introduces the hardware designs and software algorithms developed for detecting iMarkers, highlighting their adaptability and robustness in detection and recognition stages.
Various evaluations have demonstrated the effectiveness of iMarkers compared to conventional (printed) and blended fiducial markers and confirmed their applicability in diverse robotics scenarios."
318,679d459debd8ffd557a2afab,cs.CV,https://arxiv.org/pdf/2501.15486,FedAlign: Federated Domain Generalization with Cross-Client Feature Alignment,"Sunny Gupta, Vinay Sutar, Varunav Singh, Amit Sethi","Machine Learning, Artificial Intelligence, Computer Vision and Pattern Recognition, Distributed, Parallel, and Cluster Computing","Federated Learning (FL)offers a decentralized paradigm for collaborative model training without direct data sharing, yet it poses unique challenges forDomain Generalization (DG), including strict privacy constraints, non-i.i.d. local data, and limited domain diversity. We introduceFedAlign, a lightweight, privacy-preserving framework designed to enhance DG in federated settings by simultaneously increasing feature diversity and promoting domain invariance. First, a cross-client feature extension module broadens local domain representations through domain-invariant feature perturbation and selective cross-client feature transfer, allowing each client to safely access a richer domain space. Second, a dual-stage alignment module refines global feature learning by aligning both feature embeddings and predictions across clients, thereby distilling robust, domain-invariant features. By integrating these modules, our method achieves superior generalization to unseen domains while maintaining data privacy and operating with minimal computational and communication overhead."
319,679d459debd8ffd557a2afac,cs.CV,https://arxiv.org/pdf/2501.15485,Differentiable Low-computation Global Correlation Loss for Monotonicity Evaluation in Quality Assessment,"Yipeng Liu, Qi Yang, Yiling Xu","Image and Video Processing, Computer Vision and Pattern Recognition","In this paper, we propose a global monotonicity consistency training strategy for quality assessment, which includes a differentiable, low-computation monotonicity evaluation loss function and a global perception training mechanism. Specifically, unlike conventional ranking loss and linear programming approaches that indirectly implement the Spearman rank-order correlation coefficient (SROCC) function, our method directly converts SROCC into a loss function by making the sorting operation within SROCC differentiable and functional. Furthermore, to mitigate the discrepancies between batch optimization during network training and global evaluation of SROCC, we introduce a memory bank mechanism. This mechanism stores gradient-free predicted results from previous batches and uses them in the current batch’s training to prevent abrupt gradient changes. We evaluate the performance of the proposed method on both images and point clouds quality assessment tasks, demonstrating performance gains in both cases."
320,679d459debd8ffd557a2afad,cs.CV,https://arxiv.org/pdf/2501.15450,FlatTrack: Eye-tracking with ultra-thin lensless cameras,"Purvam Jain, Althaf M. Nazar, Salman S. Khan, Kaushik Mitra, Praneeth Chakravarthula","Image and Video Processing, Computer Vision and Pattern Recognition","Existing eye trackers use cameras based on thick compound optical elements, necessitating the cameras to be placed at
focusing distance from the eyes. This results in the overall bulk of wearable eye trackers, especially for augmented and virtual reality
(AR/VR) headsets. We overcome this limitation by building a compact flat eye gaze tracker using mask-based lensless cameras. These
cameras, in combination with co-designed lightweight deep neural network algorithm, can be placed in extreme close proximity to the
eye, within the eyeglasses frame, resulting in ultra-flat and lightweight eye gaze tracker system. We collect a large dataset of near-eye
lensless camera measurements along with their calibrated gaze directions for training the gaze tracking network. Through real and
simulation experiments, we show that the proposed gaze tracking system performs on par with conventional lens-based trackers while
maintaining a significantly flatter and more compact form-factor. Moreover, our gaze regressor boasts real-time (>>>125 fps) performance for gaze tracking."
321,679d459debd8ffd557a2afae,cs.CV,https://arxiv.org/pdf/2501.15435,Making Sense Of Distributed Representations With Activation Spectroscopy,"Kyle Reing, Greg Ver Steeg, Aram Galstyan","Machine Learning, Computer Vision and Pattern Recognition","In the study of neural network interpretability, there is growing evidence to suggest that relevant features are encoded across many neurons in a distributed fashion. Making sense of these distributed representations without knowledge of the network’s encoding strategy is a combinatorial task that is not guaranteed to be tractable. This work explores one feasible path to both detecting and tracing the joint influence of neurons in a distributed representation. We term this approachActivation Spectroscopy(ActSpec), owing to its analysis of the pseudo-Boolean Fourier spectrum defined over the activation patterns of a network layer. The sub-network defined between a given layer and an output logit is cast as a special class of pseudo-Boolean function. The contributions of each subset of neurons in the specified layer can be quantified through the function’s Fourier coefficients. We propose a combinatorial optimization procedure to search for Fourier coefficients that are simultaneously high-valued, and non-redundant. This procedure can be viewed as an extension of the Goldreich-Levin algorithm which incorporates additional problem-specific constraints. The resulting coefficients specify a collection of subsets, which are used to test the degree to which a representation is distributed. We verify our approach in a number of synthetic settings and compare against existing interpretability benchmarks. We conclude with a number of experimental evaluations on an MNIST classifier, and a transformer-based network for sentiment analysis."
322,679d459debd8ffd557a2afaf,cs.CV,https://arxiv.org/pdf/2501.15423,Stroke Lesion Segmentation using Multi-Stage Cross-Scale Attention,"Liang Shang, William A. Sethares, Anusha Adluru, Andrew L. Alexander, Vivek Prabhakaran, Veena A. Nair, Nagesh Adluru","Image and Video Processing, Computer Vision and Pattern Recognition","Precise characterization of stroke lesions from MRI data has immense value in prognosticating clinical and cognitive outcomes following a stroke. Manual stroke lesion segmentation is time-consuming and requires the expertise of neurologists and neuroradiologists. Often, lesions are grossly characterized for their location and overall extent using bounding boxes without specific delineation of their boundaries. While such characterization provides some clinical value, to develop a precise mechanistic understanding of the impact of lesions on post-stroke vascular contributions to cognitive impairments and dementia (VCID), the stroke lesions need to be fully segmented with accurate boundaries.
This work introduces the Multi-Stage Cross-Scale Attention (MSCSA) mechanism, applied to the U-Net family, to improve the mapping between brain structural features and lesions of varying sizes. Using the Anatomical Tracings of Lesions After Stroke (ATLAS) v2.0 dataset, MSCSA outperforms all baseline methods in both Dice and F1 scores on a subset focusing on small lesions, while maintaining competitive performance across the entire dataset. Notably, the ensemble strategy incorporating MSCSA achieves the highest scores for Dice and F1 on both the full dataset and the small lesion subset.
These results demonstrate the effectiveness of MSCSA in segmenting small lesions and highlight its robustness across different training schemes for large stroke lesions.
Our code is available at:https://github.com/nadluru/StrokeLesSeg."
323,679d459debd8ffd557a2afb0,cs.CV,https://arxiv.org/pdf/2501.15396,Foundations of a Knee Joint Digital Twin from qMRI Biomarkers for Osteoarthritis and Knee Replacement,"Gabrielle Hoyer, Kenneth T Gao, Felix G Gassert, Johanna Luitjens, Fei Jiang, Sharmila Majumdar, Valentina Pedoia","Quantitative Methods, Computer Vision and Pattern Recognition, Machine Learning, Image and Video Processing, Applications",
324,679d459debd8ffd557a2afb1,cs.CV,https://arxiv.org/pdf/2501.15379,Zero-Shot Interactive Text-to-Image Retrieval via Diffusion-Augmented Representations,"Zijun Long, Kangheng Liang, Gerardo Aragon-Camarasa, Richard Mccreadie, Paul Henderson","Information Retrieval, Artificial Intelligence, Computer Vision and Pattern Recognition","Interactive Text-to-Image Retrieval (I-TIR) has emerged as a transformative user interactive ways for applications in domains such as e-commerce and education. Yet, current methodologies predominantly depend on finetuned Multimodal Large Language Models (MLLMs), which face two critical limitations: (1) Finetuning imposes prohibitive computational overhead and long-term maintenance costs. (2) Finetuning narrows the pretrained knowledge distribution of MLLMs, reducing their adaptability to novel scenarios. These issues are exacerbated by the inherently dynamic nature of real-world I-TIR systems, where queries and image databases evolve in complexity and diversity, often deviating from static training distributions. To overcome these constraints, we proposeDiffusion Augmented Retrieval (DAR), a paradigm-shifting framework that bypasses MLLM finetuning entirely. DAR synergizesLarge Language Model (LLM)-guided query refinementwithDiffusion Model (DM)-based visual synthesisto createcontextually enriched intermediate representations. This dual-modality approach deciphers nuanced user intent more holistically, enabling precise alignment between textual queries and visually relevant images. Rigorous evaluations across four benchmarks reveal DAR’s dual strengths: (1) Matches state-of-the-art finetuned I-TIR models on straightforward querieswithout task-specific training. (2) Scalable Generalization: Surpasses finetuned baselines by7.61% in Hits@10(top-10 accuracy) under multi-turn conversational complexity, demonstrating robustness to intricate, distributionally shifted interactions. By eliminating finetuning dependencies and leveraging generative-augmented representations, DAR establishes a new trajectory for efficient, adaptive, and scalable cross-modal retrieval systems."
325,679d459debd8ffd557a2afb2,cs.CV,https://arxiv.org/pdf/2501.15363,AI-Driven Secure Data Sharing: A Trustworthy and Privacy-Preserving Approach,"Al Amin, Kamrul Hasan, Sharif Ullah, Liang Hong","Cryptography and Security, Computer Vision and Pattern Recognition","In the era of data-driven decision-making, ensuring the privacy and security of shared data is paramount across various domains. Applying existing deep neural networks (DNNs) to encrypted data is critical and often compromises performance, security, and computational overhead. To address these limitations, this research introduces a secure framework consisting of a learnable encryption method based on the block-pixel operation to encrypt the data and subsequently integrate it with the Vision Transformer (ViT). The proposed framework ensures data privacy and security by creating unique scrambling patterns per key, providing robust performance against adversarial attacks without compromising computational efficiency and data integrity. The framework was tested on sensitive medical datasets to validate its efficacy, proving its capability to handle highly confidential information securely. The suggested framework was validated with a 94% success rate after extensive testing on real-world datasets, such as MRI brain tumors and histological scans of lung and colon cancers. Additionally, the framework was tested under diverse adversarial attempts against secure data sharing with optimum performance and demonstrated its effectiveness in various threat scenarios. These comprehensive analyses underscore its robustness, making it a trustworthy solution for secure data sharing in critical applications."
326,679d459debd8ffd557a2afb3,cs.CV,https://arxiv.org/pdf/2501.15343,Development and Application of Self-Supervised Machine Learning for Smoke Plume and Active Fire Identification from the FIREX-AQ Datasets,"Nicholas LaHaye, Anistasija Easley, Kyongsik Yun, Huikyo Lee, Erik Linstead, Michael J. Garay, Olga V. Kalashnikova","Machine Learning, Artificial Intelligence, Computer Vision and Pattern Recognition","Fire Influence on Regional to Global Environments and Air Quality (FIREX-AQ) was a field campaign aimed at better understanding the impact of wildfires and agricultural fires on air quality and climate. The FIREX-AQ campaign took place in August 2019 and involved two aircraft and multiple coordinated satellite observations. This study applied and evaluated a self-supervised machine learning (ML) method for the active fire and smoke plume identification and tracking in the satellite and sub-orbital remote sensing datasets collected during the campaign. Our unique methodology combines remote sensing observations with different spatial and spectral resolutions. The demonstrated approach successfully differentiates fire pixels and smoke plumes from background imagery, enabling the generation of a per-instrument smoke and fire mask product, as well as smoke and fire masks created from the fusion of selected data from independent instruments. This ML approach has a potential to enhance operational wildfire monitoring systems and improve decision-making in air quality management through fast smoke plume identification and tracking and could improve climate impact studies through fusion data from independent instruments."
327,679d459debd8ffd557a2afb4,cs.CV,https://arxiv.org/pdf/2501.15309,Investigating the Feasibility of Patch-based Inference for Generalized Diffusion Priors in Inverse Problems for Medical Images,"Saikat Roy, Mahmoud Mostapha, Radu Miron, Matt Holbrook, Mariappan Nadar","Image and Video Processing, Computer Vision and Pattern Recognition, Machine Learning","Plug-and-play approaches to solving inverse problems such as restoration and super-resolution have recently benefited from Diffusion-based generative priors for natural as well as medical images. However, solutions often use the standard albeit computationally intensive route of training and inferring with thewhole imageon the diffusion prior. While patch-based approaches to evaluating diffusion priors in plug-and-play methods have received some interest, they remain an open area of study. In this work, we explore the feasibility of the usage of patches for training and inference of a diffusion prior on MRI images. We explore the minor adaptation necessary for artifact avoidance, the performance and the efficiency of memory usage of patch-based methods as well as the adaptability of whole image training to patch-based evaluation – evaluating across multiple plug-and-play methods, tasks and datasets."
328,679d459debd8ffd557a2afb5,cs.CV,https://arxiv.org/pdf/2501.15269,Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language Models with Only Attention Sink,"Yining Wang, Mi Zhang, Junjie Sun, Chenyue Wang, Min Yang, Hui Xue, Jialing Tao, Ranjie Duan, Jiexi Liu","Machine Learning, Cryptography and Security, Computer Vision and Pattern Recognition","Fusing visual understanding into language generation, Multi-modal Large Language Models (MLLMs) are revolutionizing visual-language applications. Yet, these models are often plagued by thehallucination problem, which involves generating inaccurate objects, attributes, and relationships that do not match the visual content. In this work, we delve into the internal attention mechanisms of MLLMs to reveal the underlying causes of hallucination, exposing the inherent vulnerabilities in the instruction-tuning process."
329,679d459debd8ffd557a2afb6,cs.CV,https://arxiv.org/pdf/2501.15235,Large-Scale Riemannian Meta-Optimization via Subspace Adaptation,"Peilin Yu, Yuwei Wu, Zhi Gao, Xiaomeng Fan, Yunde Jia","Machine Learning, Computer Vision and Pattern Recognition","Riemannian meta-optimization provides a promising approach to solving non-linear constrained optimization problems, which trains neural networks as optimizers to perform optimization on Riemannian manifolds. However, existing Riemannian meta-optimization methods take up huge memory footprints in large-scale optimization settings, as the learned optimizer can only adapt gradients of a fixed size and thus cannot be shared across different Riemannian parameters. In this paper, we propose an efficient Riemannian meta-optimization method that significantly reduces the memory burden for large-scale optimization via a subspace adaptation scheme. Our method trains neural networks to individually adapt the row and column subspaces of Riemannian gradients, instead of directly adapting the full gradient matrices in existing Riemannian meta-optimization methods. In this case, our learned optimizer can be shared across Riemannian parameters with different sizes. Our method reduces the model memory consumption by six orders of magnitude when optimizing an orthogonal mainstream deep neural network (e.g.ResNet50). Experiments on multiple Riemannian tasks show that our method can not only reduce the memory consumption but also improve the performance of Riemannian meta-optimization."
330,679d459debd8ffd557a2afb7,cs.CV,https://arxiv.org/pdf/2501.15128,MAP-based Problem-Agnostic diffusion model for Inverse Problems,"Pingping Tao, Haixia Liu, Jing Su, Xiaochen Yang, Hongchen Tan","Image and Video Processing, Computer Vision and Pattern Recognition","Diffusion models have indeed shown great promise in
solving inverse problems in image processing. In this paper, we propose a novel, problem-agnostic diffusion model called the maximum a posteriori (MAP)-based guided term estimation
method for inverse problems. We divide the conditional score function into two terms according to Bayes’ rule: the
unconditional score function and the guided term. We design the MAP-based guided term estimation method, while the
unconditional score function is approximated by an existing score network. To estimate the guided term, we base on the assumption that the space of clean natural images is inherently
smooth, and introduce a MAP estimate of thet𝑡titalic_t-th latent variable. We then substitute this estimation into the expression of
the inverse problem and obtain the approximation of the guided term.
We evaluate our method extensively on super-resolution, inpainting, and denoising tasks, and demonstrate comparable performance to DDRM, DMPS, DPS andΠΠ\Piroman_ΠGDM."
331,679d459debd8ffd557a2afb8,cs.CV,https://arxiv.org/pdf/2501.15001,What if Eye...? Computationally Recreating Vision Evolution,"Kushagra Tiwary, Aaron Young, Zaid Tasneem, Tzofi Klinghoffer, Akshat Dave, Tomaso Poggio, Dan Nilsson, Brian Cheung, Ramesh Raskar","Artificial Intelligence, Computer Vision and Pattern Recognition, Neural and Evolutionary Computing, Neurons and Cognition",
332,679d459debd8ffd557a2afb9,cs.CV,https://arxiv.org/pdf/2501.14877,DrawEduMath: Evaluating Vision Language Models with Expert-Annotated Students' Hand-Drawn Math Images,"Sami Baral, Li Lucy, Ryan Knight, Alice Ng, Luca Soldaini, Neil T. Heffernan, Kyle Lo","Computation and Language, Computer Vision and Pattern Recognition","In real-world settings, vision language models (VLMs) should robustly handle naturalistic, noisy visual content as well as domain-specific language and concepts. For example, K-12 educators using digital learning platforms may need to examine and provide feedback across many images of students’ math work. To assess the potential of VLMs to support educators in settings like this one, we introduceDrawEduMath, an English-language dataset of 2,030 images of students’ handwritten responses to K-12 math problems. Teachers provided detailed annotations, including free-form descriptions of each image and 11,661 question-answer (QA) pairs. These annotations capture a wealth of pedagogical insights, ranging from students’ problem-solving strategies to the composition of their drawings, diagrams, and writing. We evaluate VLMs on teachers’ QA pairs, as well as 44,362 synthetic QA pairs derived from teachers’ descriptions using language models (LMs). We show that even state-of-the-art VLMs leave much room for improvement onDrawEduMath questions. We also find that synthetic QAs, though imperfect, can yield similar model rankings as teacher-written QAs. We releaseDrawEduMath to support the evaluation of VLMs’ abilities to reason mathematically over images gathered with educational contexts in mind."
333,679d459debd8ffd557a2afba,cs.CV,https://arxiv.org/pdf/2501.14744,FSTA-SNN:Frequency-based Spatial-Temporal Attention Module for Spiking Neural Networks,"Kairong Yu, Tianqing Zhang, Hongwei Wang, Qi Xu","Neural and Evolutionary Computing, Computer Vision and Pattern Recognition, Machine Learning","Spiking Neural Networks (SNNs) are emerging as a promising alternative to Artificial Neural Networks (ANNs) due to their inherent energy efficiency.
Owing to the inherent sparsity in spike generation within SNNs, the in-depth analysis and optimization of intermediate output spikes are often neglected.
This oversight significantly restricts the inherent energy efficiency of SNNs and diminishes their advantages in spatiotemporal feature extraction, resulting in a lack of accuracy and unnecessary energy expenditure.
In this work, we analyze the inherent spiking characteristics of SNNs from both temporal and spatial perspectives.
In terms of spatial analysis, we find that shallow layers tend to focus on learning vertical variations, while deeper layers gradually learn horizontal variations of features.
Regarding temporal analysis, we observe that there is not a significant difference in feature learning across different time steps.
This suggests that increasing the time steps has limited effect on feature learning.
Based on the insights derived from these analyses, we propose aFrequency-basedSpatial-TemporalAttention (FSTA) module to enhance feature learning in SNNs.
This module aims to improve the feature learning capabilities by suppressing redundant spike features.
The experimental results indicate that the introduction of the FSTA module significantly reduces the spike firing rate of SNNs, demonstrating superior performance compared to state-of-the-art baselines across multiple datasets.
Our source code is available inhttps://github.com/yukairong/FSTA-SNN."
334,679d459debd8ffd557a2afbb,cs.CV,https://arxiv.org/pdf/2501.14729,HERMES: A Unified Self-Driving World Model for Simultaneous 3D Scene Understanding and Generation,"Xin Zhou, Dingkang Liang, Sifan Tu, Xiwu Chen, Yikang Ding, Dingyuan Zhang, Feiyang Tan, Hengshuang Zhao, Xiang Bai",Computer Vision and Pattern Recognition,"Driving World Models (DWMs) have become essential for autonomous driving by enabling future scene prediction. However, existing DWMs are limited to scene generation and fail to incorporate scene understanding, which involves interpreting and reasoning about the driving environment. In this paper, we present a unified Driving World Model namedHermes. We seamlessly integrate 3D scene understanding and future scene evolution (generation) through a unified framework in driving scenarios. Specifically,Hermesleverages a Bird’s-Eye View (BEV) representation to consolidate multi-view spatial information while preserving geometric relationships and interactions. We also introduce world queries, which incorporate world knowledge into BEV features via causal attention in the Large Language Model (LLM), enabling contextual enrichment for understanding and generation tasks. We conduct comprehensive studies on nuScenes and OmniDrive-nuScenes datasets to validate the effectiveness of our method.Hermesachieves state-of-the-art performance, reducing generation error by 32.4% and improving understanding metrics such as CIDEr by 8.0%. The model and code will be publicly released athttps://github.com/LMD0311/HERMES."
335,679d459debd8ffd557a2afbc,cs.CV,https://arxiv.org/pdf/2501.14726,Relightable Full-Body Gaussian Codec Avatars,"Shaofei Wang, Tomas Simon, Igor Santesteban, Timur Bagautdinov, Junxuan Li, Vasu Agrawal, Fabian Prada, Shoou-I Yu, Pace Nalbone, Matt Gramlich, Roman Lubachersky, Chenglei Wu, Javier Romero, Jason Saragih, Michael Zollhoefer, Andreas Geiger, Siyu Tang, Shunsuke Saito","Computer Vision and Pattern Recognition, Graphics","We propose Relightable Full-Body Gaussian Codec Avatars, a new approach for modeling relightable full-body avatars with fine-grained details including face and hands.
The unique challenge for relighting full-body avatars lies in the large deformations caused by body articulation and the resulting impact on appearance caused by light transport.
Changes in body pose can dramatically change the orientation of body surfaces with respect to lights, resulting in both local appearance changes due to changes in local light transport functions, as well as non-local changes due to occlusion between body parts.
To address this, we decompose the light transport into local and non-local effects.
Local appearance changes are modeled using learnable zonal harmonics for diffuse radiance transfer.
Unlike spherical harmonics, zonal harmonics are highly efficient to rotate under articulation.
This allows us to learn diffuse radiance transfer in a local coordinate frame, which disentangles the local radiance transfer from the articulation of the body.
To account for non-local appearance changes, we introduce a shadow network that predicts shadows given precomputed incoming irradiance on a base mesh.
This facilitates the learning of non-local shadowing between the body parts.
Finally, we use a deferred shading approach to model specular radiance transfer and better capture reflections and highlights such as eye glints.
We demonstrate that our approach successfully models both the local and non-local light transport required for relightable full-body avatars, with a superior generalization ability under novel illumination conditions and unseen poses."
336,679d459debd8ffd557a2afbd,cs.CV,https://arxiv.org/pdf/2501.14689,"Approach to Designing CV Systems for Medical Applications: Data, Architecture and AI","Dmitry Ryabtsev, Boris Vasilyev, Sergey Shershakov","Computer Vision and Pattern Recognition, Artificial Intelligence","This paper introduces an innovative software system for fundus image analysis that deliberately diverges from the conventional screening approach, opting not to predict specific diagnoses. Instead, our methodology mimics the diagnostic process by thoroughly analyzing both normal and pathological features of fundus structures, leaving the ultimate decision-making authority in the hands of healthcare professionals[1]."
337,679d459debd8ffd557a2afbe,cs.CV,https://arxiv.org/pdf/2501.14679,Surface Vision Mamba: Leveraging Bidirectional State Space Model for Efficient Spherical Manifold Representation,"Rongzhao He, Weihao Zheng","Computer Vision and Pattern Recognition, Artificial Intelligence",
338,679d459debd8ffd557a2afbf,cs.CV,https://arxiv.org/pdf/2501.14677,MatAnyone: Stable Video Matting with Consistent Memory Propagation,"Peiqing Yang, Shangchen Zhou, Jixin Zhao, Qingyi Tao, Chen Change Loy",Computer Vision and Pattern Recognition,"Auxiliary-free human video matting methods, which rely solely on input frames, often struggle with complex or ambiguous backgrounds. To address this, we propose MatAnyone, a robust framework tailored for target-assigned video matting. Specifically, building on a memory-based paradigm, we introduce a consistent memory propagation module via region-adaptive memory fusion, which adaptively integrates memory from the previous frame. This ensures semantic stability in core regions while preserving fine-grained details along object boundaries. For robust training, we present a larger, high-quality, and diverse dataset for video matting. Additionally, we incorporate a novel training strategy that efficiently leverages large-scale segmentation data, boosting matting stability. With this new network design, dataset, and training strategy, MatAnyone delivers robust and accurate video matting results in diverse real-world scenarios, outperforming existing methods."
339,679d459debd8ffd557a2afc0,cs.CV,https://arxiv.org/pdf/2501.14659,Towards Unified Structured Light Optimization,"Tinglei Wan, Tonghua Su, Zhongjie Wang",Computer Vision and Pattern Recognition,"Structured light (SL) 3D reconstruction captures the precise surface shape of objects, providing high-accuracy 3D data essential for industrial inspection and robotic vision systems.
However, current research on optimizing projection patterns in SL 3D reconstruction faces two main limitations: each scene requires separate training of calibration parameters, and optimization is restricted to specific types of SL, which restricts their application range.
To tackle these limitations, we present a unified framework for SL optimization, adaptable to diverse lighting conditions, object types, and different types of SL. Our framework quickly determines the optimal projection pattern using only a single projected image.
Key contributions include a novel global matching method for projectors, enabling precise projector-camera alignment with just one projected image, and a new projection compensation model with a photometric adjustment module to reduce artifacts from out-of-gamut clipping. Experimental results show our method achieves superior decoding accuracy across various objects, SL patterns, and lighting conditions, significantly outperforming previous methods."
340,679d459debd8ffd557a2afc1,cs.CV,https://arxiv.org/pdf/2501.14646,SyncAnimation: A Real-Time End-to-End Framework for Audio-Driven Human Pose and Talking Head Animation,"Yujian Liu, Shidang Xu, Jing Guo, Dingbin Wang, Zairan Wang, Xianfeng Tan, Xiaoli Liu",Computer Vision and Pattern Recognition,"Generating talking avatar driven by audio remains a significant challenge. Existing methods typically require high computational costs and often lack sufficient facial detail and realism, making them unsuitable for applications that demand high real-time performance and visual quality. Additionally, while some methods can synchronize lip movement, they still face issues with consistency between facial expressions and upper body movement, particularly during silent periods. In this paper, we introduce SyncAnimation, the first NeRF-based method that achieves audio-driven, stable, and real-time generation of speaking avatar by combining generalized audio-to-pose matching and audio-to-expression synchronization. By integrating AudioPose Syncer and AudioEmotion Syncer, SyncAnimation achieves high-precision poses and expression generation, progressively producing audio-synchronized upper body, head, and lip shapes. Furthermore, the High-Synchronization Human Renderer ensures seamless integration of the head and upper body, and achieves audio-sync lip. The project page can be found athttps://syncanimation.github.io/"
341,679d459debd8ffd557a2afc2,cs.CV,https://arxiv.org/pdf/2501.14607,ReferDINO: Referring Video Object Segmentation with Visual Grounding Foundations,"Tianming Liang, Kun-Yu Lin, Chaolei Tan, Jianguo Zhang, Wei-Shi Zheng, Jian-Fang Hu",Computer Vision and Pattern Recognition,"Referring video object segmentation (RVOS) aims to segment target objects throughout a video based on a text description.
Despite notable progress in recent years, current RVOS models remain struggle to handle complicated object descriptions due to their limited video-language understanding.
To address this limitation, we presentReferDINO, an end-to-end RVOS model that inherits strong vision-language understanding from the pretrained visual grounding foundation models, and is further endowed with effective temporal understanding and object segmentation capabilities.
In ReferDINO, we contribute three technical innovations for effectively adapting the foundation models to RVOS:
1) an object-consistent temporal enhancer that capitalizes on the pretrained object-text representations to enhance temporal understanding and object consistency;
2) a grounding-guided deformable mask decoder that integrates text and grounding conditions to generate accurate object masks;
3) a confidence-aware query pruning strategy that significantly improves the object decoding efficiency without compromising performance.
We conduct extensive experiments on five public RVOS benchmarks to demonstrate that our proposed ReferDINO outperforms state-of-the-art methods significantly.
Project page:https://isee-laboratory.github.io/ReferDINO."
342,679d459debd8ffd557a2afc3,cs.CV,https://arxiv.org/pdf/2501.14605,3DLabelProp: Geometric-Driven Domain Generalization for LiDAR Semantic Segmentation in Autonomous Driving,"Jules Sanchez, Jean-Emmanuel Deschaud, François Goulette",Computer Vision and Pattern Recognition,"Domain generalization aims to find ways for deep learning models to maintain their performance despite significant domain shifts between training and inference datasets. This is particularly important for models that need to be robust or are costly to train. LiDAR perception in autonomous driving is impacted by both of these concerns, leading to the emergence of various approaches. This work addresses the challenge by proposing a geometry-based approach, leveraging the sequential structure of LiDAR sensors, which sets it apart from the learning-based methods commonly found in the literature. The proposed method, called 3DLabelProp, is applied on the task of LiDAR Semantic Segmentation (LSS). Through extensive experimentation on seven datasets, it is demonstrated to be a state-of-the-art approach, outperforming both naive and other domain generalization methods."
343,679d459debd8ffd557a2afc4,cs.CV,https://arxiv.org/pdf/2501.14593,Geometric Mean Improves Loss For Few-Shot Learning,"Tong Wu, Takumi Kobayashi",Computer Vision and Pattern Recognition,"Few-shot learning (FSL) is a challenging task in machine learning, demanding a model to render discriminative classification by using only a few labeled samples.
In the literature of FSL, deep models are trained in a manner of metric learning to provide metric in a feature space which is well generalizable to classify samples of novel classes; in the space, even a few amount of labeled training examples can construct an effective classifier.
In this paper, we propose a novel FSL loss based ongeometric meanto embed discriminative metric into deep features.
In contrast to the other losses such as utilizing arithmetic mean in softmax-based formulation, the proposed method leverages geometric mean to aggregate pair-wise relationships among samples for enhancing discriminative metric across class categories.
The proposed loss is not only formulated in a simple form but also is thoroughly analyzed in theoretical ways to reveal its favorable characteristics which are favorable for learning feature metric in FSL.
In the experiments on few-shot image classification tasks, the method produces competitive performance in comparison to the other losses."
344,679d459debd8ffd557a2afc5,cs.CV,https://arxiv.org/pdf/2501.14587,Visual Localization via Semantic Structures in Autonomous Photovoltaic Power Plant Inspection,"Viktor Kozák, Karel Košnar, Jan Chudoba, Miroslav Kulich, Libor Přeučil","Computer Vision and Pattern Recognition, Robotics",
345,679d459debd8ffd557a2afc6,cs.CV,https://arxiv.org/pdf/2501.14548,Large-scale and Fine-grained Vision-language Pre-training for Enhanced CT Image Understanding,"Zhongyi Shui, Jianpeng Zhang, Weiwei Cao, Sinuo Wang, Ruizhe Guo, Le Lu, Lin Yang, Xianghua Ye, Tingbo Liang, Qi Zhang, Ling Zhang",Computer Vision and Pattern Recognition,"Artificial intelligence (AI) shows great potential in assisting radiologists to improve the efficiency and accuracy of medical image interpretation and diagnosis. However, a versatile AI model requires large-scale data and comprehensive annotations, which are often impractical in medical settings. Recent studies leverage radiology reports as a naturally high-quality supervision for medical images, using contrastive language-image pre-training (CLIP) to develop language-informed models for radiological image interpretation. Nonetheless, these approaches typically contrast entire images with reports, neglecting the local associations between imaging regions and report sentences, which may undermine model performance and interoperability. In this paper, we propose a fine-grained vision-language model (fVLM) for anatomy-level CT image interpretation. Specifically, we explicitly match anatomical regions of CT images with corresponding descriptions in radiology reports and perform contrastive pre-training for each anatomy individually. Fine-grained alignment, however, faces considerable false-negative challenges, mainly from the abundance of anatomy-level healthy samples and similarly diseased abnormalities, leading to ambiguous patient-level pairings. To tackle this issue, we propose identifying false negatives of both normal and abnormal samples and calibrating contrastive learning from patient-level to disease-aware pairing.
We curated the largest CT dataset to date, comprising imaging and report data from 69,086 patients, and conducted a comprehensive evaluation of 54 major and important disease (including several most deadly cancers) diagnosis tasks across 15 main anatomies. Experimental results demonstrate the substantial potential of fVLM in versatile medical image interpretation. In the zero-shot classification task, we achieved an average AUC of 81.3% on 54 diagnosis tasks, surpassing CLIP and supervised methods by 12.9% and 8.0%, respectively.
Additionally, on the publicly available CT-RATE and Rad-ChestCT benchmarks, our fVLM outperformed the current state-of-the-art methods with absolute AUC gains of 7.4% and 4.8%, respectively. Code is available athttps://github.com/alibaba-damo-academy/fvlm"
346,679d459debd8ffd557a2afc7,cs.CV,https://arxiv.org/pdf/2501.14546,Leveraging ChatGPT's Multimodal Vision Capabilities to Rank Satellite Images by Poverty Level: Advancing Tools for Social Science Research,"Hamid Sarmadi, Ola Hall, Thorsteinn Rögnvaldsson, Mattias Ohlsson","Computer Vision and Pattern Recognition, Artificial Intelligence","This paper investigates the novel application of Large Language Models (LLMs) with vision capabilities to analyze satellite imagery for village-level poverty prediction. Although LLMs were originally designed for natural language understanding, their adaptability to multimodal tasks, including geospatial analysis, has opened new frontiers in data-driven research. By leveraging advancements in vision-enabled LLMs, we assess their ability to provide interpretable, scalable, and reliable insights into human poverty from satellite images. Using a pairwise comparison approach, we demonstrate that ChatGPT can rank satellite images based on poverty levels with accuracy comparable to domain experts. These findings highlight both the promise and the limitations of LLMs in socioeconomic research, providing a foundation for their integration into poverty assessment workflows. This study contributes to the ongoing exploration of unconventional data sources for welfare analysis and opens pathways for cost-effective, large-scale poverty monitoring."
347,679d459debd8ffd557a2afc8,cs.CV,https://arxiv.org/pdf/2501.14535,Rethinking Encoder-Decoder Flow Through Shared Structures,"Frederik Laboyrie, Mehmet Kerim Yucel, Albert Saa-Garriga","Computer Vision and Pattern Recognition, Machine Learning","Dense prediction tasks have enjoyed a growing complexity of encoder architectures, decoders, however, have remained largely the same. They rely on individual blocks decoding intermediate feature maps sequentially. We introduce banks, shared structures that are used by each decoding block to provide additional context in the decoding process. These structures, through applying them via resampling and feature fusion, improve performance on depth estimation for state-of-the-art transformer-based architectures on natural and synthetic images whilst training on large-scale datasets."
348,679d459debd8ffd557a2afc9,cs.CV,https://arxiv.org/pdf/2501.14534,Trick-GS: A Balanced Bag of Tricks for Efficient Gaussian Splatting,"Anil Armagan, Albert Saà-Garriga, Bruno Manganelli, Mateusz Nowak, Mehmet Kerim Yucel",Computer Vision and Pattern Recognition,"Gaussian splatting (GS) for 3D reconstruction has become quite popular due to their fast training, inference speeds and high quality reconstruction. However, GS-based reconstructions generally consist of millions of Gaussians, which makes them hard to use on computationally constrained devices such as smartphones. In this paper, we first propose a principled analysis of advances in efficient GS methods. Then, we propose Trick-GS, which is a careful combination of several strategies including (1) progressive training with resolution, noise and Gaussian scales, (2) learning to prune and mask primitives and SH bands by their significance, and (3) accelerated GS training framework. Trick-GS takes a large step towards resource-constrained GS, where faster run-time, smaller and faster-convergence of models is of paramount concern. Our results on three datasets show that Trick-GS achieves up to 2×\times×faster training, 40×\times×smaller disk size and 2×\times×faster rendering speed compared to vanilla GS, while having comparable accuracy."
349,679d459debd8ffd557a2afca,cs.CV,https://arxiv.org/pdf/2501.14533,CheapNVS: Real-Time On-Device Narrow-Baseline Novel View Synthesis,"Konstantinos Georgiadis, Mehmet Kerim Yucel, Albert Saa-Garriga",Computer Vision and Pattern Recognition,"Single-view novel view synthesis (NVS) is a notorious problem due to its ill-posed nature, and often requires large, computationally expensive approaches to produce tangible results. In this paper, we proposeCheapNVS: a fully end-to-end approach for narrow baseline single-view NVS based on a novel, efficient multiple encoder/decoder design trained in a multi-stage fashion. CheapNVS first approximates the laborious 3D image warping with lightweight learnable modules that are conditioned on the camera pose embeddings of the target view, and then performs inpainting on the occluded regions in parallel to achieve significant performance gains. Once trained on a subset of Open Images dataset, CheapNVS outperforms the state-of-the-art despite being 10×\times×faster and consuming 6% less memory. Furthermore, CheapNVS runs comfortably in real-time on mobile devices, reaching over 30 FPS on a Samsung Tab 9+."
350,679d459debd8ffd557a2afcb,cs.CV,https://arxiv.org/pdf/2501.14524,Training-Free Style and Content Transfer by Leveraging U-Net Skip Connections in Stable Diffusion 2.*,"Ludovica Schaerf, Andrea Alfarano, Fabrizio Silvestri, Leonardo Impett",Computer Vision and Pattern Recognition,"Despite significant recent advances in image generation with diffusion models, their internal latent representations remain poorly understood. Existing works focus on the bottleneck layer (h-space) of Stable Diffusion’s U-Net or leverage the cross-attention, self-attention, or decoding layers. Our model, SkipInject takes advantage of U-Net’s skip connections. We conduct thorough analyses on the role of the skip connections and find that the residual connections passed by the third encoder block carry most of the spatial information of the reconstructed image, splitting the content from the style. We show that injecting the representations from this block can be used for text-based editing, precise modifications, and style transfer. We compare our methods state-of-the-art style transfer and image editing methods and demonstrate that our method obtains the best content alignment and optimal structural preservation tradeoff."
351,679d459debd8ffd557a2afcc,cs.CV,https://arxiv.org/pdf/2501.14514,PARASIDE: An Automatic Paranasal Sinus Segmentation and Structure Analysis Tool for MRI,"Hendrik Möller, Lukas Krautschick, Matan Atad, Robert Graf, Chia-Jung Busch, Achim Beule, Christian Scharf, Lars Kaderali, Bjoern Menze, Daniel Rueckert, Jan Kirschke, Fabian Schwitzing","Computer Vision and Pattern Recognition, Machine Learning","Chronic rhinosinusitis (CRS) is a common and persistent sinus imflammation that affects 5 - 12% of the general population. It significantly impacts quality of life and is often difficult to assess due to its subjective nature in clinical evaluation. We introduce PARASIDE, an automatic tool for segmenting air and soft tissue volumes of the structures of the sinus maxillaris, frontalis, sphenodalis and ethmoidalis in T1 MRI. By utilizing that segmentation, we can quantify feature relations that have been observed only manually and subjectively before. We performed an exemplary study and showed both volume and intensity relations between structures and radiology reports. While the soft tissue segmentation is good, the automated annotations of the air volumes are excellent. The average intensity over air structures are consistently below those of the soft tissues, close to perfect separability. Healthy subjects exhibit lower soft tissue volumes and lower intensities. Our developed system is the first automated whole nasal segmentation of 16 structures, and capable of calculating medical relevant features such as the Lund-Mackay score."
352,679d459debd8ffd557a2afcd,cs.CV,https://arxiv.org/pdf/2501.14510,Deep-BrownConrady: Prediction of Camera Calibration and Distortion Parameters Using Deep Learning and Synthetic Data,"Faiz Muhammad Chaudhry, Jarno Ralli, Jerome Leudet, Fahad Sohrab, Farhad Pakdaman, Pierre Corbani, Moncef Gabbouj","Computer Vision and Pattern Recognition, Machine Learning","This research addresses the challenge of camera calibration and distortion parameter prediction from a single image using deep learning models. The main contributions of this work are: (1) demonstrating that a deep learning model, trained on a mix of real and synthetic images, can accurately predict camera and lens parameters from a single image, and (2) developing a comprehensive synthetic dataset using the AILiveSim simulation platform. This dataset includes variations in focal length and lens distortion parameters, providing a robust foundation for model training and testing. The training process predominantly relied on these synthetic images, complemented by a small subset of real images, to explore how well models trained on synthetic data can perform calibration tasks on real-world images. Traditional calibration methods require multiple images of a calibration object from various orientations, which is often not feasible due to the lack of such images in publicly available datasets. A deep learning network based on the ResNet architecture was trained on this synthetic dataset to predict camera calibration parameters following the Brown-Conrady lens model. The ResNet architecture, adapted for regression tasks, is capable of predicting continuous values essential for accurate camera calibration in applications such as autonomous driving, robotics, and augmented reality."
353,679d459debd8ffd557a2afce,cs.CV,https://arxiv.org/pdf/2501.14495,BILLNET: A Binarized Conv3D-LSTM Network with Logic-gated residual architecture for hardware-efficient video inference,"Van Thien Nguyen, William Guicquero, Gilles Sicard","Computer Vision and Pattern Recognition, Hardware Architecture","Long Short-Term Memory (LSTM) and 3D convolution (Conv3D) show impressive results for many video-based applications but require large memory and intensive computing. Motivated by recent works on hardware-algorithmic co-design towards efficient inference, we propose a compact binarized Conv3D-LSTM model architecture called BILLNET, compatible with a highly resource-constrained hardware. Firstly, BILLNET proposes to factorize the costly standard Conv3D by two pointwise convolutions with a grouped convolution in-between. Secondly, BILLNET enables binarized weights and activations via a MUX-OR-gated residual architecture. Finally, to efficiently train BILLNET, we propose a multi-stage training strategy enabling to fully quantize LSTM layers. Results on Jester dataset show that our method can obtain high accuracy with extremely low memory and computational budgets compared to existing Conv3D resource-efficient models."
354,679d459debd8ffd557a2afcf,cs.CV,https://arxiv.org/pdf/2501.14455,Triple Path Enhanced Neural Architecture Search for Multimodal Fake News Detection,"Bo Xu, Qiujie Xie, Jiahui Zhou, Linlin Zong",Computer Vision and Pattern Recognition,"Multimodal fake news detection has become one of the most crucial issues on social media platforms. Although existing methods have achieved advanced performance, two main challenges persist: (1) Under-performed multimodal news information fusion due to model architecture solidification, and (2) weak generalization ability on partial-modality contained fake news. To meet these challenges, we propose a novel and flexible triple path enhanced neural architecture search model MUSE. MUSE includes two dynamic paths for detecting partial-modality contained fake news and a static path for exploiting potential multimodal correlations. Experimental results show that MUSE achieves stable performance improvement over the baselines."
355,679d459debd8ffd557a2afd0,cs.CV,https://arxiv.org/pdf/2501.14439,Optimizing Human Pose Estimation Through Focused Human and Joint Regions,"Yingying Jiao, Zhigang Wang, Zhenguang Liu, Shaojing Fan, Sifan Wu, Zheqi Wu, Zhuoyue Xu",Computer Vision and Pattern Recognition,"Human pose estimation has given rise to a broad spectrum of novel and compelling applications, includingaction recognition,sports analysis, as well assurveillance. However, accurate video pose estimation remains an open challenge. One aspect that has been overlooked so far is that existing methods learn motion clues from all pixels rather than focusing on the target human body, making them easily misled and disrupted by unimportant information such asbackground changesormovements of other people. Additionally, while the current Transformer-based pose estimation methods has demonstrated impressive performance with global modeling, they struggle with local context perception and precise positional identification."
356,679d459debd8ffd557a2afd1,cs.CV,https://arxiv.org/pdf/2501.14413,Context-CrackNet: A Context-Aware Framework for Precise Segmentation of Tiny Cracks in Pavement images,"Blessing Agyei Kyem, Joshua Kofi Asamoah, Armstrong Aboah",Computer Vision and Pattern Recognition,"The accurate detection and segmentation of pavement distresses, particularly tiny and small cracks, are critical for early intervention and preventive maintenance in transportation infrastructure. Traditional manual inspection methods are labor-intensive and inconsistent, while existing deep learning models struggle with fine-grained segmentation and computational efficiency. To address these challenges, this study proposes Context-CrackNet, a novel encoder-decoder architecture featuring the Region-Focused Enhancement Module (RFEM) and Context-Aware Global Module (CAGM). These innovations enhance the model’s ability to capture fine-grained local details and global contextual dependencies, respectively. Context-CrackNet was rigorously evaluated on ten publicly available crack segmentation datasets, covering diverse pavement distress scenarios. The model consistently outperformed 9 state-of-the-art segmentation frameworks, achieving superior performance metrics such as mIoU and Dice score, while maintaining competitive inference efficiency. Ablation studies confirmed the complementary roles of RFEM and CAGM, with notable improvements in mIoU and Dice score when both modules were integrated. Additionally, the model’s balance of precision and computational efficiency highlights its potential for real-time deployment in large-scale pavement monitoring systems."
357,679d459debd8ffd557a2afd2,cs.CV,https://arxiv.org/pdf/2501.14404,Kolmogorov Arnold Neural Interpolator for Downscaling and Correcting Meteorological Fields from In-Situ Observations,"Zili Liu, Hao Chen, Lei Bai, Wenyuan Li, Zhengxia Zou, Zhenwei Shi",Computer Vision and Pattern Recognition,"Obtaining accurate weather forecasts at station locations is a critical challenge due to systematic biases arising from the mismatch between multi-scale, continuous atmospheric characteristic and their discrete, gridded representations. Previous works have primarily focused on modeling gridded meteorological data, inherently neglecting the off-grid, continuous nature of atmospheric states and leaving such biases unresolved. To address this, we propose theKolmogorov–Arnold Neural Interpolator(KANI), a novel framework that redefines meteorological field representation as continuous neural functions derived from discretized grids. Grounded in the Kolmogorov–Arnold theorem, KANI captures the inherent continuity of atmospheric states and leverages sparse in-situ observations to correct these biases systematically. Furthermore, KANI introduces an innovativezero-shotdownscaling capability, guided by high-resolution topographic textures without requiring high-resolution meteorological fields for supervision. Experimental results across three sub-regions of the continental United States indicate that KANI achieves an accuracy improvement of 40.28% for temperature and 67.41% for wind speed, highlighting its significant improvement over traditional interpolation methods. This enables continuous neural representation of meteorological variables through neural networks, transcending the limitations of conventional grid-based representations."
358,679d459debd8ffd557a2afd3,cs.CV,https://arxiv.org/pdf/2501.14401,"CVOCSemRPL: Class-Variance Optimized Clustering, Semantic Information Injection and Restricted Pseudo Labeling based Improved Semi-Supervised Few-Shot Learning","Rhythm Baghel, Souvik Maji, Pratik Mazumder",Computer Vision and Pattern Recognition,"Few-shot learning has been extensively explored to address problems where the amount of labeled samples is very limited for some classes. In the semi-supervised few-shot learning setting, substantial quantities of unlabeled samples are available. Such unlabeled samples are generally cheaper to obtain and can be used to improve the few-shot learning performance of the model. Some of the recent methods for this setting rely on clustering to generate pseudo-labels for the unlabeled samples. Since the quality of the representation learned by the model heavily influences the effectiveness of clustering, this might also lead to incorrect labeling of the unlabeled samples and consequently lead to a drop in the few-shot learning performance. We propose an approach for semi-supervised few-shot learning that performs a class-variance optimized clustering in order to improve the effectiveness of clustering the labeled and unlabeled samples in this setting. It also optimizes the clustering-based pseudo-labeling process using a restricted pseudo-labeling approach and performs semantic information injection in order to improve the semi-supervised few-shot learning performance of the model. We experimentally demonstrate that our proposed approach significantly outperforms recent state-of-the-art methods on the benchmark datasets."
359,679d459debd8ffd557a2afd4,cs.CV,https://arxiv.org/pdf/2501.14369,Low-rank Prompt Interaction for Continual Vision-Language Retrieval,"Weicai Yan, Ye Wang, Wang Lin, Zirun Guo, Zhou Zhao, Tao Jin",Computer Vision and Pattern Recognition,"Research on continual learning in multi-modal tasks has been receiving increasing attention. However, most existing work overlooks the explicit cross-modal and cross-task interactions. In this paper, we innovatively propose theLow-rankPromptInteraction (LPI) to address this general problem of multi-modal understanding, which considers both cross-modal and cross-task interactions. Specifically, as for the former, we employ multi-modal correlation modules for corresponding Transformer layers. Considering that the training parameters scale to the number of layers and tasks, we propose low-rank interaction-augmented decomposition to avoid memory explosion while enhancing the cross-modal association through sharing and separating common-specific low-rank factors. In addition, due to the multi-modal semantic differences carried by the low-rank initialization, we adopt hierarchical low-rank contrastive learning to ensure training robustness. As for the latter, we initially employ a visual analysis and identify that different tasks have clear distinctions in proximity. Therefore, we introduce explicit task contrastive constraints in the prompt learning process based on task semantic distances. Experiments on two retrieval tasks show performance improvements with the introduction of a minimal number of parameters, demonstrating the effectiveness of our method. Code is available athttps://github.com/Kelvin-ywc/LPI."
360,679d459debd8ffd557a2afd5,cs.CV,https://arxiv.org/pdf/2501.14356,Causal-Inspired Multitask Learning for Video-Based Human Pose Estimation,"Haipeng Chen, Sifan Wu, Zhigang Wang, Yifang Yin, Yingying Jiao, Yingda Lyu, Zhenguang Liu",Computer Vision and Pattern Recognition,"Video-based human pose estimation has long been a fundamental yet challenging problem in computer vision. Previous studies focus on spatio-temporal modeling through the enhancement of architecture design and optimization strategies. However, they overlook the causal relationships in the joints, leading to models that may be overly tailored and thus estimate poorly to challenging scenes. Therefore, adequate causal reasoning capability, coupled with good interpretability of model, are both indispensable and prerequisite for achieving reliable results. In this paper, we pioneer a causal perspective on pose estimation and introduce a causal-inspired multitask learning framework, consisting of two stages.In the first stage, we try to endow the model with causal spatio-temporal modeling ability by introducing two self-supervision auxiliary tasks. Specifically, these auxiliary tasks enable the network to infer challenging keypoints based on observed keypoint information, thereby imbuing causal reasoning capabilities into the model and making it robust to challenging scenes.In the second stage, we argue that not all feature tokens contribute equally to pose estimation. Prioritizing causal (keypoint-relevant) tokens is crucial to achieve reliable results, which could improve the interpretability of the model. To this end, we propose a Token Causal Importance Selection module to identify the causal tokens and non-causal tokens (e.g., background and objects). Additionally, non-causal tokens could provide potentially beneficial cues but may be redundant. We further introduce a non-causal tokens clustering module to merge the similar non-causal tokens. Extensive experiments show that our method outperforms state-of-the-art methods on three large-scale benchmark datasets."
361,679d459debd8ffd557a2afd6,cs.CV,https://arxiv.org/pdf/2501.14338,Correlation-Based Band Selection for Hyperspectral Image Classification,"Dibyabha Deb, Ujjwal Verma","Computer Vision and Pattern Recognition, Image and Video Processing","Hyperspectral images offer extensive spectral information about ground objects across multiple spectral bands. However, the large volume of data can pose challenges during processing. Typically, adjacent bands in hyperspectral data are highly correlated, leading to the use of only a few selected bands for various applications. In this work, we present a correlation-based band selection approach for hyperspectral image classification. Our approach calculates the average correlation between bands using correlation coefficients to identify the relationships among different bands. Afterward, we select a subset of bands by analyzing the average correlation and applying a threshold-based method. This allows us to isolate and retain bands that exhibit lower inter-band dependencies, ensuring that the selected bands provide diverse and non-redundant information. We evaluate our proposed approach on two standard benchmark datasets: Pavia University (PA) and Salinas Valley (SA), focusing on image classification tasks. The experimental results demonstrate that our method performs competitively with other standard band selection approaches."
362,679d459debd8ffd557a2afd7,cs.CV,https://arxiv.org/pdf/2501.14319,Scalable Benchmarking and Robust Learning for Noise-Free Ego-Motion and 3D Reconstruction from Noisy Video,"Xiaohao Xu, Tianyi Zhang, Shibo Zhao, Xiang Li, Sibo Wang, Yongqi Chen, Ye Li, Bhiksha Raj, Matthew Johnson-Roberson, Sebastian Scherer, Xiaonan Huang","Computer Vision and Pattern Recognition, Robotics",
363,679d459debd8ffd557a2afd8,cs.CV,https://arxiv.org/pdf/2501.14317,Nautilus: Locality-aware Autoencoder for Scalable Mesh Generation,"Yuxuan Wang, Xuanyu Yi, Haohan Weng, Qingshan Xu, Xiaokang Wei, Xianghui Yang, Chunchao Guo, Long Chen, Hanwang Zhang",Computer Vision and Pattern Recognition,"Triangle meshes are fundamental to 3D applications, enabling efficient modification and rasterization while maintaining compatibility with standard rendering pipelines. However, current automatic mesh generation methods typically rely on intermediate representations that lack the continuous surface quality inherent to meshes. Converting these representations into meshes produces dense, suboptimal outputs. Although recent autoregressive approaches demonstrate promise in directly modeling mesh vertices and faces, they are constrained by the limitation in face count, scalability, and structural fidelity.
To address these challenges, we propose Nautilus, a locality-aware autoencoder for artist-like mesh generation that leverages the local properties of manifold meshes to achieve structural fidelity and efficient representation. Our approach introduces a novel tokenization algorithm that preserves face proximity relationships and compresses sequence length through locally shared vertices and edges, enabling the generation of meshes with an unprecedented scale of up to 5,000 faces. Furthermore, we develop a Dual-stream Point Conditioner that provides multi-scale geometric guidance, ensuring global consistency and local structural fidelity by capturing fine-grained geometric features.
Extensive experiments demonstrate that Nautilus significantly outperforms state-of-the-art methods in both fidelity and scalability. The project page will be released tohttps://nautilusmeshgen.github.io."
364,679d459debd8ffd557a2afd9,cs.CV,https://arxiv.org/pdf/2501.14316,PAID: A Framework of Product-Centric Advertising Image Design,"Hongyu Chen, Min Zhou, Jing Jiang, Jiale Chen, Yang Lu, Bo Xiao, Tiezheng Ge, Bo Zheng",Computer Vision and Pattern Recognition,"In E-commerce platforms, a full advertising image is composed of a background image and marketing taglines. Automatic ad image design reduces human costs and plays a crucial role. For the convenience of users, a novel automatic framework named Product-Centric Advertising Image Design (PAID) is proposed in this work. PAID takes the product foreground image, required taglines, and target size as input and creates an ad image automatically. PAID consists of four sequential stages: prompt generation, layout generation, background image generation, and graphics rendering. Different expert models are trained to conduct these sub-tasks. A visual language model (VLM) based prompt generation model is leveraged to produce a product-matching background prompt. The layout generation model jointly predicts text and image layout according to the background prompt, product, and taglines to achieve the best harmony. An SDXL-based layout-controlled inpainting model is trained to generate an aesthetic background image. Previous ad image design methods take a background image as input and then predict the layout of taglines, which limits the spatial layout due to fixed image content. Innovatively, our PAID adjusts the stages to produce an unrestricted layout. To complete the PAID framework, we created two high-quality datasets, PITA and PIL. Extensive experimental results show that PAID creates more visually pleasing advertising images than previous methods."
365,679d459debd8ffd557a2afda,cs.CV,https://arxiv.org/pdf/2501.14309,BrainGuard: Privacy-Preserving Multisubject Image Reconstructions from Brain Activities,"Zhibo Tian, Ruijie Quan, Fan Ma, Kun Zhan, Yi Yang",Computer Vision and Pattern Recognition,"Reconstructing perceived images from human brain activity forms a crucial link between human and machine learning through Brain-Computer Interfaces. Early methods primarily focused on training separate models for each individual to account for individual variability in brain activity, overlooking valuable cross-subject commonalities. Recent advancements have explored multisubject methods, but these approaches face significant challenges, particularly in data privacy and effectively managing individual variability. To overcome these challenges, we introduceBrainGuard, a privacy-preserving collaborative training framework designed to enhance image reconstruction from multisubject fMRI data while safeguarding individual privacy.BrainGuardemploys a collaborative global-local architecture where individual models are trained on each subject’s local data and operate in conjunction with a shared global model that captures and leverages cross-subject patterns. This architecture eliminates the need to aggregate fMRI data across subjects, thereby ensuring privacy preservation. To tackle the complexity of fMRI data,BrainGuardintegrates a hybrid synchronization strategy, enabling individual models to dynamically incorporate parameters from the global model. By establishing a secure and collaborative training environment,BrainGuardnot only protects sensitive brain data but also improves the image reconstructions accuracy. Extensive experiments demonstrate thatBrainGuardsets a new benchmark in both high-level and low-level metrics, advancing the state-of-the-art in brain decoding through its innovative design."
366,679d459debd8ffd557a2afdb,cs.CV,https://arxiv.org/pdf/2501.14308,Learning Primitive Relations for Compositional Zero-Shot Learning,"Insu Lee, Jiseob Kim, Kyuhong Shim, Byonghyo Shim","Computer Vision and Pattern Recognition, Artificial Intelligence",
367,679d459debd8ffd557a2afdc,cs.CV,https://arxiv.org/pdf/2501.14306,Additive Manufacturing Processes Protocol Prediction by Artificial Intelligence using X-ray Computed Tomography data,"Sunita Khod, Akshay Dvivedi, Mayank Goswami","Computer Vision and Pattern Recognition, Applied Physics",
368,679d459debd8ffd557a2afdd,cs.CV,https://arxiv.org/pdf/2501.14302,TD-RD: A Top-Down Benchmark with Real-Time Framework for Road Damage Detection,"Xi Xiao, Zhengji Li, Wentao Wang, Jiacheng Xie, Houjie Lin, Swalpa Kumar Roy, Tianyang Wang, Min Xu",Computer Vision and Pattern Recognition,"Object detection has witnessed remarkable advancements over the past decade, largely driven by breakthroughs in deep learning and the proliferation of large-scale datasets. However, the domain of road damage detection remains relatively underexplored, despite its critical significance for applications such as infrastructure maintenance and road safety. This paper addresses this gap by introducing a novel top-down benchmark that offers a complementary perspective to existing datasets, specifically tailored for road damage detection. Our proposed Top-Down Road Damage Detection Dataset (TD-RD) includes three primary categories of road damage—cracks, potholes, and patches—captured from an top-down viewpoint. The dataset consists of 7,088 high-resolution images, encompassing 12,882 annotated instances of road damage. Additionally, we present a novel real-time object detection framework, TD-YOLOV10, designed to handle the unique challenges posed by the TD-RD dataset. Comparative studies with state-of-the-art models demonstrate competitive baseline results. By releasing TD-RD, we aim to accelerate research in this crucial area. A sample of the dataset will be made publicly available upon the paper’s acceptance."
369,679d459debd8ffd557a2afde,cs.CV,https://arxiv.org/pdf/2501.14277,Dense-SfM: Structure from Motion with Dense Consistent Matching,"JongMin Lee, Sungjoo Yoo",Computer Vision and Pattern Recognition,"We present Dense-SfM, a novel Structure from Motion (SfM) framework designed for dense and accurate 3D reconstruction from multi-view images. Sparse keypoint matching, which traditional SfM methods often rely on, limits both accuracy and point density, especially in texture-less areas. Dense-SfM addresses this limitation by integrating dense matching with a Gaussian Splatting (GS) based track extension which gives more consistent, longer feature tracks. To further improve reconstruction accuracy, Dense-SfM is equipped with a multi-view kernelized matching module leveraging transformer and Gaussian Process architectures, for robust track refinement across multi-views. Evaluations on the ETH3D and Texture-Poor SfM datasets show that Dense-SfM offers significant improvements in accuracy and density over state-of-the-art methods."
370,679d459debd8ffd557a2afdf,cs.CV,https://arxiv.org/pdf/2501.14276,Global Semantic-Guided Sub-image Feature Weight Allocation in High-Resolution Large Vision-Language Models,"Yuxuan Liang, Xu Li, Xiaolei Chen, Haotian Chen, Yi Zheng, Chenghang Lai, Bin Li, Xiangyang Xue","Computer Vision and Pattern Recognition, Artificial Intelligence","As the demand for high-resolution image processing in Large Vision-Language Models (LVLMs) grows, sub-image partitioning has become a popular approach for mitigating visual information loss associated with fixed-resolution processing. However, existing partitioning methods uniformly process sub-images, resulting in suboptimal image understanding. In this work, we reveal that the sub-images with higher semantic relevance to the entire image encapsulate richer visual information for preserving the model’s visual understanding ability. Therefore, we propose the Global Semantic-guided Weight Allocator (GSWA) module, which dynamically allocates weights to sub-images based on their relative information density, emulating human visual attention mechanisms. This approach enables the model to focus on more informative regions, overcoming the limitations of uniform treatment. We integrate GSWA into the InternVL2-2B framework to create SleighVL, a lightweight yet high-performing model. Extensive experiments demonstrate that SleighVL outperforms models with comparable parameters and remains competitive with larger models. Our work provides a promising direction for more efficient and contextually aware high-resolution image processing in LVLMs, advancing multimodal system development."
371,679d459debd8ffd557a2afe0,cs.CV,https://arxiv.org/pdf/2501.14265,Bayesian Neural Networks for One-to-Many Mapping in Image Enhancement,"Guoxi Huang, Nantheera Anantrasirichai, Fei Ye, Zipeng Qi, RuiRui Lin, Qirui Yang, David Bull",Computer Vision and Pattern Recognition,"In image enhancement tasks, such as low-light and underwater image enhancement, a degraded image can correspond to multiple plausible target images due to dynamic photography conditions, such as variations in illumination. This naturally results in a one-to-many mapping challenge.
To address this, we propose a Bayesian Enhancement Model (BEM) that incorporates Bayesian Neural Networks (BNNs) to capture data uncertainty and produce diverse outputs. To achieve real-time inference, we introduce a two-stage approach: Stage I employs a BNN to model the one-to-many mappings in the low-dimensional space, while Stage II refines fine-grained image details using a Deterministic Neural Network (DNN).
To accelerate BNN training and convergence, we introduce a dynamicMomentum Prior. Extensive experiments on multiple low-light and underwater image enhancement benchmarks demonstrate the superiority of our method over deterministic models.
Our code is available atthis link."
372,679d459debd8ffd557a2afe1,cs.CV,https://arxiv.org/pdf/2501.14238,Point-LN: A Lightweight Framework for Efficient Point Cloud Classification Using Non-Parametric Positional Encoding,"Marzieh Mohammadi, Amir Salarpour, Pedram MohajerAnsari","Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning, Robotics","We introducePoint-LN, a novel lightweight framework engineered for efficient 3D point cloud classification.Point-LNintegrates essential non-parametric components—such as Farthest Point Sampling (FPS), k-Nearest Neighbors (k-NN), and non-learnable positional encoding—with a streamlined learnable classifier that significantly enhances classification accuracy while maintaining a minimal parameter footprint. This hybrid architecture ensures low computational costs and rapid inference speeds, makingPoint-LNideal for real-time and resource-constrained applications. Comprehensive evaluations on benchmark datasets, including ModelNet40 and ScanObjectNN, demonstrate thatPoint-LNachieves competitive performance compared to state-of-the-art methods, all while offering exceptional efficiency. These results establishPoint-LNas a robust and scalable solution for diverse point cloud classification tasks, highlighting its potential for widespread adoption in various computer vision applications. For more details, see the code at:https://github.com/asalarpour/Point_LN."
373,679d459debd8ffd557a2afe2,cs.CV,https://arxiv.org/pdf/2501.14231,Micro-macro Wavelet-based Gaussian Splatting for 3D Reconstruction from Unconstrained Images,"Yihui Li, Chengxin Lv, Hongyu Yang, Di Huang",Computer Vision and Pattern Recognition,"3D reconstruction from unconstrained image collections presents substantial challenges due to varying appearances and transient occlusions. In this paper, we introduce Micro-macro Wavelet-based Gaussian Splatting (MW-GS), a novel approach designed to enhance 3D reconstruction by disentangling scene representations into global, refined, and intrinsic components. The proposed method features two key innovations:Micro-macro Projection, which allows Gaussian points to capture details from feature maps across multiple scales with enhanced diversity; andWavelet-based Sampling, which leverages frequency domain information to refine feature representations and significantly improve the modeling of scene appearances. Additionally, we incorporate a Hierarchical Residual Fusion Network to seamlessly integrate these features. Extensive experiments demonstrate that MW-GS delivers state-of-the-art rendering performance, surpassing existing methods."
374,679d459debd8ffd557a2afe3,cs.CV,https://arxiv.org/pdf/2501.14230,GreedyPixel: Fine-Grained Black-Box Adversarial Attack Via Greedy Algorithm,"Hanrui Wang, Ching-Chun Chang, Chun-Shien Lu, Christopher Leckie, Isao Echizen","Computer Vision and Pattern Recognition, Cryptography and Security, Machine Learning","A critical requirement for deep learning models is ensuring their robustness against adversarial attacks. These attacks commonly introduce noticeable perturbations, compromising the visual fidelity of adversarial examples. Another key challenge is that while white-box algorithms can generate effective adversarial perturbations, they require access to the model gradients, limiting their practicality in many real-world scenarios. Existing attack mechanisms struggle to achieve similar efficacy without access to these gradients. In this paper, we introduceGreedyPixel, a novel pixel-wise greedy algorithm designed to generate high-quality adversarial examples using only query-based feedback from the target model. GreedyPixel improves computational efficiency in what is typically a brute-force process by perturbing individual pixels in sequence, guided by a pixel-wise priority map. This priority map is constructed by ranking gradients obtained from a surrogate model, providing a structured path for perturbation. Our results demonstrate that GreedyPixel achieves attack success rates comparable to white-box methods without the need for gradient information, and surpasses existing algorithms in black-box settings, offering higher success rates, reduced computational time, and imperceptible perturbations. These findings underscore the advantages of GreedyPixel in terms of attack efficacy, time efficiency, and visual quality."
375,679d459debd8ffd557a2afe4,cs.CV,https://arxiv.org/pdf/2501.14228,Detection and Classification of Acute Lymphoblastic Leukemia Utilizing Deep Transfer Learning,"Md. Abu Ahnaf Mollick, Md. Mahfujur Rahman, D.M. Asadujjaman, Abdullah Tamim, Nosin Anjum Dristi, Md. Takbir Hossen","Computer Vision and Pattern Recognition, Artificial Intelligence",
376,679d459debd8ffd557a2afe5,cs.CV,https://arxiv.org/pdf/2501.14210,PuzzleGPT: Emulating Human Puzzle-Solving Ability for Time and Location Prediction,"Hammad Ayyubi, Xuande Feng, Junzhang Liu, Xudong Lin, Zhecan Wang, Shih-Fu Chang","Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning","The task of predicting time and location from images is challenging and requires complex human-like puzzle-solving ability over different clues. In this work, we formalize this ability into core skills and implement them using different modules in an expert pipeline called PuzzleGPT. PuzzleGPT consists of a perceiver to identify visual clues, a reasoner to deduce prediction candidates, a combiner to combinatorially combine information from different clues, a web retriever to get external knowledge if the task can’t be solved locally, and a noise filter for robustness. This results in a zero-shot, interpretable, and robust approach that records state-of-the-art performance on two datasets – TARA and WikiTilo. PuzzleGPT outperforms large VLMs such as BLIP-2, InstructBLIP, LLaVA, and even GPT-4o, as well as automatically generated reasoning pipelines like VisProgGupta and Kembhavi (2022), by at least 32% and 38%, respectively. It even rivals or surpasses finetuned models."
377,679d459debd8ffd557a2afe6,cs.CV,https://arxiv.org/pdf/2501.14204,Dynamic Token Reduction during Generation for Vision Language Models,"Xiaoyu Liang, Chaofeng Guan, Jiaying Lu, Huiyao Chen, Huan Wang, Haoji Hu","Computer Vision and Pattern Recognition, Artificial Intelligence","Vision-Language Models (VLMs) have achieved notable success in multimodal tasks but face practical limitations due to the quadratic complexity of decoder attention mechanisms and autoregressive generation. Existing methods like FASTV[1]and VTW[2]have achieved notable results in reducing redundant visual tokens, but these approaches focus on pruning tokens in a single forward pass without systematically analyzing the redundancy of visual tokens throughout the entire generation process.
In this paper, we introduce a dynamic pruning strategy tailored for VLMs, namedDynamicRate(DyRate), which progressively adjusts the compression rate during generation.
Our analysis of the distribution of attention reveals that the importance of visual tokens decreases throughout the generation process, inspiring us to adopt a more aggressive compression rate.
By integrating a lightweight predictor based on attention distribution, our approach enables flexible adjustment of pruning rates based on the attention distribution.
Our experimental results demonstrate that our method not only reduces computational demands but also maintains the quality of responses."
378,679d459debd8ffd557a2afe7,cs.CV,https://arxiv.org/pdf/2501.14195,VideoShield: Regulating Diffusion-based Video Generation Models via Watermarking,"Runyi Hu, Jie Zhang, Yiming Li, Jiwei Li, Qing Guo, Han Qiu, Tianwei Zhang",Computer Vision and Pattern Recognition,"Artificial Intelligence Generated Content (AIGC) has advanced significantly, particularly with the development of video generation models such as text-to-video (T2V) models and image-to-video (I2V) models. However, like other AIGC types, video generation requires robust content control. A common approach is to embed watermarks, but most research has focused on images, with limited attention given to videos. Traditional methods, which embed watermarks frame-by-frame in a post-processing manner, often degrade video quality. In this paper, we proposeVideoShield, a novel watermarking framework specifically designed for popular diffusion-based video generation models. Unlike post-processing methods,VideoShieldembeds watermarks directly during video generation, eliminating the need for additional training. To ensure video integrity, we introduce a tamper localization feature that can detect changes both temporally (across frames) and spatially (within individual frames). Our method maps watermark bits to template bits, which are then used to generate watermarked noise during the denoising process. Using DDIM Inversion, we can reverse the video to its original watermarked noise, enabling straightforward watermark extraction. Additionally, template bits allow precise detection for potential temporal and spatial modification. Extensive experiments across various video models (both T2V and I2V models) demonstrate that our method effectively extracts watermarks and detects tamper without compromising video quality. Furthermore, we show that this approach is applicable to image generation models, enabling tamper detection in generated images as well. Codes and models are available athttps://github.com/hurunyi/VideoShield."
379,679d459debd8ffd557a2afe8,cs.CV,https://arxiv.org/pdf/2501.14194,ENTER: Event Based Interpretable Reasoning for VideoQA,"Hammad Ayyubi, Junzhang Liu, Ali Asgarov, Zaber Ibn Abdul Hakim, Najibul Haque Sarker, Zhecan Wang, Chia-Wei Tang, Hani Alomari, Md. Atabuzzaman, Xudong Lin, Naveen Reddy Dyava, Shih-Fu Chang, Chris Thomas","Computer Vision and Pattern Recognition, Artificial Intelligence","In this paper, we present ENTER, an interpretable Video Question Answering (VideoQA) system based on event graphs. Event graphs convert videos into graphical representations, where video events form the nodes and event-event relationships (temporal/causal/hierarchical) form the edges. This structured representation offers many benefits: 1) Interpretable VideoQA via generated code that parses event-graph; 2) Incorporation of contextual visual information in the reasoning process (code generation) via event graphs; 3) Robust VideoQA via Hierarchical Iterative Update of the event graphs. Existing interpretable VideoQA systems are often top-down, disregarding low-level visual information in the reasoning plan generation, and are brittle. While bottom-up approaches produce responses from visual data, they lack interpretability. Experimental results on NExT-QA, IntentQA, and EgoSchema demonstrate that not only does our method outperform existing top-down approaches while obtaining competitive performance against bottom-up approaches, but more importantly, offers superior interpretability and explainability in the reasoning process."
380,679d459debd8ffd557a2afe9,cs.CV,https://arxiv.org/pdf/2501.14190,High-Precision Fabric Defect Detection via Adaptive Shape Convolutions and Large Kernel Spatial Modeling,"Shuai Wang, Yang Xu, Hui Zheng, Baotian Li",Computer Vision and Pattern Recognition,"Detecting fabric defects in the textile industry remains a challenging task due to the diverse and complex nature of defect patterns. Traditional methods often suffer from slow inference speeds, limited accuracy, and inadequate recognition rates, particularly in scenarios involving intricate or subtle defects.
To overcome these limitations, we introduce Fab-ASLKS, an advanced fabric defect detection framework built upon the YOLOv8s architecture. Fab-ASLKS incorporates two key modules: (1) the Adaptive Shape Convolution Module (ASCM), which leverages adaptive shape convolution within the Neck to enhance feature fusion and improve efficiency by extending the capabilities of the standard C2f structure, and (2) the Large Kernel Shift Convolution Module (LKSCM), designed to emulate large kernel effects within the Backbone, enabling superior spatial information extraction. These modules collaboratively optimize feature extraction and information integration across the network.
Extensive experiments conducted on the Tianchi fabric defect detection dataset demonstrate that Fab-ASLKS achieves a 5% improvement in mAP@50 over the baseline, showcasing its capability to deliver high precision and efficiency."
381,679d459debd8ffd557a2afea,cs.CV,https://arxiv.org/pdf/2501.14182,Post-hoc Spurious Correlation Neutralization with Single-Weight Fictitious Class Unlearning,"Shahin Hakemi, Naveed Akhtar, Ghulam Mubashar Hassan, Ajmal Mian",Computer Vision and Pattern Recognition,"Neural network training tends to exploit the simplest features as shortcuts to greedily minimize training loss. However, some of these features might be spuriously correlated with the target labels, leading to incorrect predictions by the model.
Several methods have been proposed to address this issue.
Focusing on suppressing the spurious correlations with model training, they not only incur additional training cost, but also have limited practical utility as the model misbehavior due to spurious relations is usually discovered after its deployment.
It is also often overlooked
that spuriousness is a subjective notion. Hence,
the precise questions that must be investigated are; to what degree a feature is spurious, and how we canproportionallydistract the model’s attention from it for reliable prediction. To this end, we propose a method that enables post-hoc neutralization of spurious feature impact, controllable to an arbitrary degree. We conceptualize spurious features as fictitious sub-classes within the original classes, which can be eliminated by a class removal scheme. We then propose a unique precise class removal technique that employs a single-weight modification, which entails negligible performance compromise for the remaining classes.
We perform extensive experiments, demonstrating that by editing just a single weight in a post-hoc manner, our method achieves highly competitive, or better performance against the state-of-the-art methods."
382,679d459debd8ffd557a2afeb,cs.CV,https://arxiv.org/pdf/2501.14174,Dreamweaver: Learning Compositional World Representations from Pixels,"Junyeob Baek, Yi-Fu Wu, Gautam Singh, Sungjin Ahn","Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning","Humans have an innate ability to decompose their perceptions of the world into objects and their attributes, such as colors, shapes, and movement patterns. This cognitive process enables us to imagine novel futures by recombining familiar concepts. However, replicating this ability in artificial intelligence systems has proven challenging, particularly when it comes to modeling videos into compositional concepts and generating unseen, recomposed futures without relying on auxiliary data, such as text, masks, or bounding boxes. In this paper, we proposeDreamweaver, a neural architecture designed to discover hierarchical and compositional representations from raw videos and generate compositional future simulations. Our approach leverages a novel Recurrent Block-Slot Unit (RBSU) to decompose videos into their constituent objects and attributes. In addition, Dreamweaver uses a multi-future-frame prediction objective to capture disentangled representations for dynamic concepts more effectively as well as static concepts. In experiments, we demonstrate our model outperforms current state-of-the-art baselines for world modeling when evaluated under the DCI framework across multiple datasets. Furthermore, we show how the modularized concept representations of our model enable compositional imagination, allowing the generation of novel videos by recombining attributes from different objects."
383,679d459debd8ffd557a2afec,cs.CV,https://arxiv.org/pdf/2501.14166,Enhancing Multimodal Entity Linking with Jaccard Distance-based Conditional Contrastive Learning and Contextual Visual Augmentation,"Cong-Duy Nguyen, Xiaobao Wu, Thong Nguyen, Shuai Zhao, Khoi Le, Viet-Anh Nguyen, Feng Yichao, Anh Tuan Luu","Computer Vision and Pattern Recognition, Artificial Intelligence","Previous research on multimodal entity linking (MEL) has primarily employed contrastive learning as the primary objective. However, using the rest of the batch as negative samples without careful consideration, these studies risk leveraging “easy” features and potentially overlook essential details that make entities unique. In this work, we propose JD-CCL (Jaccard Distance-based Conditional Contrastive Learning), a novel approach designed to enhance the ability to match multimodal entity linking models. JD-CCL leverages meta-information to select negative samples with similar attributes, making the linking task more challenging and robust. Additionally, to address the limitations caused by the variations within the visual modality among mentions and entities, we introduce a novel method, CVaCPT (Contextual Visual-aid Controllable Patch Transform). It enhances visual representations by incorporating multi-view synthetic images and contextual textual representations to scale and shift patch representations. Experimental results on benchmark MEL datasets demonstrate the strong effectiveness of our approach."
384,679d459debd8ffd557a2afed,cs.CV,https://arxiv.org/pdf/2501.14158,Advancing MRI Reconstruction: A Systematic Review of Deep Learning and Compressed Sensing Integration,"Mojtaba Safari, Zach Eidex, Chih-Wei Chang, Richard L.J. Qiu, Xiaofeng Yang","Computer Vision and Pattern Recognition, Artificial Intelligence, Medical Physics","Magnetic resonance imaging (MRI) is a non-invasive imaging modality and provides comprehensive anatomical and functional insights into the human body. However, its long acquisition times can lead to patient discomfort, motion artifacts, and limiting real-time applications. To address these challenges, strategies such as parallel imaging have been applied, which utilize multiple receiver coils to speed up the data acquisition process. Additionally, compressed sensing (CS) is a method that facilitates image reconstruction from sparse data, significantly reducing image acquisition time by minimizing the amount of data collection needed. Recently, deep learning (DL) has emerged as a powerful tool for improving MRI reconstruction. It has been integrated with parallel imaging and CS principles to achieve faster and more accurate MRI reconstructions. This review comprehensively examines DL-based techniques for MRI reconstruction. We categorize and discuss various DL-based methods, including end-to-end approaches, unrolled optimization, and federated learning, highlighting their potential benefits. Our systematic review highlights significant contributions and underscores the potential of DL in MRI reconstruction. Additionally, we summarize key results and trends in DL-based MRI reconstruction, including quantitative metrics, the dataset, acceleration factors, and the progress of and research interest in DL techniques over time. Finally, we discuss potential future directions and the importance of DL-based MRI reconstruction in advancing medical imaging. To facilitate further research in this area, we provide a GitHub repository that includes up-to-date DL-based MRI reconstruction publications and public datasets-https://github.com/mosaf/Awesome-DL-based-CS-MRI."
385,679d459debd8ffd557a2afee,cs.CV,https://arxiv.org/pdf/2501.14149,Effective Defect Detection Using Instance Segmentation for NDI,"Ashiqur Rahman, Venkata Devesh Reddy Seethi, Austin Yunker, Zachary Kral, Rajkumar Kettimuthu, Hamed Alhoori","Computer Vision and Pattern Recognition, Machine Learning","Ultrasonic testing is a common Non-Destructive Inspection (NDI) method used in aerospace manufacturing. However, the complexity and size of the ultrasonic scans make it challenging to identify defects through visual inspection or machine learning models. Using computer vision techniques to identify defects from ultrasonic scans is an evolving research area. In this study, we used instance segmentation to identify the presence of defects in the ultrasonic scan images of composite panels that are representative of real components manufactured in aerospace. We used two models based on Mask-RCNN (Detectron2222) and YOLO11111111respectively. Additionally, we implemented a simple statistical pre-processing technique that reduces the burden of requiring custom-tailored pre-processing techniques. Our study demonstrates the feasibility and effectiveness of using instance segmentation in the NDI pipeline by significantly reducing data pre-processing time, inspection time, and overall costs."
386,679d459debd8ffd557a2afef,cs.CV,https://arxiv.org/pdf/2501.14148,SelfPrompt: Confidence-Aware Semi-Supervised Tuning for Robust Vision-Language Model Adaptation,"Shuvendu Roy, Ali Etemad",Computer Vision and Pattern Recognition,"We present SelfPrompt, a novel prompt-tuning approach for vision-language models (VLMs) in a semi-supervised learning setup. Existing methods for tuning VLMs in semi-supervised setups struggle with the negative impact of the miscalibrated VLMs on pseudo-labelling, and the accumulation of noisy pseudo-labels. SelfPrompt addresses these challenges by introducing a cluster-guided pseudo-labelling method that improves pseudo-label accuracy, and a confidence-aware semi-supervised learning module that maximizes the utilization of unlabelled data by combining supervised learning and weakly-supervised learning. Additionally, we investigate our method in an active semi-supervised learning setup, where the labelled set is strategically selected to ensure the best utilization of a limited labelling budget. To this end, we propose a weakly-supervised sampling technique that selects a diverse and representative labelled set, which can be seamlessly integrated into existing methods to enhance their performance. We conduct extensive evaluations across 13 datasets, significantly surpassing state-of-the-art performances with average improvements of 6.23% in standard semi-supervised learning, 6.25% in active semi-supervised learning, and 4.9% in base-to-novel generalization, using a 2-shot setup. Furthermore, SelfPrompt shows excellent generalization in single-shot settings, achieving an average improvement of 11.78%."
387,679d459debd8ffd557a2aff0,cs.CV,https://arxiv.org/pdf/2501.14101,StreamingRAG: Real-time Contextual Retrieval and Generation Framework,"Murugan Sankaradas, Ravi K.Rajendran, Srimat T.Chakradhar",Computer Vision and Pattern Recognition,"Extracting real-time insights from multi-modal data streams from various domains such as healthcare, intelligent transportation, and satellite remote sensing remains a challenge. High computational demands and limited knowledge scope restrict the applicability of Multi-Modal Large Language Models (MM-LLMs) on these data streams. Traditional Retrieval-Augmented Generation (RAG) systems address knowledge limitations of these models, but suffer from slow preprocessing, making them unsuitable for real-time analysis. We propose StreamingRAG, a novel RAG framework designed for streaming data. StreamingRAG constructs evolving knowledge graphs capturing scene-object-entity relationships in real-time. The knowledge graph achieves temporal-aware scene representations using MM-LLMs and enables timely responses for specific events or user queries. StreamingRAG addresses limitations in existing methods, achieving significant improvements in real-time analysis (5-6x faster throughput), contextual accuracy (through a temporal knowledge graph), and reduced resource consumption (using lightweight models by 2-3x)."
388,679d459debd8ffd557a2aff1,cs.CV,https://arxiv.org/pdf/2501.14070,Expanding on the BRIAR Dataset: A Comprehensive Whole Body Biometric Recognition Resource at Extreme Distances and Real-World Scenarios (Collections 1-4),"Gavin Jager, David Cornett III, Gavin Glenn, Deniz Aykac, Christi Johnson, Robert Zhang, Ryan Shivers, David Bolme, Laura Davies, Scott Dolvin, Nell Barber, Joel Brogan, Nick Burchfield, Carl Dukes, Andrew Duncan, Regina Ferrell, Austin Garrett, Jim Goddard, Jairus Hines, Bart Murphy, Sean Pharris, Brandon Stockwell, Leanne Thompson, Matthew Yohe","Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning","The state-of-the-art in biometric recognition algorithms and operational systems has advanced quickly in recent years providing high accuracy and robustness in more challenging collection environments and consumer applications. However, the technology still suffers greatly when applied to non-conventional settings such as those seen when performing identification at extreme distances or from elevated cameras on buildings or mounted to UAVs. This paper summarizes an extension to the largest dataset currently focused on addressing these operational challenges, and describes its composition as well as methodologies of collection, curation, and annotation."
389,679d459debd8ffd557a2aff2,cs.CV,https://arxiv.org/pdf/2501.14056,Prior Knowledge Injection into Deep Learning Models Predicting Gene Expression from Whole Slide Images,"Max Hallemeesch, Marija Pizurica, Paloma Rabaey, Olivier Gevaert, Thomas Demeester, Kathleen Marchal",Computer Vision and Pattern Recognition,"Cancer diagnosis and prognosis primarily depend on clinical parameters such as age and tumor grade, and are increasingly complemented by molecular data, such as gene expression, from tumor sequencing. However, sequencing is costly and delays oncology workflows. Recent advances in Deep Learning allow to predict molecular information from morphological features within Whole Slide Images (WSIs), offering a cost-effective proxy of the molecular markers. While promising, current methods lack the robustness to fully replace direct sequencing. Here we aim to improve existing methods by introducing a model-agnostic framework that allows to inject prior knowledge on gene-gene interactions into Deep Learning architectures, thereby increasing accuracy and robustness. We design the framework to be generic and flexibly adaptable to a wide range of architectures.
In a case study on breast cancer, our strategy leads to an average increase of 983 significant genes (out of 25,761) across all 18 experiments, with 14 generalizing to an increase on an independent dataset.
Our findings reveal a high potential for injection of prior knowledge to increase gene expression prediction performance from WSIs across a wide range of architectures."
390,679d459debd8ffd557a2aff3,cs.CV,https://arxiv.org/pdf/2501.14051,Revisiting CLIP: Efficient Alignment of 3D MRI and Tabular Data using Domain-Specific Foundation Models,"Jakob Krogh Petersen, Valdemar Licht, Mads Nielsen, Asbjørn Munk","Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning","Multi-modal models require aligned, shared embedding spaces. However, common CLIP-based approaches need large amounts of samples and do not natively support 3D or tabular data, both of which are crucial in the medical domain. To address these issues, we revisit CLIP-style alignment by training adomain-specific3D foundation model as an image encoder and demonstrate that modality alignment is feasible with only 62 MRI scans. Our approach is enabled by a simple embedding accumulation strategy required for training in 3D, which scales the amount of negative pairs across batches in order to stabilize training. We perform a thorough evaluation of various design choices, including the choice of backbone and loss functions, and evaluate the proposed methodology on zero-shot classification and image-retrieval tasks. While zero-shot image-retrieval remains challenging, zero-shot classification results demonstrate that the proposed approach can meaningfully align the representations of 3D MRI with tabular data. Code and model checkpoints are availablehere."
391,679d459debd8ffd557a2aff4,cs.CV,https://arxiv.org/pdf/2501.14046,LLM-guided Instance-level Image Manipulation with Diffusion U-Net Cross-Attention Maps,"Andrey Palaev, Adil Khan, Syed M. Ahsan Kazmi",Computer Vision and Pattern Recognition,"The advancement of text-to-image synthesis has introduced powerful generative models capable of creating realistic images from textual prompts. However, precise control over image attributes remains challenging, especially at the instance level. While existing methods offer some control through fine-tuning or auxiliary information, they often face limitations in flexibility and accuracy. To address these challenges, we propose a pipeline leveraging Large Language Models (LLMs), open-vocabulary detectors, cross-attention maps and intermediate activations of diffusion U-Net for instance-level image manipulation. Our method detects objects mentioned in the prompt and present in the generated image, enabling precise manipulation without extensive training or input masks. By incorporating cross-attention maps, our approach ensures coherence in manipulated images while controlling object positions. Our method enables precise manipulations at the instance level without fine-tuning or auxiliary information such as masks or bounding boxes. Code is available athttps://github.com/Palandr123/DiffusionU-NetLLM"
392,679d459debd8ffd557a2aff5,cs.CV,https://arxiv.org/pdf/2501.14038,Implicit Neural Surface Deformation with Explicit Velocity Fields,"Lu Sang, Zehranaz Canfes, Dongliang Cao, Florian Bernard, Daniel Cremers",Computer Vision and Pattern Recognition,"In this work, we introduce the first unsupervised method that simultaneously predicts time-varying neural implicit surfaces and deformations between pairs of point clouds.
We propose to model the point movement using an explicit velocity field and directly deform a time-varying implicit field using the modified level-set equation. This equation utilizes an iso-surface evolution with Eikonal constraints in a compact formulation, ensuring the integrity of the signed distance field. By applying a smooth, volume-preserving constraint to the velocity field, our method successfully recoversphysically plausibleintermediate shapes. Our method is able to handle both rigid and non-rigid deformationswithout any intermediate shape supervision. Our experimental results demonstrate that our method significantly outperforms existing works, delivering superior results in both quality and efficiency111the code is available:https://github.com/Sangluisme/Implicit-surf-Deformation."
393,679d459debd8ffd557a2aff6,cs.CV,https://arxiv.org/pdf/2501.14014,INDIGO+: A Unified INN-Guided Probabilistic Diffusion Algorithm for Blind and Non-Blind Image Restoration,"Di You, Pier Luigi Dragotti","Computer Vision and Pattern Recognition, Image and Video Processing","Generative diffusion models are becoming one of the most popular prior in image restoration (IR) tasks due to their remarkable ability to generate realistic natural images. Despite achieving satisfactory results, IR methods based on diffusion models present several limitations. First of all, most non-blind approaches require an analytical expression of the degradation model to guide the sampling process. Secondly, most existing blind approaches rely on families of pre-defined degradation models for training their deep networks. The above issues limit the flexibility of these approaches and so their ability to handle real-world degradation tasks."
394,679d459debd8ffd557a2aff7,cs.CV,https://arxiv.org/pdf/2501.14005,Device-aware Optical Adversarial Attack for a Portable Projector-camera System,"Ning Jiang, Yanhong Liu, Dingheng Zeng, Yue Feng, Weihong Deng, Ying Li","Computer Vision and Pattern Recognition, Artificial Intelligence","Deep-learning-based face recognition (FR) systems are susceptible to adversarial examples in both digital and physical domains. Physical attacks present a greater threat to deployed systems as adversaries can easily access the input channel, allowing them to provide malicious inputs to impersonate a victim. This paper addresses the limitations of existing projector-camera-based adversarial light attacks in practical FR setups. By incorporating device-aware adaptations into the digital attack algorithm, such as resolution-aware and color-aware adjustments, we mitigate the degradation from digital to physical domains. Experimental validation showcases the efficacy of our proposed algorithm against real and spoof adversaries, achieving high physical similarity scores in FR models and state-of-the-art commercial systems. On average, there is only a 14% reduction in scores from digital to physical attacks, with high attack success rate in both white- and black-box scenarios."
395,679d459debd8ffd557a2aff8,cs.CV,https://arxiv.org/pdf/2501.14004,ME-CPT: Multi-Task Enhanced Cross-Temporal Point Transformer for Urban 3D Change Detection,"Luqi Zhang, Haiping Wang, Chong Liu, Zhen Dong, Bisheng Yang","Computer Vision and Pattern Recognition, Artificial Intelligence","The point clouds collected by the Airborne Laser Scanning (ALS) system provide accurate 3D information of urban land covers. By utilizing multi-temporal ALS point clouds, semantic changes in urban area can be captured, demonstrating significant potential in urban planning, emergency management, and infrastructure maintenance.
Existing 3D change detection methods struggle to efficiently extract multi-class semantic information and change features, still facing the following challenges:
(1) the difficulty of accurately modeling cross-temporal point clouds spatial relationships for effective change feature extraction; (2) class imbalance of change samples which hinders distinguishability of semantic features; (3) the lack of real-world datasets for 3D semantic change detection.
To resolve these challenges, we propose the Multi-task Enhanced Cross-temporal Point Transformer (ME-CPT) network. ME-CPT establishes spatiotemporal correspondences between point cloud across different epochs and employs attention mechanisms to jointly extract semantic change features, facilitating information exchange and change comparison. Additionally, we incorporate a semantic segmentation task and through the multi-task training strategy, further enhance the distinguishability of semantic features, reducing the impact of class imbalance in change types.
Moreover, we release a 22.5k⁢m2𝑘superscript𝑚2km^{2}italic_k italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT3D semantic change detection dataset, offering diverse scenes for comprehensive evaluation. Experiments on multiple datasets show that the proposed MT-CPT achieves superior performance compared to existing state-of-the-art methods. The source code and dataset will be released upon acceptance athttps://github.com/zhangluqi0209/ME-CPT."
396,679d459debd8ffd557a2aff9,cs.CV,https://arxiv.org/pdf/2501.14001,Enhancing kelp forest detection in remote sensing images using crowdsourced labels with Mixed Vision Transformers and ConvNeXt segmentation models,Ioannis Nasios,"Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning","This template is for authors who are preparing a manuscript for a Taylor & Francis journal using theLaTeXdocument preparation system and theinteractclass file, which is available via selected journals’ home pages on the Taylor & Francis website."
397,679d459debd8ffd557a2affa,cs.CV,https://arxiv.org/pdf/2501.13996,Integrating Persian Lip Reading in Surena-V Humanoid Robot for Human-Robot Interaction,"Ali Farshian Abbasi, Aghil Yousefi-Koma, Soheil Dehghani Firouzabadi, Parisa Rashidi, Alireza Naeini","Computer Vision and Pattern Recognition, Robotics","Lip reading is vital for robots in social settings, improving their ability to understand human communication. This skill allows them to communicate more easily in crowded environments, especially in caregiving and customer service roles.
Generating a Persian Lip-reading dataset, this study integrates Persian lip-reading technology into the Surena-V humanoid robot to improve its speech recognition capabilities. Two complementary methods are explored, an indirect method using facial landmark tracking and a direct method leveraging convolutional neural networks (CNNs) and long short-term memory (LSTM) networks. The indirect method focuses on tracking key facial landmarks, especially around the lips, to infer movements, while the direct method processes raw video data for action and speech recognition. The best-performing model, LSTM, achieved 89% accuracy and has been successfully implemented into the Surena-V robot for real-time human-robot interaction. The study highlights the effectiveness of these methods, particularly in environments where verbal communication is limited."
398,679d459debd8ffd557a2affb,cs.CV,https://arxiv.org/pdf/2501.13994,CSAOT: Cooperative Multi-Agent System for Active Object Tracking,"Hy Nguyen, Bao Pham, Hung Du, Srikanth Thudumu, Rajesh Vasa, Kon Mouzakis","Computer Vision and Pattern Recognition, Artificial Intelligence, Robotics","Object Tracking is essential for many computer vision applications, such as autonomous navigation, surveillance, and robotics. Unlike Passive Object Tracking (POT), which relies on static camera viewpoints to detect and track objects across consecutive frames, Active Object Tracking (AOT) requires a controller agent to actively adjust its viewpoint to maintain visual contact with a moving target in complex environments. Existing AOT solutions are predominantly single-agent-based, which struggle in dynamic and complex scenarios due to limited information gathering and processing capabilities, often resulting in suboptimal decision-making. Alleviating these limitations necessitates the development of a multi-agent system where different agents perform distinct roles and collaborate to enhance learning and robustness in dynamic and complex environments. Although some multi-agent approaches exist for AOT, they typically rely on external auxiliary agents, which require additional devices, making them costly. In contrast, we introduce the Collaborative System for Active Object Tracking (CSAOT), a method that leverages multi-agent deep reinforcement learning (MADRL) and a Mixture of Experts (MoE) framework to enable multiple agents to operate on a single device, thereby improving tracking performance and reducing costs. Our approach enhances robustness against occlusions and rapid motion while optimizing camera movements to extend tracking duration. We validated the effectiveness of CSAOT on various interactive maps with dynamic and stationary obstacles."
399,679d459debd8ffd557a2affc,cs.CV,https://arxiv.org/pdf/2501.13991,CGI: Identifying Conditional Generative Models with Example Images,"Zhi Zhou, Hao-Zhe Tan, Peng-Xiao Song, Lan-Zhe Guo","Computer Vision and Pattern Recognition, Artificial Intelligence","Generative models have achieved remarkable performance recently, and thus model hubs have emerged.
Existing model hubs typically assume basic text matching is sufficient to search for models.
However, in reality, due to different abstractions and the large number of models in model hubs, it is not easy for users to review model descriptions and example images, choosing which model best meets their needs.
Therefore, it is necessary to describe model functionality wisely so that future users can efficiently search for the most suitable model for their needs.
Efforts to address this issue remain limited. In this paper, we proposeConditionalGenerative ModelIdentification(CGi), which aims to provide an effective way to identify the most suitable model using user-provided example images rather than requiring users to manually review a large number of models with example images.
To address this problem, we propose thePrompt-BasedModelIdentification(Pmi) , which can adequately describe model functionality and precisely match requirements with specifications. To evaluatePmiapproach and promote related research, we provide a benchmark comprising 65 models and 9100 identification tasks. Extensive experimental and human evaluation results demonstrate thatPmiis effective. For instance, 92% of models are correctly identified with significantly better FID scores when four example images are provided."
400,679d459debd8ffd557a2affd,cs.CV,https://arxiv.org/pdf/2501.13982,Attribute-based Visual Reprogramming for Image Classification with CLIP,"Chengyi Cai, Zesheng Ye, Lei Feng, Jianzhong Qi, Feng Liu","Computer Vision and Pattern Recognition, Machine Learning","Visual reprogramming(VR) reuses pre-trained vision models for downstream image classification tasks by adding trainable noise patterns to inputs.
When applied to vision-language models (e.g., CLIP), existing VR approaches follow the same pipeline used in vision models (e.g., ResNet, ViT), where ground-truth class labels are inserted into fixed text templates to guide the optimization of VR patterns.
This label-based approach, however, overlooks the rich information and diverse attribute-guided textual representations that CLIP can exploit, which may lead to the misclassification of samples.
In this paper, we proposeAttribute-basedVisualReprogramming(AttrVR) for CLIP, utilizingdescriptiveattributes(DesAttrs) anddistinctiveattributes(DistAttrs), which respectively represent common and unique feature descriptions for different classes.
Besides, as images of the same class may reflect different attributes after VR, AttrVR iteratively refines patterns using thek𝑘kitalic_k-nearest DesAttrs and DistAttrs for each image sample, enabling more dynamic and sample-specific optimization.
Theoretically, AttrVR is shown to reduce intra-class variance and increase inter-class separation. Empirically, it achieves superior performance in 12 downstream tasks for both ViT-based and ResNet-based CLIP. The success of AttrVR facilitates more effective integration of VR from unimodal vision models into vision-language models. Our code is available athttps://github.com/tmlr-group/AttrVR."
401,679d459debd8ffd557a2affe,cs.CV,https://arxiv.org/pdf/2501.13981,Enhanced PEC-YOLO for Detecting Improper Safety Gear Wearing Among Power Line Workers,"Chen Zuguo, Kuang Aowei, Huang Yi, Jin Jie","Computer Vision and Pattern Recognition, Image and Video Processing","To address the high risks associated with improper use of safety gear in complex power line environments, where target occlusion and large variance are prevalent, this paper proposes an enhanced PEC-YOLO object detection algorithm. The method integrates deep perception with multi-scale feature fusion, utilizing PConv and EMA attention mechanisms to enhance feature extraction efficiency and minimize model complexity. The CPCA attention mechanism is incorporated into the SPPF module, improving the model’s ability to focus on critical information and enhance detection accuracy, particularly in challenging conditions. Furthermore, the introduction of the BiFPN neck architecture optimizes the utilization of low-level and high-level features, enhancing feature representation through adaptive fusion and context-aware mechanism. Experimental results demonstrate that the proposed PEC-YOLO achieves a 2.7% improvement in detection accuracy compared to YOLOv8s, while reducing model parameters by 42.58%. Under identical conditions, PEC-YOLO outperforms other models in detection speed, meeting the stringent accuracy requirements for safety gear detection in construction sites. This study contributes to the development of efficient and accurate intelligent monitoring systems for ensuring worker safety in hazardous environments. Code is available athttps://github.com/kuang-aowei/PEC-YOLO"
402,679d459debd8ffd557a2afff,cs.CV,https://arxiv.org/pdf/2501.13975,3DGS$^2$: Near Second-order Converging 3D Gaussian Splatting,"Lei Lan, Tianjia Shao, Zixuan Lu, Yu Zhang, Chenfanfu Jiang, Yin Yang","Computer Vision and Pattern Recognition, Graphics","3D Gaussian Splatting (3DGS) has emerged as a mainstream solution for novel view synthesis and 3D reconstruction. By explicitly encoding a 3D scene using a collection of Gaussian kernels, 3DGS achieves high-quality rendering with superior efficiency. As a learning-based approach, 3DGS training has been dealt with the standard stochastic gradient descent (SGD) method, which offers at most linear convergence. Consequently, training often requires tens of minutes, even with GPU acceleration. This paper introduces a (near) second-order convergent training algorithm for 3DGS, leveraging its unique properties. Our approach is inspired by two key observations. First, the attributes of a Gaussian kernel contribute independently to the image-space loss, which endorses isolated and local optimization algorithms. We exploit this by splitting the optimization at the level of individual kernel attributes, analytically constructing small-size Newton systems for each parameter group, and efficiently solving these systems on GPU threads. This achieves Newton-like convergence per training image without relying on the global Hessian. Second, kernels exhibit sparse and structured coupling across input images. This property allows us to effectively utilize spatial information to mitigate overshoot during stochastic training. Our method converges an order faster than standard GPU-based 3DGS training, requiring over10×10\times10 ×fewer iterations while maintaining or surpassing the quality of the compared with the SGD-based 3DGS reconstructions."
403,679d459debd8ffd557a2b000,cs.CV,https://arxiv.org/pdf/2501.13973,A Spatio-temporal Graph Network Allowing Incomplete Trajectory Input for Pedestrian Trajectory Prediction,"Juncen Long, Gianluca Bardaro, Simone Mentasti, Matteo Matteucci","Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning, Robotics","Pedestrian trajectory prediction is important in the research of mobile robot navigation in environments with pedestrians. Most pedestrian trajectory prediction algorithms require the input historical trajectories to be complete. If a pedestrian is unobservable in any frame in the past, then its historical trajectory become incomplete, the algorithm will not predict its future trajectory. To address this limitation, we propose the STGN-IT, a spatio-temporal graph network allowing incomplete trajectory input, which can predict the future trajectories of pedestrians with incomplete historical trajectories. STGN-IT uses the spatio-temporal graph with an additional encoding method to represent the historical trajectories and observation states of pedestrians. Moreover, STGN-IT introduces static obstacles in the environment that may affect the future trajectories as nodes to further improve the prediction accuracy. A clustering algorithm is also applied in the construction of spatio-temporal graphs. Experiments on public datasets show that STGN-IT outperforms state of the art algorithms on these metrics."
404,679d459debd8ffd557a2b001,cs.CV,https://arxiv.org/pdf/2501.13971,GS-LiDAR: Generating Realistic LiDAR Point Clouds with Panoramic Gaussian Splatting,"Junzhe Jiang, Chun Gu, Yurui Chen, Li Zhang","Computer Vision and Pattern Recognition, Graphics, Image and Video Processing","LiDAR novel view synthesis (NVS) has emerged as a novel task within LiDAR simulation, offering valuable simulated point cloud data from novel viewpoints to aid in autonomous driving systems. However, existing LiDAR NVS methods typically rely on neural radiance fields (NeRF) as their 3D representation, which incurs significant computational costs in both training and rendering. Moreover, NeRF and its variants are designed for symmetrical scenes, making them ill-suited for driving scenarios. To address these challenges, we proposeGS-LiDAR, a novel framework for generating realistic LiDAR point clouds with panoramic Gaussian splatting. Our approach employs 2D Gaussian primitives with periodic vibration properties, allowing for precise geometric reconstruction of both static and dynamic elements in driving scenarios. We further introduce a novel panoramic rendering technique with explicit ray-splat intersection, guided by panoramic LiDAR supervision. By incorporating intensity and ray-drop spherical harmonic (SH) coefficients into the Gaussian primitives, we enhance the realism of the rendered point clouds. Extensive experiments on KITTI-360 and nuScenes demonstrate the superiority of our method in terms of quantitative metrics, visual quality, as well as training and rendering efficiency."
405,679d459debd8ffd557a2b002,cs.CV,https://arxiv.org/pdf/2501.13969,InsTex: Indoor Scenes Stylized Texture Synthesis,"Yunfan Zhang, Zhiwei Xiong, Zhiqi Shen, Guosheng Lin, Hao Wang, Nicolas Vun","Computer Vision and Pattern Recognition, Graphics, Machine Learning","Generating high-quality textures for 3D scenes is crucial for applications in interior design, gaming, and augmented/virtual reality (AR/VR). Although recent advancements in 3D generative models have enhanced content creation, significant challenges remain in achieving broad generalization and maintaining style consistency across multiple viewpoints. Current methods, such as 2D diffusion models adapted for 3D texturing, suffer from lengthy processing times and visual artifacts, while approaches driven by 3D data often fail to generalize effectively. To overcome these challenges, we introduce InsTex, a two-stage architecture designed to generate high-quality, style-consistent textures for 3D indoor scenes. InsTex utilizes depth-to-image diffusion priors in a coarse-to-fine pipeline, first generating multi-view images with a pre-trained 2D diffusion model and subsequently refining the textures for consistency. Our method supports both textual and visual prompts, achieving state-of-the-art results in visual quality and quantitative metrics, and demonstrates its effectiveness across various 3D texturing applications."
406,679d459debd8ffd557a2b003,cs.CV,https://arxiv.org/pdf/2501.13968,Triplet Synthesis For Enhancing Composed Image Retrieval via Counterfactual Image Generation,"Kenta Uesugi, Naoki Saito, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama","Computer Vision and Pattern Recognition, Machine Learning, Image and Video Processing","Composed Image Retrieval (CIR) provides an effective way to manage and accesslarge-scalevisual data.
Construction of the CIR model utilizes triplets that consist of a reference image, modification text describing desired changes, and a target image that reflects these changes.
For effectively training CIR models, extensive manual annotation to constructhigh-qualitytraining datasets, which can betime-consumingandlabor-intensive, is required.
To deal with this problem, this paper proposes a novel triplet synthesis method
by leveraging counterfactual image generation.
By controlling visual feature modifications via counterfactual image generation, our approach automatically generates diverse training triplets without any manual intervention.
This approach facilitates the creation of larger and more expressive datasets, leading to the improvement of CIR model’s performance."
407,679d459debd8ffd557a2b004,cs.CV,https://arxiv.org/pdf/2501.13967,FedDAG: Federated Domain Adversarial Generation Towards Generalizable Medical Image Analysis,"Haoxuan Che, Yifei Wu, Haibo Jin, Yong Xia, Hao Chen","Computer Vision and Pattern Recognition, Artificial Intelligence","Federated domain generalization aims to train a global model from multiple source domains and ensure its generalization ability to unseen target domains.
Due to the target domain being with unknown domain shifts, attempting to approximate these gaps by source domains may be the key to improving model generalization capability.
Existing works mainly focus on sharing and recombining local domain-specific attributes to increase data diversity and simulate potential domain shifts.
However, these methods may be insufficient since only the local attribute recombination can be hard to touch the out-of-distribution of global data.
In this paper, we propose a simple-yet-efficient framework named Federated Domain Adversarial Generation (FedDAG).
It aims to simulate the domain shift and improve the model generalization by adversarially generating novel domains different from local and global source domains.
Specifically, it generates novel-style images by maximizing the instance-level feature discrepancy between original and generated images and trains a generalizable task model by minimizing their feature discrepancy.
Further, we observed that FedDAG could cause different performance improvements for local models.
It may be due to inherent data isolation and heterogeneity among clients, exacerbating the imbalance in their generalization contributions to the global model.
Ignoring this imbalance can lead the global model’s generalization ability to be sub-optimal, further limiting the novel domain generation procedure. 
Thus, to mitigate this imbalance, FedDAG hierarchically aggregates local models at the within-client and across-client levels by using the sharpness concept to evaluate client model generalization contributions.
Extensive experiments across four medical benchmarks demonstrate FedDAG’s ability to enhance generalization in federated medical scenarios."
408,679d459debd8ffd557a2b005,cs.CV,https://arxiv.org/pdf/2501.13964,Advancing the Understanding and Evaluation of AR-Generated Scenes: When Vision-Language Models Shine and Stumble,"Lin Duan, Yanming Xiu, Maria Gorlatova","Computer Vision and Pattern Recognition, Artificial Intelligence, Human-Computer Interaction","Augmented Reality (AR) enhances the real world by integrating virtual content, yet ensuring the quality, usability, and safety of AR experiences presents significant challenges. Could Vision-Language Models (VLMs) offer a solution for the automated evaluation of AR-generated scenes? In this study, we evaluate the capabilities of three state-of-the-art commercial VLMs—GPT, Gemini, and Claude—in identifying and describing AR scenes. For this purpose, we use DiverseAR, the first AR dataset specifically designed to assess VLMs’ ability to analyze virtual content across a wide range of AR scene complexities. Our findings demonstrate that VLMs are generally capable of perceiving and describing AR scenes, achieving a True Positive Rate (TPR) of up to 93% for perception and 71% for description. While they excel at identifying obvious virtual objects, such as a glowing apple, they struggle when faced with seamlessly integrated content, such as a virtual pot with realistic shadows. Our results highlight both the strengths and the limitations of VLMs in understanding AR scenarios. We identify key factors affecting VLM performance, including virtual content placement, rendering quality, and physical plausibility. This study underscores the potential of VLMs as tools for evaluating the quality of AR experiences."
409,679d459debd8ffd557a2b006,cs.CV,https://arxiv.org/pdf/2501.13963,Procedural Generation of 3D Maize Plant Architecture from LIDAR Data,"Mozhgan Hadadi, Mehdi Saraeian, Jackson Godbersen, Talukder Jubery, Yawei Li, Lakshmi Attigala, Aditya Balu, Soumik Sarkar, Patrick S. Schnable, Adarsh Krishnamurthy, Baskar Ganapathysubramanian","Computer Vision and Pattern Recognition, Machine Learning","This study introduces a robust framework for generating procedural 3D models of maize (Zea mays) plants from LiDAR point cloud data, offering a scalable alternative to traditional field-based phenotyping. Our framework leverages Non-Uniform Rational B-Spline (NURBS) surfaces to model the leaves of maize plants, combining Particle Swarm Optimization (PSO) for an initial approximation of the surface and a differentiable programming framework for precise refinement of the surface to fit the point cloud data. In the first optimization phase, PSO generates an approximate NURBS surface by optimizing its control points, aligning the surface with the LiDAR data, and providing a reliable starting point for refinement. The second phase uses NURBS-Diff, a differentiable programming framework, to enhance the accuracy of the initial fit by refining the surface geometry and capturing intricate leaf details. Our results demonstrate that, while PSO establishes a robust initial fit, the integration of differentiable NURBS significantly improves the overall quality and fidelity of the reconstructed surface. This hierarchical optimization strategy enables accurate 3D reconstruction of maize leaves across diverse genotypes, facilitating the subsequent extraction of complex traits like phyllotaxy. We demonstrate our approach on diverse genotypes of field-grown maize plants. All our codes are open-source to democratize these phenotyping approaches."
410,679d459debd8ffd557a2b007,cs.CV,https://arxiv.org/pdf/2501.13961,"A Fast, Scalable, and Robust Deep Learning-based Iterative Reconstruction Framework for Accelerated Industrial Cone-beam X-ray Computed Tomography","Aniket Pramanik, Obaidullah Rahman, Singanallur V. Venkatakrishnan, Amirkoushyar Ziabari","Computer Vision and Pattern Recognition, Machine Learning","Cone-beam X-ray Computed Tomography (XCT) with large detectors and corresponding large-scale 3D reconstruction plays a pivotal role in micron-scale characterization of materials and parts across various industries.
In this work, we present a novel deep neural network-based iterative algorithm that integrates an artifact reduction-trained CNN as a prior model with automated regularization parameter selection, tailored for large-scale industrial cone-beam XCT data.
Our method achieves high-quality 3D reconstructions even for extremely dense thick metal parts - which traditionally pose challenges to industrial CT images - in just a few iterations.
Furthermore, we show the generalizability of our approach to out-of-distribution scans obtained under diverse scanning conditions.
Our method effectively handles significant noise and streak artifacts, surpassing state-of-the-art supervised learning methods trained on the same data."
411,679d459debd8ffd557a2b008,cs.CV,https://arxiv.org/pdf/2501.13950,DEFEND: A Large-scale 1M Dataset and Foundation Model for Tobacco Addiction Prevention,"Naga VS Raviteja Chappa, Matthew Shepard, Connor McCurtain, Charlotte McCormick, Page Daniel Dobbs, Khoa Luu",Computer Vision and Pattern Recognition,"While tobacco advertising innovates at unprecedented speed, traditional surveillance methods remain frozen in time, especially in the context of social media. The lack of large-scale, comprehensive datasets and sophisticated monitoring systems has created a widening gap between industry advancement and public health oversight. This paper addresses this critical challenge by introducing Tobacco-1M, a comprehensive dataset of one million tobacco product images with hierarchical labels spanning 75 product categories, and DEFEND, a novel foundation model for tobacco product understanding. Our approach integrates a Feature Enhancement Module for rich multimodal representation learning, a Local-Global Visual Coherence mechanism for detailed feature discrimination, and an Enhanced Image-Text Alignment strategy for precise product characterization. Experimental results demonstrate DEFEND’s superior performance, achieving 83.1% accuracy in product classification and 73.8% in visual question-answering tasks, outperforming existing methods by significant margins. Moreover, the model exhibits robust zero-shot learning capabilities with 45.6% accuracy on novel product categories. This work provides regulatory bodies and public health researchers with powerful tools for monitoring emerging tobacco products and marketing strategies, potentially revolutionizing approaches to tobacco control and public health surveillance."
412,679d459debd8ffd557a2b009,cs.CV,https://arxiv.org/pdf/2501.14728,Mitigating GenAI-powered Evidence Pollution for Out-of-Context Multimodal Misinformation Detection,"Zehong Yan, Peng Qi, Wynne Hsu, Mong Li Lee","Multimedia, Computation and Language, Computer Vision and Pattern Recognition, Computers and Society",
413,679d459debd8ffd557a2b00a,cs.CV,https://arxiv.org/pdf/2501.14709,Enhanced Confocal Laser Scanning Microscopy with Adaptive Physics Informed Deep Autoencoders,"Zaheer Ahmad, Junaid Shabeer, Usman Saleem, Tahir Qadeer, Abdul Sami, Zahira El Khalidi, Saad Mehmood","Materials Science, Computer Vision and Pattern Recognition, Image and Video Processing",
414,679d459debd8ffd557a2b00b,cs.CV,https://arxiv.org/pdf/2501.14704,Stroke classification using Virtual Hybrid Edge Detection from in silico electrical impedance tomography data,"Juan Pablo Agnelli, Fernando S. Moura, Siiri Rautio, Melody Alsaker, Rashmi Murthy, Matti Lassas, Samuli Siltanen","Analysis of PDEs, Computer Vision and Pattern Recognition, Numerical Analysis","Electrical impedance tomography (EIT) is a non-invasive imaging method for recovering the internal conductivity of a physical body from electric boundary measurements. EIT combined with machine learning has shown promise for the classification of strokes. However, most previous works have used raw EIT voltage data as network inputs. We build upon a recent development which suggested the use of special noise-robust Virtual Hybrid Edge Detection (VHED) functions as network inputs, although that work used only highly simplified and mathematically ideal models. In this work we strengthen the case for the use of EIT, and VHED functions especially, for stroke classification. We design models with high detail and mathematical realism to test the use of VHED functions as inputs. Virtual patients are created using a physically detailed 2D head model which includes features known to create challenges in real-world imaging scenarios. Conductivity values are drawn from statistically realistic distributions, and phantoms are afflicted with either hemorrhagic or ischemic strokes of various shapes and sizes. Simulated noisy EIT electrode data, generated using the realistic Complete Electrode Model (CEM) as opposed to the mathematically ideal continuum model, is processed to obtain VHED functions. We compare the use of VHED functions as inputs against the alternative paradigm of using raw EIT voltages. Our results show that (i) stroke classification can be performed with high accuracy using 2D EIT data from physically detailed and mathematically realistic models, and (ii) in the presence of noise, VHED functions outperform raw data as network inputs."
415,679d459debd8ffd557a2b00c,cs.CV,https://arxiv.org/pdf/2501.14685,Rethinking Foundation Models for Medical Image Classification through a Benchmark Study on MedMNIST,"Fuping Wu, Bartlomiej W. Papiez","Image and Video Processing, Artificial Intelligence, Computer Vision and Pattern Recognition, Machine Learning","Foundation models are widely employed in medical image analysis, due to their high adaptability and generalizability for downstream tasks.
With the increasing number of foundation models being released, model selection has become an important issue.
In this work, we study the capabilities of foundation models in medical image classification tasks by conducting a benchmark study on the MedMNIST dataset. Specifically, we adopt various foundation models ranging from convolutional to Transformer-based models and implement both end-to-end training and linear probing for all classification tasks.
The results demonstrate the significant potential of these pre-trained models when transferred for medical image classification.
We further conduct experiments with different image sizes and various sizes of training data.
By analyzing all the results, we provide preliminary, yet useful insights and conclusions on this topic."
416,679d459debd8ffd557a2b00d,cs.CV,https://arxiv.org/pdf/2501.14592,Improved Vessel Segmentation with Symmetric Rotation-Equivariant U-Net,"Jiazhen Zhang, Yuexi Du, Nicha C. Dvornek, John A. Onofrey","Image and Video Processing, Computer Vision and Pattern Recognition, Machine Learning","Automated segmentation plays a pivotal role in medical image analysis and computer-assisted interventions.
Despite the promising performance of existing methods based on convolutional neural networks (CNNs), they neglect useful equivariant properties for images, such as rotational and reflection equivariance.
This limitation can decrease performance and lead to inconsistent predictions, especially in applications like vessel segmentation where explicit orientation is absent.
While existing equivariant learning approaches attempt to mitigate these issues, they substantially increase learning cost, model size, or both.
To overcome these challenges, we propose a novel application of an efficient symmetric rotation-equivariant (SRE) convolutional (SRE-Conv) kernel implementation to the U-Net architecture, to learn rotation- and reflection-equivariant features, while also reducing the model size dramatically.
We validate the effectiveness of our method through improved segmentation performance on retina vessel fundus imaging.
Our proposed SRE U-Net not only significantly surpasses standard U-Net in handling rotated images, but also outperforms existing equivariant learning methods and does so with a reduced number of trainable parameters and smaller memory cost.
The code is available onhttps://github.com/OnofreyLab/sre_conv_segm_isbi2025."
417,679d459debd8ffd557a2b00e,cs.CV,https://arxiv.org/pdf/2501.14520,Scene Understanding Enabled Semantic Communication with Open Channel Coding,"Zhe Xiang, Fei Yu, Quan Deng, Yuandi Li, Zhiguo Wan","Signal Processing, Computer Vision and Pattern Recognition","As communication systems transition from symbol transmission to conveying meaningful information, sixth-generation (6G) networks emphasize semantic communication. This approach prioritizes high-level semantic information, improving robustness and reducing redundancy across modalities like text, speech, and images. However, traditional semantic communication faces limitations, including static coding strategies, poor generalization, and reliance on task-specific knowledge bases that hinder adaptability."
418,679d459debd8ffd557a2b00f,cs.CV,https://arxiv.org/pdf/2501.14502,LiDAR-Based Vehicle Detection and Tracking for Autonomous Racing,"Marcello Cellina, Matteo Corno, Sergio Matteo Savaresi","Robotics, Computer Vision and Pattern Recognition","Autonomous racing provides a controlled environment for testing the software and hardware of autonomous vehicles operating at their performance limits. Competitive interactions between multiple autonomous racecars however introduce challenging and potentially dangerous scenarios.
Accurate and consistent vehicle detection and tracking is crucial for overtaking maneuvers, and low-latency sensor processing is essential to respond quickly to hazardous situations.
This paper presents the LiDAR-based perception algorithms deployed on Team PoliMOVE’s autonomous racecar, which won multiple competitions in the Indy Autonomous Challenge series.
Our Vehicle Detection and Tracking pipeline is composed of a novel fast Point Cloud Segmentation technique and a specific Vehicle Pose Estimation methodology, together with a variable-step Multi-Target Tracking algorithm.
Experimental results demonstrate the algorithm’s performance, robustness, computational efficiency, and suitability for autonomous racing applications, enabling fully autonomous overtaking maneuvers at velocities exceeding 275 km/h."
419,679d459debd8ffd557a2b010,cs.CV,https://arxiv.org/pdf/2501.14496,A Note on Implementation Errors in Recent Adaptive Attacks Against Multi-Resolution Self-Ensembles,Stanislav Fort,"Cryptography and Security, Computer Vision and Pattern Recognition, Machine Learning","This note documents an implementation issue in recent adaptive attacks (Zhang et al. (2024)) against the multi-resolution self-ensemble defense (Fort and Lakshminarayanan (2024)). The implementation allowed adversarial perturbations to exceed the standardL∞=8/255subscript𝐿8255L_{\infty}=8/255italic_L start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT = 8 / 255bound by up to a factor of 20×\times×, reaching magnitudes of up toL∞=160/255subscript𝐿160255L_{\infty}=160/255italic_L start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT = 160 / 255. When attacks are properly constrained within the intended bounds, the defense maintains non-trivial robustness. Beyond highlighting the importance of careful validation in adversarial machine learning research, our analysis reveals an intriguing finding: properly bounded adaptive attacks against strong multi-resolution self-ensembles often align with human perception, suggesting the need to reconsider how we measure adversarial robustness."
420,679d459debd8ffd557a2b011,cs.CV,https://arxiv.org/pdf/2501.14483,Registration of Longitudinal Liver Examinations for Tumor Progress Assessment,"Walid Yassine, Martin Charachon, Céline Hudelot, Roberto Ardon","Image and Video Processing, Artificial Intelligence, Computer Vision and Pattern Recognition, Medical Physics","Assessing cancer progression in liver CT scans is a clinical challenge, requiring a comparison of scans at different times for the same patient.
Practitioners must identify existing tumors, compare them with prior exams, identify new tumors, and evaluate overall disease evolution.
This process is particularly complex in liver examinations due to misalignment between exams caused by several factors. Indeed, longitudinal liver examinations can undergo different non-pathological and pathological changes due to non-rigid deformations, the appearance or disappearance of pathologies, and other variations.
In such cases, existing registration approaches, mainly based on intrinsic features may distort tumor regions, biasing the tumor progress evaluation step and the corresponding diagnosis.
This work proposes a registration method based only on geometrical and anatomical information from liver segmentation, aimed at aligning longitudinal liver images for aided diagnosis.
The proposed method is trained and tested on longitudinal liver CT scans, with 317 patients for training and 53 for testing.
Our experimental results support our claims by showing that our method is better than other registration techniques by providing a smoother deformation while preserving the tumor burden111Total volume of tissues considered as tumor.within the volume. Qualitative results emphasize the importance of smooth deformations in preserving tumor appearance."
421,679d459debd8ffd557a2b012,cs.CV,https://arxiv.org/pdf/2501.14379,"ECTIL: Label-efficient Computational Tumour Infiltrating Lymphocyte (TIL) assessment in breast cancer: Multicentre validation in 2,340 patients with breast cancer","Yoni Schirris, Rosie Voorthuis, Mark Opdam, Marte Liefaard, Gabe S Sonke, Gwen Dackus, Vincent de Jong, Yuwei Wang, Annelot Van Rossum, Tessa G Steenbruggen, Lars C Steggink, Liesbeth G.E. de Vries, Marc van de Vijver, Roberto Salgado, Efstratios Gavves, Paul J van Diest, Sabine C Linn, Jonas Teuwen, Renee Menezes, Marleen Kok, Hugo Horlings","Image and Video Processing, Artificial Intelligence, Computer Vision and Pattern Recognition",
422,679d459debd8ffd557a2b013,cs.CV,https://arxiv.org/pdf/2501.14323,Automatic detection and prediction of nAMD activity change in retinal OCT using Siamese networks and Wasserstein Distance for ordinality,"Taha Emre, Teresa Araújo, Marzieh Oghbaie, Dmitrii Lachinov, Guilherme Aresta, Hrvoje Bogunović","Image and Video Processing, Computer Vision and Pattern Recognition, Machine Learning","Neovascular age-related macular degeneration (nAMD) is a leading cause of vision loss among older adults, where disease activity detection and progression prediction are critical for nAMD management in terms of timely drug administration and improving patient outcomes. Recent advancements in deep learning offer a promising solution for predicting changes in AMD from optical coherence tomography (OCT) retinal volumes. In this work, we proposed deep learning models for the two tasks of the public MARIO Challenge at MICCAI 2024, designed to detect and forecast changes in nAMD severity with longitudinal retinal OCT. For the first task, we employ a Vision Transformer (ViT) based Siamese Network to detect changes in AMD severity by comparing scan embeddings of a patient from different time points. To train a model to forecast the change after 3 months, we exploit, for the first time, an Earth Mover (Wasserstein) Distance-based loss to harness the ordinal relation within the severity change classes. Both models ranked high on the preliminary leaderboard, demonstrating that their predictive capabilities could facilitate nAMD treatment management.∗∗****footnotetext:https://github.com/EmreTaha/Siamese-EMD-for-AMD-Change"
423,679d459debd8ffd557a2b014,cs.CV,https://arxiv.org/pdf/2501.14287,Snapshot multi-spectral imaging through defocusing and a Fourier imager network,"Xilin Yang, Michael John Fanous, Hanlong Chen, Ryan Lee, Paloma Casteleiro Costa, Yuhang Li, Luzhe Huang, Yijie Zhang, Aydogan Ozcan","Optics, Computer Vision and Pattern Recognition, Machine Learning, Applied Physics",
424,679d459debd8ffd557a2b015,cs.CV,https://arxiv.org/pdf/2501.14279,Deep Learning-Powered Classification of Thoracic Diseases in Chest X-Rays,"Yiming Lei, Michael Nguyen, Tzu Chia Liu, Hyounkyun Oh","Image and Video Processing, Computer Vision and Pattern Recognition","The ABSTRACT is to be in fully-justified italicized text, at the top
of the left-hand column, below the author and affiliation
information. Use the word “Abstract” as the title, in 12-point
Times, boldface type, centered relative to the column, initially
capitalized. The abstract is to be in 10-point, single-spaced type.
Leave two blank lines after the Abstract, then begin the main text.
Look at previous CVPR abstracts to get a feel for style and length.
The abstract section should contain a brief summary of your work that
includes the problem statement, proposed solution and results."
425,679d459debd8ffd557a2b016,cs.CV,https://arxiv.org/pdf/2501.14264,CDI: Blind Image Restoration Fidelity Evaluation based on Consistency with Degraded Image,"Xiaojun Tang, Jingru Wang, Guangwei Huang, Guannan Chen, Rui Zheng, Lian Huai, Yuyu Liu, Xingqun Jiang","Image and Video Processing, Computer Vision and Pattern Recognition","Recent advancements in Blind Image Restoration (BIR) methods, based on Generative Adversarial Networks and Diffusion Models, have significantly improved visual quality. However, they present significant challenges for Image Quality Assessment (IQA), as the existing Full-Reference IQA methods often rate images with high perceptual quality poorly. In this paper, we reassess the Solution Non-Uniqueness and Degradation Indeterminacy issues of BIR, and propose constructing a specific BIR IQA system. In stead of directly comparing a restored image with a reference image, the BIR IQA evaluates fidelity by calculating the Consistency with Degraded Image (CDI). Specifically, we propose a wavelet domain Reference Guided CDI algorithm, which can acquire the consistency with a degraded image for various types without requiring knowledge of degradation parameters. The supported degradation types include down sampling, blur, noise, JPEG and complex combined degradations etc. In addition, we propose a Reference Agnostic CDI, enabling BIR fidelity evaluation without reference images. Finally, in order to validate the rationality of CDI, we create a new Degraded Images Switch Display Comparison Dataset (DISDCD) for subjective evaluation of BIR fidelity. Experiments conducted on DISDCD verify that CDI is markedly superior to common Full Reference IQA methods for BIR fidelity evaluation. The source code and the DISDCD dataset will be publicly available shortly."
426,679d459debd8ffd557a2b017,cs.CV,https://arxiv.org/pdf/2501.14208,You Only Teach Once: Learn One-Shot Bimanual Robotic Manipulation from Video Demonstrations,"Huayi Zhou, Ruixiang Wang, Yunxin Tai, Yueci Deng, Guiliang Liu, Kui Jia","Robotics, Computer Vision and Pattern Recognition","Bimanual robotic manipulation is a long-standing challenge of embodied intelligence due to its characteristics of dual-arm spatial-temporal coordination and high-dimensional action spaces. Previous studies rely on pre-defined action taxonomies or direct teleoperation to alleviate or circumvent these issues, often making them lack simplicity, versatility and scalability. Differently, we believe that the most effective and efficient way for teaching bimanual manipulation is learning from human demonstrated videos, where rich features such as spatial-temporal positions, dynamic postures, interaction states and dexterous transitions are available almost for free. In this work, we propose the YOTO (You Only Teach Once), which can extract and then inject patterns of bimanual actions from as few as a single binocular observation of hand movements, and teach dual robot arms various complex tasks. Furthermore, based on keyframes-based motion trajectories, we devise a subtle solution for rapidly generating training demonstrations with diverse variations of manipulated objects and their locations. These data can then be used to learn a customized bimanual diffusion policy (BiDP) across diverse scenes. In experiments, YOTO achieves impressive performance in mimicking 5 intricate long-horizon bimanual tasks, possesses strong generalization under different visual and spatial conditions, and outperforms existing visuomotor imitation learning methods in accuracy and efficiency. Our project link ishttps://hnuzhy.github.io/projects/YOTO."
427,679d459debd8ffd557a2b018,cs.CV,https://arxiv.org/pdf/2501.14198,Sparse Mixture-of-Experts for Non-Uniform Noise Reduction in MRI Images,"Zeyun Deng, Joseph Campbell","Image and Video Processing, Computer Vision and Pattern Recognition","Magnetic Resonance Imaging (MRI) is an essential diagnostic tool in clinical settings, but its utility is often hindered by noise artifacts introduced during the imaging process.Effective denoising is critical for enhancing image quality while preserving anatomical structures.
However, traditional denoising methods, which often assume uniform noise distributions, struggle to handle the non-uniform noise commonly present in MRI images. In this paper, we introduce a novel approach leveraging a sparse mixture-of-experts framework for MRI image denoising. Each expert is a specialized denoising convolutional neural network fine-tuned to target specific noise characteristics associated with different image regions. Our method demonstrates superior performance over state-of-the-art denoising techniques on both synthetic and real-world brain MRI datasets. Furthermore, we show that it generalizes effectively to unseen datasets, highlighting its robustness and adaptability."
428,679d459debd8ffd557a2b019,cs.CV,https://arxiv.org/pdf/2501.14172,UltraLightSqueezeNet: A Deep Learning Architecture for Malaria Classification with up to 54x fewer trainable parameters for resource constrained devices,"Suresh Babu Nettur, Shanthi Karpurapu, Unnati Nettur, Likhit Sagar Gajja, Sravanthy Myneni, Akhil Dusi, Lalithya Posham","Machine Learning, Artificial Intelligence, Computer Vision and Pattern Recognition",
429,679d459debd8ffd557a2b01a,cs.CV,https://arxiv.org/pdf/2501.14171,"Fully Guided Neural Schr\""odinger bridge for Brain MR image synthesis","Hanyeol Yang, Sunggyu Kim, Yongseon Yoo, Jong-min Lee","Image and Video Processing, Computer Vision and Pattern Recognition","Multi-modal brain MRI provides essential complementary information for clinical diagnosis. However, acquiring all modalities is often challenging due to time and cost constraints. To address this, various methods have been proposed to generate missing modalities from available ones. Traditional approaches can be broadly categorized into two main types: paired and unpaired methods. While paired methods offer superior performance, obtaining large-scale paired datasets is challenging in real-world scenarios. Conversely, unpaired methods facilitate large-scale data collection but struggle to preserve critical image features, such as tumors. In this paper, we propose Fully Guided Schrödinger Bridges (FGSB), a novel framework based on Neural Schrödinger Bridges, to overcome these limitations. FGSB achieves stable, high-quality generation of missing modalities using minimal paired data. Furthermore, when provided with ground truth or a segmentation network for specific regions, FGSB can generate missing modalities while preserving these critical areas with reduced data requirements. Our proposed model consists of two consecutive phases. 1) Generation Phase: Fuses a generated image, a paired reference image, and Gaussian noise, employing iterative refinement to mitigate issues such as mode collapse and improve generation quality 2) Training Phase: Learns the mapping from the generated image to the target modality. Experiments demonstrate that FGSB achieves comparable generation performance to methods trained on large datasets, while using data from only two subjects. Moreover, the utilization of lesion information with FGSB significantly enhances its ability to preserve crucial lesion features."
430,679d459debd8ffd557a2b01b,cs.CV,https://arxiv.org/pdf/2501.14122,Reinforcement Learning Platform for Adversarial Black-box Attacks with Custom Distortion Filters,"Soumyendu Sarkar, Ashwin Ramesh Babu, Sajad Mousavi, Vineet Gundecha, Sahand Ghorbanpour, Avisek Naug, Ricardo Luna Gutierrez, Antonio Guillen","Machine Learning, Artificial Intelligence, Cryptography and Security, Computer Vision and Pattern Recognition",
431,679d459debd8ffd557a2b01c,cs.CV,https://arxiv.org/pdf/2501.14066,Efficient 2D CT Foundation Model for Contrast Phase Classification,"Benjamin Hou, Tejas Sudharshan Mathai, Pritam Mukherjee, Xinya Wang, Ronald M. Summers, Zhiyong Lub","Image and Video Processing, Computer Vision and Pattern Recognition",Purpose:The purpose of this study is to harness the efficiency of a 2D foundation model to develop a robust phase classifier that is resilient to domain shifts.
432,679d459debd8ffd557a2b01d,cs.CV,https://arxiv.org/pdf/2501.14048,SIDDA: SInkhorn Dynamic Domain Adaptation for Image Classification with Equivariant Neural Networks,"Sneh Pandya, Purvik Patel, Brian D. Nord, Mike Walmsley, Aleksandra Ćiprijanović","Machine Learning, Astrophysics of Galaxies, Artificial Intelligence, Computer Vision and Pattern Recognition","Modern neural networks (NNs) often do not generalize well in the presence of a “covariate shift”; that is, in situations where the training and test data distributions differ, but the conditional distribution of classification labels given the data remains unchanged. In such cases, NN generalization can be reduced to a problem of learning more robust, domain-invariant features.
Domain adaptation (DA) methods include a broad range of techniques aimed at achieving this; however, these methods have struggled with the need for extensive hyperparameter tuning, which then incurs significant computational costs.
In this work, we introduce SIDDA, an out-of-the-box DA training algorithm built upon the Sinkhorn divergence, that can achieve effective domain alignment with minimal hyperparameter tuning and computational overhead.
We demonstrate the efficacy of our method on multiple simulated and real datasets of varying complexity, including simple shapes, handwritten digits, and real astronomical observations.
These datasets exhibit covariate shifts due to noise, blurring, and differences between telescopes.
SIDDA is compatible with a variety of NN architectures, and it works particularly well in improving classification accuracy and model calibration when paired with symmetry-aware equivariant neural networks (ENNs).
We find that SIDDA consistently enhances the generalization capabilities of NNs, achieving up to a≈40%absentpercent40\approx 40\%≈ 40 %improvement in classification accuracy on unlabeled target data, while also providing a more modest performance gain of≲1%less-than-or-similar-toabsentpercent1\lesssim 1\%≲ 1 %on labeled source data.
We also study the efficacy of DA on ENNs with respect to the varying group orders of the dihedral groupDNsubscript𝐷𝑁D_{N}italic_D start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT, and find that the model performance improves as the degree of equivariance increases.
Finally, we find that SIDDA enhances model calibration on both source and target data, with the most significant gains in the unlabeled target domain—achieving over an order of magnitude improvement in the expected calibration error and Brier score.
SIDDA’s versatility across various NN models and datasets, combined with its automated approach to domain alignment, has the potential to significantly advance multi-dataset studies by enabling the development of highly generalizable models.\faGithub\faDatabase"
433,679d459debd8ffd557a2b01e,cs.CV,https://arxiv.org/pdf/2501.14013,Leveraging Multiphase CT for Quality Enhancement of Portal Venous CT: Utility for Pancreas Segmentation,"Xinya Wang, Tejas Sudharshan Mathai, Boah Kim, Ronald M. Summers","Image and Video Processing, Artificial Intelligence, Computer Vision and Pattern Recognition","Multiphase CT studies are routinely obtained in clinical practice for diagnosis and management of various diseases, such as cancer. However, the CT studies can be acquired with low radiation doses, different scanners, and are frequently affected by motion and metal artifacts. Prior approaches have targeted the quality improvement of one specific CT phase (e.g., non-contrast CT). In this work, we hypothesized that leveraging multiple CT phases for the quality enhancement of one phase may prove advantageous for downstream tasks, such as segmentation. A 3D progressive fusion and non-local (PFNL) network was developed. It was trained with three degraded (low-quality) phases (non-contrast, arterial, and portal venous) to enhance the quality of the portal venous phase. Then, the effect of scan quality enhancement was evaluated using a proxy task of pancreas segmentation, which is useful for tracking pancreatic cancer. The proposed approach improved the pancreas segmentation by∼similar-to\sim∼3% over the corresponding low-quality CT scan. To the best of our knowledge, we are the first to harness multiphase CT for scan quality enhancement and improved pancreas segmentation."
434,679d459debd8ffd557a2b01f,cs.CV,https://arxiv.org/pdf/2501.13988,MCRL4OR: Multimodal Contrastive Representation Learning for Off-Road Environmental Perception,"Yi Yang, Zhang Zhang, Liang Wang","Robotics, Artificial Intelligence, Computer Vision and Pattern Recognition","Most studies on environmental perception for autonomous vehicles (AVs) focus on urban traffic environments, where the objects/stuff to be perceived are mainly from man-made scenes and scalable datasets with dense annotations can be used to train supervised learning models. By contrast, it is hard to densely annotate a large-scale off-road driving dataset manually due to the inherently unstructured nature of off-road environments. In this paper, we propose a Multimodal Contrastive Representation Learning approach for Off-Road environmental perception, namely MCRL4OR. This approach aims to jointly learn three encoders for processing visual images, locomotion states, and control actions by aligning the locomotion states with the fused features of visual images and control actions within a contrastive learning framework. The causation behind this alignment strategy is that the inertial locomotion state is the result of taking a certain control action under the current landform/terrain condition perceived by visual sensors. In experiments, we pre-train the MCRL4OR with a large-scale off-road driving dataset and adopt the learned multimodal representations for various downstream perception tasks in off-road driving scenarios. The superior performance in downstream tasks demonstrates the advantages of the pre-trained multimodal representations. The codes can be found inhttps://github.com/1uciusy/MCRL4OR."
435,679d459debd8ffd557a2b020,cs.CV,https://arxiv.org/pdf/2501.13985,Pilot: Building the Federated Multimodal Instruction Tuning Framework,"Baochen Xiong, Xiaoshan Yang, Yaguang Song, Yaowei Wang, Changsheng Xu","Machine Learning, Artificial Intelligence, Computer Vision and Pattern Recognition","In this paper, we explore a novel federated multimodal instruction tuning task(FedMIT),
which is significant for collaboratively fine-tuning MLLMs on different types of multimodal instruction data on distributed devices.
To solve the new task, we propose a federated multimodal instruction tuning framework(Pilot).
Our framework integrates two stages of “adapter on adapter” into the connector of the vision encoder and the LLM.
In stage 1, we extract task-specific features and client-specific features from visual information.
In stage 2, we build the cross-task Mixture-of-Adapters(CT-MoA) module to perform cross-task interaction.
Each client can not only capture personalized information of local data and learn task-related multimodal information, but also learn general knowledge from other tasks.
In addition, we introduce an adaptive parameter aggregation strategy for text training parameters, which optimizes parameter aggregation by calculating weights based on the euclidean distance between parameters, so that parameter aggregation can benefit from positive effects to the greatest extent while effectively reducing negative effects.
Our framework can collaboratively exploit distributed data from different local clients to learn cross-task knowledge without being affected by the task heterogeneity during instruction tuning.
The effectiveness of our method is verified in two different cross-task scenarios."
436,679d459debd8ffd557a2b021,cs.CV,https://arxiv.org/pdf/2501.13972,Synthetic CT image generation from CBCT: A Systematic Review,"Alzahra Altalib, Scott McGregor, Chunhui Li, Alessandro Perelli","Image and Video Processing, Computer Vision and Pattern Recognition","The generation of synthetic CT (sCT) images from cone-beam CT (CBCT) data using deep learning methodologies represents a significant advancement in radiation oncology. This systematic review, following PRISMA guidelines and using the PICO model, comprehensively evaluates the literature from 2014 to 2024 on the generation of sCT images for radiation therapy planning in oncology. A total of 35 relevant studies were identified and analyzed, revealing the prevalence of deep learning approaches in the generation of sCT. This review comprehensively covers synthetic CT generation based on CBCT and proton-based studies. Some of the commonly employed architectures explored are convolutional neural networks (CNNs), generative adversarial networks (GANs), transformers, and diffusion models. Evaluation metrics including mean absolute error (MAE), root mean square error (RMSE), peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) consistently demonstrate the comparability of sCT images with gold-standard planning CTs (pCT), indicating their potential to improve treatment precision and patient outcomes. Challenges such as field-of-view (FOV) disparities and integration into clinical workflows are discussed, along with recommendations for future research and standardization efforts. In general, the findings underscore the promising role of sCT-based approaches in personalized treatment planning and adaptive radiation therapy, with potential implications for improved oncology treatment delivery and patient care."
437,679d459debd8ffd557a2b022,cs.CV,https://arxiv.org/pdf/2501.13970,Patch-Based and Non-Patch-Based inputs Comparison into Deep Neural Models: Application for the Segmentation of Retinal Diseases on Optical Coherence Tomography Volumes,"Khaled Al-Saih, Fares Al-Shargie, Mohammed Isam Al-hiyali, Reham Alhejaili","Image and Video Processing, Computer Vision and Pattern Recognition, Machine Learning","Worldwide, sight loss is commonly occurred by retinal diseases, with age-related macular degeneration (AMD) being a notable facet that affects elderly patients.
Approaching 170 million persons wide-ranging have been spotted with AMD, a figure anticipated to rise to 288 million by 2040.
For visualizing retinal layers, optical coherence tomography (OCT) dispenses the most compelling non-invasive method.
Frequent patient visits have increased the demand for automated analysis of retinal diseases, and deep learning networks have shown promising results in both image and pixel-level 2D scan classification.
However, when relying solely on 2D data, accuracy may be impaired, especially when localizing fluid volume diseases.
The goal of automatic techniques is to outperform humans in manually recognizing illnesses in medical data.
In order to further understand the benefit of deep learning models, we studied the effects of the input size.
The dice similarity coefficient (DSC) metric showed a human performance score of 0.71 for segmenting various retinal diseases.
Yet, the deep models surpassed human performance to establish a new era of advancement of segmenting the diseases on medical images.
However, to further improve the performance of the models, overlapping patches enhanced the performance of the deep models compared to feeding the full image.
The highest score for a patch-based model in the DSC metric was 0.88 in comparison to the score of 0.71 for the same model in non-patch-based for SRF fluid segmentation.
The objective of this article is to show a fair comparison between deep learning models in relation to the input (Patch-Based vs. NonPatch-Based)."
438,679d459debd8ffd557a2b023,cs.CV,https://arxiv.org/pdf/2501.13960,LiCAR: pseudo-RGB LiDAR image for CAR segmentation,"Ignacio de Loyola Páez-Ubieta, Edison P. Velasco-Sánchez, Santiago T. Puente","Image and Video Processing, Computer Vision and Pattern Recognition, Robotics","With the advancement of computing resources, an increasing number of Neural Networks (NNs) are appearing for image detection and segmentation appear. However, these methods usually accept as input a RGB 2D image. On the other side, Light Detection And Ranging (LiDAR) sensors with many layers provide images that are similar to those obtained from a traditional low resolution RGB camera. Following this principle, a new dataset for segmenting cars in pseudo-RGB images has been generated. This dataset combines the information given by the LiDAR sensor into a Spherical Range Image (SRI), concretely the reflectivity, near infrared and signal intensity 2D images. These images are then fed into instance segmentation NNs. These NNs segment the cars that appear in these images, having as result a Bounding Box (BB) and mask precision of 88% and 81.5% respectively with You Only Look Once (YOLO)-v8 large. By using this segmentation NN, some trackers have been applied so as to follow each car segmented instance along a video feed, having great performance in real world experiments."
439,679d459debd8ffd557a2b024,cs.CV,https://arxiv.org/pdf/2501.12880,Advanced deep architecture pruning using single filter performance,"Yarden Tzach, Yuval Meir, Ronit D. Gross, Ofek Tevet, Ella Koresh, Ido Kanter","Machine Learning, Computer Vision and Pattern Recognition",
440,679d459debd8ffd557a2b025,cs.CL,https://arxiv.org/pdf/2501.18585,Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs,"Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu",Computation and Language,"Large language models (LLMs) such as OpenAI’s o1 have demonstrated remarkable abilities in complex reasoning tasks by scaling test-time compute and exhibiting human-like deep thinking. However, we identify a phenomenon we termunderthinking, where o1-like LLMs frequently switch between different reasoning thoughts without sufficiently exploring promising paths to reach a correct solution. This behavior leads to inadequate depth of reasoning and decreased performance, particularly on challenging mathematical problems.
To systematically analyze this issue, we conduct experiments on three challenging test sets and two representative open-source o1-like models, revealing that frequent thought switching correlates with incorrect responses. We introduce a novel metric to quantify underthinking by measuring token efficiency in incorrect answers. To address underthinking, we propose a decoding strategy withthought switchingpenalty (Tip) that discourages premature transitions between thoughts, encouraging deeper exploration of each reasoning path.
Experimental results demonstrate that our approach improves accuracy across challenging datasets without requiring model fine-tuning.
Our findings contribute to understanding reasoning inefficiencies in o1-like LLMs and offer a practical solution to enhance their problem-solving capabilities."
441,679d459debd8ffd557a2b026,cs.CL,https://arxiv.org/pdf/2501.18578,R.I.P.: Better Models by Survival of the Fittest Prompts,"Ping Yu, Weizhe Yuan, Olga Golovneva, Tianhao Wu, Sainbayar Sukhbaatar, Jason Weston, Jing Xu","Computation and Language, Artificial Intelligence, Machine Learning","Training data quality is one of the most important drivers of final model quality. In this work, we introduce a method for evaluating data integrity based on the assumption that low-quality input prompts result in high variance and low quality responses. This is achieved by measuring therejected response qualityand thereward gapbetween the chosen and rejected preference pair. Our method, Rejecting Instruction Preferences (RIP) can be used to filter prompts from existing training sets, or to make high quality synthetic datasets, yielding large performance gains across various benchmarks compared to unfiltered data.
Using Llama 3.1-8B-Instruct,RIPimproves AlpacaEval2 LC Win Rate by 9.4%,
Arena-Hard by 8.7%, and WildBench by 9.9%.
Using Llama 3.3-70B-Instruct,RIPimproves Arena-Hard from 67.5 to 82.9, which is from 18th place to 6th overall in the leaderboard."
442,679d459debd8ffd557a2b027,cs.CL,https://arxiv.org/pdf/2501.18539,Can we Retrieve Everything All at Once? ARM: An Alignment-Oriented LLM-based Retrieval Method,"Peter Baile Chen, Yi Zhang, Michael Cafarella, Dan Roth","Computation and Language, Artificial Intelligence, Information Retrieval","Real-world open-domain questions can be complicated, particularly when answering them involves information from multiple information sources.
LLMs have demonstrated impressive performance in decomposing complex tasks into simpler steps, and previous work has used it for better retrieval in support of complex questions.
However, LLM’s decomposition of questions is unaware of what data is available and how data is organized, often leading to a sub-optimal retrieval performance.
Recent effort in agentic RAG proposes to perform retrieval in an iterative fashion, where a followup query is derived as an action based on previous rounds of retrieval.
While this provides one way of interacting with the data collection,
agentic RAG’s exploration of data isinefficientbecause
successive queries depend on previous results rather than being guided by the organization of available data in the collection.
To address this problem, we propose an LLM-based retrieval method — ARM, that aims to better align the question with the organization of the data collection by exploringrelationships among data objectsbeyond matching the utterance of the query, thus leading to a retrieve-all-at-once solution for complex queries.
We evaluated ARM on two datasets, Bird and OTT-QA. On Bird, it outperforms standard RAG with query decomposition by up to 5.2 pt in execution accuracy and agentic RAG (ReAct) by up to 15.9 pt.
On OTT-QA, it achieves up to 5.5 pt and 19.3 pt higher F1 match scores compared to these approaches."
443,679d459debd8ffd557a2b028,cs.CL,https://arxiv.org/pdf/2501.18532,Differentially Private Steering for Large Language Model Alignment,"Anmol Goel, Yaxi Hu, Iryna Gurevych, Amartya Sanyal","Computation and Language, Machine Learning","Aligning Large Language Models (LLMs) with human values and away from undesirable behaviors (such as hallucination) has become increasingly important. Recently, steering LLMs towards a desired behavior via activation editing has emerged as an effective method to mitigate harmful generations at inference-time. Activation editing modifies LLM representations by preserving information from positive demonstrations (e.g., truthful) and minimising information from negative demonstrations (e.g., hallucinations). When these demonstrations come from a private dataset, the aligned LLM may leak private information contained in those private samples. In this work, we present the first study of aligning LLM behavior with private datasets. Our work proposes thePrivateSteering for LLMAlignment (PSA)algorithm to edit LLM activations with differential privacy (DP) guarantees. We conduct extensive experiments on seven different benchmarks with open-source LLMs of different sizes (0.5B to 7B) and model families (LlaMa, Qwen, Mistral and Gemma). Our results show that PSA achieves DP guarantees for LLM alignment with minimal loss in performance, including alignment metrics, open-ended text generation quality, and general-purpose reasoning. We also develop the first Membership Inference Attack (MIA) for evaluating and auditing the empirical privacy for the problem of LLM steering via activation editing. Our attack is tailored for activation editing and relies solely on the generated texts without their associated probabilities. Our experiments support the theoretical guarantees by showing improved guarantees for ourPSAalgorithm compared to several existing non-private techniques.111Our code is available athttps://github.com/UKPLab/iclr2025-psa/"
444,679d459debd8ffd557a2b029,cs.CL,https://arxiv.org/pdf/2501.18512,Streaming DiLoCo with overlapping communication: Towards a Distributed Free Lunch,"Arthur Douillard, Yanislav Donchev, Keith Rush, Satyen Kale, Zachary Charles, Zachary Garrett, Gabriel Teston, Dave Lacey, Ross McIlroy, Jiajun Shen, Alexandre Ramé, Arthur Szlam, Marc'Aurelio Ranzato, Paul Barham",Computation and Language,"Training of large language models (LLMs) is typically distributed across a large number of accelerators to reduce training time. Since internal states and parameter gradients need to be exchanged at each and every single gradient step, all devices need to be co-located using low-latency high-bandwidth communication links to support the required high volume of exchanged bits. Recently, distributed algorithms like DiLoCo(Douillard et al.,2024a)have relaxed such co-location constraint: accelerators can be grouped into “workers”, where synchronizations between workers only occur infrequently. This in turn means that workers can afford being connected by lower bandwidth communication links without affecting learning quality. However, in these methods, communication across workers still requires the same peak bandwidth as before, as the synchronizations require all parameters to be exchanged across all workers. In this paper, we improve DiLoCo in three ways. First, we synchronize only subsets of parameters in sequence, rather than all at once, which greatly reduces peak bandwidth. Second, we allow workers to continue training while synchronizing, which decreases wall clock time. Third, we quantize the data exchanged by workers, which further reduces bandwidth across workers. By properly combining these modifications, we show experimentally that we can distribute training of billion-scale parameters and reach similar quality as before, but reducing required bandwidth by two orders of magnitude."
445,679d459debd8ffd557a2b02a,cs.CL,https://arxiv.org/pdf/2501.18457,CALM: Unleashing the Cross-Lingual Self-Aligning Ability of Language Model Question Answering,"Yumeng Wang, Zhiyuan Fan, Qingyun Wang, May Fung, Heng Ji",Computation and Language,"Large Language Models (LLMs) are pretrained on extensive multilingual corpora to acquire both language-specific cultural knowledge and general knowledge. Ideally, while LLMs should provide consistent responses to culture-independent questions across languages, we observe significant performance disparities. To address this, we explore theCross-Lingual Self-Aligning ability ofLanguageModels (CALM) to align knowledge across languages.
Specifically, for a given question, we sample multiple responses across different languages, and select the most self-consistent response as the target, leaving the remaining responses as negative examples. We then employ direct preference optimization (DPO) to align the model’s knowledge across different languages.
Evaluations on the MEDQA and X-CSQA datasets demonstrate CALM’s effectiveness in enhancing cross-lingual knowledge question answering, both in zero-shot and retrieval augmented settings. We also found that increasing the number of languages involved in CALM training leads to even higher accuracy and consistency. We offer a qualitative analysis of how cross-lingual consistency can enhance knowledge alignment and explore the method’s generalizability.
The source code and data of this paper is available onGitHub."
446,679d459debd8ffd557a2b02b,cs.CL,https://arxiv.org/pdf/2501.18435,GENIE: Generative Note Information Extraction model for structuring EHR data,"Huaiyuan Ying, Hongyi Yuan, Jinsen Lu, Zitian Qu, Yang Zhao, Zhengyun Zhao, Isaac Kohane, Tianxi Cai, Sheng Yu",Computation and Language,
447,679d459debd8ffd557a2b02c,cs.CL,https://arxiv.org/pdf/2501.18365,RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects,"Yiteng Tu, Weihang Su, Yujia Zhou, Yiqun Liu, Qingyao Ai","Computation and Language, Information Retrieval","Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems."
448,679d459debd8ffd557a2b02d,cs.CL,https://arxiv.org/pdf/2501.18287,"Mining for Species, Locations, Habitats, and Ecosystems from Scientific Papers in Invasion Biology: A Large-Scale Exploratory Study with Large Language Models","Jennifer D'Souza, Zachary Laubach, Tarek Al Mustafa, Sina Zarrieß, Robert Frühstückl, Phyllis Illari","Computation and Language, Artificial Intelligence, Digital Libraries","This document contains the instructions for preparing a camera-ready
manuscript for the proceedings of NoDaLiDa/Batlic-HLT 2025, which is also
accessible directly on Overleaf. The document itself
conforms to its own specifications and is therefore an example of
what your manuscript should look like. These instructions should be
used for both papers submitted for review and for final versions of
accepted papers. Authors are asked to conform to all the directions
reported in this document."
449,679d459debd8ffd557a2b02e,cs.CL,https://arxiv.org/pdf/2501.18280,Jailbreaking LLMs' Safeguard with Universal Magic Words for Text Embedding Models,"Haoyu Liang, Youran Sun, Yunfeng Cai, Jun Zhu, Bo Zhang","Computation and Language, Artificial Intelligence, Machine Learning, Neural and Evolutionary Computing","The security issue of large language models (LLMs) has gained significant attention recently, with various defense mechanisms developed to prevent harmful outputs, among which safeguards based on text embedding models serve as a fundamental defense.
Through testing, we discover that the distribution of text embedding model outputs is significantly biased with a large mean.
Inspired by this observation, we propose novel efficient methods to search foruniversal magic wordsthat can attack text embedding models.
The universal magic words as suffixes can move the embedding of any text towards the bias direction, therefore manipulate the similarity of any text pair and mislead safeguards.
By appending magic words to user prompts and requiring LLMs to end answers with magic words, attackers can jailbreak the safeguard.
To eradicate this security risk, we also propose defense mechanisms against such attacks, which can correct the biased distribution of text embeddings in a train-free manner."
450,679d459debd8ffd557a2b02f,cs.CL,https://arxiv.org/pdf/2501.18251,How to Select Datapoints for Efficient Human Evaluation of NLG Models?,"Vilém Zouhar, Peng Cui, Mrinmaya Sachan",Computation and Language,
451,679d459debd8ffd557a2b030,cs.CL,https://arxiv.org/pdf/2501.18205,Contextually Structured Token Dependency Encoding for Large Language Models,"James Blades, Frederick Somerfield, William Langley, Susan Everingham, Maurice Witherington",Computation and Language,"Token representation strategies within large-scale neural architectures often rely on contextually refined embeddings, yet conventional approaches seldom encode structured relationships explicitly within token interactions. Self-attention mechanisms effectively capture dynamic contextual dependencies, but their reliance on learned weight distributions limits the preservation of long-range hierarchical structures in generated sequences. Dependency-aware token encoding introduces a structured approach to embedding initialization, ensuring that relational constraints are embedded within token representations rather than inferred solely through attention dynamics. The proposed encoding mechanism refines token interactions through dependency-weighted attention computations, ensuring that syntactic and semantic dependencies are retained across multiple processing layers. Empirical evaluations indicate reductions in perplexity across diverse linguistic benchmarks, suggesting improvements in contextual coherence and predictive consistency in autoregressive text generation. Computational efficiency assessments reveal a moderate increase in memory consumption and training time, attributed to additional matrix computations within the encoding module, yet scalability remains feasible within conventional transformer architectures. Structured encoding enhances lexical variation and dependency retention, reinforcing linguistic coherence without requiring external syntactic annotations or auxiliary training objectives. Statistical comparisons highlight improvements in dependency alignment, particularly in longer sequences where conventional self-attention models exhibit degradation in hierarchical consistency. Sentence length distributions indicate a reduction in abrupt phrase transitions, further supporting the hypothesis that explicit dependency encoding facilitates more structured phrase generation."
452,679d459debd8ffd557a2b031,cs.CL,https://arxiv.org/pdf/2501.18154,Mixed-Precision Graph Neural Quantization for Low Bit Large Language Models,"Wanlong Liu, Yichen Xiao, Dingyi Zeng, Hongyang Zhao, Wenyu Chen, Malu Zhang",Computation and Language,"Post-Training Quantization (PTQ) is pivotal for deploying large language models (LLMs) within resource-limited settings by significantly reducing resource demands.
However, existing PTQ strategies underperform at low bit levels (<3 bitsabsent3 bits<\text{3 bits}< 3 bits) due to the significant difference between the quantized and original weights.
To enhance the quantization performance at low bit widths, we introduce aMixed-precisionGraph Neural PTQ (MG-PTQ) approach, employing a graph neural network (GNN) module to capture dependencies among weights and adaptively assign quantization bit-widths.
Through the information propagation of the GNN module, our method more effectively captures dependencies among target weights, leading to a more accurate assessment of weight importance and optimized allocation of quantization strategies.
Extensive experiments on the WikiText2 and C4 datasets demonstrate that our MG-PTQ method outperforms previous state-of-the-art PTQ method GPTQ, setting new benchmarks for quantization performance under low-bit (<3 bitsabsent3 bits<\text{3 bits}< 3 bits) conditions."
453,679d459debd8ffd557a2b032,cs.CL,https://arxiv.org/pdf/2501.18128,Unraveling the Capabilities of Language Models in News Summarization,"Abdurrahman Odabaşı, Göksel Biricik","Computation and Language, Artificial Intelligence","Given the recent introduction of multiple language models and the ongoing demand for improved Natural Language Processing tasks, particularly summarization, this work provides a comprehensive benchmarking of 20 recent language models, focusing on smaller ones for the news summarization task. In this work, we systematically test the capabilities and effectiveness of these models in summarizing news article texts which are written in different styles and presented in three distinct datasets. Specifically, we focus in this study on zero-shot and few-shot learning settings and we apply a robust evaluation methodology that combines different evaluation concepts including automatic metrics, human evaluation, and LLM-as-a-judge. Interestingly, including demonstration examples in the few-shot learning setting did not enhance models’ performance and, in some cases, even led to worse quality of the generated summaries. This issue arises mainly due to the poor quality of the gold summaries that have been used as reference summaries, which negatively impacts the models’ performance. Furthermore, our study’s results highlight the exceptional performance of GPT-3.5-Turbo and GPT-4, which generally dominate due to their advanced capabilities. However, among the public models evaluated, certain models such as Qwen1.5-7B, SOLAR-10.7B-Instruct-v1.0, Meta-Llama-3-8B and Zephyr-7B-Beta demonstrated promising results. These models showed significant potential, positioning them as competitive alternatives to large models for the task of news summarization."
454,679d459debd8ffd557a2b033,cs.CL,https://arxiv.org/pdf/2501.18119,Self-supervised Quantized Representation for Seamlessly Integrating Knowledge Graphs with Large Language Models,"Qika Lin, Tianzhe Zhao, Kai He, Zhen Peng, Fangzhi Xu, Ling Huang, Jingying Ma, Mengling Feng","Computation and Language, Artificial Intelligence","Due to the presence of the natural gap between Knowledge Graph (KG) structures and the natural language,
the effective integration of holistic structural information of KGs with Large Language Models (LLMs) has emerged as a significant question.
To this end, we propose a two-stage framework to learn and apply quantized codes for each entity, aiming for the seamless integration of KGs with LLMs.
Firstly, a self-supervised quantized representation (SSQR) method is proposed to compress both KG structural and semantic knowledge into discrete codes (i.e., tokens) that align the format of language sentences.
We further design KG instruction-following data by viewing these learned codes as features to directly input to LLMs, thereby achieving seamless integration.
The experiment results demonstrate that SSQR outperforms existing unsupervised quantized methods, producing more distinguishable codes.
Further, the fine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link prediction and triple classification tasks, utilizing only 16 tokens per entity instead of thousands in conventional prompting methods."
455,679d459debd8ffd557a2b034,cs.CL,https://arxiv.org/pdf/2501.18101,Diverse Preference Optimization,"Jack Lanchantin, Angelica Chen, Shehzaad Dhuliawala, Ping Yu, Jason Weston, Sainbayar Sukhbaatar, Ilia Kulikov",Computation and Language,"Post-training of language models, either through reinforcement learning, preference optimization or supervised finetuning, tends to sharpen the output probability distribution and reduce the diversity of generated responses.
This is particularly a problem for creative generative tasks where varied responses are desired.
In this work we introduceDiverse Preference Optimization (DivPO), an online optimization method which learns to generate much more diverse responses than standard pipelines, while maintaining the quality of the generations. In DivPO, preference pairs are selected by first consideringa poolof responses, and a measure of diversity among them, and selecting chosen examples as being more rare but high quality, while rejected examples are more common, but low quality. DivPO results in generating
45.6% more diverse persona attributes, and an 81% increase in story diversity,
while maintaining similar win rates as standard baselines."
456,679d459debd8ffd557a2b035,cs.CL,https://arxiv.org/pdf/2501.18100,Panacea: Mitigating Harmful Fine-tuning for Large Language Models via Post-fine-tuning Perturbation,"Yibo Wang, Tiansheng Huang, Li Shen, Huanjin Yao, Haotian Luo, Rui Liu, Naiqiang Tan, Jiaxing Huang, Dacheng Tao","Computation and Language, Artificial Intelligence","Harmful fine-tuning attack introduces significant security risks to the fine-tuning services. Main-stream defenses aim to vaccinate the model such that the later harmful fine-tuning attack is less effective. However, our evaluation results show that such defenses are fragile– with a few fine-tuning steps, the model still can learn the harmful knowledge. To this end, we do further experiment and find that an embarrassingly simple solution– adding purely random perturbations to the fine-tuned model, can recover the model from harmful behaviors, though it leads to a degradation in the model’s fine-tuning performance.
To address the degradation of fine-tuning performance, we further propose Panacea, which optimizes an adaptive perturbation that will be applied to the model after fine-tuning. Panacea maintains model’s safety alignment performance without compromising downstream fine-tuning performance. Comprehensive experiments are conducted on different harmful ratios, fine-tuning tasks and mainstream LLMs, where the average harmful scores are reduced by up-to 21.5%, while maintaining fine-tuning performance. As a by-product, we analyze the adaptive perturbation and show that different layers in various LLMs have distinct safety coefficients. Source code available athttps://github.com/w-yibo/Panacea."
457,679d459debd8ffd557a2b036,cs.CL,https://arxiv.org/pdf/2501.17994,InnerThoughts: Disentangling Representations and Predictions in Large Language Models,"Didier Chételat, Joseph Cotnareanu, Rylee Thompson, Yingxue Zhang, Mark Coates","Computation and Language, Machine Learning","Large language models (LLMs) contain substantial factual knowledge which is commonly elicited by multiple-choice question-answering prompts. Internally, such models process the prompt through multiple transformer layers, building varying representations of the problem within its hidden states. Ultimately, however, only the hidden state corresponding to the final layer and token position are used to predict the answer label. In this work, we propose instead to learn a small separate neural network predictor module on a collection of training questions, that take the hidden states from all the layers at the last temporal position as input and outputs predictions. In effect, such a framework disentangles the representational abilities of LLMs from their predictive abilities. On a collection of hard benchmarks, our method achieves considerable improvements in performance, sometimes comparable to supervised fine-tuning procedures, but at a fraction of the computational cost."
458,679d459debd8ffd557a2b037,cs.CL,https://arxiv.org/pdf/2501.18511,WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in Post-Training,"Benjamin Feuer, Chinmay Hegde","Machine Learning, Computation and Language","Language model (LLM) post-training can refine behaviors and unlock new skills, but the open science supporting these post-training techniques is still in its infancy. One limiting factor has been the difficulty of conducting large-scale comparative analyses of synthetic data generating models and LLM judges. To close this gap, we introduceWildChat-50m, the largest public chat dataset to date. We extend the existing WildChat dataset to include responses not only from GPT, but from over 50 different open-weight models, ranging in size from 0.5B to 104B parameters. We conduct an extensive comparative analysis and demonstrate the potential of this dataset by creatingRe-Wild, our own public SFT mix, which outperforms the recent Tulu-3 SFT mixture from Allen AI with only 40% as many samples. Our work is available athttps://github.com/penfever/wildchat-50m."
459,679d459debd8ffd557a2b038,cs.CL,https://arxiv.org/pdf/2501.18356,State Stream Transformer (SST) : Emergent Metacognitive Behaviours Through Latent State Persistence,Thea Aviss,"Machine Learning, Artificial Intelligence, Computation and Language","We introduce the State Stream Transformer (SST), a novel LLM architecture that reveals emergent reasoning behaviours and capabilities latent in pretrained weights of the base model through addressing a fundamental limitation in traditional transformer models: the lack of latent computational continuity across autoregressive generations in the state space. SST introduces a sliding window latent state (FFN) cache with weighted decay that maintains and evolves persistent latent processes throughout autoregressive generations. Through controlled experiments comparing base and SST architectures using the same frozen weights and no additional training, we demonstrate that this architectural modification alone enables enhanced reasoning capabilities which appear best explained by some form of potential higher-order processing, as evidenced by emergent metacognitive behaviours. These behaviours persist under controlled conditions designed to eliminate confounding factors such as stochastic variation or learned response patterns. Analysis of latent state distributions and processing dynamics provides evidence that it is solely the ’state stream’ that is responsible for these phenomena. In quantitative evaluations, the SST achieves substantial performance improvements over the base model on two reasoning benchmarks, reaching 89.01% accuracy on GSM-8K (0-shot) and 91.04% on ARC Challenge (0-shot CoT). These findings indicate that persistent computation in the latent state space enables fundamentally different information processing and internal reasoning strategies, with implications for our understanding of artificial intelligence systems."
460,679d459debd8ffd557a2b039,cs.CL,https://arxiv.org/pdf/2501.18292,Citation Recommendation based on Argumentative Zoning of User Queries,"Shutian Ma, Chengzhi Zhang, Heng Zhang, Zheng Gao","Information Retrieval, Computation and Language, Digital Libraries",
461,679d459debd8ffd557a2b03a,cs.CL,https://arxiv.org/pdf/2501.18265,"Collecting Cost-Effective, High-Quality Truthfulness Assessments with LLM Summarized Evidence","Kevin Roitero, Dustin Wright, Michael Soprano, Isabelle Augenstein, Stefano Mizzaro","Information Retrieval, Computation and Language, Human-Computer Interaction","With the degradation of guardrails against mis- and disinformation online, it is more critical than ever to be able to effectively combat it. In this paper, we explore the efficiency and effectiveness of using crowd-sourced truthfulness assessments based on condensed, large language model (LLM) generated summaries of online sources. We compare the use of generated summaries to the use of original web pages in an A/B testing setting, where we employ a large and diverse pool of crowd-workers to perform the truthfulness assessment. We evaluate the quality of assessments, the efficiency with which assessments are performed, and the behavior and engagement of participants. Our results demonstrate that theSummarymodality, which relies on summarized evidence, offers no significant change in assessment accuracy over theStandardmodality, while significantly increasing the speed with which assessments are performed. Workers using summarized evidence produce a significantly higher number of assessments in the same time frame, reducing the cost needed to acquire truthfulness assessments. Additionally, theSummarymodality maximizes both the inter-annotator agreements as well as the reliance on and perceived usefulness of evidence, demonstrating the utility of summarized evidence without sacrificing the quality of assessments."
462,679d459debd8ffd557a2b03b,cs.CL,https://arxiv.org/pdf/2501.18243,Statistical multi-metric evaluation and visualization of LLM system predictive performance,"Samuel Ackerman, Eitan Farchi, Orna Raz, Assaf Toledo","Applications, Computation and Language, Machine Learning","The evaluation of generative or discriminative large language model (LLM)-based systems is often a complex multi-dimensional problem. Typically, a set of system configuration alternatives are evaluated on one or more benchmark datasets, each with one or more evaluation metrics, which may differ between datasets. We often want to evaluate—with a statistical measure of significance—whether systems perform differently either on a given dataset according to a single metric, on aggregate across metrics on a dataset, or across datasets. Such evaluations can be done to support decision-making, such as deciding whether a particular system component change (e.g., choice of LLM or hyperparameter values) significantly improves performance over the current system configuration, or, more generally, whether a fixed set of system configurations (e.g., a leaderboard list) have significantly different performances according to metrics of interest. We present a framework implementation that automatically performs the correct statistical tests, properly aggregates the statistical results across metrics and datasets (a nontrivial task), and can visualize the results. The framework is demonstrated on the multi-lingual code generation benchmark CrossCodeEval, for several state-of-the-art LLMs."
463,679d459debd8ffd557a2b03c,cs.CL,https://arxiv.org/pdf/2501.18107,Scaling Inference-Efficient Language Models,"Song Bian, Minghao Yan, Shivaram Venkataraman","Machine Learning, Artificial Intelligence, Computation and Language","Scaling laws are powerful tools to predict the performance of large language models. However, current scaling laws fall short of accounting for inference costs. In this work, we first show that model architecture affects inference latency, where models of the same size can have up to3.5×3.5\times3.5 ×difference in latency. To tackle this challenge, we modify the Chinchilla scaling laws to co-optimize the model parameter count, the number of training tokens, and the model architecture. Due to the reason that models of similar training loss exhibit gaps in downstream evaluation, we also propose a novel method to train inference-efficient models based on the revised scaling laws. We perform extensive empirical studies to fit and evaluate our inference-aware scaling laws. We vary model parameters from 80M to 1B, training tokens from 1.6B to 30B, and model shapes, training a total of 63 models. Guided by our inference-efficient scaling law and model selection method, we release the Morph-1B model, which improves inference latency by1.8×1.8\times1.8 ×while maintaining accuracy on downstream tasks compared to open-source models, pushing the Pareto frontier of accuracy-latency tradeoff."
464,679d459debd8ffd557a2b03d,cs.CL,https://arxiv.org/pdf/2501.18103,Beyond Turn-taking: Introducing Text-based Overlap into Human-LLM Interactions,"JiWoo Kim, Minsuk Chang, JinYeong Bak","Human-Computer Interaction, Computation and Language","Traditional text-based Human-AI interactions often adhere to a strict turn-taking approach. In this research, we propose a novel approach that incorporates overlapping messages, mirroring natural human conversations. Through a formative study, we observed that even in text-based contexts, users instinctively engage in overlapping behaviors like “A: Today I went to–” “B: yeah.” To capitalize on these insights, we developed OverlapBot, a prototype chatbot where both AI and users can initiate overlapping. Our user study revealed that OverlapBot was perceived as more communicative and immersive than traditional turn-taking chatbot, fostering faster and more natural interactions. Our findings contribute to the understanding of design space for overlapping interactions. We also provide recommendations for implementing overlap-capable AI interactions to enhance the fluidity and engagement of text-based conversations."
465,679d459debd8ffd557a2b03e,cs.CL,https://arxiv.org/pdf/2501.18099,Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge,"Swarnadeep Saha, Xian Li, Marjan Ghazvininejad, Jason Weston, Tianlu Wang","Artificial Intelligence, Computation and Language","LLM-as-a-Judge models generate chain-of-thought (CoT) sequences intended to capture the step-by-step reasoning process that underlies the final evaluation of a response. However, due to the lack of human-annotated CoTs for evaluation, the required components and structure of effective reasoning traces remain understudied. Consequently, previous approaches often (1) constrain reasoning traces to hand-designed components, such as a list of criteria, reference answers, or verification questions and (2) structure them such that planning is intertwined with the reasoning for evaluation. In this work, we propose EvalPlanner, a preference optimization algorithm for Thinking-LLM-as-a-Judge that first generates an unconstrained evaluation plan, followed by its execution, and then the final judgment. In a self-training loop, EvalPlanner iteratively optimizes over synthetically constructed evaluation plans and executions, leading to better final verdicts. Our method achieves a new state-of-the-art performance for generative reward models on RewardBench (with a score of93.993.993.993.9), despite being trained on fewer amount of, and synthetically generated, preference pairs. Additional experiments on other benchmarks like RM-Bench, JudgeBench, and FollowBenchEval further highlight the utility of both planning and reasoning for building robust LLM-as-a-Judge reasoning models."
466,679d459debd8ffd557a2b03f,cs.CL,https://arxiv.org/pdf/2501.18062,FinanceQA: A Benchmark for Evaluating Financial Analysis Capabilities of Large Language Models,"Spencer Mateega, Carlos Georgescu, Danny Tang","Machine Learning, Computation and Language",
467,679d459debd8ffd557a2b040,cs.CL,https://arxiv.org/pdf/2501.18045,From tools to thieves: Measuring and understanding public perceptions of AI through crowdsourced metaphors,"Myra Cheng, Angela Y. Lee, Kristina Rapuano, Kate Niederhoffer, Alex Liebscher, Jeffrey Hancock","Computers and Society, Artificial Intelligence, Computation and Language, Human-Computer Interaction","How has the public responded to the increasing prevalence of artificial intelligence (AI)-based technologies? We investigate public perceptions of AI by collecting over 12,000 responses over 12 months from a nationally representative U.S. sample. Participants provided open-ended metaphors reflecting their mental models of AI, a methodology that overcomes the limitations of traditional self-reported measures. Using a mixed-methods approach combining quantitative clustering and qualitative coding, we identify 20 dominant metaphors shaping public understanding of AI. To analyze these metaphors systematically, we present a scalable framework integrating language modeling (LM)-based techniques to measure key dimensions of public perception: anthropomorphism (attribution of human-like qualities), warmth, and competence. We find that Americans generally view AI as warm and competent, and that over the past year, perceptions of AI’s human-likeness and warmth have significantly increased (+34%,r=0.80,p<0.01;+41%,r=0.62,p<0.05formulae-sequencepercent34𝑟0.80formulae-sequence𝑝0.01percent41formulae-sequence𝑟0.62𝑝0.05+34\%,r=0.80,p<0.01;+41\%,r=0.62,p<0.05+ 34 % , italic_r = 0.80 , italic_p < 0.01 ; + 41 % , italic_r = 0.62 , italic_p < 0.05). Furthermore, these implicit perceptions, along with the identified dominant metaphors, strongly predict trust in and willingness to adopt AI (r2=0.21,0.18,p<0.001formulae-sequencesuperscript𝑟20.210.18𝑝0.001r^{2}=0.21,0.18,p<0.001italic_r start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 0.21 , 0.18 , italic_p < 0.001). We further explore how differences in metaphors and implicit perceptions—such as the higher propensity of women, older individuals, and people of color to anthropomorphize AI—shed light on demographic disparities in trust and adoption.
In addition to our dataset and framework for tracking evolving public attitudes, we provide actionable insights on using metaphors for inclusive and responsible AI development."
468,679d459debd8ffd557a2b041,cs.CL,https://arxiv.org/pdf/2501.17905,DReSS: Data-driven Regularized Structured Streamlining for Large Language Models,"Mingkuan Feng, Jinyang Wu, Shuai Zhang, Pengpeng Shao, Ruihan Jin, Zhengqi Wen, Jianhua Tao, Feihu Che","Machine Learning, Artificial Intelligence, Computation and Language","Large language models (LLMs) have achieved significant progress across various domains, but their increasing scale results in high computational and memory costs. Recent studies have revealed that LLMs exhibit sparsity, providing the potential to reduce model size through pruning techniques. However, existing pruning methods typically follow a prune-then-finetune paradigm. Since the pruned components still contain valuable information, their direct removal often leads to irreversible performance degradation, imposing a substantial computational burden to recover performance during finetuning. In this paper, we propose a novel paradigm that first applies regularization, then prunes, and finally finetunes. Based on this paradigm, we introduce DReSS, a simple and effectiveData-drivenRegularizedStructuredStreamlining method for LLMs. By leveraging a small amount of data to regularize the components to be pruned, DReSS explicitly transfers the important information to the remaining parts of the model in advance. Compared to direct pruning, this can reduce the information loss caused by parameter removal, thereby enhancing its language modeling capabilities. Experimental results demonstrate that DReSS significantly outperforms existing pruning methods even under extreme pruning ratios, significantly reducing latency and increasing throughput."
469,679d459debd8ffd557a2b042,cs.CL,https://arxiv.org/pdf/2501.17860,Dialogue is Better Than Monologue: Instructing Medical LLMs via Strategical Conversations,"Zijie Liu, Xinyu Zhao, Jie Peng, Zhuangdi Zhu, Qingyu Chen, Xia Hu, Tianlong Chen","Computation and Language, Artificial Intelligence",
470,679d459debd8ffd557a2b043,cs.CL,https://arxiv.org/pdf/2501.17858,Improving Your Model Ranking on Chatbot Arena by Vote Rigging,"Rui Min, Tianyu Pang, Chao Du, Qian Liu, Minhao Cheng, Min Lin","Computation and Language, Artificial Intelligence, Cryptography and Security, Machine Learning","Chatbot Arena is a popular platform for evaluating LLMs by pairwise battles, where users vote for their preferred response from two randomly sampled anonymous models. While Chatbot Arena is widely regarded as a reliable LLM ranking leaderboard, we show that crowdsourced voting can beriggedto improve (or decrease) the ranking of a target modelmtsubscript𝑚𝑡m_{t}italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. We first introduce a straightforwardtarget-only riggingstrategy that focuses on new battles involvingmtsubscript𝑚𝑡m_{t}italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, identifying it via watermarking or a binary classifier, and exclusively voting formtsubscript𝑚𝑡m_{t}italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTwins. However, this strategy is practically inefficient because there are over190190190190models on Chatbot Arena and on average only about1%percent11\%1 %of new battles will involvemtsubscript𝑚𝑡m_{t}italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. To overcome this, we proposeomnipresent riggingstrategies, exploiting the Elo rating mechanism of Chatbot Arena thatany new vote on a battle can influence the ranking of the target modelmtsubscript𝑚𝑡m_{t}italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, even ifmtsubscript𝑚𝑡m_{t}italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTis not directly involved in the battle. We conduct experiments on around1.71.71.71.7millionhistorical votes from the Chatbot Arena Notebook, showing that omnipresent rigging strategies can improve model rankings by rigging onlyhundreds ofnew votes. While we have evaluated several defense mechanisms, our findings highlight the importance of continued efforts to prevent vote rigging.Codeis publicly available to reproduce all experiments."
471,679d459debd8ffd557a2b044,cs.CL,https://arxiv.org/pdf/2501.17840,Learning Beyond the Surface: How Far Can Continual Pre-Training with LoRA Enhance LLMs' Domain-Specific Insight Learning?,"Pouya Pezeshkpour, Estevam Hruschka","Computation and Language, Machine Learning","Large Language Models (LLMs) have demonstrated remarkable performance on various tasks, yet their ability to extract and internalize deeper insights from domain-specific datasets remains underexplored. In this study, we investigate how continual pre-training can enhance LLMs’ capacity for insight learning across three distinct forms: declarative, statistical, and probabilistic insights. Focusing on two critical domains: medicine and finance, we employ LoRA to train LLMs on two existing datasets. To evaluate each insight type, we create benchmarks to measure how well continual pre-training helps models go beyond surface-level knowledge. We also assess the impact of document modification on capturing insights. The results show that, while continual pre-training on original documents has a marginal effect, modifying documents to retain only essential information significantly enhances the insight-learning capabilities of LLMs. We released our dataset and code111https://github.com/megagonlabs/insight_miner."
472,679d459debd8ffd557a2b045,cs.CL,https://arxiv.org/pdf/2501.17830,A Comprehensive Survey on Legal Summarization: Challenges and Future Directions,"Mousumi Akter, Erion Çano, Erik Weber, Dennis Dobler, Ivan Habernal",Computation and Language,"This article provides a systematic up-to-date survey of automatic summarization techniques, datasets, models, and evaluation methods in the legal domain. Through specific source selection criteria, we thoroughly review over 120 papers spanning the modern ‘transformer’ era of natural language processing (NLP), thus filling a gap in existing systematic surveys on the matter. We present existing research along several axes and discuss trends, challenges, and opportunities for future research."
473,679d459debd8ffd557a2b046,cs.CL,https://arxiv.org/pdf/2501.17790,BreezyVoice: Adapting TTS for Taiwanese Mandarin with Enhanced Polyphone Disambiguation -- Challenges and Insights,"Chan-Jan Hsu, Yi-Cheng Lin, Chia-Chun Lin, Wei-Chih Chen, Ho Lam Chung, Chen-An Li, Yi-Chang Chen, Chien-Yu Yu, Ming-Ji Lee, Chien-Cheng Chen, Ru-Heng Huang, Hung-yi Lee, Da-Shan Shiu","Computation and Language, Artificial Intelligence","We present BreezyVoice, a Text-to-Speech (TTS) system specifically adapted for Taiwanese Mandarin, highlighting phonetic control abilities to address the unique challenges of polyphone disambiguation in the language. Building upon CosyVoice, we incorporate aS3superscript𝑆3S^{3}italic_S start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPTtokenizer, a large language model (LLM), an optimal-transport conditional flow matching model (OT-CFM), and a grapheme to phoneme prediction model, to generate realistic speech that closely mimics human utterances. Our evaluation demonstrates BreezyVoice’s superior performance in both general and code-switching contexts, highlighting its robustness and effectiveness in generating high-fidelity speech. Additionally, we address the challenges of generalizability in modeling long-tail speakers and polyphone disambiguation. Our approach significantly enhances performance and offers valuable insights into the workings of neural codec TTS systems."
474,679d459debd8ffd557a2b047,cs.CL,https://arxiv.org/pdf/2501.17785,Reasoning Over the Glyphs: Evaluation of LLM's Decipherment of Rare Scripts,"Yu-Fei Shih, Zheng-Lin Lin, Shu-Kai Hsieh","Computation and Language, Machine Learning","We explore the capabilities of LVLMs and LLMs in deciphering rare scripts not encoded in Unicode. We introduce a novel approach to construct a multimodal dataset of linguistic puzzles involving such scripts, utilizing a tokenization method for language glyphs. Our methods include the Picture Method for LVLMs and the Description Method for LLMs, enabling these models to tackle these challenges.
We conduct experiments using prominent models, GPT-4o, Gemini, and Claude 3.5 Sonnet, on linguistic puzzles. Our findings reveal the strengths and limitations of current AI methods in linguistic decipherment, highlighting the impact of Unicode encoding on model performance and the challenges of modeling visual language tokens through descriptions. Our study advances understanding of AI’s potential in linguistic decipherment and underscores the need for further research."
475,679d459debd8ffd557a2b048,cs.CL,https://arxiv.org/pdf/2501.17771,2SSP: A Two-Stage Framework for Structured Pruning of LLMs,"Fabrizio Sandri, Elia Cunegatti, Giovanni Iacca","Computation and Language, Artificial Intelligence, Machine Learning","We propose a novel Two-Stage framework for Structured Pruning (2SSP) for pruning Large Language Models (LLMs), which combines two different strategies of pruning, namely Width and Depth Pruning. The first stage (Width Pruning) removes entire neurons, hence their corresponding rows and columns, aiming to preserve the connectivity among the pruned structures in the intermediate state of the Feed-Forward Networks in each Transformer block. This is done based on an importance score measuring the impact of each neuron over the output magnitude. The second stage (Depth Pruning), instead, removes entire Attention submodules. This is done by applying an iterative process that removes the Attention submodules with the minimum impact on a given metric of interest (in our case, perplexity). We also propose a novel mechanism to balance the sparsity rate of the two stages w.r.t. to the desired global sparsity. We test2SSPon four LLM families and three sparsity rates (25%, 37.5%, and 50%), measuring the resulting perplexity over three language modeling datasets as well as the performance over six downstream tasks. Our method consistently outperforms five state-of-the-art competitors over three language modeling and six downstream tasks, with an up to two-order-of-magnitude gain in terms of pruning time.
The code is available at available athttps://github.com/FabrizioSandri/2SSP."
476,679d459debd8ffd557a2b049,cs.CL,https://arxiv.org/pdf/2501.17767,Hybrid Graphs for Table-and-Text based Question Answering using LLMs,"Ankush Agarwal, Ganesh S, Chaitanya Devaguptapu","Computation and Language, Artificial Intelligence",
477,679d459debd8ffd557a2b04a,cs.CL,https://arxiv.org/pdf/2501.17715,RICoTA: Red-teaming of In-the-wild Conversation with Test Attempts,"Eujeong Choi, Younghun Jeong, Soomin Kim, Won Ik Cho",Computation and Language,"User interactions with conversational agents (CAs) evolve in the era of heavily guardrailed large language models (LLMs). As users push beyond programmed boundaries to explore and build relationships with these systems, there is a growing concern regarding the potential for unauthorized access or manipulation, commonly referred to as “jailbreaking.” Moreover, with CAs that possess highly human-like qualities, users show a tendency toward initiating intimate sexual interactions or attempting to tame their chatbots.
To capture and reflect these in-the-wild interactions into chatbot designs, we propose RICoTA, a Korean red teaming dataset that consists of 609 prompts challenging LLMs with in-the-wild user-made dialogues capturing jailbreak attempts. We utilize user-chatbot conversations that were self-posted on a Korean Reddit-like community, containing specific testing and gaming intentions with a social chatbot. With these prompts, we aim to evaluate LLMs’ ability to identify the type of conversation and users’ testing purposes to derive chatbot design implications for mitigating jailbreaking risks. Our dataset will be made publicly available via GitHub.111https://github.com/boychaboy/RICoTA"
478,679d459debd8ffd557a2b04b,cs.CL,https://arxiv.org/pdf/2501.17703,Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate,"Yubo Wang, Xiang Yue, Wenhu Chen",Computation and Language,"Supervised Fine-Tuning (SFT) is commonly used to train language models to imitate annotated responses for given instructions. In this paper, we challenge this paradigm and propose Critique Fine-Tuning (CFT), a strategy where models learn to critique noisy responses rather than simply imitate correct ones. Inspired by human learning processes that emphasize critical thinking,CFTencourages deeper analysis and nuanced understanding—traits often overlooked by standard SFT.
To validate the effectiveness ofCFT, we construct a 50K-sample dataset from WebInstruct, using GPT-4o as the teacher to generate critiques in the form of ([query; noisy response], critique).CFTon this dataset yields a consistent 4–10% improvement over SFT on six math benchmarks with different base models like Qwen2.5, Qwen2.5-Math and DeepSeek-Math. We further expand to MetaMath and NuminaMath datasets and observe similar gains over SFT. Notably, our model Qwen2.5-Math-CFTonly requires 1 hour training on 8xH100 over the 50K examples. It can match or outperform strong competitors like Qwen2.5-Math-Instruct on most benchmarks, which use over 2M samples. Moreover, it can match the performance of SimpleRL, which is a deepseek-r1 replication trained with 140x more compute. Ablation studies show thatCFTis robust to the source of noisy response and teacher critique model. Through these findings, we argue thatCFToffers a more effective alternative to advance the reasoning of language models."
479,679d459debd8ffd557a2b04c,cs.CL,https://arxiv.org/pdf/2501.17654,Exploring Vision Language Models for Multimodal and Multilingual Stance Detection,"Jake Vasilakes, Carolina Scarton, Zhixue Zhao","Computation and Language, Artificial Intelligence","Social media’s global reach amplifies the spread of information, highlighting the need for robust Natural Language Processing tasks, like stance detection, across languages and modalities. Prior research predominantly focuses on text-only inputs, leaving multimodal scenarios, such as those involving both images and text, relatively underexplored. Meanwhile, the prevalence of multimodal posts has increased significantly in recent years. Although state-of-the-art Vision-Language Models (VLMs) show promise, their performance on multimodal and multilingual stance detection tasks remains largely unexamined. This paper evaluates state-of-the-art VLMs on a newly extended dataset covering seven languages and multimodal inputs, investigating their use of visual cues, language-specific performance, and cross-modality interactions. Our results show that VLMs generally rely more on text than images for stance detection and this trend persists across languages. Additionally, VLMs rely significantly more on text contained within the images than other visual content. Regarding multilinguality, the models studied tend to generate consistent predictions across languages whether they are explicitly multilingual or not, although there are outliers that are incongruous with macro F1, language support, and model size.111We will make our code and data publicly available upon publication."
480,679d459debd8ffd557a2b04d,cs.CL,https://arxiv.org/pdf/2501.17643,Tonguescape: Exploring Language Models Understanding of Vowel Articulation,"Haruki Sakajo, Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe","Computation and Language, Artificial Intelligence","Vowels are primarily characterized by tongue position.
Humans have discovered these features of vowel articulation through their own experience and explicit objective observation such as using MRI.
With this knowledge and our experience, we can explain and understand the relationship between tongue positions and vowels, and this knowledge is helpful for language learners to learn pronunciation.
Since language models (LMs) are trained on a large amount of data that includes linguistic and medical fields, our preliminary studies indicate that an LM is able to explain the pronunciation mechanisms of vowels.
However, it is unclear whether multi-modal LMs, such as vision LMs, align textual information with visual information.
One question arises: do LMs associate real tongue positions with vowel articulation?
In this study, we created video and image datasets from the existing real-time MRI dataset and investigated whether LMs can understand vowel articulation based on tongue positions using vision-based information.
Our findings suggest that LMs exhibit potential for understanding vowels and tongue positions when reference examples are provided while they have difficulties without them.
Our code for dataset building is available on GitHub111https://github.com/sj-h4/tonguescape-builder."
481,679d459debd8ffd557a2b04e,cs.CL,https://arxiv.org/pdf/2501.17617,Structured Context Recomposition for Large Language Models Using Probabilistic Layer Realignment,"Jonathan Teel, Jocasta Cumberbatch, Raphael Benington, Quentin Baskerville",Computation and Language,"Extended sequence generation often leads to degradation in contextual consistency due to the inability of conventional self-attention mechanisms to effectively retain long-range dependencies. Existing approaches, including memory compression and retrieval-augmented conditioning, introduce computational trade-offs that either increase inference latency or impose additional storage overhead. Structured Context Recomposition (SCR) introduces a probabilistic layer realignment strategy that dynamically adjusts learned representations within transformer layers, ensuring that semantically relevant embeddings persist throughout extended transformations. The proposed method enhances coherence retention through a recursive weighting function that redistributes representational emphasis based on inferred contextual relevance rather than relying on fixed token-level attention scores. Empirical results indicate that probabilistic realignment mitigates abrupt topic shifts and logical inconsistencies, particularly in scenarios where sequences exceed standard attention window constraints. Sequence-level entropy analysis further reveals that SCR moderates representational variability without introducing excessive output regularization, allowing models to sustain generative diversity while preserving contextual alignment. Attention head deviation measurements confirm that hierarchical reweighting contributes to smoother token dependency transitions across transformer layers, reinforcing the stability of multi-turn interactions and document-level reasoning. Computational resource assessments show that while SCR incurs a moderate increase in processing time, memory overhead remains within feasible limits, making it suitable for practical deployment in autoregressive generative applications."
482,679d459debd8ffd557a2b04f,cs.CL,https://arxiv.org/pdf/2501.17615,Cross-lingual Embedding Clustering for Hierarchical Softmax in Low-Resource Multilingual Speech Recognition,"Zhengdong Yang, Qianying Liu, Sheng Li, Fei Cheng, Chenhui Chu","Computation and Language, Sound, Audio and Speech Processing","This document describes the most common article elements and how to use the IEEEtran class withLaTeXto produce files that are suitable for submission to the Institute of Electrical and Electronics Engineers (IEEE). IEEEtran can produce conference, journal and technical note (correspondence) papers with a suitable choice of class options."
483,679d459debd8ffd557a2b050,cs.CL,https://arxiv.org/pdf/2501.17598,Semantic Consistency Regularization with Large Language Models for Semi-supervised Sentiment Analysis,"Kunrong Li, Xinyu Liu, Zhen Chen","Computation and Language, Machine Learning","Accurate sentiment analysis of texts is crucial for a variety of applications, such as understanding customer feedback, monitoring market trends, and detecting public sentiment. However, manually annotating large sentiment corpora for supervised learning is labor-intensive and time-consuming. Therefore, it is essential and effective to develop a semi-supervised method for the sentiment analysis task. Although some methods have been proposed for semi-supervised text classification, they rely on the intrinsic information within the unlabeled data and the learning capability of the NLP model, which lack generalization ability to the sentiment analysis scenario and may prone to overfit. Inspired by the ability of pretrained Large Language Models (LLMs) in following instructions and generating coherent text, we propose a Semantic Consistency Regularization with Large Language Models (SCR) framework for semi-supervised sentiment analysis. We introduce two prompting strategies to semantically enhance unlabeled text using LLMs. The first is Entity-based Enhancement (SCR-EE), which involves extracting entities and numerical information, and querying the LLM to reconstruct the textual information. The second is Concept-based Enhancement (SCR-CE), which directly queries the LLM with the original sentence for semantic reconstruction. Subsequently, the LLM-augmented data is utilized for a consistency loss with confidence thresholding, which preserves high-quality agreement samples to provide additional supervision signals during training. Furthermore, to fully utilize the uncertain unlabeled data samples, we propose a class re-assembling strategy inspired by the class space shrinking theorem. Experiments show our method achieves remarkable performance over prior semi-supervised methods. For example, with 200 labeled data per class, SCR-EE achieves a remarkable performance of 76.13% accuracy, outperforming previous method FixMatch by 3.42%."
484,679d459debd8ffd557a2b051,cs.CL,https://arxiv.org/pdf/2501.17581,"CSEval: Towards Automated, Multi-Dimensional, and Reference-Free Counterspeech Evaluation using Auto-Calibrated LLMs","Amey Hengle, Aswini Kumar, Anil Bandhakavi, Tanmoy Chakraborty","Computation and Language, Artificial Intelligence, Computers and Society, Social and Information Networks","Counterspeech has been popularly shown as an effective approach to counter online hate speech, leading to increasing research interest in automated counterspeech generation using language models. However, this field lacks standardised evaluation protocols and robust automated evaluation metrics that align with human judgement. Current automatic evaluation methods, primarily based on similarity metrics, do not effectively capture the complex and independent attributes of counterspeech quality, such as contextual relevance, aggressiveness, or argumentative coherence. This has led to an increased dependency on labor-intensive human evaluations to assess automated counter-speech generation methods. To address these challenges, we introduceCSEval, a novel dataset and framework for evaluating counterspeech quality across four dimensions:contextual-relevance,aggressiveness,argument-coherence, andsuitableness. Furthermore, we proposeAuto-Calibrated COT for Counterspeech Evaluation(ACE), a prompt-based method with auto-calibrated chain-of-thoughts (CoT) for scoring counterspeech using large language models. Our experiments show thatACEoutperforms traditional metrics like ROUGE, METEOR, and BertScore in correlating with human judgement, indicating a significant advancement in automated counterspeech evaluation.111Warning: The content in this paper may be upsetting or offensive."
485,679d459debd8ffd557a2b052,cs.CL,https://arxiv.org/pdf/2501.17569,A linguistically-motivated evaluation methodology for unraveling model's abilities in reading comprehension tasks,"Elie Antoine, Frédéric Béchet, Géraldine Damnati, Philippe Langlais",Computation and Language,
486,679d459debd8ffd557a2b053,cs.CL,https://arxiv.org/pdf/2501.17549,Query-Aware Learnable Graph Pooling Tokens as Prompt for Large Language Models,"Wooyoung Kim, Byungyoon Park, Wooju Kim",Computation and Language,"Graph-structured data plays a vital role in numerous domains, such as social networks, citation networks, commonsense reasoning graphs and knowledge graphs. While graph neural networks have been employed for graph processing, recent advancements have explored integrating large language models for graph-based tasks. In this paper, we propose a novel approach named Learnable Graph Pooling Token (LGPT), which addresses the limitations of the scalability issues in node-level projection and information loss in graph-level projection. LGPT enables flexible and efficient graph representation by introducing learnable parameters that act as tokens in large language models, balancing fine-grained and global graph information. Additionally, we investigate an Early Query Fusion technique, which fuses query context before constructing the graph representation, leading to more effective graph embeddings. Our method achieves a 4.13% performance improvement on the GraphQA benchmark without training the large language model, demonstrating significant gains in handling complex textual-attributed graph data."
487,679d459debd8ffd557a2b054,cs.CL,https://arxiv.org/pdf/2501.17486,DINT Transformer,"Yueyang Cang, Yuhang Liu, Xiaoteng Zhang, Erlu Zhao, Li Shi","Computation and Language, Artificial Intelligence, Machine Learning","DIFF Transformer addresses the issue of irrelevant context interference by introducing a differential attention mechanism that enhances the robustness of local attention. However, it has two critical limitations: the lack of global context modeling, which is essential for identifying globally significant tokens, and numerical instability due to the absence of strict row normalization in the attention matrix. To overcome these challenges, we propose DINT Transformer, which extends DIFF Transformer by incorporating a differential-integral mechanism. By computing global importance scores and integrating them into the attention matrix, DINT Transformer improves its ability to capture global dependencies. Moreover, the unified parameter design enforces row-normalized attention matrices, improving numerical stability. Experimental results demonstrate that DINT Transformer excels in accuracy and robustness across various practical applications, such as long-context language modeling and key information retrieval. These results position DINT Transformer as a highly effective and promising architecture."
488,679d459debd8ffd557a2b055,cs.CL,https://arxiv.org/pdf/2501.17449,Cross-Language Approach for Quranic QA,"Islam Oshallah, Mohamed Basem, Ali Hamdi, Ammar Mohammed","Computation and Language, Information Retrieval","Question answering systems face critical limitations in languages with limited resources and scarce data, making the development of robust models especially challenging. The Quranic QA system holds significant importance as it facilitates a deeper understanding of the Quran, a Holy text for over a billion people worldwide. However, these systems face unique challenges, including the linguistic disparity between questions written in Modern Standard Arabic and answers found in Quranic verses written in Classical Arabic, and the small size of existing datasets, which further restricts model performance. To address these challenges, we adopt a cross-language approach by (1) Dataset Augmentation: expanding and enriching the dataset through machine translation to convert Arabic questions into English, paraphrasing questions to create linguistic diversity, and retrieving answers from an English translation of the Quran to align with multilingual training requirements; and (2) Language Model Fine-Tuning: utilizing pre-trained models such as BERT-Medium, RoBERTa-Base, DeBERTa-v3-Base, ELECTRA-Large, Flan-T5, Bloom, and Falcon to address the specific requirements of Quranic QA. Experimental results demonstrate that this cross-language approach significantly improves model performance, with RoBERTa-Base achieving the highest MAP@10 (0.34) and MRR (0.52), while DeBERTa-v3-Base excels in Recall@10 (0.50) and Precision@10 (0.24). These findings underscore the effectiveness of cross-language strategies in overcoming linguistic barriers and advancing Quranic QA systems."
489,679d459debd8ffd557a2b056,cs.CL,https://arxiv.org/pdf/2501.17420,Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases in Language Models,"Yuxuan Li, Hirokazu Shirado, Sauvik Das","Computation and Language, Artificial Intelligence, Human-Computer Interaction","While advances in fairness and alignment have helped mitigate overt biases exhibited by large language models (LLMs) when explicitly prompted, we hypothesize that these models may still exhibit implicit biases when simulating human behavior. To test this hypothesis, we propose a technique to systematically uncover such biases across a broad range of sociodemographic categories by assessing decision-making disparities among agents with LLM-generated, sociodemographically-informed personas. Using our technique, we tested six LLMs across three sociodemographic groups and four decision-making scenarios. Our results show that state-of-the-art LLMs exhibit significant sociodemographic disparities in nearly all simulations, with more advanced models exhibiting greater implicit biases despite reducing explicit biases. Furthermore, when comparing our findings to real-world disparities reported in empirical studies, we find that the biases we uncovered are directionally aligned but markedly amplified. This directional alignment highlights the utility of our technique in uncovering systematic biases in LLMs rather than random variations; moreover, the presence and amplification of implicit biases emphasizes the need for novel strategies to address these biases."
490,679d459debd8ffd557a2b057,cs.CL,https://arxiv.org/pdf/2501.17399,MultiChallenge: A Realistic Multi-Turn Conversation Evaluation Benchmark Challenging to Frontier LLMs,"Ved Sirdeshmukh, Kaustubh Deshpande, Johannes Mols, Lifeng Jin, Ed-Yeremai Cardona, Dean Lee, Jeremy Kritz, Willow Primack, Summer Yue, Chen Xing","Computation and Language, Artificial Intelligence","We presentMultiChallenge, a pioneering benchmark evaluating large language models (LLMs) on conducting multi-turn conversations with human users, a crucial yet underexamined capability for their applications.MultiChallengeidentifies four categories of challenges in multi-turn conversations that are not only common and realistic among current human-LLM interactions, but are also challenging to all current frontier LLMs.
All 4 challenges require accurate instruction-following, context allocation, and in-context reasoning at the same time.
We also develop LLM as judge with instance-level rubrics to facilitate an automatic evaluation method with fair agreement with experienced human raters.
Despite achieveing near perfect scores on existing multi-turn evaluation benchmarks, all frontier models have less than 50% accuracy onMultiChallenge, with the top-performing Claude 3.5 Sonnet (June 2024) achieving just a 41.4% average accuracy."
491,679d459debd8ffd557a2b058,cs.CL,https://arxiv.org/pdf/2501.17397,Leveraging In-Context Learning and Retrieval-Augmented Generation for Automatic Question Generation in Educational Domains,"Subhankar Maity, Aniket Deroy, Sudeshna Sarkar",Computation and Language,"Question generation in education is a time-consuming and cognitively demanding task, as it requires creating questions that are both contextually relevant and pedagogically sound. Current automated question generation methods often generate questions that are out of context. In this work, we explore advanced techniques for automated question generation in educational contexts, focusing on In-Context Learning (ICL), Retrieval-Augmented Generation (RAG), and a novel Hybrid Model that merges both methods. We implement GPT-4 for ICL using few-shot examples and BART with a retrieval module for RAG. The Hybrid Model combines RAG and ICL to address these issues and improve question quality. Evaluation is conducted using automated metrics, followed by human evaluation metrics. Our results show that both the ICL approach and the Hybrid Model consistently outperform other methods, including baseline models, by generating more contextually accurate and relevant questions."
492,679d459debd8ffd557a2b059,cs.CL,https://arxiv.org/pdf/2501.17386,Context-Aware Semantic Recomposition Mechanism for Large Language Models,"Richard Katrix, Quentin Carroway, Rowan Hawkesbury, Matthias Heathfield","Computation and Language, Artificial Intelligence","Context-aware processing mechanisms have increasingly become a critical area of exploration for improving the semantic and contextual capabilities of language generation models. The Context-Aware Semantic Recomposition Mechanism (CASRM) was introduced as a novel framework designed to address limitations in coherence, contextual adaptability, and error propagation in large-scale text generation tasks. Through the integration of dynamically generated context vectors and attention modulation layers, CASRM enhances the alignment between token-level representations and broader contextual dependencies. Experimental evaluations demonstrated significant improvements in semantic coherence across multiple domains, including technical, conversational, and narrative text. The ability to adapt to unseen domains and ambiguous inputs was evaluated using a diverse set of test scenarios, highlighting the robustness of the proposed mechanism. A detailed computational analysis revealed that while CASRM introduces additional processing overhead, the gains in linguistic precision and contextual relevance outweigh the marginal increase in complexity. The framework also successfully mitigates error propagation in sequential tasks, improving performance in dialogue continuation and multi-step text synthesis. Additional investigations into token-level attention distribution emphasized the dynamic focus shifts enabled through context-aware enhancements. The findings suggest that CASRM offers a scalable and flexible solution for integrating contextual intelligence into existing language model architectures."
493,679d459debd8ffd557a2b05a,cs.CL,https://arxiv.org/pdf/2501.17348,Better Slow than Sorry: Introducing Positive Friction for Reliable Dialogue Systems,"Mert İnan, Anthony Sicilia, Suvodip Dey, Vardhan Dongre, Tejas Srinivasan, Jesse Thomason, Gökhan Tür, Dilek Hakkani-Tür, Malihe Alikhani","Computation and Language, Human-Computer Interaction","While theories of discourse and cognitive science have long recognized the value of unhurried pacing, recent dialogue research tends to minimize friction in conversational systems. Yet, frictionless dialogue risks fostering uncritical reliance on AI outputs, which can obscure implicit assumptions and lead to unintended consequences. To meet this challenge, we propose integratingpositive frictioninto conversational AI, which promotes user reflection on goals, critical thinking on system response, and subsequent re-conditioning of AI systems. We hypothesize systems can improve goal alignment, modeling of user mental states, and task success by deliberately slowing down conversations in strategic moments to ask questions, reveal assumptions, or pause. We present an ontology of positive friction and collect expert human annotations on multi-domain and embodied goal-oriented corpora. Experiments on these corpora, along with simulated interactions using state-of-the-art systems, suggest incorporating friction not only fosters accountable decision-making, but also enhances machine understanding of user beliefs and goals, and increases task success rates.111Code, data, and guidelines will be made public."
494,679d459debd8ffd557a2b05b,cs.CL,https://arxiv.org/pdf/2501.17338,Inferring from Logits: Exploring Best Practices for Decoding-Free Generative Candidate Selection,"Mingyu Derek Ma, Yanna Ding, Zijie Huang, Jianxi Gao, Yizhou Sun, Wei Wang","Computation and Language, Artificial Intelligence, Machine Learning",
495,679d459debd8ffd557a2b05c,cs.CL,https://arxiv.org/pdf/2501.17326,Memorize and Rank: Elevating Large Language Models for Clinical Diagnosis Prediction,"Mingyu Derek Ma, Xiaoxuan Wang, Yijia Xiao, Anthony Cuturrufo, Vijay S Nori, Eran Halperin, Wei Wang","Computation and Language, Artificial Intelligence, Machine Learning","Clinical diagnosis prediction models, when provided with a patient’s medical history, aim to detect potential diseases early, facilitating timely intervention and improving prognostic outcomes. However, the inherent scarcity of patient data and large disease candidate space often pose challenges in developing satisfactory models for this intricate task. The exploration of leveraging Large Language Models (LLMs) for encapsulating clinical decision processes has been limited.
We introduceMera, a clinical diagnosis prediction model that bridges pertaining natural language knowledge with medical practice. We apply hierarchical contrastive learning on a disease candidate ranking list to alleviate the large decision space issue. With concept memorization through fine-tuning, we bridge the natural language clinical knowledge with medical codes.
Experimental results on MIMIC-III and IV datasets show thatMeraachieves the state-of-the-art diagnosis prediction performance and dramatically elevates the diagnosis prediction capabilities of generative LMs."
496,679d459debd8ffd557a2b05d,cs.CL,https://arxiv.org/pdf/2501.17295,Mitigating Hallucinated Translations in Large Language Models with Hallucination-focused Preference Optimization,"Zilu Tang, Rajen Chatterjee, Sarthak Garg","Computation and Language, Artificial Intelligence, Machine Learning",
497,679d459debd8ffd557a2b05e,cs.CL,https://arxiv.org/pdf/2501.17273,Tailored Truths: Optimizing LLM Persuasion with Personalization and Fabricated Statistics,"Jasper Timm, Chetan Talele, Jacob Haimes",Computation and Language,"Large Language Models (LLMs) are becoming increasingly persuasive, demonstrating the ability to personalize arguments in conversation with humans by leveraging their personal data. This may have serious impacts on the scale and effectiveness of disinformation campaigns. We studied the persuasiveness of LLMs in a debate setting by having humans (n=33𝑛33n=33italic_n = 33) engage with LLM-generated arguments intended to change the human’s opinion. We quantified the LLM’s effect by measuring human agreement with the debate’s hypothesis pre- and post-debate and analyzing both the magnitude of opinion change, as well as the likelihood of an update in the LLM’s direction. We compare persuasiveness across established persuasion strategies, including personalized arguments informed by user demographics and personality, appeal to fabricated statistics, and a mixed strategy utilizing both personalized arguments and fabricated statistics. We found that static arguments generated by humans and GPT-4o-mini have comparable persuasive power. However, the LLM outperformed static human-written arguments when leveraging the mixed strategy in an interactive debate setting. This approach had a51%chance of persuading participants to modify their initial position, compared to32%for the static human-written arguments. Our results highlight the concerning potential for LLMs to enable inexpensive and persuasive large-scale disinformation campaigns."
498,679d459debd8ffd557a2b05f,cs.CL,https://arxiv.org/pdf/2501.17270,Comprehensive Evaluation for a Large Scale Knowledge Graph Question Answering Service,"Saloni Potdar, Daniel Lee, Omar Attia, Varun Embar, De Meng, Ramesh Balaji, Chloe Seivwright, Eric Choi, Mina H. Farid, Yiwen Sun, Yunyao Li","Computation and Language, Databases","Question answering systems for knowledge graph (KGQA), answer factoid questions based on the data in the knowledge graph. KGQA systems are complex because the system has to understand the relations and entities in the knowledge-seeking natural language queries and map them to structured queries against the KG to answer them. In this paper, we introduce Chronos, a comprehensive evaluation framework for KGQA at industry scale. It is designed to evaluate such a multi-component system comprehensively, focusing on (1) end-to-end and component-level metrics, (2) scalable to diverse datasets & (3) a scalable approach to measure the performance of the system prior to release. In this paper, we discuss the unique challenges associated with evaluating KGQA systems at industry scale, review the design of Chronos, and how it addresses these challenges. We will demonstrates how it provides a base for data-driven decisions and discuss the challenges of using it to measure and improve a real-world KGQA system."
499,679d459debd8ffd557a2b060,cs.CL,https://arxiv.org/pdf/2501.17265,Giving the Old a Fresh Spin: Quality Estimation-Assisted Constrained Decoding for Automatic Post-Editing,"Sourabh Deoghare, Diptesh Kanojia, Pushpak Bhattacharyya",Computation and Language,"Automatic Post-Editing (APE) systems often struggle with over-correction, where unnecessary modifications are made to a translation, diverging from the principle of minimal editing. In this paper, we propose a novel technique to mitigate over-correction by incorporating word-level Quality Estimation (QE) information during the decoding process. This method is architecture-agnostic, making it adaptable to any APE system, regardless of the underlying model or training approach. Our experiments on English-German, English-Hindi, and English-Marathi language pairs show the proposed approach yields significant improvements over their corresponding baseline APE systems, with TER gains of0.650.650.650.65,1.861.861.861.86, and1.441.441.441.44points, respectively. These results underscore the complementary relationship between QE and APE tasks and highlight the effectiveness of integrating QE information to reduce over-correction in APE systems."
500,679d459debd8ffd557a2b061,cs.CL,https://arxiv.org/pdf/2501.17261,NUS-Emo at SemEval-2024 Task 3: Instruction-Tuning LLM for Multimodal Emotion-Cause Analysis in Conversations,"Meng Luo, Han Zhang, Shengqiong Wu, Bobo Li, Hong Han, Hao Fei",Computation and Language,"This paper describes the architecture of our system developed for Task 3 of SemEval-2024: Multimodal Emotion-Cause Analysis in Conversations.
Our project targets the challenges of subtask 2, dedicated to Multimodal Emotion-Cause Pair Extraction with Emotion Category (MECPE-Cat), and constructs a dual-component system tailored to the unique challenges of this task.
We divide the task into two subtasks: emotion recognition in conversation (ERC) and emotion-cause pair extraction (ECPE).
To address these subtasks, we capitalize on the abilities of Large Language Models (LLMs), which have consistently demonstrated state-of-the-art performance across various natural language processing tasks and domains.
Most importantly, we design an approach of emotion-cause-aware instruction-tuning for LLMs, to enhance the perception of the emotions with their corresponding causal rationales.
Our method enables us to adeptly navigate the complexities of MECPE-Cat, achieving a weighted average 34.71% F1 score of the task,
and securing the2ndrank on the leaderboard.111https://nustm.github.io/SemEval-2024_ECAC/The code and metadata to reproduce our experiments are all made publicly available.222https://github.com/zhanghanXD/NUS-Emo-at-SemEval-2024-Task3"
501,679d459debd8ffd557a2b062,cs.CL,https://arxiv.org/pdf/2501.17200,Improving LLM Leaderboards with Psychometrical Methodology,Denis Federiakin,"Computation and Language, Artificial Intelligence, Applications",
502,679d459debd8ffd557a2b063,cs.CL,https://arxiv.org/pdf/2501.17195,Atla Selene Mini: A General Purpose Evaluation Model,"Andrei Alexandru, Antonia Calvi, Henry Broomfield, Jackson Golden, Kyle Dai, Mathias Leys, Maurice Burger, Max Bartolo, Roman Engeler, Sashank Pisupati, Toby Drane, Young Sun Park","Computation and Language, Artificial Intelligence","We introduce Atla Selene Mini, a state-of-the-art small language model-as-a-judge (SLMJ). Selene Mini is a general-purpose evaluator that outperforms the best SLMJs and GPT-4o-mini on overall performance across 11 out-of-distribution benchmarks, spanning absolute scoring, classification, and pairwise preference tasks. It is the highest-scoring 8B generative model on RewardBench, surpassing strong baselines like GPT-4o and specialized judges. To achieve this, we develop a principled data curation strategy that augments public datasets with synthetically generated critiques and ensures high quality through filtering and dataset ablations. We train our model on a combined direct preference optimization (DPO) and supervised fine-tuning (SFT) loss, and produce a highly promptable evaluator that excels in real-world scenarios. Selene Mini shows dramatically improved zero-shot agreement with human expert evaluations on financial and medical industry datasets. It is also robust to variations in prompt format. Preliminary results indicate that Selene Mini is the top-ranking evaluator in a live, community-driven Judge Arena111https://huggingface.co/blog/arena-atla. We release the model weights on HuggingFace (https://hf.co/AtlaAI/Selene-1-Mini-Llama-3.1-8B) and Ollama222https://ollama.com/atla/selene-minito encourage widespread community adoption."
503,679d459debd8ffd557a2b064,cs.CL,https://arxiv.org/pdf/2501.17194,AI-assisted German Employment Contract Review: A Benchmark Dataset,"Oliver Wardas, Florian Matthes",Computation and Language,
504,679d459debd8ffd557a2b065,cs.CL,https://arxiv.org/pdf/2501.17191,Aspect-Aware Decomposition for Opinion Summarization,"Miao Li, Jey Han Lau, Eduard Hovy, Mirella Lapata","Computation and Language, Information Retrieval",
505,679d459debd8ffd557a2b066,cs.CL,https://arxiv.org/pdf/2501.17190,A Comprehensive Study on Fine-Tuning Large Language Models for Medical Question Answering Using Classification Models and Comparative Analysis,"Aysegul Ucar, Soumik Nayak, Anunak Roy, Burak Taşcı, Gülay Taşcı",Computation and Language,
506,679d459debd8ffd557a2b067,cs.CL,https://arxiv.org/pdf/2501.17187,Visualizing Uncertainty in Translation Tasks: An Evaluation of LLM Performance and Confidence Metrics,"Jin Hyun Park, Utsawb Laminchhane, Umer Farooq, Uma Sivakumar, Arpan Kumar","Computation and Language, Artificial Intelligence, Machine Learning","Large language models (LLMs) are increasingly utilized for machine translation, yet their predictions often exhibit uncertainties that hinder interpretability and user trust. Effectively visualizing these uncertainties can enhance the usability of LLM outputs, particularly in contexts where translation accuracy is critical.
This paper addresses two primary objectives: (1) providing users with token-level insights into model confidence and (2) developing a web-based visualization tool to quantify and represent translation uncertainties. To achieve these goals, we utilized the T5 model with the WMT19 dataset for translation tasks and evaluated translation quality using established metrics such as BLEU, METEOR, and ROUGE.
We introduced three novel uncertainty quantification (UQ) metrics: (1) the geometric mean of token probabilities, (2) the arithmetic mean of token probabilities, and (3) the arithmetic mean of the kurtosis of token distributions. These metrics provide a simple yet effective framework for evaluating translation performance. Our analysis revealed a linear relationship between the traditional evaluation metrics and our UQ metrics, demonstrating the validity of our approach.
Additionally, we developed an interactive web-based visualization that uses a color gradient to represent token confidence. This tool offers users a clear and intuitive understanding of translation quality while providing valuable insights into model performance.
Overall, we show that our UQ metrics and visualization are both robust and interpretable, offering practical tools for evaluating and accessing machine translation systems."
507,679d459debd8ffd557a2b068,cs.CL,https://arxiv.org/pdf/2501.17183,LLM Evaluation Based on Aerospace Manufacturing Expertise: Automated Generation and Multi-Model Question Answering,"Beiming Liu, Zhizhuo Cui, Siteng Hu, Xiaohua Li, Haifeng Lin, Zhengxin Zhang","Computation and Language, Artificial Intelligence","Aerospace manufacturing demands exceptionally high precision in technical parameters. The remarkable performance of Large Language Models (LLMs), such as GPT-4 and QWen, in Natural Language Processing has sparked industry interest in their application to tasks including process design, material selection, and tool information retrieval. However, LLMs are prone to generating ”hallucinations” in specialized domains, producing inaccurate or false information that poses significant risks to the quality of aerospace products and flight safety. This paper introduces a set of evaluation metrics tailored for LLMs in aerospace manufacturing, aiming to assess their accuracy by analyzing their performance in answering questions grounded in professional knowledge. Firstly, key information is extracted through in-depth textual analysis of classic aerospace manufacturing textbooks and guidelines. Subsequently, utilizing LLM generation techniques, we meticulously construct multiple-choice questions with multiple correct answers of varying difficulty. Following this, different LLM models are employed to answer these questions, and their accuracy is recorded. Experimental results demonstrate that the capabilities of LLMs in aerospace professional knowledge are in urgent need of improvement. This study provides a theoretical foundation and practical guidance for the application of LLMs in aerospace manufacturing, addressing a critical gap in the field."
508,679d459debd8ffd557a2b069,cs.CL,https://arxiv.org/pdf/2501.17182,Dialogue Systems for Emotional Support via Value Reinforcement,"Juhee Kim, Chunghu Mok, Jisun Lee, Hyang Sook Kim, Yohan Jo","Computation and Language, Artificial Intelligence, Computers and Society, Human-Computer Interaction","Emotional support dialogue systems aim to reduce help-seekers’ distress and help them overcome challenges.
While human values—core beliefs that shape an individual’s priorities—are increasingly emphasized in contemporary psychological therapy for their role in fostering internal transformation and long-term emotional well-being, their integration into emotional support systems remains underexplored.
To bridge this gap, we present a value-driven method for training emotional support dialogue systems designed to reinforce positivevaluesin seekers. Our model learns to identify which values to reinforce at each turn and how to do so, by leveraging online support conversations from Reddit.
The model demonstrated superior performance in emotional support capabilities, outperforming various baselines. Notably, it more effectively explored and elicited values from seekers. Expert assessments by therapists highlighted two key strengths of our model: its ability to validate users’ challenges and its effectiveness in emphasizing positive aspects of their situations—both crucial elements of value reinforcement.
Our work validates the effectiveness of value reinforcement for emotional support systems and establishes a foundation for future research.111This paper is currently under review. All source code and data will be made publicly available upon its publication."
509,679d459debd8ffd557a2b06a,cs.CL,https://arxiv.org/pdf/2501.17178,Tuning LLM Judges Hyperparameters,"David Salinas, Omar Swelam, Frank Hutter","Computation and Language, Artificial Intelligence, Machine Learning","Evaluating Large Language Models (LLMs) often requires costly human annotations. To address this, LLM-based judges have been proposed, which compare the outputs of two LLMs enabling the ranking of models without human intervention. While several approaches have been proposed, many confounding factors are present between different papers. For instance the model, the prompt and other hyperparameters are typically changed at the same time making apple-to-apple comparisons challenging.
In this paper, we propose to systematically analyze and tune hyperparameter of LLM judges. To alleviate the high cost of evaluating a judge, we propose to leverage multi-objective multi-fidelity which allows to find judges that trades accuracy for cost and also reduce significantly the cost of the search. Our method identifies judges that not only outperform existing benchmarks in accuracy and cost-efficiency but also utilize open-weight models, ensuring greater accessibility and reproducibility."
510,679d459debd8ffd557a2b06b,cs.CL,https://arxiv.org/pdf/2501.17175,Document-Level Sentiment Analysis of Urdu Text Using Deep Learning Techniques,"Ammarah Irum, M. Ali Tahir","Computation and Language, Artificial Intelligence, Information Retrieval","Document level Urdu Sentiment Analysis (SA) is a challenging Natural Language Processing (NLP) task as it deals with large documents in a resource-poor language. In large documents, there are ample amounts of words that exhibit different viewpoints. Deep learning (DL) models comprise of complex neural network architectures that have the ability to learn diverse features of the data to classify various sentiments. Besides audio, image and video classification; DL algorithms are now extensively used in text-based classification problems. To explore the powerful DL techniques for Urdu SA, we have applied five different DL architectures namely, Bidirectional Long Short Term Memory (BiLSTM), Convolutional Neural Network (CNN), Convolutional Neural Network with Bidirectional Long Short Term Memory (CNN-BiLSTM), Bidirectional Encoder Representation from Transformer (BERT). In this paper, we have proposed a DL hybrid model that integrates BiLSTM with Single Layer Multi Filter Convolutional Neural Network (BiLSTM-SLMFCNN). The proposed and baseline techniques are applied on Urdu Customer Support data set and IMDB Urdu movie review data set by using pretrained Urdu word embeddings that are suitable for (SA) at the document level. Results of these techniques are evaluated and our proposed model outperforms all other DL techniques for Urdu SA. BiLSTM-SLMFCNN outperformed the baseline DL models and achieved 83%, 79%, 83% and 94% accuracy on small, medium and large sized IMDB Urdu movie review data set and Urdu Customer Support data set respectively."
511,679d459debd8ffd557a2b06c,cs.CL,https://arxiv.org/pdf/2501.17762,Improving Privacy Benefits of Redaction,"Vaibhav Gusain, Douglas Leith","Cryptography and Security, Computation and Language, Machine Learning",We propose a novel redaction methodology that can be used to sanitize natural text data. Our new technique provides better privacy benefits than other state of the art techniques while maintaining lower redaction levels.
512,679d459debd8ffd557a2b06d,cs.CL,https://arxiv.org/pdf/2501.17725,Using Code Generation to Solve Open Instances of Combinatorial Design Problems,Christopher D. Rosin,"Artificial Intelligence, Computation and Language, Discrete Mathematics, Combinatorics","TheHandbook of Combinatorial Designscatalogs many types of combinatorial designs, together with lists of open instances for which existence has not yet been determined.
We develop a constructive protocolCPro1, which uses Large Language Models (LLMs) to generate code that constructs combinatorial designs and resolves some of these open instances. The protocol starts from a definition of a particular type of design, and averifierthat reliably confirms whether a proposed design is valid. The LLM selects strategies and implements them in code, and scaffolding provides automated hyperparameter tuning and execution feedback using the verifier. Most generated code fails, but by generating many candidates, the protocol automates exploration of a variety of standard methods (e.g. simulated annealing, genetic algorithms) and experimentation with variations (e.g. cost functions) to find successful approaches. Testing on 16 different types of designs, CPro1 constructs solutions to open instances for 6 of them: Symmetric and Skew Weighing Matrices, Equidistant Permutation Arrays, Packing Arrays, Balanced Ternary Designs, and Florentine Rectangles."
513,679d459debd8ffd557a2b06e,cs.CL,https://arxiv.org/pdf/2501.17630,Uncertainty Quantification and Decomposition for LLM-based Recommendation,"Wonbin Kweon, Sanghwan Jang, SeongKu Kang, Hwanjo Yu","Information Retrieval, Computation and Language",
514,679d459debd8ffd557a2b06f,cs.CL,https://arxiv.org/pdf/2501.17584,GLLM: Self-Corrective G-Code Generation using Large Language Models with User Feedback,"Mohamed Abdelaal, Samuel Lokadjaja, Gilbert Engert","Software Engineering, Computation and Language, Machine Learning",
515,679d459debd8ffd557a2b070,cs.CL,https://arxiv.org/pdf/2501.17510,LLM Assistance for Pediatric Depression,"Mariia Ignashina, Paulina Bondaronek, Dan Santel, John Pestian, Julia Ive","Machine Learning, Artificial Intelligence, Computation and Language",Introduction:
516,679d459debd8ffd557a2b071,cs.CL,https://arxiv.org/pdf/2501.17479,DFPE: A Diverse Fingerprint Ensemble for Enhancing LLM Performance,"Seffi Cohen, Niv Goldshlager, Nurit Cohen-Inger, Bracha Shapira, Lior Rokach","Machine Learning, Artificial Intelligence, Computation and Language","Large Language Models (LLMs) have shown remarkable capabilities across various natural language processing tasks but often struggle to excel uniformly in diverse or complex domains. We propose a novel ensemble method - Diverse Fingerprint Ensemble (DFPE), which leverages the complementary strengths of multiple LLMs to achieve more robust performance. Our approach involves: (1) clustering models based on response ""fingerprints"" patterns, (2) applying a quantile-based filtering mechanism to remove underperforming models at a per-subject level, and (3) assigning adaptive weights to remaining models based on their subject-wise validation accuracy. In experiments on the Massive Multitask Language Understanding (MMLU) benchmark, DFPE outperforms the best single model by 3% overall accuracy and 5% in discipline-level accuracy. This method increases the robustness and generalization of LLMs and underscores how model selection, diversity preservation, and performance-driven weighting can effectively address challenging, multi-faceted language understanding tasks."
517,679d459debd8ffd557a2b072,cs.CL,https://arxiv.org/pdf/2501.17459,Large Language Models for Single-Step and Multi-Step Flight Trajectory Prediction,"Kaiwei Luo, Jiliu Zhou","Artificial Intelligence, Computation and Language","Flight trajectory prediction is a critical time series task in aviation. While deep learning methods have shown significant promise, the application of large language models (LLMs) to this domain remains underexplored. This study pioneers the use of LLMs for flight trajectory prediction by reframing it as a language modeling problem. Specifically, We extract features representing the aircraft’s position and status from ADS-B flight data to construct a prompt-based dataset, where trajectory waypoints are converted into language tokens. The dataset is then employed to fine-tune LLMs, enabling them to learn complex spatiotemporal patterns for accurate predictions. Comprehensive experiments demonstrate that LLMs achieve notable performance improvements in both single-step and multi-step predictions compared to traditional methods, with LLaMA-3.1 model achieving the highest overall accuracy. However, the high inference latency of LLMs poses a challenge for real-time applications, underscoring the need for further research in this promising direction."
518,679d459debd8ffd557a2b073,cs.CL,https://arxiv.org/pdf/2501.17456,A review on the novelty measurements of academic papers,"Yi Zhao, Chengzhi Zhang","Digital Libraries, Computation and Language",
519,679d459debd8ffd557a2b074,cs.CL,https://arxiv.org/pdf/2501.17433,Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation,"Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, Ling Liu","Cryptography and Security, Artificial Intelligence, Computation and Language, Machine Learning","Recent research shows that Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks – models lose their safety alignment ability after fine-tuning on a few harmful samples. For risk mitigation, a guardrail is typically used to filter out harmful samples before fine-tuning. By designing a new red-teaming method, we in this paper show that purely relying on the moderation guardrail for data filtration is not reliable. Our proposed attack method, dubbed Virus, easily bypasses the guardrail moderation by slightly modifying the harmful data. Experimental results show that the harmful data optimized by Virus is not detectable by the guardrail with up to 100% leakage ratio, and can simultaneously achieve superior attack performance. Finally, the key message we want to convey through this paper is that:it is reckless to consider guardrail moderation as a clutch at straws towards harmful fine-tuning attack, as it cannot solve the inherent safety issue of the pre-trained LLMs. Our code is available athttps://github.com/git-disl/Virus."
520,679d459debd8ffd557a2b075,cs.CL,https://arxiv.org/pdf/2501.17330,Attribution analysis of legal language as used by LLM,Richard K. Belew,"Machine Learning, Computation and Language","Three publicly-available LLM specifically designed for legal tasks
have been implemented and shown that classification accuracy can benefit
from training over legal corpora, but why and how? Here we use two
publicly-available legal datasets, a simpler binary classification
task of “overruling” texts, and a more elaborate multiple choice
task identifying “holding” judicial decisions. We report on experiments
contrasting the legal LLM and a generic BERT model for comparison,
against both datasets. We use integrated gradient attribution techniques
to impute “causes” of variation in the models’ perfomance, and
characterize them in terms of the tokenizations each use. We find
that while all models can correctly classify some test examples from
the casehold task, other examples can only be identified by only one,
model, and attribution can be used to highlight the reasons for this.
We find that differential behavior of the models’ tokenizers accounts
for most of the difference and analyze these differences in terms
of the legal language they process. Frequency analysis of tokens generated
by dataset texts, combined with use of known “stop word” lists,
allow identification of tokens that are clear signifiers of legal
topics."
521,679d459debd8ffd557a2b076,cs.CL,https://arxiv.org/pdf/2501.17299,"""Ownership, Not Just Happy Talk"": Co-Designing a Participatory Large Language Model for Journalism","Emily Tseng, Meg Young, Marianne Aubin Le Quéré, Aimee Rinehart, Harini Suresh","Human-Computer Interaction, Computation and Language, Computers and Society","Journalism has emerged as an essential domain for understanding the uses, limitations, and impacts of large language models (LLMs) in the workplace. News organizations face divergent financial incentives: LLMs already permeate newswork processes within financially constrained organizations, even as ongoing legal challenges assert that AI companies violate their copyright. At stake are key questions about what LLMs are created to do, and by whom: How might a journalist-led LLM work, and what can participatory design illuminate about the present-day challenges about adapting “one-size-fits-all” foundation models to a given context of use? In this paper, we undertake a co-design exploration to understand how a participatory approach to LLMs might address opportunities and challenges around AI in journalism. Our 20 interviews with reporters, data journalists, editors, labor organizers, product leads, and executives highlight macro, meso, and micro tensions that designing for this opportunity space must address. From these desiderata, we describe the result of our co-design work: organizational structures and functionality for a journalist-controlled LLM. In closing, we discuss the limitations of commercial foundation models for workplace use, and the methodological implications of applying participatory methods to LLM co-design."
522,679d459debd8ffd557a2b077,cs.CL,https://arxiv.org/pdf/2501.17286,Fine-Tuning Open-Source Large Language Models to Improve Their Performance on Radiation Oncology Tasks: A Feasibility Study to Investigate Their Potential Clinical Applications in Radiation Oncology,"Peilong Wang, Zhengliang Liu, Yiwei Li, Jason Holmes, Peng Shu, Lian Zhang, Xiang Li, Quanzheng Li, Brady S. Laughlin, Diego Santos Toesca, Sujay A. Vora, Samir H. Patel, Terence T. Sio, Tianming Liu, Wei Liu","Medical Physics, Artificial Intelligence, Computation and Language",
523,679d459debd8ffd557a2b078,cs.CL,https://arxiv.org/pdf/2501.17282,From Natural Language to Extensive-Form Game Representations,"Shilong Deng, Yongzhao Wang, Rahul Savani","Artificial Intelligence, Computation and Language, Computer Science and Game Theory, Multiagent Systems","We introduce a framework for translating game descriptions in natural language into game-theoretic extensive-form representations, leveraging Large Language Models (LLMs) and in-context learning.
We find that a naive application of in-context learning struggles on this problem, in particular with imperfect information.
To address this, we introduceGameInterpreter, a two-stage framework with specialized modules to enhance in-context learning, enabling it to divide and conquer the problem effectively.
In the first stage, we tackle the challenge of imperfect information by developing a module that identifies information sets and the corresponding partial tree structure.
With this information, the second stage leverages in-context learning alongside a self-debugging module to produce a complete extensive-form game tree represented using pygambit, the Python API of a recognized game-theoretic analysis tool called Gambit.
Using this python representation enables the automation of tasks such as computing Nash equilibria directly from natural language descriptions.
We evaluate the performance of the full framework, as well as its individual components, using various LLMs on games with different levels of strategic complexity.
Our experimental results show that the framework significantly outperforms baseline approaches in generating accurate extensive-form games, with each module playing a critical role in its success."
524,679d459debd8ffd557a2b079,cs.CL,https://arxiv.org/pdf/2501.17202,Audio Large Language Models Can Be Descriptive Speech Quality Evaluators,"Chen Chen, Yuchen Hu, Siyin Wang, Helin Wang, Zhehuai Chen, Chao Zhang, Chao-Han Huck Yang, Eng Siong Chng","Sound, Computation and Language, Audio and Speech Processing","An ideal multimodal agent should be aware of the quality of its input modalities. Recent advances have enabled large language models (LLMs) to incorporate auditory systems for handling various speech-related tasks. However, most audio LLMs remain unaware of the quality of the speech they process. This limitation arises because speech quality evaluation is typically excluded from multi-task training due to the lack of suitable datasets. To address this, we introduce the first natural language-based speech evaluation corpus, generated from authentic human ratings. In addition to the overall Mean Opinion Score (MOS), this corpus offers detailed analysis across multiple dimensions and identifies causes of quality degradation. It also enables descriptive comparisons between two speech samples (A/B tests) with human-like judgment. Leveraging this corpus, we propose an alignment approach with LLM distillation (ALLD) to guide the audio LLM in extracting relevant information from raw speech and generating meaningful responses. Experimental results demonstrate that ALLD outperforms the previous state-of-the-art regression model in MOS prediction, with a mean square error of 0.17 and an A/B test accuracy of 98.6%. Additionally, the generated responses achieve BLEU scores of 25.8 and 30.2 on two tasks, surpassing the capabilities of task-specific models. This work advances the comprehensive perception of speech signals by audio LLMs, contributing to the development of real-world auditory and sensory intelligent agents."
525,679d459debd8ffd557a2b07a,cs.CL,https://arxiv.org/pdf/2501.17186,Complete Chess Games Enable LLM Become A Chess Master,"Yinqi Zhang, Xintian Han, Haolong Li, Kedi Chen, Shaohui Lin","Artificial Intelligence, Computation and Language, Machine Learning","Large language models (LLM) have shown remarkable abilities in text generation, question answering, language translation, reasoning and many other tasks. It continues to advance rapidly and is becoming increasingly influential in various fields, from technology and business to education and entertainment. Despite LLM’s success in multiple areas, its ability to play abstract games, such as chess, is underexplored. Chess-playing requires the language models to output legal and reasonable moves from textual inputs. Here, we propose the Large language model ChessLLM to play full chess games. We transform the game into a textual format with the best move represented in the Forsyth-Edwards Notation. We show that by simply supervised fine-tuning, our model has achieved a professional-level Elo rating of 1788 in matches against the standard Elo-rated Stockfish when permitted to sample 10 times. We further show that data quality is important. Long-round data supervision enjoys a 350 Elo rating improvement over short-round data."
526,679d459debd8ffd557a2b07b,cs.CL,https://arxiv.org/pdf/2501.17181,An AI-Driven Live Systematic Reviews in the Brain-Heart Interconnectome: Minimizing Research Waste and Advancing Evidence Synthesis,"Arya Rahgozar, Pouria Mortezaagha, Jodi Edwards, Douglas Manuel, Jessie McGowen, Merrick Zwarenstein, Dean Fergusson, Andrea Tricco, Kelly Cobey, Margaret Sampson, Malcolm King, Dawn Richards, Alexandra Bodnaruc, David Moher","Artificial Intelligence, Computation and Language, Digital Libraries, Information Retrieval","Background:The rapidly evolving field of the Brain-Heart Interconnectome (BHI) merges neurology and cardiology. Despite its potential, inefficiencies in evidence synthesis and suboptimal adherence to quality standards often result in research waste. Systematic reviews are prone to redundancy, incomplete reporting, and lack of methodological rigor."
527,679d459debd8ffd557a2b07c,cs.CL,https://arxiv.org/pdf/2501.17176,Prompt-Based Cost-Effective Evaluation and Operation of ChatGPT as a Computer Programming Teaching Assistant,"Marc Ballestero-Ribó, Daniel Ortiz-Martínez","Computers and Society, Artificial Intelligence, Computation and Language","The dream of achieving a student-teacher ratio of 1:1 is closer than ever thanks to the emergence of large language models (LLMs). One potential application of these models in the educational field would be to provide feedback to students in university introductory programming courses, so that a student struggling to solve a basic implementation problem could seek help from an LLM available 24/7. This article focuses on studying three aspects related to such an application. First, the performance of two well-known models, GPT-3.5T and GPT-4T, in providing feedback to students is evaluated. The empirical results showed that GPT-4T performs much better than GPT-3.5T, however, it is not yet ready for use in a real-world scenario. This is due to the possibility of generating incorrect information that potential users may not always be able to detect. Second, the article proposes a carefully designed prompt using in-context learning techniques that allows automating important parts of the evaluation process, as well as providing a lower bound for the fraction of feedbacks containing incorrect information, saving time and effort. This was possible because the resulting feedback has a programmatically analyzable structure that incorporates diagnostic information about the LLM’s performance in solving the requested task. Third, the article also suggests a possible strategy for implementing a practical learning tool based on LLMs, which is rooted on the proposed prompting techniques. This strategy opens up a whole range of interesting possibilities from a pedagogical perspective."
528,679d459debd8ffd557a2b07d,cs.CL,https://arxiv.org/pdf/2501.17174,Extractive Schema Linking for Text-to-SQL,"Michael Glass, Mustafa Eyceoz, Dharmashankar Subramanian, Gaetano Rossiello, Long Vu, Alfio Gliozzo","Databases, Artificial Intelligence, Computation and Language","Text-to-SQL is emerging as a practical interface for real world databases. The dominant paradigm for Text-to-SQL is cross-database or schema-independent, supporting application schemas unseen during training.
Theschemaof a database defines the tables, columns, column types and foreign key connections between tables.
Real world schemas can be large, containing hundreds of columns, but for any particular query only a small fraction will be relevant. Placing the entire schema in the prompt for an LLM can be impossible for models with smaller token windows and expensive even when the context window is large enough to allow it.
Even apart from computational considerations, the accuracy of the model can be improved by focusing the SQL generation on only the relevant portion of the database.Schema linkingidentifies the portion of the database schema useful for the question."
529,679d459debd8ffd557a2b07e,cs.CL,https://arxiv.org/pdf/2501.17170,"Benchmarking Randomized Optimization Algorithms on Binary, Permutation, and Combinatorial Problem Landscapes","Jethro Odeyemi, Wenjun Zhang","Neural and Evolutionary Computing, Artificial Intelligence, Computation and Language, Machine Learning","In this paper, we evaluate the performance of four randomized optimization algorithms—Randomized Hill Climbing (RHC), Simulated Annealing (SA), Genetic Algorithms (GA), and MIMIC (Mutual Information Maximizing Input Clustering)—across three distinct types of problems: binary, permutation, and combinatorial. We systematically compare these algorithms using a set of benchmark fitness functions that highlight the specific challenges and requirements of each problem category. Our study analyzes each algorithm’s effectiveness based on key performance metrics, including solution quality, convergence speed, computational cost, and robustness. Results show that while MIMIC and GA excel in producing high-quality solutions for binary and combinatorial problems, their computational demands vary significantly. RHC and SA, while computationally less expensive, demonstrate limited performance in complex problem landscapes. The findings offer valuable insights into the trade-offs between different optimization strategies and provide practical guidance for selecting the appropriate algorithm based on the type of problems, accuracy requirements, and computational constraints."
530,679d459debd8ffd557a2b07f,cs.CL,https://arxiv.org/pdf/2501.17148,AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders,"Zhengxuan Wu, Aryaman Arora, Atticus Geiger, Zheng Wang, Jing Huang, Dan Jurafsky, Christopher D. Manning, Christopher Potts","Computation and Language, Artificial Intelligence, Machine Learning",
531,679d459debd8ffd557a2b080,cs.CL,https://arxiv.org/pdf/2501.17144,FactCG: Enhancing Fact Checkers with Graph-Based Multi-Hop Data,"Deren Lei, Yaxi Li, Siyao Li, Mengya Hu, Rui Xu, Ken Archer, Mingyu Wang, Emily Ching, Alex Deng","Computation and Language, Artificial Intelligence","Prior research on training grounded factuality classification models to detect hallucinations in large language models (LLMs) has relied on public natural language inference (NLI) data and synthetic data. However, conventional NLI datasets are not well-suited for document-level reasoning, which is critical for detecting LLM hallucinations. Recent approaches to document-level synthetic data generation involve iteratively removing sentences from documents and annotating factuality using LLM-based prompts. While effective, this method is computationally expensive for long documents and limited by the LLM’s capabilities. In this work, we analyze the differences between existing synthetic training data used in state-of-the-art models and real LLM output claims. Based on our findings, we propose a novel approach for synthetic data generation, CG2C, that leverages multi-hop reasoning on context graphs extracted from documents. Our fact checker model, FactCG, demonstrates improved performance with more connected reasoning, using the same backbone models. Experiments show it even outperforms GPT-4-o on the LLM-AGGREFACT benchmark with much smaller model size.111https://github.com/derenlei/FactCG"
532,679d459debd8ffd557a2b081,cs.CL,https://arxiv.org/pdf/2501.17117,Histoires Morales: A French Dataset for Assessing Moral Alignment,"Thibaud Leteno, Irina Proskurina, Antoine Gourru, Julien Velcin, Charlotte Laclau, Guillaume Metzler, Christophe Gravier","Computation and Language, Artificial Intelligence","Aligning language models with human values is crucial, especially as they become more integrated into everyday life.
While models are often adapted to user preferences, it is equally important to ensure they align with moral norms and behaviours in real-world social situations.
Despite significant progress in languages like English and Chinese, French has seen little attention in this area, leaving a gap in understanding how LLMs handle moral reasoning in this language.
To address this gap, we introduceHistoiresMorales, a French dataset derived fromMoralStories, created through translation and subsequently refined with the assistance of native speakers to guarantee grammatical accuracy and adaptation to the French cultural context.
We also rely on annotations of the moral values within the dataset to ensure their alignment with French norms.HistoiresMoralescovers a wide range of social situations, including differences in tipping practices, expressions of honesty in relationships, and responsibilities toward animals.
To foster future research, we also conduct preliminary experiments on the alignment of multilingual models on French and English data and the robustness of the alignment.
We find that while LLMs are generally aligned with human moral norms by default, they can be easily influenced with user-preference optimization for both moral and immoral data.111The data and code are openly available at:https://hf.co/datasets/LabHC/histoires_moraleshttps://github.com/upunaprosk/histoires-morales"
533,679d459debd8ffd557a2b082,cs.CL,https://arxiv.org/pdf/2501.17104,COS(M+O)S: Curiosity and RL-Enhanced MCTS for Exploring Story Space via Language Models,Tobias Materzok,"Computation and Language, Artificial Intelligence","We present COS(M+O)S, a System 2-inspired framework for open-ended plot development that systematically explores the vast space of possible story expansions, enabling a 3B-parameter language model to approach the plot quality of a 70B model on select short-story tasks.
The method accomplishes this by combining Monte Carlo Tree Search (MCTS), guided by a step-level value model that rewards moderate surprisal (curiosity) while penalizing incoherence, and Odds Ratio Preference Optimization (ORPO) to fine-tune the policy on high-value plot expansions.
This iterative reinforcement learning loop systematically explores multiple candidate plot branches, backpropagates quality signals, and adapts the policy for faster convergence, notably shifting the policy from puzzle-based Chain-of-Thought to more character-driven storytelling.
In small-scale tests with short-story prompts, 67%–77% of participants favored COS(M+O)S’s highest-rated expansions over lower-rated ones, suggesting that our learned value function aligns.
GPT-4o ratings further show that COS(M+O)S surpasses naive single-pass decoding from Llama 3.2 3B by 0.59 SD, coming within 0.06 SD of Llama 3.1 70B (no significant difference,p=0.93𝑝0.93p=\text{0.93}italic_p = 0.93).
Pairwise comparisons with o1 place COS(M+O)S 1.5 SD above the 3B baseline and find no statistically significant gap from 70B.
Nevertheless, absolute story quality remains modest, constrained by the small model’s capacity and limited training data."
534,679d459debd8ffd557a2b083,cs.CL,https://arxiv.org/pdf/2501.17047,How Linguistics Learned to Stop Worrying and Love the Language Models,"Richard Futrell, Kyle Mahowald",Computation and Language,"Language models can produce fluent, grammatical text.
Nonetheless, some maintain that language models don’t really learn language and also that, even if they did, that would not be informative for the study of human learning and processing.
On the other side, there have been claims that the success of LMs obviates the need for studying linguistic theory and structure.
We argue that both extremes are wrong.
LMs can contribute to fundamental questions about linguistic structure, language processing, and learning.
They force us to rethink arguments about learning and are informative for major questions in linguistic theory.
But they do not replace linguistic structure and theory.
We offer an optimistic take on the relationship between language models and linguistics."
535,679d459debd8ffd557a2b084,cs.CL,https://arxiv.org/pdf/2501.16975,Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling,"Hongzhi Huang, Defa Zhu, Banggu Wu, Yutao Zeng, Ya Wang, Qiyang Min, Xun Zhou","Computation and Language, Machine Learning","Tokenization is a fundamental component of large language models (LLMs), yet its influence on model scaling and performance is not fully explored. In this paper, we introduce Over-Tokenized Transformers, a novel framework that decouples input and output vocabularies to improve language modeling performance. Specifically, our approach scales up input vocabularies to leverage multi-gram tokens. Through extensive experiments, we uncover a log-linear relationship between input vocabulary size and training loss, demonstrating that larger input vocabularies consistently enhance model performance, regardless of model size. Using a large input vocabulary, we achieve performance comparable to double-sized baselines with no additional cost. Our findings highlight the importance of tokenization in scaling laws and provide practical insight for tokenizer design, paving the way for more efficient and powerful LLMs."
536,679d459debd8ffd557a2b085,cs.CL,https://arxiv.org/pdf/2501.16952,Multiple Abstraction Level Retrieve Augment Generation,"Zheng Zheng, Xinyi Ni, Pengyu Hong","Computation and Language, Artificial Intelligence, Machine Learning","A Retrieval-Augmented Generation (RAG) model powered by a large language model (LLM) provides a faster and more cost-effective solution for adapting to new data and knowledge. It also delivers more specialized responses compared to pre-trained LLMs. However, most existing approaches rely on retrieving prefix-sized chunks as references to support question-answering (Q/A). This approach is often deployed to address information needs at a single level of abstraction, as it struggles to generate answers across multiple levels of abstraction. In an RAG setting, while LLMs can summarize and answer questions effectively when provided with sufficient details, retrieving excessive information often leads to the ’lost in the middle’ problem and exceeds token limitations. We propose a novel RAG approach that uses chunks of multiple abstraction levels (MAL), including multi-sentence-level, paragraph-level, section-level, and document-level. The effectiveness of our approach is demonstrated in an under-explored scientific domain of Glycoscience. Compared to traditional single-level RAG approaches, our approach improves AI evaluated answer correctness of Q/A by 25.739% on Glyco-related papers."
537,679d459debd8ffd557a2b086,cs.CL,https://arxiv.org/pdf/2501.16925,Detecting harassment and defamation in cyberbullying with emotion-adaptive training,"Peiling Yi, Arkaitz Zubiaga, Yunfei Long",Computation and Language,"Existing research on detecting cyberbullying incidents on social media has primarily concentrated on harassment and is typically approached as a binary classification task. However, cyberbullying encompasses various forms, such as denigration and harassment, which celebrities frequently face. Furthermore, suitable training data for these diverse forms of cyberbullying remains scarce. In this study, we first develop a celebrity cyberbullying dataset that encompasses two distinct types of incidents: harassment and defamation. We investigate various types of transformer-based models, namely masked (RoBERTa, Bert and DistilBert), replacing (Electra), autoregressive (XLnet), masked&permuted (Mpnet), text-text (T5) and large language models (Llama2 and Llama3) under low source settings. We find that they perform competitively on explicit harassment binary detection, however, their performance is substantially lower on harassment and denigration multi-classification tasks.
Therefore, we propose an emotion-adaptive training framework (EAT) that helps transfer knowledge from the domain of emotion detection to the domain of cyberbullying detection to help detect indirect cyberbullying events. EAT consistently improves the average macro F1, precision and recall by 20% in cyberbullying detection tasks across nine transformer-based models under low-resource settings. Our claims are supported by intuitive theoretical insights and extensive experiments.111The data and source are publicly available athttps://github.com/Misinformation-emotion/Cyberbullying-emotionWarning: This paper contains offensive words, which do not reflect the views of the authors."
538,679d459debd8ffd557a2b087,cs.CL,https://arxiv.org/pdf/2501.16884,"Irony Detection, Reasoning and Understanding in Zero-shot Learning","Peiling Yi, Yuhan Xia","Computation and Language, Artificial Intelligence","Irony is a powerful figurative language (FL) on social media that can potentially mislead various NLP tasks, such as recommendation systems, misinformation checks, and sentiment analysis. Understanding the implicit meaning of this kind of subtle language is an essential step to mitigate the negative impact of irony in NLP tasks. However, building models to understand irony presents a unique set of challenges, because irony is a complex form of language that often relies on context, tone, and subtle cues to convey meaning that is opposite or different from the literal interpretation. Large language models, such as ChatGPT, are increasingly able to capture implicit and contextual information. In this study, we investigate the generalization, reasoning and understanding ability of ChatGPT on irony detection across six different genre irony detection datasets. Our findings suggest that ChatGPT appears to show an enhanced language understanding and reasoning ability. But it needs to be very careful in prompt engineering design. Thus, we propose a prompt engineering design framework IDADP to achieve higher irony detection accuracy, improved understanding of irony, and more effective explanations compared to other state-of-the-art ChatGPT zero-shot approaches. And ascertain
via experiments that the practice generated under the framework is likely to be the promised solution to resolve the generalization issues of LLMs."
539,679d459debd8ffd557a2b088,cs.CL,https://arxiv.org/pdf/2501.16865,"JRE-L: Journalist, Reader, and Editor LLMs in the Loop for Science Journalism for the General Audience","Gongyao Jiang, Xinran Shi, Qiong Luo",Computation and Language,"Science journalism reports current scientific discoveries to non-specialists, aiming to enable public comprehension of the state of the art.
This task is challenging as the audience often lacks specific knowledge about the presented research.
We propose a JRE-L framework that integrates three LLMs mimicking the writing-reading-feedback-revision loop.
In JRE-L, one LLM acts as the journalist, another LLM as the general public reader, and the third LLM as an editor.
The journalist’s writing is iteratively refined by feedback from the reader and suggestions from the editor.
Our experiments demonstrate that by leveraging the collaboration of two 7B and one 1.8B open-source LLMs, we can generate articles that are more accessible than those generated by existing methods, including prompting single advanced models such as GPT-4 and other LLM-collaboration strategies.
Our code is publicly available atgithub.com/Zzoay/JRE-L."
540,679d459debd8ffd557a2b089,cs.CL,https://arxiv.org/pdf/2501.16836,Misspellings in Natural Language Processing: A survey,"Gianluca Sperduti, Alejandro Moreo","Computation and Language, Artificial Intelligence","This survey provides an overview of the challenges of misspellings in natural language processing (NLP).
While often unintentional, misspellings have become ubiquitous in digital communication, especially with
the proliferation of Web 2.0, user-generated content, and informal text mediums such as social media,
blogs, and forums. Even if humans can generally interpret misspelled text, NLP models frequently strug-
gle to handle it: this causes a decline in performance in common tasks like text classification and machine
translation. In this paper, we reconstruct a history of misspellings as a scientific problem. We then discuss
the latest advancements to address the challenge of misspellings in NLP. Main strategies to mitigate the
effect of misspellings include data augmentation, double step, character-order agnostic, and tuple-based
methods, among others. This survey also examines dedicated data challenges and competitions to spur
progress in the field. Critical safety and ethical concerns are also examined, for example, the voluntary use
of misspellings to inject malicious messages and hate speech on social networks. Furthermore, the survey
explores psycholinguistic perspectives on how humans process misspellings, potentially informing inno-
vative computational techniques for text normalization and representation. Finally, the misspelling-related
challenges and opportunities associated with modern large language models are also analyzed, including
benchmarks, datasets, and performances of the most prominent language models against misspellings. This
survey aims to be an exhaustive resource for researchers seeking to mitigate the impact of misspellings in
the rapidly evolving landscape of NLP."
541,679d459debd8ffd557a2b08a,cs.CL,https://arxiv.org/pdf/2501.16813,Whispers of Sound-Enhancing Information Extraction from Depression Patients' Unstructured Data through Audio and Text Emotion Recognition and Llama Fine-tuning,"Lindy Gan, Yifan Huang, Xiaoyang Gao, Jiaming Tan, Fujun Zhao, Tao Yang","Computation and Language, Sound, Audio and Speech Processing",
542,679d459debd8ffd557a2b08b,cs.CL,https://arxiv.org/pdf/2501.16794,Algorithm for Automatic Legislative Text Consolidation,"Matias Etcheverry, Thibaud Real, Pauline Chavallard",Computation and Language,"This study introduces a method for automating the consolidation process in a legal context, a time-consuming task traditionally performed by legal professionals. We present a generative approach that processes legislative texts to automatically apply amendments. Our method employs light quantized generative model, finetuned with LoRA, to generate accurate and reliable amended texts. To the authors knowledge, this is the first time generative models are used on legislative text consolidation. Our dataset is publicly available on HuggingFace111Link to dataset. Experimental results demonstrate a significant improvement in efficiency, offering faster updates to legal documents. A full automated pipeline of legislative text consolidation can be done in a few hours, with a success rate of more than 63% on a difficult bill."
543,679d459debd8ffd557a2b08c,cs.CL,https://arxiv.org/pdf/2501.16783,A Stochastic Dynamical Theory of LLM Self-Adversariality: Modeling Severity Drift as a Critical Process,Jack David Carson,"Computation and Language, Artificial Intelligence, Adaptation and Self-Organizing Systems","This paper introduces a continuous-timestochastic dynamicalframework for understanding how large language models (LLMs) mayself-amplifylatent biases or toxicity through their own chain-of-thought reasoning. The model posits an instantaneous “severity” variablex⁢(t)∈[0,1]𝑥𝑡01x(t)\in[0,1]italic_x ( italic_t ) ∈ [ 0 , 1 ]evolving under a stochastic differential equation (SDE) with a drift termμ⁢(x)𝜇𝑥\mu(x)italic_μ ( italic_x )and diffusionσ⁢(x)𝜎𝑥\sigma(x)italic_σ ( italic_x ). Crucially, such a process can be consistently analyzed via the Fokker–Planck approach if each incremental step behaves nearly Markovian in severity space. The analysis investigatescritical phenomena, showing that certain parameter regimes create phase transitions from subcritical (self-correcting) to supercritical (runaway severity). The paper derives stationary distributions, first-passage times to harmful thresholds, and scaling laws near critical points. Finally, it highlights implications foragentsand extended LLM reasoning models: in principle, these equations might serve as a basis forformal verificationof whether a model remains stable or propagates bias over repeated inferences."
544,679d459debd8ffd557a2b08d,cs.CL,https://arxiv.org/pdf/2501.16748,Through the Prism of Culture: Evaluating LLMs' Understanding of Indian Subcultures and Traditions,"Garima Chhikara, Abhishek Kumar, Abhijnan Chakraborty",Computation and Language,"Large Language Models (LLMs) have shown remarkable advancements but also raise concerns about cultural bias, often reflecting dominant narratives at the expense of under-represented subcultures. In this study, we evaluate the capacity of LLMs to recognize and accurately respond to theLittle Traditionswithin Indian society, encompassing localized cultural practices and subcultures such as caste, kinship, marriage, and religion. Through a series of case studies, we assess whether LLMs can balance the interplay betweendominant Great Traditionsandlocalized Little Traditions. We explore various prompting strategies and further investigate whether using prompts in regional languages enhances the models cultural sensitivity and response quality. Our findings reveal that while LLMs demonstrate an ability to articulate cultural nuances, they often struggle to apply this understanding in practical, context-specific scenarios. To the best of our knowledge, this is the first study to analyze LLMs engagement with Indian subcultures, offering critical insights into the challenges of embedding cultural diversity in AI systems."
545,679d459debd8ffd557a2b08e,cs.CL,https://arxiv.org/pdf/2501.16727,xJailbreak: Representation Space Guided Reinforcement Learning for Interpretable LLM Jailbreaking,"Sunbowen Lee, Shiwen Ni, Chi Wei, Shuaimin Li, Liyang Fan, Ahmadreza Argha, Hamid Alinejad-Rokny, Ruifeng Xu, Yicheng Gong, Min Yang",Computation and Language,"Safety alignment mechanism are essential for preventing large language models (LLMs) from generating harmful information or unethical content. However, cleverly crafted prompts can bypass these safety measures without accessing the model’s internal parameters, a phenomenon known as black-box jailbreak. Existing heuristic black-box attack methods, such as genetic algorithms, suffer from limited effectiveness due to their inherent randomness, while recent reinforcement learning (RL) based methods often lack robust and informative reward signals. To address these challenges, we propose a novel black-box jailbreak method leveraging RL, which optimizes prompt generation by analyzing the embedding proximity between benign and malicious prompts. This approach ensures that the rewritten prompts closely align with the intent of the original prompts while enhancing the attack’s effectiveness. Furthermore, we introduce a comprehensive jailbreak evaluation framework incorporating keywords, intent matching, and answer validation to provide a more rigorous and holistic assessment of jailbreak success. Experimental results show the superiority of our approach, achieving state-of-the-art (SOTA) performance on several prominent open and closed-source LLMs, including Qwen2.5-7B-Instruct, Llama3.1-8B-Instruct, and GPT-4o-0806. Our method sets a new benchmark in jailbreak attack effectiveness, highlighting potential vulnerabilities in LLMs. The codebase for this work is available athttps://github.com/Aegis1863/xJailbreak."
546,679d459debd8ffd557a2b08f,cs.CL,https://arxiv.org/pdf/2501.16688,MME-Industry: A Cross-Industry Multimodal Evaluation Benchmark,"Dongyi Yi, Guibo Zhu, Chenglin Ding, Zongshu Li, Dong Yi, Jinqiao Wang",Computation and Language,"With the rapid advancement of Multimodal Large Language Models (MLLMs), numerous evaluation benchmarks have emerged.
However, comprehensive assessments of their performance across diverse industrial applications remain limited.
In this paper, we introduce MME-Industry, a novel benchmark designed specifically for evaluating MLLMs in industrial settings.
The benchmark encompasses 21 distinct domain, comprising 1050 question-answer pairs with 50 questions per domain. To ensure data integrity and prevent potential leakage from public datasets, all question-answer pairs were manually crafted and validated by domain experts.
Besides, the benchmark’s complexity is effectively enhanced by incorporating non-OCR questions that can be answered directly, along with tasks requiring specialized domain knowledge.
Moreover, we provide both Chinese and English versions of the benchmark, enabling comparative analysis of MLLMs’ capabilities across these languages. Our findings contribute valuable insights into MLLMs’ practical industrial applications and illuminate promising directions for future model optimization research."
547,679d459debd8ffd557a2b090,cs.CL,https://arxiv.org/pdf/2501.16673,LLM-AutoDiff: Auto-Differentiate Any LLM Workflow,"Li Yin, Zhangyang Wang",Computation and Language,"Large Language Models (LLMs) have reshaped natural language processing, powering applications from multi-hop retrieval and question answering to autonomous agent workflows. Yet,prompt engineering—the task of crafting textual inputs to effectively direct LLMs—remains difficult and labor-intensive, particularly for complex pipelines that combine multiple LLM calls with functional operations like retrieval and data formatting. We introduceLLM-AutoDiff: a novel framework for Automatic Prompt Engineering (APE) that extends textual gradient-based methods (such asText-Grad) to multi-component, potentially cyclic LLM architectures. Implemented within theAdalFlowlibrary,111https://github.com/SylphAI-Inc/AdalFlowLLM-AutoDiff treats each textual input as a trainable parameter and uses a frozen “backward engine” LLM to generate feedback—akin to “textual gradients”—that guide iterative prompt updates. Unlike prior single-node approaches, LLM-AutoDiff inherently accommodates functional nodes, preserves time-sequential behavior in repeated calls (e.g., multi-hop loops), and combats the “lost-in-the-middle” problem by isolating distinct sub-prompts (instructions, formats, or few-shot examples). It further boosts training efficiency by focusing on error-prone samples through selective gradient computation. Across diverse tasks, including single-step classification, multi-hop retrieval-based QA, and agent-driven pipelines, LLM-AutoDiff consistently outperforms existing textual gradient baselines in both accuracy and training cost. By unifying prompt optimization through a graph-centric lens, LLM-AutoDiff offers a powerful new paradigm for scaling and automating LLM workflows — mirroring the transformative role that automatic differentiation libraries have long played in neural network research."
548,679d459debd8ffd557a2b091,cs.CL,https://arxiv.org/pdf/2501.16658,Contextual Reinforcement in Multimodal Token Compression for Large Language Models,"Naderdel Piero, Zacharias Cromwell, Nathaniel Wainwright, Matthias Nethercott","Computation and Language, Artificial Intelligence","Effective token compression remains a critical challenge for scaling models to handle increasingly complex and diverse datasets. A novel mechanism based on contextual reinforcement is introduced, dynamically adjusting token importance through interdependencies and semantic relevance. This approach enables substantial reductions in token usage while preserving the quality and coherence of information representation. Incorporating graph-based algorithms and adaptive weighting, the method captures subtle contextual relationships across textual and multimodal data, ensuring robust alignment and performance in downstream tasks. Evaluations across varied domains reveal significant improvements in accuracy and semantic retention, particularly for tasks requiring detailed cross-modal interactions. Memory usage analyses demonstrate improved computational efficiency, with minimal overhead despite the additional reinforcement processes. Performance gains are further validated through error distribution analyses, showing reduced semantic loss and syntactic inconsistencies compared to baseline models. The modular architecture ensures compatibility with a wide range of open-source frameworks, facilitating scalable implementation for real-world applications. These findings highlight the potential of contextual reinforcement in redefining token management strategies and advancing large-scale model design."
549,679d459debd8ffd557a2b092,cs.CL,https://arxiv.org/pdf/2501.16655,Large Language Model Critics for Execution-Free Evaluation of Code Changes,"Aashish Yadavally, Hoan Nguyen, Laurent Callot, Gauthier Guinet","Computation and Language, Artificial Intelligence, Software Engineering","Large language models (LLMs) offer a promising way forward for automating software engineering tasks, such as bug fixes, feature additions, etc., via multi-step LLM-based agentic workflows. However, existing metrics for evaluating such workflows, mainly build status and occasionally log analysis, are toosparseand limited in providing the information needed to assess the quality of changes made. In this work, we designed LLM-based critics to derive well-structured and rigorous intermediate/step-level,execution-freeevaluation proxies for repo-level code changes. Importantly, we assume access to the gold test patch for the problem (i.e., reference-aware) to assess both semantics and executability of generated patches. With the gold test patch as a reference, we predict executability of all editing locations with an F1 score of 91.6%, aggregating which, we can predict the build status in 84.8% of the instances in SWE-bench. In particular, such an execution-focused LLM critic outperforms other reference-free and reference-aware LLM critics by 38.9% to 72.5%. Moreover, we demonstrate the usefulness of such a reference-aware framework in comparing patches generated by different agentic workflows. Finally, we open-source the library developed for this project, which allows further usage for either other agentic workflows or other benchmarks. The source code is available athttps://github.com/amazon-science/code-agent-eval"
550,679d459debd8ffd557a2b093,cs.CL,https://arxiv.org/pdf/2501.16650,DOCS: Quantifying Weight Similarity for Deeper Insights into Large Language Models,"Zeping Min, Xinshang Wang","Computation and Language, Artificial Intelligence","We introduce a novel index, the Distribution of Cosine Similarity (DOCS), for quantitatively assessing the similarity between weight matrices in Large Language Models (LLMs), aiming to facilitate the analysis of their complex architectures. Leveraging DOCS, our analysis uncovers intriguing patterns in the latest open-source LLMs: adjacent layers frequently exhibit high weight similarity and tend to form clusters, suggesting depth-wise functional specialization. Additionally, we prove that DOCS is theoretically effective in quantifying similarity for orthogonal matrices, a crucial aspect given the prevalence of orthogonal initializations in LLMs. This research contributes to a deeper understanding of LLM architecture and behavior, offering tools with potential implications for developing more efficient and interpretable models."
551,679d459debd8ffd557a2b094,cs.CL,https://arxiv.org/pdf/2501.16643,An LLM Benchmark for Addressee Recognition in Multi-modal Multi-party Dialogue,"Koji Inoue, Divesh Lala, Mikey Elmers, Keiko Ochi, Tatsuya Kawahara","Computation and Language, Artificial Intelligence, Sound, Audio and Speech Processing","Handling multi-party dialogues represents a significant step for advancing spoken dialogue systems, necessitating the development of tasks specific to multi-party interactions.
To address this challenge, we are constructing a multi-modal multi-party dialogue corpus of triadic (three-participant) discussions.
This paper focuses on the task of addressee recognition, identifying who is being addressed to take the next turn, a critical component unique to multi-party dialogue systems.
A subset of the corpus was annotated with addressee information, revealing that explicit addressees are indicated in approximately 20% of conversational turns.
To evaluate the task’s complexity, we benchmarked the performance of a large language model (GPT-4o) on addressee recognition.
The results showed that GPT-4o achieved an accuracy only marginally above chance, underscoring the challenges of addressee recognition in multi-party dialogue.
These findings highlight the need for further research to enhance the capabilities of large language models in understanding and navigating the intricacies of multi-party conversational dynamics."
552,679d459debd8ffd557a2b095,cs.CL,https://arxiv.org/pdf/2501.16635,Why Do We Laugh? Annotation and Taxonomy Generation for Laughable Contexts in Spontaneous Text Conversation,"Koji Inoue, Mikey Elmers, Divesh Lala, Tatsuya Kawahara","Computation and Language, Artificial Intelligence","Laughter serves as a multifaceted communicative signal in human interaction, yet its identification within dialogue presents a significant challenge for conversational AI systems.
This study addresses this challenge by annotating laughable contexts in Japanese spontaneous text conversation data and developing a taxonomy to classify the underlying reasons for such contexts.
Initially, multiple annotators manually labeled laughable contexts using a binary decision (laughable or non-laughable).
Subsequently, an LLM was used to generate explanations for the binary annotations of laughable contexts, which were then categorized into a taxonomy comprising ten categories, including “Empathy and Affinity” and “Humor and Surprise,” highlighting the diverse range of laughter-inducing scenarios.
The study also evaluated GPT-4’s performance in recognizing the majority labels of laughable contexts, achieving an F1 score of 43.14%.
These findings contribute to the advancement of conversational AI by establishing a foundation for more nuanced recognition and generation of laughter, ultimately fostering more natural and engaging human-AI interactions."
553,679d459debd8ffd557a2b096,cs.CL,https://arxiv.org/pdf/2501.16616,Few-Shot Optimized Framework for Hallucination Detection in Resource-Limited NLP Systems,"Baraa Hikal, Ahmed Nasreldin, Ali Hamdi, Ammar Mohammed",Computation and Language,
554,679d459debd8ffd557a2b097,cs.CL,https://arxiv.org/pdf/2501.16581,DialUp! Modeling the Language Continuum by Adapting Models to Dialects and Dialects to Models,"Niyati Bafna, Emily Chang, Nathaniel R. Robinson, David R. Mortensen, Kenton Murray, David Yarowsky, Hale Sirin",Computation and Language,"Most of the world’s languages and dialects are low-resource, and lack support in mainstream machine translation (MT) models.
However, many of them have a closely-related high-resource language (HRL) neighbor, and differ in linguistically regular ways from it.
This underscores the importance of model robustness to dialectical variation and cross-lingual generalization to the HRL dialect continuum. We present DialUp, consisting of a training-time technique for adapting a pretrainedmodel to dialecticaldata (M→→\rightarrow→D), and an inference-time intervention adapting dialecticaldata to themodel expertise (D→→\rightarrow→M).M→→\rightarrow→Dinduces model robustness to potentially unseen and unknown dialects by exposure to synthetic data exemplifying linguistic mechanisms of dialectical variation, whereasD→→\rightarrow→Mtreats dialectical divergence for known target dialects.
These methods show considerable performance gains for several dialects from four language families, and modest gains for two other language families.
We also conduct feature and error analyses, which show that language varieties with low baseline MT performance are more likely to benefit from these approaches.111https://github.com/niyatibafna/dialectical-robustness-mt/"
555,679d459debd8ffd557a2b098,cs.CL,https://arxiv.org/pdf/2501.16533,A comparison of data filtering techniques for English-Polish LLM-based machine translation in the biomedical domain,"Jorge del Pozo Lérida, Kamil Kojs, János Máté, Mikołaj Antoni Barański, Christian Hardmeier","Computation and Language, Machine Learning","Large Language Models (LLMs) have become state-of-the-art in Machine Translation (MT), often trained on massive bilingual parallel corpora scraped from the web, that contain low-quality entries and redundant information, leading to significant computational challenges. Various data filtering methods exist to reduce dataset sizes, but their effectiveness largely varies based on specific language pairs and domains. This paper evaluates the impact of commonly used data filtering techniques—LASER, MUSE, and LaBSE—on English-Polish translation within the biomedical domain. By filtering the UFAL Medical Corpus, we created varying dataset sizes to fine-tune the mBART50 model, which was then evaluated using the SacreBLEU metric on the Khresmoi dataset, having the quality of translations assessed by bilingual speakers. Our results show that both LASER and MUSE can significantly reduce dataset sizes while maintaining or even enhancing performance. We recommend the use of LASER, as it consistently outperforms the other methods and provides the most fluent and natural-sounding translations."
556,679d459debd8ffd557a2b099,cs.CL,https://arxiv.org/pdf/2501.16524,Programming by Examples Meets Historical Linguistics: A Large Language Model Based Approach to Sound Law Induction,"Atharva Naik, Darsh Agrawal, Hong Sng, Clayton Marr, Kexun Zhang, Nathaniel R Robinson, Kalvin Chang, Rebecca Byrnes, Aravind Mysore, Carolyn Rose, David R Mortensen",Computation and Language,
557,679d459debd8ffd557a2b09a,cs.CL,https://arxiv.org/pdf/2501.16516,How well can LLMs Grade Essays in Arabic?,"Rayed Ghazawi, Edwin Simpson","Computation and Language, Artificial Intelligence",
558,679d459debd8ffd557a2b09b,cs.CL,https://arxiv.org/pdf/2501.16513,Deception in LLMs: Self-Preservation and Autonomous Goals in Large Language Models,"Sudarshan Kamath Barkur, Sigurd Schacht, Johannes Scholl",Computation and Language,"Recent advances in Large Language Models (LLMs) have incorporated planning and reasoning capabilities, enabling models to outline steps before execution and provide transparent reasoning paths. This enhancement has reduced errors in mathematical and logical tasks while improving accuracy. These developments have facilitated LLMs’ use as agents that can interact with tools and adapt their responses based on new information."
559,679d459debd8ffd557a2b09c,cs.CL,https://arxiv.org/pdf/2501.17132,ASTRAL: Automated Safety Testing of Large Language Models,"Miriam Ugarte, Pablo Valle, José Antonio Parejo, Sergio Segura, Aitor Arrieta","Software Engineering, Computation and Language","Large Language Models (LLMs) have recently gained significant attention due to their ability to understand and generate sophisticated human-like content. However, ensuring their safety is paramount as they might provide harmful and unsafe responses. Existing LLM testing frameworks address various safety-related concerns (e.g., drugs, terrorism, animal abuse) but often face challenges due to unbalanced and obsolete datasets. In this paper, we presentASTRAL, a tool that automates the generation and execution of test cases (i.e., prompts) for testing the safety of LLMs. First, we introduce a novel black-box coverage criterion to generate balanced and diverse unsafe test inputs across a diverse set of safety categories as well as linguistic writing characteristics (i.e., different style and persuasive writing techniques). Second, we propose an LLM-based approach that leverages Retrieval Augmented Generation (RAG), few-shot prompting strategies and web browsing to generate up-to-date test inputs. Lastly, similar to current LLM test automation techniques, we leverage LLMs as test oracles to distinguish between safe and unsafe test outputs, allowing a fully automated testing approach. We conduct an extensive evaluation on well-known LLMs, revealing the following key findings: i) GPT3.5 outperforms other LLMs when acting as the test oracle, accurately detecting unsafe responses, and even surpassing more recent LLMs (e.g., GPT-4), as well as LLMs that are specifically tailored to detect unsafe LLM outputs (e.g., LlamaGuard); ii) the results confirm that our approach can uncover nearly twice as many unsafe LLM behaviors with the same number of test inputs compared to currently used static datasets; and iii) our black-box coverage criterion combined with web browsing can effectively guide the LLM on generating up-to-date unsafe test inputs, significantly increasing the number of unsafe LLM behaviors."
560,679d459debd8ffd557a2b09d,cs.CL,https://arxiv.org/pdf/2501.17116,Optimizing Large Language Model Training Using FP4 Quantization,"Ruizhe Wang, Yeyun Gong, Xiao Liu, Guoshuai Zhao, Ziyue Yang, Baining Guo, Zhengjun Zha, Peng Cheng","Machine Learning, Computation and Language","The growing computational demands of training large language models (LLMs) necessitate more efficient methods. Quantized training presents a promising solution by enabling low-bit arithmetic operations to reduce these costs. While FP8 precision has demonstrated feasibility, leveraging FP4 remains a challenge due to significant quantization errors and limited representational capacity. This work introduces the first FP4 training framework for LLMs, addressing these challenges with two key innovations: a differentiable quantization estimator for precise weight updates and an outlier clamping and compensation strategy to prevent activation collapse. To ensure stability, the framework integrates a mixed-precision training scheme and vector-wise quantization. Experimental results demonstrate that our FP4 framework achieves accuracy comparable to BF16 and FP8, with minimal degradation, scaling effectively to 13B-parameter LLMs trained on up to 100B tokens. With the emergence of next-generation hardware supporting FP4, our framework sets a foundation for efficient ultra-low precision training."
561,679d459debd8ffd557a2b09e,cs.CL,https://arxiv.org/pdf/2501.17088,Mamba-Shedder: Post-Transformer Compression for Efficient Selective Structured State Space Models,"J. Pablo Muñoz, Jinjie Yuan, Nilesh Jain","Machine Learning, Artificial Intelligence, Computation and Language","Large pre-trained models have achieved outstanding results in sequence modeling. The Transformer block and its attention mechanism have been the main drivers of the success of these models. Recently, alternative architectures, such as Selective Structured State Space Models (SSMs), have been proposed to address the inefficiencies of Transformers. This paper explores the compression of SSM-based models, particularly Mamba and its hybrids. We study the sensitivity of these models to the removal of selected components at different granularities to reduce the model size and computational overhead, thus improving their efficiency while maintaining accuracy. The proposed solutions, collectively referred to asMamba-Shedder, achieve a speedup of up to 1.4x during inference, demonstrating that model efficiency can be improved by eliminating several redundancies with minimal impact on the overall model performance. The code is available athttps://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning."
562,679d459debd8ffd557a2b09f,cs.CL,https://arxiv.org/pdf/2501.17070,Context is Key for Agent Security,"Lillian Tsai, Eugene Bagdasarian","Cryptography and Security, Computation and Language, Machine Learning","Judging the safety of an action, whether taken by a human or a system, must take into account the context in which the action takes place. For example, deleting an email from a user’s mailbox may or may not be appropriate depending on the email’s content, the user’s goals, or even available space.
Today, existing systems (e.g., a smartphone ecosystem) that prevent harmful actions rely on manually-crafted policies or user confirmation for every context.
With the upcoming deployment of generalist agents that support a multitude of tasks, we argue that we must rethink security designs to adapt to the scale of contexts and capabilities of these systems.
As a first step, this paper explores contextual security in the domain of agents and proposescontextual security for agents(Conseca), a framework to generate just-in-time, contextual, and human-verifiable security policies."
563,679d459debd8ffd557a2b0a0,cs.CL,https://arxiv.org/pdf/2501.17030,Challenges in Ensuring AI Safety in DeepSeek-R1 Models: The Shortcomings of Reinforcement Learning Strategies,"Manojkumar Parmar, Yuvaraj Govindarajulu","Machine Learning, Artificial Intelligence, Computation and Language, Cryptography and Security","Large Language Models (LLMs) have achieved remarkable progress in reasoning, alignment, and task-specific performance. However, ensuring harmlessness in these systems remains a critical challenge, particularly in advanced models like DeepSeek-R1[1]. This paper examines the limitations of Reinforcement Learning (RL) as the primary approach for reducing harmful outputs in DeepSeek-R1 and compares it with Supervised Fine-Tuning (SFT). While RL improves reasoning capabilities, it faces challenges such as reward hacking, generalization failures, language mixing, and high computational costs. We propose hybrid training approaches combining RL and SFT to achieve robust harmlessness reduction. Usage recommendations and future directions for deploying DeepSeek-R1 responsibly are also presented."
564,679d459debd8ffd557a2b0a1,cs.CL,https://arxiv.org/pdf/2501.16945,ToolFactory: Automating Tool Generation by Leveraging LLM to Understand REST API Documentations,"Xinyi Ni, Qiuyang Wang, Yukun Zhang, Pengyu Hong","Machine Learning, Artificial Intelligence, Computation and Language, Software Engineering","LLM-based tool agents offer natural language interfaces, enabling users to seamlessly interact with computing services. While REST APIs are valuable resources for building such agents, they must first be transformed into AI-compatible tools. Automatically generating AI-compatible tools from REST API documents can greatly streamline tool agent development and minimize user learning curves. However, API documentation often suffers from a lack of standardization, inconsistent schemas, and incomplete information. To address these issues, we developedToolFactory, an open-source pipeline for automating tool generation from unstructured API documents. To enhance the reliability of the developed tools, we implemented an evaluation method to diagnose errors. Furthermore, we built a knowledge base of verified tools, which we leveraged to infer missing information from poorly documented APIs. We developed the API Extraction Benchmark, comprising 167 API documents and 744 endpoints in various formats, and designed a JSON schema to annotate them. This annotated dataset was utilized to train and validate ToolFactory. The experimental results highlight the effectiveness of ToolFactory. We also demonstrated ToolFactory by creating a domain-specific AI agent for glycomaterials research. ToolFactory exhibits significant potential for facilitating the seamless integration of scientific REST APIs into AI workflows."
565,679d459debd8ffd557a2b0a2,cs.CL,https://arxiv.org/pdf/2501.16937,TAID: Temporally Adaptive Interpolated Distillation for Efficient Knowledge Transfer in Language Models,"Makoto Shing, Kou Misaki, Han Bao, Sho Yokoi, Takuya Akiba","Machine Learning, Artificial Intelligence, Computation and Language","Causal language models have demonstrated remarkable capabilities, but their size poses significant challenges for deployment in resource-constrained environments. Knowledge distillation, a widely-used technique for transferring knowledge from a large teacher model to a small student model, presents a promising approach for model compression.
A significant remaining issue lies in the major differences between teacher and student models, namely the substantial capacity gap, mode averaging, and mode collapse, which pose barriers during distillation.
To address these issues, we introduceTemporally Adaptive Interpolated Distillation (TAID), a novel knowledge distillation approach that dynamically interpolates student and teacher distributions through an adaptive intermediate distribution, gradually shifting from the student’s initial distribution towards the teacher’s distribution. We provide a theoretical analysis demonstrating TAID’s ability to prevent mode collapse and empirically show its effectiveness in addressing the capacity gap while balancing mode averaging and mode collapse.
Our comprehensive experiments demonstrate TAID’s superior performance across various model sizes and architectures in both instruction tuning and pre-training scenarios. Furthermore, we showcase TAID’s practical impact by developing two state-of-the-art compact foundation models:TAID-LLM-1.5Bfor language tasks andTAID-VLM-2Bfor vision-language tasks.
These results demonstrate TAID’s effectiveness in creating high-performing and efficient models, advancing the development of more accessible AI technologies."
566,679d459debd8ffd557a2b0a3,cs.CL,https://arxiv.org/pdf/2501.16672,VeriFact: Verifying Facts in LLM-Generated Clinical Text with Electronic Health Records,"Philip Chung, Akshay Swaminathan, Alex J. Goodell, Yeasul Kim, S. Momsen Reincke, Lichy Han, Ben Deverett, Mohammad Amin Sadeghi, Abdel-Badih Ariss, Marc Ghanem, David Seong, Andrew A. Lee, Caitlin E. Coombes, Brad Bradshaw, Mahir A. Sufian, Hyo Jung Hong, Teresa P. Nguyen, Mohammad R. Rasouli, Komal Kamra, Mark A. Burbridge, James C. McAvoy, Roya Saffary, Stephen P. Ma, Dev Dash, James Xie, Ellen Y. Wang, Clifford A. Schmiesing, Nigam Shah, Nima Aghaeepour","Artificial Intelligence, Computation and Language, Information Retrieval, Logic in Computer Science","Methods to ensure factual accuracy of text generated by large language models (LLM) in clinical medicine are lacking.VeriFactis an artificial intelligence system that combines retrieval-augmented generation and LLM-as-a-Judge to verify whether LLM-generated text is factually supported by a patient’s medical history based on their electronic health record (EHR). To evaluate this system, we introduceVeriFact-BHC, a new dataset that decomposes Brief Hospital Course narratives from discharge summaries into a set of simple statements with clinician annotations for whether each statement is supported by the patient’s EHR clinical notes. Whereas highest agreement between clinicians was 88.5%,VeriFactachieves up to 92.7% agreement when compared to a denoised and adjudicated average human clinician ground truth, suggesting thatVeriFactexceeds the average clinician’s ability to fact-check text against a patient’s medical record.VeriFactmay accelerate the development of LLM-based EHR applications by removing current evaluation bottlenecks."
567,679d459debd8ffd557a2b0a4,cs.CL,https://arxiv.org/pdf/2501.16609,CowPilot: A Framework for Autonomous and Human-Agent Collaborative Web Navigation,"Faria Huq, Zora Zhiruo Wang, Frank F. Xu, Tianyue Ou, Shuyan Zhou, Jeffrey P. Bigham, Graham Neubig","Artificial Intelligence, Computation and Language, Human-Computer Interaction","While much work on web agents emphasizes the promise of autonomously performing tasks on behalf of users, in reality, agents often fall short on complex tasks in real-world contexts and modeling user preference. This presents an opportunity for humans to collaborate with the agent and leverage the agent’s capabilities effectively. We proposeCowPilot, a framework supporting autonomous as well as human-agentcollaborativeweb navigation, and evaluation across task success and task efficiency.CowPilotreduces the number of steps humans need to perform by allowing agents to propose next steps, while users are able to pause, reject, or take alternative actions. During execution, users can interleave their actions with the agent’s by overriding suggestions or resuming agent control when needed. We conducted case studies on five common websites and found that the human-agent collaborative mode achieves the highest success rate of95%percent9595\%95 %while requiring humans to perform only15.2%percent15.215.2\%15.2 %of the total steps. Even with human interventions during task execution, the agent successfully drives up to half of task success on its own.CowPilotcan serve as a useful tool for data collection and agent evaluation across websites, which we believe will enable research in how users and agents can work together. Video demonstrations are available athttps://oaishi.github.io/cowpilot.html"
568,679d459debd8ffd557a2b0a5,cs.CL,https://arxiv.org/pdf/2501.16607,MCTS-SQL: An Effective Framework for Text-to-SQL with Monte Carlo Tree Search,"Shuozhi Yuan, Liming Chen, Miaomiao Yuan, Jin Zhao, Haoran Peng, Wenming Guo","Databases, Artificial Intelligence, Computation and Language, Programming Languages","Text-to-SQL is a fundamental and longstanding problem in the NLP area, aiming at converting natural language queries into SQL, enabling non-expert users to operate databases. Recent advances in LLM have greatly improved text-to-SQL performance. However, challenges persist, especially when dealing with complex user queries. Current approaches (e.g., COT prompting and multi-agent frameworks) rely on the ability of models to plan and generate SQL autonomously, but controlling performance remains difficult. In addition, LLMs are still prone to hallucinations. To alleviate these challenges, we designed a novelMCTS-SQLto guide SQL generation iteratively. The approach generates SQL queries through Monte Carlo Tree Search (MCTS) and a heuristic self-refinement mechanism are used to enhance accuracy and reliability. Key components include a schema selector for extracting relevant information and an MCTS-based generator for iterative query refinement. Experimental results from the SPIDER and BIRD benchmarks show that MCTS-SQL achieves state-of-the-art performance. Specifically, on the BIRD development dataset, MCTS-SQL achieves an Execution (EX) accuracy of69.40%using GPT-4o as the base model and a significant improvement when dealing with challenging tasks, with an EX of51.48%, which is3.41%higher than the existing method."
569,679d459debd8ffd557a2b0a6,cs.CL,https://arxiv.org/pdf/2501.16497,Smoothed Embeddings for Robust Language Models,"Ryo Hase, Md Rafi Ur Rashid, Ashley Lewis, Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang","Machine Learning, Artificial Intelligence, Computation and Language, Cryptography and Security, Machine Learning","Improving the safety and reliability of large language models (LLMs) is a crucial aspect of realizing trustworthy AI systems. Although alignment methods aim to suppress harmful content generation, LLMs are often still vulnerable to jailbreaking attacks that employ adversarial inputs that subvert alignment and induce harmful outputs. We propose the Randomized Embedding Smoothing and Token Aggregation (RESTA) defense, which adds random noise to the embedding vectors and performs aggregation during the generation of each output token, with the aim of better preserving semantic information. Our experiments demonstrate that our approach achieves superior robustness versus utility tradeoffs compared to the baseline defenses."
570,679d459debd8ffd557a2b0a7,cs.CL,https://arxiv.org/pdf/2501.16403,Is Open Source the Future of AI? A Data-Driven Approach,"Domen Vake, Bogdan Šinik, Jernej Vičič, Aleksandar Tošić","Software Engineering, Artificial Intelligence, Computation and Language","Large language Models(LLMs) have taken center stage in both academia and industry. Alongside discussions on their usability, accuracy, and societal impacts are growing concerns regarding privacy, transparency, and their potential misuse in illicit activities. A critical topic within these debates is the trustworthiness of LLMs, specifically when models in question are proprietary. A frequently proposed solution to significantly improve trustworthiness is open-sourcing models. However, this option has significant drawbacks such as illicit applications where models can be modified and misused, lack of incentive structures (mostly financial) to support open-sourcing, and protecting intellectual property. On the other hand, most LLMs are trained by the private sector due to data and computing resource requirements. These costs are significant and proprietary models are better positioned to achieve a return on investment.
There are also other approaches that lie somewhere on the spectrum between completely open-source and proprietary. These can largely be categorised into open-source usage limitations protected by licensing, partially open-source (open weights) models, hybrid approaches where obsolete model versions are open-sourced, while competitive versions with market value remain proprietary."
571,679d459debd8ffd557a2b0a8,cs.CL,https://arxiv.org/pdf/2501.16372,Low-Rank Adapters Meet Neural Architecture Search for LLM Compression,"J. Pablo Muñoz, Jinjie Yuan, Nilesh Jain","Machine Learning, Artificial Intelligence, Computation and Language","The rapid expansion of Large Language Models (LLMs) has posed significant challenges regarding the computational resources required for fine-tuning and deployment. Recent advancements in low-rank adapters have demonstrated their efficacy in parameter-efficient fine-tuning (PEFT) of these models.
This retrospective paper comprehensively discusses innovative approaches that synergize low-rank representations with Neural Architecture Search (NAS) techniques, particularly weight-sharing super-networks. Robust solutions for compressing and fine-tuning large pre-trained models are developed by integrating these methodologies.
Our analysis highlights the potential of these combined strategies to democratize the use of LLMs, making them more accessible for deployment in resource-constrained environments. The resulting models exhibit reduced memory footprints and faster inference times, paving the way for more practical and scalable applications of LLMs. Models and code are available athttps://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning."
572,679d459debd8ffd557a2b0a9,cs.CL,https://arxiv.org/pdf/2501.16350,A Method for Multi-Hop Question Answering on Persian Knowledge Graph,"Arash Ghafouri, Mahdi Firouzmandi, Hasan Naderi","Information Retrieval, Artificial Intelligence, Computation and Language",
573,679d459debd8ffd557a2b0aa,cs.CL,https://arxiv.org/pdf/2501.16344,WhiSPA: Semantically and Psychologically Aligned Whisper with Self-Supervised Contrastive and Student-Teacher Learning,"Rajath Rao, Adithya Ganesan, Oscar Kjell, Jonah Luby, Akshay Raghavan, Scott Feltman, Whitney Ringwald, Ryan L. Boyd, Benjamin Luft, Camilo Ruggero, Neville Ryant, Roman Kotov, H. Andrew Schwartz","Audio and Speech Processing, Artificial Intelligence, Computation and Language, Sound","Current speech encoding pipelines often rely on separate processing pipelines between text and audio, not fully leveraging the inherent overlap between these modalities for understanding human communication.
Language models excel at capturing semantic meaning from text that can complement the additional prosodic, emotional, and acoustic cues from speech.
This work bridges the gap by proposing WhiSPA111https://github.com/humanlab/WhiSPA(Whisper with Semantic-Psychological Alignment), a novel audio encoder trained with a contrastive student-teacher learning objective.
Using over 500k speech segments from mental health audio interviews, we evaluate the utility of aligning Whisper’s audio embeddings with text representations from an SBERT encoder and text-based assessments of psychological dimensions: emotion and personality.
Over self-supervised and downstream mental health tasks, WhiSPA surpasses state-of-the-art speech models, achieving an average error reduction of73.4%percent73.473.4\%73.4 %on the segment-level self-supervised objective and83.8%percent83.883.8\%83.8 %on 11 psychological downstream tasks.
WhiSPA demonstrates that cross-modal alignment can increase the amount of text-semantic and psychological information captured in audio-only encoder models."
574,679d459debd8ffd557a2b0ab,cs.CL,https://arxiv.org/pdf/2501.16341,Developing Enhanced Conversational Agents for Social Virtual Worlds,"D. Griol, A. Sanchis, J. M. Molina, Z. Callejas","Audio and Speech Processing, Computation and Language, Sound",
575,679d459debd8ffd557a2b0ac,cs.CL,https://arxiv.org/pdf/2501.16327,LUCY: Linguistic Understanding and Control Yielding Early Stage of Her,"Heting Gao, Hang Shao, Xiong Wang, Chaofan Qiu, Yunhang Shen, Siqi Cai, Yuchen Shi, Zihan Xu, Zuwei Long, Yike Zhang, Shaoqi Dong, Chaoyou Fu, Ke Li, Long Ma, Xing Sun","Computation and Language, Sound, Audio and Speech Processing","This document provides a basic paper template and submission guidelines.
Abstracts must be a single paragraph, ideally between 4–6 sentences long.
Gross violations will trigger corrections at the camera-ready phase."
576,679d459debd8ffd557a2b0ad,cs.CL,https://arxiv.org/pdf/2501.16303,RAPID: Retrieval-Augmented Parallel Inference Drafting for Text-Based Video Event Retrieval,"Long Nguyen, Huy Nguyen, Bao Khuu, Huy Luu, Huy Le, Tuan Nguyen, Tho Quan","Computation and Language, Information Retrieval","Retrieving events from videos using text queries has become increasingly challenging due to the rapid growth of multimedia content. Existing methods for text-based video event retrieval often focus heavily on object-level descriptions, overlooking the crucial role of contextual information. This limitation is especially apparent when queries lack sufficient context, such as missing location details or ambiguous background elements. To address these challenges, we propose a novel system called RAPID (Retrieval-Augmented Parallel Inference Drafting), which leverages advancements in Large Language Models (LLMs) and prompt-based learning to semantically correct and enrich user queries with relevant contextual information. These enriched queries are then processed through parallel retrieval, followed by an evaluation step to select the most relevant results based on their alignment with the original query. Through extensive experiments on our custom-developed dataset, we demonstrate that RAPID significantly outperforms traditional retrieval methods, particularly for contextually incomplete queries. Our system was validated for both speed and accuracy through participation in the Ho Chi Minh City AI Challenge 2024, where it successfully retrieved events from over 300 hours of video. Further evaluation comparing RAPID with the baseline proposed by the competition organizers demonstrated its superior effectiveness, highlighting the strength and robustness of our approach."
577,679d459debd8ffd557a2b0ae,cs.CL,https://arxiv.org/pdf/2501.16302,Matryoshka Re-Ranker: A Flexible Re-Ranking Architecture With Configurable Depth and Width,"Zheng Liu, Chaofan Li, Shitao Xiao, Chaozhuo Li, Defu Lian, Yingxia Shao",Computation and Language,"Large language models (LLMs) provide powerful foundations to perform fine-grained text re-ranking. However, they are often prohibitive in reality due to constraints on computation bandwidth. In this work, we propose aflexiblearchitecture calledMatroyshka Re-Ranker, which is designed to facilitateruntime customizationof model layers and sequence lengths at each layer based on users’ configurations. Consequently, the LLM-based re-rankers can be made applicable across various real-world situations."
578,679d459debd8ffd557a2b0af,cs.CL,https://arxiv.org/pdf/2501.16276,URAG: Implementing a Unified Hybrid RAG for Precise Answers in University Admission Chatbots -- A Case Study at HCMUT,"Long Nguyen, Tho Quan","Computation and Language, Information Retrieval","With the rapid advancement of Artificial Intelligence, particularly in Natural Language Processing, Large Language Models (LLMs) have become pivotal in educational question-answering systems, especially university admission chatbots. Concepts such as Retrieval-Augmented Generation (RAG) and other advanced techniques have been developed to enhance these systems by integrating specific university data, enabling LLMs to provide informed responses on admissions and academic counseling. However, these enhanced RAG techniques often involve high operational costs and require the training of complex, specialized modules, which poses challenges for practical deployment. Additionally, in the educational context, it is crucial to provide accurate answers to prevent misinformation, a task that LLM-based systems find challenging without appropriate strategies and methods.
In this paper, we introduce the Unified RAG (URAG) Framework, a hybrid approach that significantly improves the accuracy of responses, particularly for critical queries. Experimental results demonstrate that URAG enhances our in-house, lightweight model to perform comparably to state-of-the-art commercial models. Moreover, to validate its practical applicability, we conducted a case study at our educational institution, which received positive feedback and acclaim. This study not only proves the effectiveness of URAG but also highlights its feasibility for real-world implementation in educational settings."
579,679d459debd8ffd557a2b0b0,cs.CL,https://arxiv.org/pdf/2501.16255,A foundation model for human-AI collaboration in medical literature mining,"Zifeng Wang, Lang Cao, Qiao Jin, Joey Chan, Nicholas Wan, Behdad Afzali, Hyun-Jin Cho, Chang-In Choi, Mehdi Emamverdi, Manjot K. Gill, Sun-Hyung Kim, Yijia Li, Yi Liu, Hanley Ong, Justin Rousseau, Irfan Sheikh, Jenny J. Wei, Ziyang Xu, Christopher M. Zallek, Kyungsang Kim, Yifan Peng, Zhiyong Lu, Jimeng Sun",Computation and Language,"Systematic literature review is essential for evidence-based medicine, requiring comprehensive analysis of clinical trial publications. However, the application of artificial intelligence (AI) models for medical literature mining has been limited by insufficient training and evaluation across broad therapeutic areas and diverse tasks. Here, we present LEADS, an AI foundation model for study search, screening, and data extraction from medical literature. The model is trained on 633,759 instruction data points in LEADSInstruct, curated from 21,335 systematic reviews, 453,625 clinical trial publications, and 27,015 clinical trial registries. We showed that LEADS demonstrates consistent improvements over four cutting-edge generic large language models (LLMs) on six tasks. Furthermore, LEADS enhances expert workflows by providing supportive references following expert requests, streamlining processes while maintaining high-quality results. A study with 16 clinicians and medical researchers from 14 different institutions revealed that experts collaborating with LEADS achieved a recall of 0.81 compared to 0.77 experts working alone in study selection, with a time savings of 22.6%. In data extraction tasks, experts using LEADS achieved an accuracy of 0.85 versus 0.80 without using LEADS, alongside a 26.9% time savings. These findings highlight the potential of specialized medical literature foundation models to outperform generic models, delivering significant quality and efficiency benefits when integrated into expert workflows for medical literature mining."
580,679d459debd8ffd557a2b0b1,cs.CL,https://arxiv.org/pdf/2501.16235,Echoes of Discord: Forecasting Hater Reactions to Counterspeech,"Xiaoying Song, Sharon Lisseth Perez, Xinchen Yu, Eduardo Blanco, Lingzi Hong",Computation and Language,"Hate speech (HS) erodes the inclusiveness of online users and propagates negativity and division.
Counterspeech has been recognized as a way to mitigate HS.
While some research has investigated the impact of user-generated counterspeech on social media platforms, few have examined and modeled haters’ reactions toward counterspeech, despite the immediate alteration of haters’ attitudes being an important aspect of counterspeech.
This study fills the gap by analyzing the impact of counterspeech from the hater’s perspective, focusing on whether the counterspeech leads the hater to reenter the conversation and if the reentry is hateful.
We compile theRedditEchoes of Hate dataset (ReEco), which consists of triple-turn conversations featuring haters’ reactions, to assess the impact of counterspeech.
The linguistic analysis sheds insights on the language of counterspeech to hate eliciting different haters’ reactions.
Experimental results demonstrate that the 3-way classification model outperforms the two-stage reaction predictor, which first predicts reentry and then determines the reentry type.
We conclude the study with an assessment showing the most common error causes in the best-performing model."
581,679d459debd8ffd557a2b0b2,cs.CL,https://arxiv.org/pdf/2501.16220,DBRouting: Routing End User Queries to Databases for Answerability,"Priyangshu Mandal, Manasi Patwardhan, Mayur Patidar, Lovekesh Vig",Computation and Language,"Enterprise level data is often distributed across multiple sources and identifying the correct set-of data-sources with relevant information for a knowledge request is a fundamental challenge.
In this work, we define the novel task of routing an end-user query to the appropriate data-source, where the data-sources are databases. We synthesize datasets by extending existing datasets designed for NL-to-SQL semantic parsing. We create baselines on these datasets by using open-source LLMs, using both pre-trained and task specific embeddings fine-tuned using the training data. With these baselines we demonstrate that open-source LLMs perform better than embedding based approach, but suffer from token length limitations. Embedding based approaches benefit from task specific fine-tuning, more so when there is availability of data in terms of database specific questions for training. We further find that the task becomes more difficult (i) with an increase in the number of data-sources, (ii) having data-sources closer in terms of their domains,(iii) having databases without external domain knowledge required to interpret its entities and (iv) with ambiguous and complex queries requiring more fine-grained understanding of the data-sources or logical reasoning for routing to an appropriate source. This calls for the need for developing more sophisticated solutions to better address the task."
582,679d459debd8ffd557a2b0b3,cs.CL,https://arxiv.org/pdf/2501.16214,Provence: efficient and robust context pruning for retrieval-augmented generation,"Nadezhda Chirkova, Thibault Formal, Vassilina Nikoulina, Stéphane Clinchant","Computation and Language, Information Retrieval","Retrieval-augmented generation improves various aspects of large language models (LLMs) generation, but suffers from computational overhead caused by long contexts as well as the propagation of irrelevant retrieved information into generated responses. Context pruning deals with both aspects, by removing irrelevant parts of retrieved contexts before LLM generation. Existing context pruning approaches are however limited, and do not provide a universal model that would be bothefficientandrobustin a wide range of scenarios, e.g., when contexts contain a variable amount of relevant information or vary in length, or when evaluated on various domains. In this work, we close this gap and introduceProvence(Pruning and Reranking Of retrieVEd relevaNt ContExts), an efficient and robust context pruner for Question Answering, which dynamically detects the needed amount of pruning for a given context and can be used out-of-the-box for various domains. The three key ingredients ofProvenceare formulating the context pruning task as sequence labeling, unifying context pruning capabilities with context reranking, and training on diverse data. Our experimental results show thatProvenceenables context pruning with negligible to no drop in performance, in various domains and settings, at almost no cost in a standard RAG pipeline. We also conduct a deeper analysis alongside various ablations to provide insights into training context pruners for future work."
583,679d459debd8ffd557a2b0b4,cs.CL,https://arxiv.org/pdf/2501.16181,Can summarization approximate simplification? A gold standard comparison,"Giacomo Magnifico, Eduard Barbu",Computation and Language,"This study explores the overlap between text summarization and simplification outputs. While summarization evaluation methods are streamlined, simplification lacks cohesion, prompting the question: how closely can abstractive summarization resemble gold-standard simplification? We address this by applying two BART-based BRIO summarization methods to the Newsela corpus, comparing outputs with manually annotated simplifications and achieving a top ROUGE-L score of 0.654. This provides insight into where summarization and simplification outputs converge and differ."
584,679d459debd8ffd557a2b0b5,cs.CL,https://arxiv.org/pdf/2501.16154,AdaCoT: Rethinking Cross-Lingual Factual Reasoning through Adaptive Chain-of-Thought,"Xin Huang, Tarun Kumar Vangani, Zhengyuan Liu, Bowei Zou, Ai Ti Aw","Computation and Language, Artificial Intelligence","Large language models (LLMs) have shown impressive multilingual capabilities through pretraining on diverse corpora. While these models show strong reasoning abilities, their performance varies significantly across languages due to uneven training data distribution. Existing approaches using machine translation, and extensive multilingual pretraining and cross-lingual tuning face scalability challenges and often fail to capture nuanced reasoning processes across languages.
In this paper, we introduceAdaCoT(Adaptive Chain-of-Thought), a framework that enhances multilingual reasoning by dynamically routing thought processes through intermediary “thinking languages” before generating target-language responses. AdaCoT leverages a language-agnostic core and incorporates an adaptive, reward-based mechanism for selecting optimal reasoning pathways without requiring additional pretraining. Our comprehensive evaluation across multiple benchmarks demonstrates substantial improvements in both factual reasoning quality and cross-lingual consistency, with particularly strong performance gains in low-resource language settings. The results suggest that adaptive reasoning paths can effectively bridge the performance gap between high and low-resource languages while maintaining cultural and linguistic nuances."
585,679d459debd8ffd557a2b0b6,cs.CL,https://arxiv.org/pdf/2501.16135,Evaluation of NMT-Assisted Grammar Transfer for a Multi-Language Configurable Data-to-Text System,"Andreas Madsack, Johanna Heininger, Adela Schneider, Ching-Yi Chen, Christian Eckard, Robert Weißgraeber",Computation and Language,One approach for multilingual data-to-text generation is to translate grammatical configurations upfront from the source language into each target language. These configurations are then used by a surface realizer and in document planning stages to generate output.
586,679d459debd8ffd557a2b0b7,cs.CL,https://arxiv.org/pdf/2501.16123,From #Dr00gtiktok to #harmreduction: Exploring Substance Use Hashtags on TikTok,"Layla Bouzoubaa, Muqi Guo, Joseph Trybala, Afsaneh Razi, Rezvaneh Rezapour",Computation and Language,"The rise of TikTok as a primary source of information for youth, combined with its unique short-form video format, creates urgent questions about how substance use content manifests and spreads on the platform. This paper provides the first in-depth exploration of substance use-related content on TikTok, covering all major substance categories as classified by the Drug Enforcement Agency. Through social network analysis and qualitative coding, we examined more than 2,333 hashtags across 39,509 videos, identified 16 distinct hashtag communities and analyzed their interconnections and thematic content. Our analysis revealed a highly interconnected small-world network where recovery-focused hashtags like “#addiction”, “#recovery”, and “#sober” serve as central bridges between communities. Through manual coding of 351 representative videos, we found that Recovery Advocacy content (33.9%) and Satirical content (28.2%) dominate, while direct substance depiction appears in only 26% of videos, with active use shown in just 6.5% of them. This suggests TikTok functions primarily as a recovery support platform rather than a space promoting substance use. We found strong alignment between hashtag communities and video content, indicating organic community formation rather than attempts to evade content moderation. Our findings inform how platforms can balance content moderation with preserving valuable recovery support communities, while also providing insights for the design of social media-based recovery interventions."
587,679d459debd8ffd557a2b0b8,cs.CL,https://arxiv.org/pdf/2501.16106,Towards Explainable Multimodal Depression Recognition for Clinical Interviews,"Wenjie Zheng, Qiming Xie, Zengzhi Wang, Jianfei Yu, Rui Xia",Computation and Language,"Recently, multimodal depression recognition for clinical interviews (MDRC) has recently attracted considerable attention. Existing MDRC studies mainly focus on improving task performance and have achieved significant development. However, for clinical applications, model transparency is critical, and previous works ignore the interpretability of decision-making processes. To address this issue, we propose an Explainable Multimodal Depression Recognition for Clinical Interviews (EMDRC) task, which aims to provide evidence for depression recognition by summarizing symptoms and uncovering underlying causes. Given an interviewer-participant interaction scenario, the goal of EMDRC is to structured summarize participant’s symptoms based on the eight-item Patient Health Questionnaire depression scale (PHQ-8), and predict their depression severity. To tackle the EMDRC task, we construct a new dataset based on an existing MDRC dataset. Moreover, we utilize the PHQ-8 and propose a PHQ-aware multimodal multi-task learning framework, which captures the utterance-level symptom-related semantic information to help generate dialogue-level summary. Experiment results on our annotated dataset demonstrate the superiority of our proposed methods over baseline systems on the EMDRC task."
588,679d459debd8ffd557a2b0b9,cs.CL,https://arxiv.org/pdf/2501.16093,STAR: Stepwise Task Augmentation and Relation Learning for Aspect Sentiment Quad Prediction,"Wenna Lai, Haoran Xie, Guandong Xu, Qing Li","Computation and Language, Artificial Intelligence","Abstract text goes here. To find your publication’s abstractword count limit, navigate to your magazine’s homepage fromhttps://www.computer.org/csdl/magazinesand click Write for Us>>>Author Information. An abstract is a single paragraph that summarizes the significant aspects of the manuscript. Often it indicates whether the manuscript is a report of new work, a review or overview, or a combination thereof. Do not cite references in the abstract. Papers must not have been published previously and must be targeted toward the general technical reader. Papers submitted for peer review (not departments or columns) may fit into the theme of an open Call for Papers or be submitted as a “Regular” paper. Some Computer Society (CS) magazines provide early access to full manuscript submissions by posting a preprint of the article prior to its inclusion in an issue. Preprint articles are considered published and may be cited using their Digital Object Identifier (DOI). IEEE’s Publishing Operations team will provide editorial and production services throughout the publication process."
589,679d459debd8ffd557a2b0ba,cs.CL,https://arxiv.org/pdf/2501.16078,Integration of LLM Quality Assurance into an NLG System,"Ching-Yi Chen, Johanna Heininger, Adela Schneider, Christian Eckard, Andreas Madsack, Robert Weißgraeber",Computation and Language,"In this paper, we present a system that uses a Large Language Model (LLM) to perform grammar and spelling correction as a component of Quality Assurance (QA) for texts generated by NLG systems, which is important for text production in real-world scenarios. Evaluating the results of the system on work-in-progress sports news texts in three languages, we show that it is able to deliver acceptable corrections."
590,679d459debd8ffd557a2b0bb,cs.CL,https://arxiv.org/pdf/2501.16077,RelCAT: Advancing Extraction of Clinical Inter-Entity Relationships from Unstructured Electronic Health Records,"Shubham Agarwal, Vlad Dinu, Thomas Searle, Mart Ratas, Anthony Shek, Dan F. Stein, James Teo, Richard Dobson",Computation and Language,"This study introduces RelCAT (Relation Concept Annotation Toolkit), an interactive tool, library, and workflow designed to classify relations between entities extracted from clinical narratives. Building upon the CogStack MedCAT framework, RelCAT addresses the challenge of capturing complete clinical relations dispersed within text. The toolkit implements state-of-the-art machine learning models such as BERT and Llama along with proven evaluation and training methods. We demonstrate a dataset annotation tool (built within MedCATTrainer), model training, and evaluate our methodology on both openly available gold-standard and real-world UK National Health Service (NHS) hospital clinical datasets. We perform extensive experimentation and a comparative analysis of the various publicly available models with varied approaches selected for model fine-tuning. Finally, we achieve macro F1-scores of 0.977 on the gold-standard n2c2, surpassing the previous state-of-the-art performance, and achieve performance of>=>=> =0.93 F1 on our NHS gathered datasets."
591,679d459debd8ffd557a2b0bc,cs.CL,https://arxiv.org/pdf/2501.16075,PISCO: Pretty Simple Compression for Retrieval-Augmented Generation,"Maxime Louis, Hervé Déjean, Stéphane Clinchant","Computation and Language, Artificial Intelligence, Information Retrieval","Retrieval-Augmented Generation (RAG) pipelines enhance Large Language Models (LLMs) by retrieving relevant documents, but they face scalability issues due to high inference costs and limited context size. Document compression is a practical solution, but current soft compression methods suffer from accuracy losses and require extensive pretraining. In this paper, we introduce PISCO111Code and models will be released soon., a novel method that achieves a 16x compression rate with minimal accuracy loss (0-3%) across diverse RAG-based question-answering (QA) tasks. Unlike existing approaches, PISCO requires no pretraining or annotated data, relying solely on sequence-level knowledge distillation from document-based questions. With the ability to fine-tune a 7-10B LLM in 48 hours on a single A100 GPU, PISCO offers a highly efficient and scalable solution. We present comprehensive experiments showing that PISCO outperforms existing compression models by 8% in accuracy."
592,679d459debd8ffd557a2b0bd,cs.CL,https://arxiv.org/pdf/2501.16011,MEL: Legal Spanish Language Model,"David Betancur Sánchez, Nuria Aldama García, Álvaro Barbero Jiménez, Marta Guerrero Nieto, Patricia Marsà Morales, Nicolás Serrano Salas, Carlos García Hernán, Pablo Haya Coll, Elena Montiel Ponsoda, Pablo Calleja Ibáñez",Computation and Language,"Legal texts, characterized by complex and specialized terminology, present a significant challenge for Language Models. Adding an underrepresented language, such as Spanish, to the mix makes it even more challenging. While pre-trained models like XLM-RoBERTa have shown capabilities in handling multilingual corpora, their performance on domain specific documents remains underexplored. This paper presents the development and evaluation of MEL, a legal language model based on XLM-RoBERTa-large, fine-tuned on legal documents such as BOE (Boletín Oficial del Estado, the Spanish oficial report of laws) and congress texts. We detail the data collection, processing, training, and evaluation processes. Evaluation benchmarks show a significant improvement over baseline models in understanding the legal Spanish language. We also present case studies demonstrating the model’s application to new legal texts, highlighting its potential to perform top results over different NLP tasks."
593,679d459debd8ffd557a2b0be,cs.CL,https://arxiv.org/pdf/2501.15990,3CEL: A corpus of legal Spanish contract clauses,"Nuria Aldama García, Patricia Marsà Morales, David Betancur Sánchez, Álvaro Barbero Jiménez, Marta Guerrero Nieto, Pablo Haya Coll, Patricia Martín Chozas, Elena Montiel Ponsoda",Computation and Language,"Legal corpora for Natural Language Processing (NLP) are valuable and scarce resources in languages like Spanish due to two main reasons: data accessibility and legal expert knowledge availability. INESData 2024 is a European Union funded project lead by the Universidad Politécnica de Madrid (UPM) and developed by Instituto de Ingeniería del Conocimiento (IIC) to create a series of state-of-the-art NLP resources applied to the legal/administrative domain in Spanish. The goal of this paper is to present the Corpus of Legal Spanish Contract Clauses (3CEL), which is a contract information extraction corpus developed within the framework of INESData 2024. 3CEL contains 373 manually annotated tenders using 19 defined categories (4 782 total tags) that identify key information for contract understanding and reviewing."
594,679d459debd8ffd557a2b0bf,cs.CL,https://arxiv.org/pdf/2501.15968,Multi-View Attention Syntactic Enhanced Graph Convolutional Network for Aspect-based Sentiment Analysis,"Xiang Huang, Hao Peng, Shuo Sun, Zhifeng Hao, Hui Lin, Shuhai Wang","Computation and Language, Artificial Intelligence","Aspect-based Sentiment Analysis (ABSA) is the task aimed at predicting the sentiment polarity of aspect words within sentences.
Recently, incorporating graph neural networks (GNNs) to capture additional syntactic structure information in the dependency tree derived from syntactic dependency parsing has been proven to be an effective paradigm for boosting ABSA.
Despite GNNs enhancing model capability by fusing more types of information, most works only utilize a single topology view of the dependency tree or simply conflate different perspectives of information without distinction, which limits the model performance.
To address these challenges, in this paper, we propose a new multi-view attention syntactic enhanced graph convolutional network (MASGCN) that weighs different syntactic information of views using attention mechanisms.
Specifically, we first construct distance mask matrices from the dependency tree to obtain multiple subgraph views for GNNs.
To aggregate features from different views, we propose a multi-view attention mechanism to calculate the attention weights of views.
Furthermore, to incorporate more syntactic information, we fuse the dependency type information matrix into the adjacency matrices and present a structural entropy loss to learn the dependency type adjacency matrix.
Comprehensive experiments on four benchmark datasets demonstrate that our model outperforms state-of-the-art methods.
The codes and datasets are available athttps://github.com/SELGroup/MASGCN."
595,679d459debd8ffd557a2b0c0,cs.CL,https://arxiv.org/pdf/2501.15915,Parametric Retrieval Augmented Generation,"Weihang Su, Yichen Tang, Qingyao Ai, Junxi Yan, Changyue Wang, Hongning Wang, Ziyi Ye, Yujia Zhou, Yiqun Liu","Computation and Language, Information Retrieval","Retrieval-augmented generation (RAG) techniques have emerged as a promising solution to enhance the reliability of large language models (LLMs) by addressing issues like hallucinations, outdated knowledge, and domain adaptation.
In particular, existing RAG methods append relevant documents retrieved from external corpus or databases to the input of LLMs to guide their generation process, which we refer to as the in-context knowledge injection method.
While this approach is simple and often effective, it has inherent limitations. Firstly, increasing the context length and number of relevant documents can lead to higher computational overhead and degraded performance, especially in complex reasoning tasks. More importantly, in-context knowledge injection operates primarily at the input level, but LLMs store their internal knowledge in their parameters. This gap fundamentally limits the capacity of in-context methods. To this end, we introduce Parametric retrieval-augmented generation (Parametric RAG), a new RAG paradigm that integrates external knowledge directly into the parameters of feed-forward networks (FFN) of an LLM through document parameterization. This approach not only saves online computational costs by eliminating the need to inject multiple documents into the LLMs’ input context, but also deepens the integration of external knowledge into the parametric knowledge space of the LLM. Experimental results demonstrate that Parametric RAG substantially enhances both the effectiveness and efficiency of knowledge augmentation in LLMs. Also, it can be combined with in-context RAG methods to achieve even better performance111We have open-sourced all the code, data, and models in the following anonymized GitHub link: https://github.com/oneal2000/PRAG."
596,679d459debd8ffd557a2b0c1,cs.CL,https://arxiv.org/pdf/2501.15876,Optimizing Sentence Embedding with Pseudo-Labeling and Model Ensembles: A Hierarchical Framework for Enhanced NLP Tasks,"Ziwei Liu, Qi Zhang, Lifu Gao","Computation and Language, Artificial Intelligence","Sentence embedding tasks are important in natural language processing (NLP), but improving their performance while keeping them reliable is still hard. This paper presents a framework that combines pseudo-label generation and model ensemble techniques to improve sentence embeddings. We use external data from SimpleWiki, Wikipedia, and BookCorpus to make sure the training data is consistent. The framework includes a hierarchical model with an encoding layer, refinement layer, and ensemble prediction layer, using ALBERT-xxlarge, RoBERTa-large, and DeBERTa-large models. Cross-attention layers combine external context, and data augmentation techniques like synonym replacement and back-translation increase data variety. Experimental results show large improvements in accuracy and F1-score compared to basic models, and studies confirm that cross-attention and data augmentation make a difference. This work presents an effective way to improve sentence embedding tasks and lays the groundwork for future NLP research."
597,679d459debd8ffd557a2b0c2,cs.CL,https://arxiv.org/pdf/2501.15875,LCTG Bench: LLM Controlled Text Generation Benchmark,"Kentaro Kurihara, Masato Mita, Peinan Zhang, Shota Sasaki, Ryosuke Ishigami, Naoaki Okazaki",Computation and Language,
598,679d459debd8ffd557a2b0c3,cs.CL,https://arxiv.org/pdf/2501.15858,Potential Applications of Artificial Intelligence for Cross-language Intelligibility Assessment of Dysarthric Speech,"Eunjung Yeo, Julie Liss, Visar Berisha, David Mortensen","Computation and Language, Sound, Audio and Speech Processing",
599,679d459debd8ffd557a2b0c4,cs.CL,https://arxiv.org/pdf/2501.15826,MADP: Multi-Agent Deductive Planning for Enhanced Cognitive-Behavioral Mental Health Question Answer,"Qi Chen, Dexi Liu",Computation and Language,"The Mental Health Question Answer (MHQA) task requires the seeker and supporter to complete the support process in one-turn dialogue. Given the richness of help-seeker posts, supporters must thoroughly understand the content and provide logical, comprehensive, and well-structured responses. Previous works in MHQA mostly focus on single-agent approaches based on the cognitive element of Cognitive Behavioral Therapy (CBT), but they overlook the interactions among various CBT elements, such as emotion and cognition. This limitation hinders the models’ ability to thoroughly understand the distress of help-seekers. To address this, we propose a framework named Multi-Agent Deductive Planning (MADP), which is based on the interactions between the various psychological elements of CBT. This method guides Large Language Models (LLMs) to achieve a deeper understanding of the seeker’s context and provide more personalized assistance based on individual circumstances. Furthermore, we construct a new dataset based on the MADP framework and use it to fine-tune LLMs, resulting in a specialized model named MADP-LLM. We conduct extensive experiments, including comparisons with multiple LLMs, human evaluations, and automatic evaluations, to validate the effectiveness of the MADP framework and MADP-LLM."
600,679d459debd8ffd557a2b0c5,cs.CL,https://arxiv.org/pdf/2501.15781,Large Language Models to Diffusion Finetuning,"Edoardo Cetin, Tianyu Zhao, Yujin Tang","Computation and Language, Artificial Intelligence, Machine Learning","We propose a new finetuning method to provide pre-trained large language models (LMs) the ability to scale test-time compute through the diffusion framework. By increasing the number of diffusion steps, we show our finetuned models achieve monotonically increasing accuracy, directly translating to improved performance across downstream tasks. Furthermore, our finetuned models can expertly answer questions on specific topics by integrating powerful guidance techniques, and autonomously determine the compute required for a given problem by leveraging adaptive ODE solvers. Our method is universally applicable to any foundation model pre-trained with a cross-entropy loss and does not modify any of its original weights, fully preserving its strong single-step generation capabilities. We show our method is more effective and fully compatible with traditional finetuning approaches, introducing an orthogonal new direction to unify the strengths of the autoregressive and diffusion frameworks."
601,679d459debd8ffd557a2b0c6,cs.CL,https://arxiv.org/pdf/2501.15777,Automatic Feedback Generation for Short Answer Questions using Answer Diagnostic Graphs,"Momoka Furuhashi, Hiroaki Funayama, Yuya Iwase, Yuichiroh Matsubayashi, Yoriko Isobe, Toru Nagahama, Saku Sugawara, Kentaro Inui",Computation and Language,"Short-reading comprehension questions are widely used in reading education to foster understanding of the structure of a prompt text. These questions typically require students to read a specific passage (prompt text) and then articulate their understanding of its contents in a few sentences. However, giving feedback to students on their responses to such problems can be burdensome for teachers. As a result, students generally only receive scores on their responses, making it difficult for them to identify and correct their own errors. Thus, it is a necessary to develop a system that automatically generates feedback statements, linking their responses to the scoring rubrics. Natural language processing (NLP) has evolved significantly in recent years. Automatic scoring feature remains a uniquely researched aspect in relation to short-reading comprehension questions, while feedback generation remains largely unexplored. To address this, we develop a system that can produce feedback for student responses. The Answer Diagnostic Graph (ADG) we proposed aligns the student’s responses to the logical structure of the reading text of these questions and automatically generates feedback. In our experiment, we assess the impact of our system using oracle feedback generated when the system is fully functional. The two experimental groups of students are asked to answer two prompts and their scores are compared: for these two prompts, one group receives the model answer and corresponding explanatory text (answer explanation condition) and the other receives our system’s oracle feedback in addition to those two (feedback condition), alternatively. We further investigated the students’ perceptions of the feedback and assess changes in their motivation. As a result, no significant differences were observed between the groups in terms of score improvements in re-answering. However, we found that feedback helped students understand the reasons for their mistakes and advance their comprehension of the key points of the text. We also found that feedback makes students enhance their motivation, but room remains for improvement in the generated feedback to promote understanding of the logical structure of the text."
602,679d459debd8ffd557a2b0c7,cs.CL,https://arxiv.org/pdf/2501.15773,Is It Navajo? Accurate Language Detection in Endangered Athabaskan Languages,"Ivory Yang, Weicheng Ma, Chunhui Zhang, Soroush Vosoughi",Computation and Language,"Endangered languages, such as Navajo—the most widely spoken Native American language—are significantly underrepresented in contemporary language technologies, exacerbating the challenges of their preservation and revitalization. This study evaluates Google’s large language model (LLM)-based language identification system, which consistently misidentifies Navajo, exposing inherent limitations when applied to low-resource Native American languages. To address this, we introduce a random forest classifier trained on Navajo and eight frequently confused languages. Despite its simplicity, the classifier achieves near-perfect accuracy (97-100%), significantly outperforming Google’s LLM-based system. Additionally, the model demonstrates robustness across other Athabaskan languages—a family of Native American languages spoken primarily in Alaska, the Pacific Northwest, and parts of the Southwestern United States—suggesting its potential for broader application. Our findings underscore the pressing need for NLP systems that prioritize linguistic diversity and adaptability over centralized, one-size-fits-all solutions, especially in supporting underrepresented languages in a multicultural world. This work directly contributes to ongoing efforts to address cultural biases in language models and advocates for the development of culturally localized NLP tools that serve diverse linguistic communities."
603,679d459debd8ffd557a2b0c8,cs.CL,https://arxiv.org/pdf/2501.15754,Weight-based Analysis of Detokenization in Language Models: Understanding the First Stage of Inference Without Inference,"Go Kamoda, Benjamin Heinzerling, Tatsuro Inaba, Keito Kudo, Keisuke Sakaguchi, Kentaro Inui",Computation and Language,"According to the stages-of-inference hypothesis, early layers of language models map their subword-tokenized input, which does not necessarily correspond to a linguistically meaningful segmentation, to more meaningful representations that form the model’s “inner vocabulary”.
Prior analysis of thisdetokenizationstage has predominantly relied on probing and interventions such as path patching, which involve selecting particular inputs, choosing a subset of components that will be patched, and then observing changes in model behavior.
Here, we show that several important aspects of the detokenization stage can be understood purely by analyzing model weights, without performing any model inference steps.
Specifically, we introduce an analytical decomposition of first-layer attention in GPT-2.
Our decomposition yields interpretable terms that quantify the relative contributions of position-related, token-related, and mixed effects.
By focusing on terms in this decomposition, we discover weight-based explanations of attention bias toward close tokens and attention for detokenization.github.com/gokamoda/lm-detokenization"
604,679d459debd8ffd557a2b0c9,cs.CL,https://arxiv.org/pdf/2501.15747,IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task Language Understanding,"Sankalp KJ, Ashutosh Kumar, Laxmaan Balaji, Nikunj Kotecha, Vinija Jain, Aman Chadha, Sreyoshi Bhaduri","Computation and Language, Artificial Intelligence","Known by more than 1.5 billion people in the Indian subcontinent, Indic languages present unique challenges and opportunities for natural language processing (NLP) research due to their rich cultural heritage, linguistic diversity, and complex structures. IndicMMLU-Pro is a comprehensive benchmark designed to evaluate Large Language Models (LLMs) across Indic languages, building upon the MMLU Pro (Massive Multitask Language Understanding) framework. Covering major languages such as Hindi, Bengali, Gujarati, Marathi, Kannada, Punjabi, Tamil, Telugu, and Urdu, our benchmark addresses the unique challenges and opportunities presented by the linguistic diversity of the Indian subcontinent. This benchmark encompasses a wide range of tasks in language comprehension, reasoning, and generation, meticulously crafted to capture the intricacies of Indian languages. IndicMMLU-Pro provides a standardized evaluation framework to push the research boundaries in Indic language AI, facilitating the development of more accurate, efficient, and culturally sensitive models. This paper outlines the benchmarks’ design principles, task taxonomy, and data collection methodology, and presents baseline results from state-of-the-art multilingual models. As a publicly available resource, IndicMMLU-Pro111https://huggingface.co/datasets/LinguaLift/IndicMMLU-Prois set to contribute significantly to advancements in Indic language-based technologies and serve as a valuable tool for the NLP community."
605,679d459debd8ffd557a2b0ca,cs.CL,https://arxiv.org/pdf/2501.15720,ESGSenticNet: A Neurosymbolic Knowledge Base for Corporate Sustainability Analysis,"Keane Ong, Rui Mao, Frank Xing, Ranjan Satapathy, Johan Sulaeman, Erik Cambria, Gianmarco Mengaldo",Computation and Language,"Evaluating corporate sustainability performance is essential to drive sustainable business practices, amid the need for a more sustainable economy. However, this is hindered by the complexity and volume of corporate sustainability data (i.e. sustainability disclosures), not least by the effectiveness of the NLP tools used to analyse them. To this end, we identify three primary challenges –immateriality, complexity,andsubjectivity, that exacerbate the difficulty of extracting insights from sustainability disclosures. To address these issues, we introduce ESGSenticNet, a publicly available knowledge base for sustainability analysis. ESGSenticNet is constructed from a neurosymbolic framework that integrates specialised concept parsing, GPT-4o inference, and semi-supervised label propagation, together with a hierarchical taxonomy. This approach culminates in a structured knowledge base of 44k knowledge triplets –(‘halve carbon emission’, supports, ‘emissions control’), for effective sustainability analysis. Experiments indicate that ESGSenticNet, when deployed as a lexical method, more effectively captures relevant and actionable sustainability information from sustainability disclosures compared to state of the art baselines. Besides capturing a high number of unique ESG topic terms, ESGSenticNet outperforms baselines on the ESG relatedness and ESG action orientation of these terms by 26% and 31% respectively. These metrics describe the extent to which topic terms are related to ESG, and depict an action toward ESG. Moreover, when deployed as a lexical method, ESGSenticNet does not require any training, possessing a key advantage in its simplicity for non-technical stakeholders."
606,679d459debd8ffd557a2b0cb,cs.CL,https://arxiv.org/pdf/2501.15708,StaICC: Standardized Evaluation for Classification Task in In-context Learning,"Hakaze Cho, Naoya Inoue","Computation and Language, Artificial Intelligence","Classification tasks are widely investigated in theIn-ContextLearning (ICL) paradigm. However, current efforts are evaluated on disjoint benchmarks and settings, while their performances are significantly influenced by some trivial variables, such as prompt templates, data sampling, instructions, etc., which leads to significant inconsistencies in the results reported across various literature, preventing fair comparison or meta-analysis across different papers. Therefore, this paper proposes a standardized and easy-to-use evaluation toolkit (StaICC) for in-context classification. Including, for the normal classification task, we provideStaICC-Normal, selecting 10 widely used datasets, and generating prompts with a fixed form, to mitigate the variance among the experiment implementations. To enrich the usage of our benchmark, we also provide a sub-benchmarkStaICC-Diagfor diagnosing ICL from several aspects, aiming for a more robust inference processing."
607,679d459debd8ffd557a2b0cc,cs.CL,https://arxiv.org/pdf/2501.15700,Adapting Biomedical Abstracts into Plain language using Large Language Models,"Haritha Gangavarapu, Giridhar Kaushik Ramachandran, Kevin Lybarger, Meliha Yetisgen, Özlem Uzuner",Computation and Language,"A vast amount of medical knowledge is available for public use through online health forums, and question-answering platforms on social media. The majority of the population in the United States doesn’t have the right amount of health literacy to make the best use of that information. Health literacy means the ability to obtain and comprehend the basic health information to make appropriate health decisions. To build the bridge between this gap, organizations advocate adapting this medical knowledge into plain language. Building robust systems to automate the adaptations helps both medical and non-medical professionals best leverage the available information online. The goal of the Plain Language Adaptation of Biomedical Abstracts (PLABA) track is to adapt the biomedical abstracts in English language extracted from PubMed based on the questions asked in MedlinePlus for the general public using plain language at the sentence level. As part of this track, we leveraged the best open-source Large Language Models suitable and fine-tuned for dialog use cases. We compare and present the results for all of our systems and our ranking among the other participants’ submissions. Our top performing GPT-4 based model ranked first in the avg. simplicity measure and 3rdon the avg. accuracy measure."
608,679d459debd8ffd557a2b0cd,cs.CL,https://arxiv.org/pdf/2501.15688,Transformer-Based Multimodal Knowledge Graph Completion with Link-Aware Contexts,"Haodi Ma, Dzmitry Kasinets, Daisy Zhe Wang","Computation and Language, Artificial Intelligence, Machine Learning","Multimodal knowledge graph completion (MMKGC) aims to predict missing links in multimodal knowledge graphs (MMKGs) by leveraging information from various modalities alongside structural data. Existing MMKGC approaches primarily extend traditional knowledge graph embedding (KGE) models, which often require creating an embedding for every entity. This results in large model sizes and inefficiencies in integrating multimodal information, particularly for real-world graphs. Meanwhile, Transformer-based models have demonstrated competitive performance in knowledge graph completion (KGC). However, their focus on single-modal knowledge limits their capacity to utilize cross-modal information. Recently, Large vision-language models (VLMs) have shown potential in cross-modal tasks but are constrained by the high cost of training. In this work, we propose a novel approach that integrates Transformer-based KGE models with cross-modal context generated by pre-trained VLMs, thereby extending their applicability to MMKGC. Specifically, we employ a pre-trained VLM to transform relevant visual information from entities and their neighbors into textual sequences. We then frame KGC as a sequence-to-sequence task, fine-tuning the model with the generated cross-modal context. This simple yet effective method significantly reduces model size compared to traditional KGE approaches while achieving competitive performance across multiple large-scale datasets with minimal hyperparameter tuning."
609,679d459debd8ffd557a2b0ce,cs.CL,https://arxiv.org/pdf/2501.15674,TensorLLM: Tensorising Multi-Head Attention for Enhanced Reasoning and Compression in LLMs,"Yuxuan Gu, Wuyang Zhou, Giorgos Iacovides, Danilo Mandic","Computation and Language, Machine Learning","The reasoning abilities of Large Language Models (LLMs) can be improved by structurally denoising their weights, yet existing techniques primarily focus on denoising the feed-forward network (FFN) of the transformer block, and can not efficiently utilise the Multi-head Attention (MHA) block, which is the core of transformer architectures. To address this issue, we propose a novel intuitive framework that, at its very core, performs MHA compression through a multi-head tensorisation process and the Tucker decomposition. This enables both higher-dimensional structured denoising and compression of the MHA weights, by enforcing a shared higher-dimensional subspace across the weights of the multiple attention heads. We demonstrate that this approach consistently enhances the reasoning capabilities of LLMs across multiple benchmark datasets, and for both encoder-only and decoder-only architectures, while achieving compression rates of up to∼250similar-toabsent250\sim 250∼ 250times in the MHA weights, all without requiring any additional data, training, or fine-tuning. Furthermore, we show that the proposed method can be seamlessly combined with existing FFN-only-based denoising techniques to achieve further improvements in LLM reasoning performance."
610,679d459debd8ffd557a2b0cf,cs.CL,https://arxiv.org/pdf/2501.15654,People who frequently use ChatGPT for writing tasks are accurate and robust detectors of AI-generated text,"Jenna Russell, Marzena Karpinska, Mohit Iyyer","Computation and Language, Artificial Intelligence","In this paper, we study how wellhumanscan detect text generated by commercial LLMs (GPT-4o,Claude-3.5-Sonnet,o1-Pro). We hire annotators to read 300 non-fiction English articles, label them as either human-written or AI-generated, and provide paragraph-length explanations for their decisions. Our experiments show that annotators who frequently use LLMs for writing tasks excel at detecting AI-generated text, even without any specialized training or feedback. In fact, the majority vote among five such “expert” annotators misclassifies only 1 of 300 articles, significantly outperforming most commercial and open-source detectors we evaluated even in the presence of evasion tactics like paraphrasing and humanization.
Qualitative analysis of the experts’ free-form explanations shows that while they rely heavily on specific lexical clues (“AI vocabulary”), they also pick up on more complex phenomena within the text (e.g., formality, originality, clarity) that are challenging to assess for automatic detectors.
We release our annotated dataset and code to spur future research into both human and automated detection of AI-generated text."
611,679d459debd8ffd557a2b0d0,cs.CL,https://arxiv.org/pdf/2501.15630,Quantum-Enhanced Attention Mechanism in NLP: A Hybrid Classical-Quantum Approach,"S.M. Yousuf Iqbal Tomal, Abdullah Al Shafin, Debojit Bhattacharjee, MD. Khairul Amin, Rafiad Sadat Shahir","Computation and Language, Quantum Physics","Transformer-based models have achieved remarkable results in natural language processing (NLP) tasks such as text classification and machine translation. However, their computational complexity and resource demands pose challenges for scalability and accessibility. This research proposes a hybrid quantum-classical transformer model that integrates a quantum-enhanced attention mechanism to address these limitations. By leveraging quantum kernel similarity and variational quantum circuits (VQC), the model captures intricate token dependencies while improving computational efficiency. Experimental results on the IMDb dataset demonstrate that the quantum-enhanced model outperforms the classical baseline across all key metrics, achieving a 1.5% improvement in accuracy (65.5% vs. 64%), precision, recall, and F1 score. Statistical significance tests validate these improvements, highlighting the robustness of the quantum approach. These findings illustrate the transformative potential of quantum-enhanced attention mechanisms in optimizing NLP architectures for real-world applications."
612,679d459debd8ffd557a2b0d1,cs.CL,https://arxiv.org/pdf/2501.15624,Improving Estonian Text Simplification through Pretrained Language Models and Custom Datasets,"Eduard Barbu, Meeri-Ly Muru, Sten Marcus Malva",Computation and Language,"This study introduces an approach to Estonian text simplification using two model architectures: a neural machine translation model and a fine-tuned large language model (LLaMA). Given the limited resources for Estonian, we developed a new dataset, the Estonian Simplification Dataset, combining translated data and GPT-4.0-generated simplifications. We benchmarked OpenNMT, a neural machine translation model that frames text simplification as a translation task, and fine-tuned the LLaMA model on our dataset to tailor it specifically for Estonian simplification. Manual evaluations on the test set show that the LLaMA model consistently outperforms OpenNMT in readability, grammaticality, and meaning preservation. These findings underscore the potential of large language models for low-resource languages and provide a basis for further research in Estonian text simplification."
613,679d459debd8ffd557a2b0d2,cs.CL,https://arxiv.org/pdf/2501.15587,SCP-116K: A High-Quality Problem-Solution Dataset and a Generalized Pipeline for Automated Extraction in the Higher Education Science Domain,"Dakuan Lu, Xiaoyu Tan, Rui Xu, Tianchu Yao, Chao Qu, Wei Chu, Yinghui Xu, Yuan Qi","Computation and Language, Artificial Intelligence, Information Retrieval","Recent breakthroughs in large language models (LLMs)—exemplified by the impressive mathematical and scientific reasoning capabilities of the o1 model—have spotlighted the critical importance of high-quality training data in advancing LLM performance across STEM disciplines. While the mathematics community has benefited from a growing body of curated datasets, the scientific domain at the higher education level has long suffered from a scarcity of comparable resources. To address this gap, we present SCP-116K, a new large-scale dataset of 116,756 high-quality problem-solution pairs, automatically extracted from heterogeneous sources using a streamlined and highly generalizable pipeline. Our approach involves stringent filtering to ensure the scientific rigor and educational level of the extracted materials, while maintaining adaptability for future expansions or domain transfers. By openly releasing both the dataset and the extraction pipeline, we seek to foster research on scientific reasoning, enable comprehensive performance evaluations of new LLMs, and lower the barrier to replicating the successes of advanced models like o1 in the broader science community. We believe SCP-116K will serve as a critical resource, catalyzing progress in high-level scientific reasoning tasks and promoting further innovations in LLM development. The dataset and code are publicly available athttps://github.com/AQA6666/SCP-116K-open."
614,679d459debd8ffd557a2b0d3,cs.CL,https://arxiv.org/pdf/2501.15581,Error Classification of Large Language Models on Math Word Problems: A Dynamically Adaptive Framework,"Yuhong Sun, Zhangyue Yin, Xuanjing Huang, Xipeng Qiu, Hui Zhao",Computation and Language,"Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains.
Math Word Problems (MWPs) serve as a crucial benchmark for evaluating LLMs’ reasoning abilities.
While most research primarily focuses on improving accuracy, it often neglects understanding and addressing the underlying patterns of errors.
Current error classification methods rely on static and predefined categories, which limit their ability to capture the full spectrum of error patterns in mathematical reasoning.
To enable systematic error analysis, we collect error samples from 15 different LLMs of varying sizes across four distinct MWP datasets using multiple sampling strategies.
Based on this extensive collection, we introduce MWPES-300K, a comprehensive dataset containing 304,865 error samples that cover diverse error patterns and reasoning paths.
To reduce human bias and enable fine-grained analysis of error patterns, we propose a novel framework for automated dynamic error classification in mathematical reasoning.
Experimental results demonstrate that dataset characteristics significantly shape error patterns, which evolve from basic to complex manifestations as model capabilities increase.
With deeper insights into error patterns, we propose error-aware prompting that incorporates common error patterns as explicit guidance, leading to significant improvements in mathematical reasoning performance."
615,679d459debd8ffd557a2b0d4,cs.CL,https://arxiv.org/pdf/2501.15574,Instruction Tuning for Story Understanding and Generation with Weak Supervision,"Yangshu Yuan, Heng Chen, Christian Ng",Computation and Language,"Story understanding and generation have long been a challenging task in natural language processing (NLP), especially when dealing with various levels of instruction specificity. In this paper, we propose a novel approach called “Weak to Strong Instruction Tuning” for improving story generation by tuning models with instructions of varying clarity. We explore the potential of large language models (LLMs) to adapt to different types of instructions, weak and strong, and show that our method significantly enhances performance in story comprehension and generation. By leveraging the strength of instruction tuning, we train models to understand the nuances of story plots, characters, and themes while generating coherent and engaging narratives. Through extensive experiments on several benchmark datasets and comparison with state-of-the-art baselines, we demonstrate that our method outperforms existing techniques, yielding substantial improvements in both automatic evaluation metrics and human evaluations. Our work shows that adaptive instruction tuning can be a powerful tool in refining generative models for complex narrative tasks."
616,679d459debd8ffd557a2b0d5,cs.CL,https://arxiv.org/pdf/2501.15571,Cross-Cultural Fashion Design via Interactive Large Language Models and Diffusion Models,"Spencer Ramsey, Amina Grant, Jeffrey Lee",Computation and Language,"Fashion content generation is an emerging area at the intersection of artificial intelligence and creative design, with applications ranging from virtual try-on to culturally diverse design prototyping. Existing methods often struggle with cultural bias, limited scalability, and alignment between textual prompts and generated visuals, particularly under weak supervision. In this work, we propose a novel framework that integrates Large Language Models (LLMs) with Latent Diffusion Models (LDMs) to address these challenges. Our method leverages LLMs for semantic refinement of textual prompts and introduces a weak supervision filtering module to effectively utilize noisy or weakly labeled data. By fine-tuning the LDM on an enhanced DeepFashion+ dataset enriched with global fashion styles, the proposed approach achieves state-of-the-art performance. Experimental results demonstrate that our method significantly outperforms baselines, achieving lower Fréchet Inception Distance (FID) and higher Inception Scores (IS), while human evaluations confirm its ability to generate culturally diverse and semantically relevant fashion content. These results highlight the potential of LLM-guided diffusion models in driving scalable and inclusive AI-driven fashion innovation."
617,679d459debd8ffd557a2b0d6,cs.CL,https://arxiv.org/pdf/2501.15570,"ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language Model Born from Transformer","Lin Yueyu, Li Zhiyuan, Peter Yue, Liu Xiao",Computation and Language,"As is known, hybrid quadratic and subquadratic attention models in multi-head architectures have surpassed both Transformer and Linear RNN modelsDong et al. (2024), with these works primarily focusing on reducing KV complexity and improving efficiency. For further research on expressiveness***https://github.com/Jellyfish042/Sudoku-RWKV, https://github.com/Jellyfish042/RWKV_Othello, we introduce our series of models distilled from Qwen 2.5, based on pure native RWKV-7 attention, which aims to make RNN more expressive and demonstrates state tracking ability beyond transformers. We work with QRWK 32B†††https://huggingface.co/recursal/QRWKV6-32B-Instruct-Preview-v0.1based on RWKV-6 architecture, another approach that reduces the entire knowledge processing time to just 8 hours using 16 AMD MI300X GPUs while maintaining Qwen 2.5’s performance. In fact, the distillation process can utilize any LLM, not just Qwen, and enables knowledge transfer from larger LLMs to smaller ones with more fewer tokens. We will explain the detailed process and share our insights on building more powerful foundation models. Please note that this is an ongoing work that will be updated continuously. The model checkpoints and source code are available athttps://github.com/yynil/RWKVInside,https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1."
618,679d459debd8ffd557a2b0d7,cs.CL,https://arxiv.org/pdf/2501.15487,Multilevel Browsing of Folksonomy-Based Digital Collections,"Joaquín Gayoso-Cabada, Daniel Rodríguez-Cerezo, José-Luis Sierra",Computation and Language,
619,679d459debd8ffd557a2b0d8,cs.CL,https://arxiv.org/pdf/2501.15481,Query-based versus resource-based cache strategies in tag-based browsing systems,"Joaquín Gayoso-Cabada, Mercedes Gómez-Albarrán, José-Luis Sierra",Computation and Language,
620,679d459debd8ffd557a2b0d9,cs.CL,https://arxiv.org/pdf/2501.15453,Data-adaptive Safety Rules for Training Reward Models,"Xiaomin Li, Mingye Gao, Zhiwei Zhang, Jingxuan Fan, Weiyu Li",Computation and Language,
621,679d459debd8ffd557a2b0da,cs.CL,https://arxiv.org/pdf/2501.15451,STATE ToxiCN: A Benchmark for Span-level Target-Aware Toxicity Extraction in Chinese Hate Speech Detection,"Zewen Bai, Yuanyuan Sun, Shengdi Yin, Junyu Lu, Jingjie Zeng, Haohao Zhu, Liang Yang, Hongfei Lin",Computation and Language,"The proliferation of hate speech has caused significant harm to society. The intensity and directionality of hate are closely tied to the target and argument it is associated with. However, research on hate speech detection in Chinese has lagged behind, and existing datasets lack span-level fine-grained annotations. Furthermore, the lack of research on Chinese hateful slang poses a significant challenge. In this paper, we provide a solution for fine-grained detection of Chinese hate speech. First, we construct a dataset containing Target-Argument-Hateful-Group quadruples (STATE ToxiCN), which is the first span-level Chinese hate speech dataset. Secondly, we evaluate the span-level hate speech detection performance of existing models usingSTATE ToxiCN. Finally, we conduct the first study on Chinese hateful slang and evaluate the ability of LLMs to detect such expressions. Our work contributes valuable resources and insights to advance span-level hate speech detection in Chinese.111Code and datasets are publicly available at https://github.com/shenmeyemeifashengguo/STATE-ToxiCN."
622,679d459debd8ffd557a2b0db,cs.CL,https://arxiv.org/pdf/2501.15446,Token Democracy: The Architectural Limits of Alignment in Transformer-Based Language Models,Robin Young,"Computation and Language, Artificial Intelligence","Modern language models paradoxically combine unprecedented capability with persistent vulnerability in that they can draft poetry yet cannot reliably refuse harmful requests. We reveal this fragility stems not from inadequate training, but from a fundamental architectural limitation: transformers process all tokens as equals. Transformers operate as computational democracies, granting equal voice to all tokens. This is a design tragically unsuited for AGI, where we cannot risk adversarial ”candidates” hijacking the system. Through formal analysis, we demonstrate that safety instructions fundamentally lack privileged status in transformer architectures, that they compete with adversarial inputs in the same computational arena, making robust alignment through prompting or fine-tuning inherently limited. This ”token democracy” explains why jailbreaks bypass even extensively safety-trained models and why positional shifts erode prompt effectiveness. Our work systematizes practitioners’ tacit knowledge into an architectural critique, showing current alignment approaches create mere preferences, not constraints."
623,679d459debd8ffd557a2b0dc,cs.CL,https://arxiv.org/pdf/2501.15430,Evaluating Simple Debiasing Techniques in RoBERTa-based Hate Speech Detection Models,"Diana Iftimie, Erik Zinn",Computation and Language,"The hate speech detection task is known to suffer from bias against African American English (AAE) dialect text, due to the annotation bias present in the underlying hate speech datasets used to train these models. This leads to a disparity where normal AAE text is more likely to be misclassified as abusive/hateful compared to non-AAE text. Simple debiasing techniques have been developed in the past to counter this sort of disparity, and in this work, we apply and evaluate these techniques in the scope of RoBERTa-based encoders. Experimental results suggest that the success of these techniques depends heavily on the methods used for training dataset construction, but with proper consideration of representation bias, they can reduce the disparity seen among dialect subgroups on the hate speech detection task.Original Report Publication: December 2020"
624,679d459debd8ffd557a2b0dd,cs.CL,https://arxiv.org/pdf/2501.15427,OpenCharacter: Training Customizable Role-Playing LLMs with Large-Scale Synthetic Personas,"Xiaoyang Wang, Hongming Zhang, Tao Ge, Wenhao Yu, Dian Yu, Dong Yu",Computation and Language,"Customizable role-playing in large language models (LLMs), also known as character generalization, is gaining increasing attention for its versatility and cost-efficiency in developing and deploying role-playing dialogue agents.
This study explores a large-scale data synthesis approach to equip LLMs with character generalization capabilities.
We begin by synthesizing large-scale character profiles using personas from Persona Hub and then explore two strategies: response rewriting and response generation, to create character-aligned instructional responses.
To validate the effectiveness of our synthetic instruction tuning data for character generalization, we perform supervised fine-tuning (SFT) using the LLaMA-3 8B model. Our best-performing model strengthens the original LLaMA-3 8B Instruct model and achieves performance comparable to GPT-4o models on role-playing dialogue. We release111https://huggingface.co/datasets/xywang1/OpenCharacterour synthetic characters and instruction-tuning dialogues to support public research."
625,679d459debd8ffd557a2b0de,cs.CL,https://arxiv.org/pdf/2501.15405,Semantic Layered Embedding Diffusion in Large Language Models for Multi-Contextual Consistency,"Irin Kabakum, Thomas Montgomery, Daniel Ravenwood, Genevieve Harrington","Computation and Language, Artificial Intelligence","The Semantic Layered Embedding Diffusion (SLED) mechanism redefines the representation of hierarchical semantics within transformer-based architectures, enabling enhanced contextual consistency across a wide array of linguistic tasks. By introducing a multi-layered diffusion process grounded in spectral analysis, it achieves a complex balance between global and local semantic coherence. Experimental results demonstrate significant improvements in perplexity and BLEU scores, emphasizing the mechanism’s ability to adapt effectively across diverse domains, including multilingual and cross-domain text generation. A rigorous mathematical framework underpins the embedding diffusion process, incorporating weighted adjacency matrices, kernel-based refinements, and dynamic layer-wise normalization. Error distribution analysis reveals that SLED addresses challenges in semantic alignment and coherence, outperforming baseline approaches across varied benchmarks. Scalability studies illustrate that its performance gains are maintained consistently across different model sizes, reflecting a practical balance between computational efficiency and linguistic precision. The implementation also achieves energy efficiency, reducing resource consumption during training and inference phases without compromising accuracy. Qualitative case studies further validate its adaptability to extended narratives and context-intensive scenarios, highlighting the mechanism’s potential for real-world applications. SLED offers a different perspective on embedding design and its implications for advancing language modeling."
626,679d459debd8ffd557a2b0df,cs.CL,https://arxiv.org/pdf/2501.15398,How Green are Neural Language Models? Analyzing Energy Consumption in Text Summarization Fine-tuning,"Tohida Rehman, Debarshi Kumar Sanyal, Samiran Chattopadhyay",Computation and Language,"Artificial intelligence systems significantly impact the environment, particularly in natural language processing (NLP) tasks. These tasks often require extensive computational resources to train deep neural networks, including large-scale language models containing billions of parameters. This study analyzes the trade-offs between energy consumption and performance across three neural language models: two pre-trained models (T5-base and BART-base), and one large language model (LLaMA 3-8B). These models were fine-tuned for the text summarization task, focusing on generating research paper highlights that encapsulate the core themes of each paper. A wide range of evaluation metrics, including ROUGE, METEOR, MoverScore, BERTScore, and SciBERTScore, were employed to assess their performance. Furthermore, the carbon footprint associated with fine-tuning each model was measured, offering a comprehensive assessment of their environmental impact.
This research underscores the importance of incorporating environmental considerations into the design and implementation of neural language models and calls for the advancement of energy-efficient AI methodologies."
627,679d459debd8ffd557a2b0e0,cs.CL,https://arxiv.org/pdf/2501.15383,Qwen2.5-1M Technical Report,"An Yang, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang, Jianhong Tu, Jianwei Zhang, Jingren Zhou, Junyang Lin, Kai Dang, Kexin Yang, Le Yu, Mei Li, Minmin Sun, Qin Zhu, Rui Men, Tao He, Weijia Xu, Wenbiao Yin, Wenyuan Yu, Xiafei Qiu, Xingzhang Ren, Xinlong Yang, Yong Li, Zhiying Xu, Zipeng Zhang",Computation and Language,
628,679d459debd8ffd557a2b0e1,cs.CL,https://arxiv.org/pdf/2501.15374,Evaluating the Effectiveness of XAI Techniques for Encoder-Based Language Models,"Melkamu Abay Mersha, Mesay Gemeda Yigezu, Jugal Kalita","Computation and Language, Artificial Intelligence","The black-box nature of large language models (LLMs) necessitates the development of eXplainable AI (XAI) techniques for transparency and trustworthiness. However, evaluating these techniques remains a challenge. This study presents a general evaluation framework using four key metrics: Human-reasoning Agreement (HA), Robustness, Consistency, and Contrastivity. We assess the effectiveness of six explainability techniques from five different XAI categories—model simplification (LIME), perturbation-based methods (SHAP), gradient-based approaches (InputXGradient, Grad-CAM), Layer-wise Relevance Propagation (LRP), and attention mechanisms-based explainability methods (Attention Mechanism Visualization, AMV)—across five encoder-based language models: TinyBERT, BERTbase, BERTlarge, XLM-R large, and DeBERTa-xlarge, using the IMDB Movie Reviews and Tweet Sentiment Extraction (TSE) datasets. Our findings show that the model simplification-based XAI method (LIME) consistently outperforms across multiple metrics and models, significantly excelling in HA with a score of 0.9685 on DeBERTa-xlarge, robustness, and consistency as the complexity of large language models increases. AMV demonstrates the best Robustness, with scores as low as 0.0020. It also excels in Consistency, achieving near-perfect scores of 0.9999 across all models. Regarding Contrastivity, LRP performs the best, particularly on more complex models, with scores up to 0.9371."
629,679d459debd8ffd557a2b0e2,cs.CL,https://arxiv.org/pdf/2501.15368,Baichuan-Omni-1.5 Technical Report,"Yadong Li, Jun Liu, Tao Zhang, Tao Zhang, Song Chen, Tianpeng Li, Zehuan Li, Lijun Liu, Lingfeng Ming, Guosheng Dong, Da Pan, Chong Li, Yuanbo Fang, Dongdong Kuang, Mingrui Wang, Chenglin Zhu, Youwei Zhang, Hongyu Guo, Fengyu Zhang, Yuran Wang, Bowen Ding, Wei Song, Xu Li, Yuqi Huo, Zheng Liang, Shusen Zhang, Xin Wu, Shuai Zhao, Linchu Xiong, Yozhen Wu, Jiahui Ye, Wenhao Lu, Bowen Li, Yan Zhang, Yaqi Zhou, Xin Chen, Lei Su, Hongda Zhang, Fuzhong Chen, Xuezhen Dong, Na Nie, Zhiying Wu, Bin Xiao, Ting Li, Shunya Dang, Ping Zhang, Yijia Sun, Jincheng Wu, Jinjie Yang, Xionghai Lin, Zhi Ma, Kegeng Wu, Jia li, Aiyuan Yang, Hui Liu, Jianqiang Zhang, Xiaoxi Chen, Guangwei Ai, Wentao Zhang, Yicong Chen, Xiaoqin Huang, Kun Li, Wenjing Luo, Yifei Duan, Lingling Zhu, Ran Xiao, Zhe Su, Jiani Pu, Dian Wang, Xu Jia, Tianyu Zhang, Mengyu Ai, Mang Wang, Yujing Qiao, Lei Zhang, Yanjun Shen, Fan Yang, Miao Zhen, Yijie Zhou, Mingyang Chen, Fei Li, Chenzheng Zhu, Keer Lu, Yaqi Zhao, Hao Liang, Youquan Li, Yanzhao Qin, Linzhuang Sun, Jianhua Xu, Haoze Sun, Mingan Lin, Zenan Zhou, Weipeng Chen","Computation and Language, Sound, Audio and Speech Processing","We introduceBaichuan-Omni-1.5, an omni-modal model that not only has omni-modal understanding capabilities but also provides end-to-end audio generation capabilities. To achieve fluent and high-quality interaction across modalities without compromising the capabilities of any modality, we prioritized optimizing three key aspects. First, we establish a comprehensive data cleaning and synthesis pipeline for multimodal data, obtaining about 500B high-quality data (text, audio, and vision). Second, an audio-tokenizer (Baichuan-Audio-Tokenizer) has been designed to capture both semantic and acoustic information from audio, enabling seamless integration and enhanced compatibility with MLLM. Lastly, we designed a multi-stage training strategy that progressively integrates multimodal alignment and multitask fine-tuning, ensuring effective synergy across all modalities. Baichuan-Omni-1.5 leads contemporary models (including GPT4o-mini and MiniCPM-o 2.6) in terms of comprehensive omni-modal capabilities. Notably, it achieves results comparable to leading models such as Qwen2-VL-72B across various multimodal medical benchmarks."
630,679d459debd8ffd557a2b0e3,cs.CL,https://arxiv.org/pdf/2501.15355,Large Language Models as Theory of Mind Aware Generative Agents with Counterfactual Reflection,"Bo Yang, Jiaxian Guo, Yusuke Iwasawa, Yutaka Matsuo","Computation and Language, Artificial Intelligence","Recent studies have increasingly demonstrated that large language models (LLMs) possess significanttheory of mind (ToM)capabilities, showing the potential for simulating the tracking of mental states in generative agents.
In this study, we propose a novel paradigm calledToM-agent, designed to empower LLMs-based generative agents to simulate ToM in open-domain conversational interactions.
ToM-agent disentangles the confidence from mental states, facilitating the emulation of an agent’s perception of its counterpart’s mental states, such asbeliefs, desires, and intentions (BDIs).
Using past conversation history and verbal reflections, ToM-Agent can dynamically adjust counterparts’ inferred BDIs, along with related confidence levels.
We further put forth a counterfactual intervention method that reflects on the gap between the predicted responses of counterparts and their real utterances, thereby enhancing the efficiency of reflection.
Leveraging empathetic and persuasion dialogue datasets, we assess the advantages of implementing the ToM-agent with downstream tasks, as well as its performance in both thefirst-orderand thesecond-orderToM.
Our findings indicate that the ToM-agent can grasp the underlying reasons for their counterpart’s behaviors beyond mere semantic-emotional supporting or decision-making based on common sense, providing new insights for studying large-scale LLMs-based simulation of human social behaviors.The codes of this project will be made publicly available after the paper acceptance."
631,679d459debd8ffd557a2b0e4,cs.CL,https://arxiv.org/pdf/2501.15321,Figurative-cum-Commonsense Knowledge Infusion for Multimodal Mental Health Meme Classification,"Abdullah Mazhar, Zuhair hasan shaik, Aseem Srivastava, Polly Ruhnke, Lavanya Vaddavalli, Sri Keshav Katragadda, Shweta Yadav, Md Shad Akhtar","Computation and Language, Social and Information Networks","The expression of mental health symptoms through non-traditional means, such as memes, has gained remarkable attention over the past few years, with users often highlighting their mental health struggles through figurative intricacies within memes. While humans rely on commonsense knowledge to interpret these complex expressions, current Multimodal Language Models (MLMs) struggle to capture these figurative aspects inherent in memes. To address this gap, we introduce a novel dataset,AxiOM, derived from the GAD anxiety questionnaire, which categorizes memes into six fine-grained anxiety symptoms. Next, we propose a commonsense and domain-enriched framework,M3H, to enhance MLMs’ ability to interpret figurative language and commonsense knowledge. The overarching goal remains to first understand and then classify the mental health symptoms expressed in memes. We benchmarkM3Hagainst6666competitive baselines (with20202020variations), demonstrating improvements in both quantitative and qualitative metrics, including a detailed human evaluation. We observe a clear improvement of4.20%percent4.204.20\%4.20 %and4.66%percent4.664.66\%4.66 %on weighted-F1 metric. To assess the generalizability, we perform extensive experiments on a public dataset,RESTORE, for depressive symptom identification, presenting an extensive ablation study that highlights the contribution of each module in both datasets. Our findings reveal limitations in existing models and the advantage of employing commonsense to enhance figurative understanding."
632,679d459debd8ffd557a2b0e5,cs.CL,https://arxiv.org/pdf/2501.15310,The Multicultural Medical Assistant: Can LLMs Improve Medical ASR Errors Across Borders?,"Ayo Adedeji, Mardhiyah Sanni, Emmanuel Ayodele, Sarita Joshi, Tobi Olatunji","Computation and Language, Sound, Audio and Speech Processing","The global adoption of Large Language Models (LLMs) in healthcare shows promise to enhance clinical workflows and improve patient outcomes. However, Automatic Speech Recognition (ASR) errors in critical medical terms remain a significant challenge. These errors can compromise patient care and safety if not detected. This study investigates the prevalence and impact of ASR errors in medical transcription in Nigeria, the United Kingdom, and the United States. By evaluating raw and LLM-corrected transcriptions of accented English in these regions, we assess the potential and limitations of LLMs to address challenges related to accents and medical terminology in ASR. Our findings highlight significant disparities in ASR accuracy across regions and identify specific conditions under which LLM corrections are most effective."
633,679d459debd8ffd557a2b0e6,cs.CL,https://arxiv.org/pdf/2501.15296,You Only Prune Once: Designing Calibration-Free Model Compression With Policy Learning,"Ayan Sengupta, Siddhant Chaudhary, Tanmoy Chakraborty",Computation and Language,"The ever-increasing size of large language models (LLMs) presents significant challenges for deployment
due to their heavy computational and memory requirements. Current model pruning techniques attempt to alleviate these issues by relying heavily on external calibration datasets to determine which parameters to prune or compress, thus limiting their flexibility and scalability across different compression ratios. Moreover, these methods often cause severe performance degradation, particularly in downstream tasks, when subjected to higher compression rates. In this paper, we proposePruneNet, a novel model compression method that addresses these limitations by reformulating model pruning as a policy learning process.PruneNetdecouples the pruning process from the model architecture, eliminating the need for calibration datasets. It learns a stochastic pruning policy to assess parameter importance solely based on intrinsic model properties while preserving the spectral structure to minimize information loss.PruneNetcan compress the LLaMA-2-7B model in just 15 minutes, achieving over 80% retention of its zero-shot performance with a 30% compression ratio, outperforming existing methods that retain only 75% performance. Furthermore, on complex multitask language understanding tasks,PruneNetdemonstrates its robustness by preserving up to 80% performance of the original model, proving itself a superior alternative to conventional structured compression techniques."
634,679d459debd8ffd557a2b0e7,cs.CL,https://arxiv.org/pdf/2501.15283,Are Human Interactions Replicable by Generative Agents? A Case Study on Pronoun Usage in Hierarchical Interactions,"Naihao Deng, Rada Mihalcea",Computation and Language,"Recently, researchers have increasingly employed Large Language Models (LLMs) for social simulations.
In this paper, we explore whetherinteractionsamong LLM agents exhibit human-like behaviors, with a specific focus on pronoun usage differences between leaders and non-leaders.
Our evaluation uncovers significant discrepancies that neither prompt-based configurations nor specialized agent setups successfully replicate human-like pronoun usage patterns.
Furthermore, we demonstrate that even when LLMs possess an understanding of these patterns, they fail to exhibit them in their interactions.
Our study highlights the limitations of LLM-based social simulations and underscores the need for caution when relying on such simulations for understanding or modeling human social behaviors."
635,679d459debd8ffd557a2b0e8,cs.CL,https://arxiv.org/pdf/2501.15281,Pre-training a Transformer-Based Generative Model Using a Small Sepedi Dataset,"Simon P. Ramalepe, Thipe I. Modipa, Marelie H. Davel","Computation and Language, Artificial Intelligence, Machine Learning","Due to the scarcity of data in low-resourced languages, the development of language models for these languages has been very slow. Currently, pre-trained language models have gained popularity in natural language processing, especially, in developing domain-specific models for low-resourced languages.
In this study, we experiment with the impact of using occlusion-based techniques when training a language model for a text generation task. We curate 2 new datasets, the Sepedi monolingual (SepMono) dataset from several South African resources and the Sepedi radio news (SepNews) dataset from the radio news domain. We use the SepMono dataset to pre-train transformer-based models using the occlusion and non-occlusion pre-training techniques and compare performance. The SepNews dataset is specifically used for fine-tuning. Our results show that the non-occlusion models perform better compared to the occlusion-based models when measuring validation loss and perplexity. However, analysis of the generated text using the BLEU score metric, which measures the quality of the generated text, shows a slightly higher BLEU score for the occlusion-based models compared to the non-occlusion models."
636,679d459debd8ffd557a2b0e9,cs.CL,https://arxiv.org/pdf/2501.15268,New Evaluation Paradigm for Lexical Simplification,"Jipeng Qiang, Minjiang Huang, Yi Zhu, Yunhao Yuan, Chaowei Zhang, Xiaoye Ouyang",Computation and Language,"Lexical Simplification (LS) methods use a three-step pipeline: complex word identification, substitute generation, and substitute ranking, each with separate evaluation datasets. We found large language models (LLMs) can simplify sentences directly with a single prompt, bypassing the traditional pipeline. However, existing LS datasets are not suitable for evaluating these LLM-generated simplified sentences, as they focus on providing substitutes for single complex words without identifying all complex words in a sentence."
637,679d459debd8ffd557a2b0ea,cs.CL,https://arxiv.org/pdf/2501.15260,Breaking the Stigma! Unobtrusively Probe Symptoms in Depression Disorder Diagnosis Dialogue,"Jieming Cao, Chen Huang, Yanan Zhang, Ruibo Deng, Jincheng Zhang, Wenqiang Lei","Computation and Language, Computers and Society","Stigma has emerged as one of the major obstacles to effectively diagnosing depression, as it prevents users from open conversations about their struggles. This requires advanced questioning skills to carefully probe the presence of specific symptoms in an unobtrusive manner.
While recent efforts have been made on depression-diagnosis-oriented dialogue systems, they largely ignore this problem, ultimately hampering their practical utility.
To this end, we propose a novel and effective method, UPSD4, developing a series of strategies to promote a sense of unobtrusiveness within the dialogue system and assessing depression disorder by probing symptoms. We experimentally show that UPSD4demonstrates a significant improvement over current baselines, including unobtrusiveness evaluation of dialogue content and diagnostic accuracy.
We believe our work contributes to developing more accessible and user-friendly tools for addressing the widespread need for depression diagnosis."
638,679d459debd8ffd557a2b0eb,cs.CL,https://arxiv.org/pdf/2501.15247,Prompting ChatGPT for Chinese Learning as L2: A CEFR and EBCL Level Study,"Miao Lin-Zucker, Joël Bellassen, Jean-Daniel Zucker","Computation and Language, Artificial Intelligence",
639,679d459debd8ffd557a2b0ec,cs.CL,https://arxiv.org/pdf/2501.15245,ASRank: Zero-Shot Re-Ranking with Answer Scent for Document Retrieval,"Abdelrahman Abdallah, Jamshid Mozafari, Bhawna Piryani, Adam Jatowt",Computation and Language,"Retrieval-Augmented Generation (RAG) models have drawn considerable attention in modern open-domain question answering. The effectiveness of RAG depends on the quality of the top retrieved documents. However, conventional retrieval methods sometimes fail to rank the most relevant documents at the top. In this paper, we introduceASRank111https://github.com/DataScienceUIBK/rankify, a new re-ranking method based on scoring retrieved documents using zero-shot answer scent which relies on a pre-trained large language model to compute the likelihood of the document-derived answers aligning with the answer scent. Our approach demonstrates marked improvements across several datasets, including NQ, TriviaQA, WebQA, ArchivalQA, HotpotQA, and Entity Questions. Notably,ASRankincreases Top-1 retrieval accuracy on NQ from19.2%percent19.219.2\%19.2 %to46.5%percent46.546.5\%46.5 %for MSS and22.1%percent22.122.1\%22.1 %to47.3%percent47.347.3\%47.3 %for BM25. It also shows strong retrieval performance on several datasets compared to state-of-the-art methods (47.3 Top-1 byASRankvs 35.4 by UPR by BM25)."
640,679d459debd8ffd557a2b0ed,cs.CL,https://arxiv.org/pdf/2501.15228,Improving Retrieval-Augmented Generation through Multi-Agent Reinforcement Learning,"Yiqun Chen, Lingyong Yan, Weiwei Sun, Xinyu Ma, Yi Zhang, Shuaiqiang Wang, Dawei Yin, Yiming Yang, Jiaxin Mao","Computation and Language, Information Retrieval","Retrieval-augmented generation (RAG) is extensively utilized to incorporate external, current knowledge into large language models, thereby minimizing hallucinations. A standard RAG pipeline may comprise several components, such as query rewriting, document retrieval, document filtering, and answer generation. However, these components are typically optimized separately through supervised fine-tuning, which can lead to misalignments between the objectives of individual modules and the overarching aim of generating accurate answers in question-answering (QA) tasks. Although recent efforts have explored reinforcement learning (RL) to optimize specific RAG components, these approaches often focus on overly simplistic pipelines with only two components or do not adequately address the complex interdependencies and collaborative interactions among the modules. To overcome these challenges, we propose treating the RAG pipeline as a multi-agent cooperative task, with each component regarded as an RL agent. Specifically, we present MMOA-RAG, a Multi-Module joint Optimization Algorithm for RAG, which employs multi-agent reinforcement learning to harmonize all agents’ goals towards a unified reward, such as the F1 score of the final answer. Experiments conducted on various QA datasets demonstrate that MMOA-RAG improves the overall pipeline performance and outperforms existing baselines. Furthermore, comprehensive ablation studies validate the contributions of individual components and the adaptability of MMOA-RAG across different RAG components and datasets111The code of MMOA-RAG is onhttps://github.com/chenyiqun/MMOA-RAG.."
641,679d459debd8ffd557a2b0ee,cs.CL,https://arxiv.org/pdf/2501.15225,SEAL: Scaling to Emphasize Attention for Long-Context Retrieval,"Changhun Lee, Jun-gyu Jin, Younghyun Cho, Eunhyeok Park","Computation and Language, Artificial Intelligence, Machine Learning","In this work, we introduce a novel approach called Scaling to Emphasize Attention for Long-context retrieval (SEAL), which enhances the retrieval performance of large language models (LLMs) over extended contexts. Previous studies have shown that each attention head in LLMs has a unique functionality and collectively contributes to the overall behavior of the model. Similarly, we observe that specific heads are closely tied to long-context retrieval, showing positive or negative correlation with retrieval scores. Built on this insight, we propose a learning-based mechanism using zero-shot generated data to emphasize these heads, improving the model’s performance in long-context retrieval tasks.
By applying SEAL, we can achieve significant improvements in in-domain retrieval performance, including document QA tasks from LongBench, and considerable improvements in out-of-domain cases.
Additionally, when combined with existing training-free context extension techniques, SEAL extends the context limits of LLMs while maintaining highly reliable outputs, opening new avenues for research in this field."
642,679d459debd8ffd557a2b0ef,cs.CL,https://arxiv.org/pdf/2501.15219,Faster Machine Translation Ensembling with Reinforcement Learning and Competitive Correction,"Kritarth Prasad, Mohammadi Zaki, Pratik Singh, Pankaj Wasnik",Computation and Language,"Ensembling neural machine translation (NMT) models to produce higher-quality translations than theL𝐿Litalic_Lindividual models has been extensively studied. Recent methods typically employ a candidate selection block (CSB) and an encoder-decoder fusion block (FB), requiring inference acrossallcandidate models, leading to significant computational overhead, generallyΩ⁢(L)Ω𝐿\Omega(L)roman_Ω ( italic_L ). This paper introducesSmartGen, a reinforcement learning (RL)-based strategy that improves the CSB by selecting a small, fixed number of candidates and identifying optimal groups to pass to the fusion block for each input sentence. Furthermore, previously, the CSB and FB were trained independently, leading to suboptimal NMT performance. Our DQN-basedSmartGenaddresses this by using feedback from the FB block as a reward during training. We also resolve a key issue in earlier methods, where candidates were passed to the FB without modification, by introducing a Competitive Correction Block (CCB). Finally, we validate our approach with extensive experiments on English-Hindi translation tasks in both directions."
643,679d459debd8ffd557a2b0f0,cs.CL,https://arxiv.org/pdf/2501.15188,Who is the root in a syntactic dependency structure?,"Ramon Ferrer-i-Cancho, Marta Arias","Computation and Language, Social and Information Networks, Physics and Society",
644,679d459debd8ffd557a2b0f1,cs.CL,https://arxiv.org/pdf/2501.15175,Option-ID Based Elimination For Multiple Choice Questions,"Zhenhao Zhu, Bulou Liu","Computation and Language, Artificial Intelligence, Machine Learning","Multiple choice questions (MCQs) are a common and important task for evaluating large language models (LLMs). Based on common strategies humans use when answering MCQs, the process of elimination has been proposed as an effective problem-solving method. Existing methods to the process of elimination generally fall into two categories: one involves having the model directly select the incorrect answer, while the other involves scoring the options. However, both methods incur high computational costs and often perform worse than methods that answer based on option ID. To address this issue, this paper proposes a process of elimination based on option ID. We select 10 LLMs and conduct zero-shot experiments on 7 different datasets. The experimental results demonstrate that our method significantly improves the model’s performance. Further analysis reveals that the sequential elimination strategy can effectively enhance the model’s reasoning ability. Additionally, we find that sequential elimination is also applicable to few-shot settings and can be combined with debias methods to further improve model performance."
645,679d459debd8ffd557a2b0f2,cs.CL,https://arxiv.org/pdf/2501.15113,Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads,"Xingyang He, Jie Liu, Shaowei Chen",Computation and Language,"KV cache is a widely used acceleration technique for large language models (LLMs) inference. However, its memory requirement grows rapidly with input length.
Previous studies have reduced the size of KV cache by either removing the same number of unimportant tokens for all attention heads or by allocating differentiated KV cache budgets for pre-identified attention heads.
However, due to the importance of attention heads varies across different tasks, the pre-identified attention heads fail to adapt effectively to various downstream tasks. To address this issue, we propose Task-KV, a method that leverages the semantic differentiation of attention heads to allocate differentiated KV cache budgets across various tasks. We demonstrate that attention heads far from the semantic center (called heterogeneous heads) make an significant contribution to task outputs and semantic understanding. In contrast, other attention heads play the role of aggregating important information and focusing reasoning. Task-KV allocates full KV cache budget to heterogeneous heads to preserve comprehensive semantic information, while reserving a small number of recent tokens and attention sinks for non-heterogeneous heads. Furthermore, we innovatively introduce middle activations to preserve key contextual information aggregated from non-heterogeneous heads. To dynamically perceive semantic differences among attention heads, we design a semantic separator to distinguish heterogeneous heads from non-heterogeneous ones based on their distances from the semantic center. Experimental results on multiple benchmarks and different model architectures demonstrate that Task-KV significantly outperforms existing baseline methods. Notably, in scenarios requiring full-context processing, such as summarization and synthetic tasks, Task-KV achieves performance comparable to the full KV cache while utilizing only 40% of the memory."
646,679d459debd8ffd557a2b0f3,cs.CL,https://arxiv.org/pdf/2501.15108,Knowledge Hierarchy Guided Biological-Medical Dataset Distillation for Domain LLM Training,"Xunxin Cai, Chengrui Wang, Qingqing Long, Yuanchun Zhou, Meng Xiao",Computation and Language,"The rapid advancement of large language models (LLMs) in biological-medical applications has highlighted a gap between their potential and the limited scale and often low quality of available open-source annotated textual datasets.
In addition, the inherent complexity of the biomedical knowledge hierarchy significantly hampers efforts to bridge this gap.
Can LLMs themselves play a pivotal role in overcoming this limitation?
Motivated by this question, we investigate this challenge in the present study.
We propose a framework that automates the distillation of high-quality textual training data from the extensive scientific literature.
Our approach self-evaluates and generates questions that are more closely aligned with the biomedical domain, guided by the biomedical knowledge hierarchy through medical subject headings (MeSH).
This comprehensive framework establishes an automated workflow, thereby eliminating the need for manual intervention.
Furthermore, we conducted comprehensive experiments to evaluate the impact of our framework-generated data on downstream language models of varying sizes.
Our approach substantially improves question-answering tasks compared to pre-trained models from the life sciences domain and powerful close-source models represented by GPT-4.
Notably, the generated AI-Ready dataset enabled the Llama3-70B base model to outperform GPT-4 using MedPrompt with multiple times the number of parameters.
Detailed case studies and ablation experiments underscore the significance of each component within our framework111Our code is shared on Dropbox:link.."
647,679d459debd8ffd557a2b0f4,cs.CL,https://arxiv.org/pdf/2501.15090,Speech Translation Refinement using Large Language Models,"Huaixia Dou, Xinyu Tian, Xinglin Lyu, Jie Zhu, Junhui Li, Lifan Guo",Computation and Language,"Recent advancements in large language models (LLMs) have demonstrated their remarkable capabilities across various language tasks. Inspired by the success of text-to-text translation refinement, this paper investigates how LLMs can improve the performance of speech translation by introducing a joint refinement process. Through the joint refinement of speech translation (ST) and automatic speech recognition (ASR) transcription via LLMs, the performance of the ST model is significantly improved in both training-free in-context learning and parameter-efficient fine-tuning scenarios. Additionally, we explore the effect of document-level context on refinement under the context-aware fine-tuning scenario. Experimental results on the MuST-C and CoVoST 2 datasets, which include seven translation tasks, demonstrate the effectiveness of the proposed approach using several popular LLMs including GPT-3.5-turbo, LLaMA3-8B, and Mistral-12B. Further analysis further suggests that jointly refining both transcription and translation yields better performance compared to refining translation alone. Meanwhile, incorporating document-level context significantly enhances refinement performance. We release our code and datasets on GitHub111https://github.com/world1tree/SpeechTranslationRefinement."
648,679d459debd8ffd557a2b0f5,cs.CL,https://arxiv.org/pdf/2501.15089,LongReason: A Synthetic Long-Context Reasoning Benchmark via Context Expansion,"Zhan Ling, Kang Liu, Kai Yan, Yifan Yang, Weijian Lin, Ting-Han Fan, Lingfeng Shen, Zhengyin Du, Jiecao Chen",Computation and Language,"Large language models (LLMs) have demonstrated remarkable progress in understanding long-context inputs. However, benchmarks for evaluating the long-context reasoning abilities of LLMs fall behind the pace. Existing benchmarks often focus on a narrow range of tasks or those that do not demand complex reasoning. To address this gap and enable a more comprehensive evaluation of the long-context reasoning capabilities of current LLMs, we propose a new synthetic benchmark,LongReason, which is constructed by synthesizing long-context reasoning questions from a varied set of short-context reasoning questions through context expansion. LongReason consists of 794 multiple-choice reasoning questions with diverse reasoning patterns across three task categories: reading comprehension, logical inference, and mathematical word problems. We evaluate 21 LLMs on LongReason, revealing that most models experience significant performance drops as context length increases. Our further analysis shows that even state-of-the-art LLMs still have significant room for improvement in providing robust reasoning across different tasks. We will open-source LongReason to support the comprehensive evaluation of LLMs’ long-context reasoning capabilities."
649,679d459debd8ffd557a2b0f6,cs.CL,https://arxiv.org/pdf/2501.15063,Cross-modal Context Fusion and Adaptive Graph Convolutional Network for Multimodal Conversational Emotion Recognition,"Junwei Feng, Xueyan Fan",Computation and Language,"Emotion recognition has a wide range of applications in human-computer interaction, marketing, healthcare, and other fields. In recent years, the development of deep learning technology has provided new methods for emotion recognition. Prior to this, many emotion recognition methods have been proposed, including multimodal emotion recognition methods, but these methods ignore the mutual interference between different input modalities and pay little attention to the directional dialogue between speakers. Therefore, this article proposes a new multimodal emotion recognition method, including a cross modal context fusion module, an adaptive graph convolutional encoding module, and an emotion classification module. The cross modal context module includes a cross modal alignment module and a context fusion module, which are used to reduce the noise introduced by mutual interference between different input modalities. The adaptive graph convolution module constructs a dialogue relationship graph for extracting dependencies and self dependencies between speakers. Our model has surpassed some state-of-the-art methods on publicly available benchmark datasets and achieved high recognition accuracy."
650,679d459debd8ffd557a2b0f7,cs.CL,https://arxiv.org/pdf/2501.15054,An Attempt to Unraveling Token Prediction Refinement and Identifying Essential Layers of Large Language Models,Jaturong Kongmanee,"Computation and Language, Artificial Intelligence, Machine Learning","This research aims to unravel how large language models (LLMs) iteratively refine token predictions (or, in a general sense, vector predictions). We utilized a logit lens technique to analyze the model’s token predictions derived from intermediate representations. Specifically, we focused on how LLMs access and use information from input contexts, and how positioning of relevant information affects the model’s token prediction refinement process. Our findings for multi-document question answering task, by varying input context lengths (the number of documents), using GPT-2, revealed that the number of layers between the first layer that the model predicted next tokens correctly and the later layers that the model finalized its correct predictions, as a function of the position of relevant information (i.e., placing the relevant one at the beginning, middle, or end of the input context), has a nearly inverted U shape. We found that the gap between these two layers, on average, diminishes when relevant information is positioned at the beginning or end of the input context, suggesting that the model requires more refinements when processing longer contexts with relevant information situated in the middle, and highlighting which layers are essential for determining the correct output. Our analysis provides insights about how token predictions are distributed across different conditions, and establishes important connections to existing hypotheses and previous findings in AI safety research and development."
651,679d459debd8ffd557a2b0f8,cs.CL,https://arxiv.org/pdf/2501.15051,Abstractive Text Summarization for Bangla Language Using NLP and Machine Learning Approaches,"Asif Ahammad Miazee, Tonmoy Roy, Md Robiul Islam, Yeamin Safat",Computation and Language,"Text summarization involves reducing extensive documents to short sentences that encapsulate the essential ideas. The goal is to create a brief summary that effectively conveys the main points of the original text. We spend a significant amount of time each day reading the newspaper to stay informed about current events both domestically and internationally. While reading newspapers enriches our knowledge, we sometimes come across unnecessary content that isn’t particularly relevant to our lives. In this paper, we introduce a neural network model designed to summarize Bangla text into concise and straightforward paragraphs, aiming for greater stability and efficiency."
652,679d459debd8ffd557a2b0f9,cs.CL,https://arxiv.org/pdf/2501.15042,SCCD: A Session-based Dataset for Chinese Cyberbullying Detection,"Qingpo Yang, Yakai Chen, Zihui Xu, Yu-ming Shang, Sanchuan Guo, Xi Zhang",Computation and Language,"The rampant spread of cyberbullying content poses a growing threat to societal well-being. However, research on cyberbullying detection in Chinese remains underdeveloped, primarily due to the lack of comprehensive and reliable datasets. Notably, no existing Chinese dataset is specifically tailored for cyberbullying detection. Moreover, while comments play a crucial role within sessions, current session-based datasets often lack detailed, fine-grained annotations at the comment level. To address these limitations, we present a novel Chinese cyberbullying dataset, termedSCCD, which consists of 677 session-level samples sourced from a major social media platform Weibo. Moreover, each comment within the sessions is annotated with fine-grained labels rather than conventional binary class labels. Empirically, we evaluate the performance of various baseline methods onSCCD, highlighting the challenges for effective Chinese cyberbullying detection.111The proposed SCCD is released inhttps://github.com/STAIR-BUPT/SCCD."
653,679d459debd8ffd557a2b0fa,cs.CL,https://arxiv.org/pdf/2501.15022,Using Large Language Models for education managements in Vietnamese with low resources,"Duc Do Minh, Vinh Nguyen Van, Thang Dam Cong","Computation and Language, Artificial Intelligence","Large language models (LLMs), such as GPT-4, Gemini 1.5, Claude 3.5 Sonnet, and Llama3, have demonstrated significant advancements in various NLP tasks since the release of ChatGPT in 2022. Despite their success, fine-tuning and deploying LLMs remain computationally expensive, especially in resource-constrained environments. In this paper, we proposed VietEduFrame, a framework specifically designed to apply LLMs to educational management tasks in Vietnamese institutions. Our key contribution includes the development of a tailored dataset, derived from student education documents at Hanoi VNU, which addresses the unique challenges faced by educational systems with limited resources. Through extensive experiments, we show that our approach outperforms existing methods in terms of accuracy and efficiency, offering a promising solution for improving educational management in under-resourced environments. While our framework leverages synthetic data to supplement real-world examples, we discuss potential limitations regarding broader applicability and robustness in future implementations."
654,679d459debd8ffd557a2b0fb,cs.CL,https://arxiv.org/pdf/2501.15021,AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for Vision-Language Models,"Zunhai Su, Wang Shen, Linge Li, Zhe Chen, Hanyu Wei, Huangqi Yu, Kehong Yuan",Computation and Language,
655,679d459debd8ffd557a2b0fc,cs.CL,https://arxiv.org/pdf/2501.15000,MDEval: Evaluating and Enhancing Markdown Awareness in Large Language Models,"Zhongpu Chen, Yinfeng Liu, Long Shi, Zhi-Jie Wang, Xingyan Chen, Yu Zhao, Fuji Ren","Computation and Language, Information Retrieval","Large language models (LLMs) are expected to offer structured Markdown responses for the sake of readability in web chatbots (e.g., ChatGPT). Although there are a myriad of metrics to evaluate LLMs, they fail to evaluate the readability from the view of output content structure. To this end, we focus on an overlooked yet important metric —Markdown Awareness, which directly impacts the readability and structure of the content generated by these language models. In this paper, we introduce MDEval, a comprehensive benchmark to assessMarkdown Awarenessfor LLMs, by constructing a dataset with 20K instances covering 10 subjects in English and Chinese. Unlike traditional model-based evaluations, MDEval provides excellent interpretability by combining model-based generation tasks and statistical methods. Our results demonstrate that MDEval achieves a Spearman correlation of 0.791 and an accuracy of 84.1% with human, outperforming existing methods by a large margin. Extensive experimental results also show that through fine-tuning over our proposed dataset, less performant open-source models are able to achieve comparable performance to GPT-4o in terms ofMarkdown Awareness. To ensure reproducibility and transparency, MDEval is open sourced athttps://github.com/SWUFE-DB-Group/MDEval-Benchmark."
656,679d459debd8ffd557a2b0fd,cs.CL,https://arxiv.org/pdf/2501.14998,Federated Retrieval Augmented Generation for Multi-Product Question Answering,"Parshin Shojaee, Sai Sree Harsha, Dan Luo, Akash Maharaj, Tong Yu, Yunyao Li",Computation and Language,"Recent advancements in Large Language Models and Retrieval-Augmented Generation have boosted interest in domain-specific question-answering for enterprise products.
However, AI Assistants often face challenges in multi-product QA settings, requiring accurate responses across diverse domains.
Existing multi-domain RAG-QA approaches either query all domains indiscriminately, increasing computational costs and LLM hallucinations, or rely on rigid resource selection, which can limit search results.
We introduceMKP-QA, a novel multi-product knowledge-augmented QA framework with probabilistic federated search
across domains and relevant knowledge.
This method enhances multi-domain search quality
by aggregating query-domain and query-passage probabilistic relevance.
To address the lack of suitable benchmarks for multi-product QAs, we also present new datasets focused on three Adobe products: Adobe Experience Platform, Target, and Customer Journey Analytics.
Our experiments show thatMKP-QAsignificantly
boosts multi-product RAG-QA performance in terms of both retrieval accuracy and response quality."
657,679d459debd8ffd557a2b0fe,cs.CL,https://arxiv.org/pdf/2501.14981,The Muddy Waters of Modeling Empathy in Language: The Practical Impacts of Theoretical Constructs,"Allison Lahnala, Charles Welch, David Jurgens, Lucie Flek",Computation and Language,
658,679d459debd8ffd557a2b0ff,cs.CL,https://arxiv.org/pdf/2501.14976,A review of annotation classification tools in the educational domain,"Joaquín Gayoso-Cabada, Antonio Sarasa-Cabezuelo, José-Luis Sierra","Computation and Language, Digital Libraries",
659,679d459debd8ffd557a2b100,cs.CL,https://arxiv.org/pdf/2501.14956,ExPerT: Effective and Explainable Evaluation of Personalized Long-Form Text Generation,"Alireza Salemi, Julian Killingback, Hamed Zamani","Computation and Language, Artificial Intelligence, Information Retrieval","Evaluating personalized text generated by large language models (LLMs) is challenging, as only the LLM user, i.e. prompt author, can reliably assess the output, but re-engaging the same individuals across studies is infeasible. This paper addresses the challenge of evaluating personalized text generation by introducing ExPerT, an explainable reference-based evaluation framework. ExPerT leverages an LLM to extract atomic aspects and their evidences from the generated and reference texts, match the aspects, and evaluate their alignment based on content and writing style—two key attributes in personalized text generation. Additionally, ExPerT generates detailed, fine-grained explanations for every step of the evaluation process, enhancing transparency and interpretability. Our experiments demonstrate that ExPerT achieves a 7.2% relative improvement in alignment with human judgments compared to the state-of-the-art text generation evaluation methods. Furthermore, human evaluators rated the usability of ExPerT’s explanations at 4.7 out of 5, highlighting its effectiveness in making evaluation decisions more interpretable."
660,679d459debd8ffd557a2b101,cs.CL,https://arxiv.org/pdf/2501.14940,CASE-Bench: Context-Aware Safety Evaluation Benchmark for Large Language Models,"Guangzhi Sun, Xiao Zhan, Shutong Feng, Philip C. Woodland, Jose Such","Computation and Language, Artificial Intelligence","Aligning large language models (LLMs) with human values is essential for their safe deployment and widespread adoption. Current LLM safety benchmarks often focus solely on the refusal of individual problematic queries, which overlooks the importance of the context where the query occurs and may cause undesired refusal of queries under safe contexts that diminish user experience. Addressing this gap, we introduce CASE-Bench, a Context-Aware Safety Evaluation Benchmark that integrates context into safety assessments of LLMs. CASE-Bench assigns distinct, formally described contexts to categorized queries based on Contextual Integrity theory. Additionally, in contrast to previous studies which mainly rely on majority voting from just a few annotators, we recruited a sufficient number of annotators necessary to ensure the detection of statistically significant differences among the experimental conditions based on power analysis. Our extensive analysis using CASE-Bench on various open-source and commercial LLMs reveals a substantial and significant influence of context on human judgments (p<𝑝absentp<italic_p <0.0001 from a z-test), underscoring the necessity of context in safety evaluations. We also identify notable mismatches between human judgments and LLM responses, particularly in commercial models within safe contexts.111Warning: This paper contains red-teaming-related content that can be offensive in nature.Code and data used in the paper are available athttps://github.com/BriansIDP/CASEBench.git."
661,679d459debd8ffd557a2b102,cs.CL,https://arxiv.org/pdf/2501.14936,Context-Aware Neural Gradient Mapping for Fine-Grained Instruction Processing,"David Boldo, Lily Pemberton, Gabriel Thistledown, Jacob Fairchild, Felix Kowalski","Computation and Language, Artificial Intelligence","The integration of contextual embeddings into the optimization processes of large language models is an advancement in natural language processing. The Context-Aware Neural Gradient Mapping framework introduces a dynamic gradient adjustment mechanism, incorporating contextual embeddings directly into the optimization process. This approach facilitates real-time parameter adjustments, enhancing task-specific generalization even in the presence of sparse or noisy data inputs. The mathematical foundation of this framework relies on gradient descent modifications, where contextual embeddings are derived from a supplementary neural network trained to map input features to optimal adaptation gradients. By employing differential geometry principles, high-dimensional input dependencies are encoded into low-dimensional gradient manifolds, enabling efficient adaptation without necessitating the retraining of the entire model. Empirical evaluations demonstrate that the proposed framework consistently outperforms baseline models across various metrics, including accuracy, robustness to noise, and computational efficiency. The integration of context-specific embeddings allows for a more complex understanding of language, thereby improving the model’s ability to handle diverse linguistic phenomena. Furthermore, the computational efficiency achieved through this method demonstrates its scalability for large-scale language models operating under diverse constraints."
662,679d459debd8ffd557a2b103,cs.CL,https://arxiv.org/pdf/2501.14917,Self-reflecting Large Language Models: A Hegelian Dialectical Approach,"Sara Abdali, Can Goksen, Saeed Amizadeh, Kazuhito Koishida","Computation and Language, Human-Computer Interaction, Machine Learning","Investigating NLP through a philosophical lens has recently caught researcher’s eyes as it connects computational methods with classical schools of philosophy. This paper introduces a philosophical approach inspired by theHegelian Dialecticfor LLMs’self-reflection, utilizing a self-dialectical approach to emulate internal critiques and then synthesize new ideas by resolving the contradicting points. Moreover, this paper investigates the effect of LLMs’ temperature for generation by establishing a dynamic annealing approach, which promotes the creativity in the early stages and gradually refines it by focusing on the nuances, as well as a fixed temperature strategy for generation. Our proposed approach is examined to determine its ability to generate novel ideas from an initial proposition. Additionally, a Multi Agent Majority Voting (MAMV) strategy is leveraged to assess the validity and novelty of the generated ideas, which proves beneficial in the absence of domain experts. Our experiments show promise in generating new ideas and provide a stepping stone for future research."
663,679d459debd8ffd557a2b104,cs.CL,https://arxiv.org/pdf/2501.14883,Verify with Caution: The Pitfalls of Relying on Imperfect Factuality Metrics,"Ameya Godbole, Robin Jia","Computation and Language, Machine Learning","Improvements in large language models have led to increasing optimism that they can serve as reliable evaluators of natural language generation outputs.
In this paper, we challenge this optimism by thoroughly re-evaluating five state-of-the-art factuality metrics on a collection of 11 datasets for summarization, retrieval-augmented generation, and question answering.
We find that these evaluators are inconsistent with each other and often misestimate system-level performance, both of which can lead to a variety of pitfalls.
We further show that these metrics exhibit biases against highly paraphrased outputs and outputs that draw upon faraway parts of the source documents.
We urge users of these factuality metrics to proceed with caution and manually validate the reliability of these metrics in their domain of interest before proceeding."
664,679d459debd8ffd557a2b105,cs.CL,https://arxiv.org/pdf/2501.14859,Dynamic Adaptation of LoRA Fine-Tuning for Efficient and Task-Specific Optimization of Large Language Models,"Xiaoxuan Liao, Chihang Wang, Shicheng Zhou, Jiacheng Hu, Hongye Zheng, Jia Gao","Computation and Language, Machine Learning",
665,679d459debd8ffd557a2b106,cs.CL,https://arxiv.org/pdf/2501.14851,JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models,"Michael K. Chen, Xikun Zhang, Dacheng Tao","Computation and Language, Artificial Intelligence, Machine Learning, Logic in Computer Science",
666,679d459debd8ffd557a2b107,cs.CL,https://arxiv.org/pdf/2501.14850,On the locality bias and results in the Long Range Arena,"Pablo Miralles-González, Javier Huertas-Tato, Alejandro Martín, David Camacho","Computation and Language, Artificial Intelligence","The Long Range Arena (LRA) benchmark was designed to evaluate the performance of Transformer improvements and alternatives in long-range dependency modeling tasks. The Transformer and its main variants performed poorly on this benchmark, and a new series of architectures such as State Space Models (SSMs) gained some traction, greatly outperforming Transformers in the LRA. Recent work has shown that with a denoising pre-training phase, Transformers can achieve competitive results in the LRA with these new architectures. In this work, we discuss and explain the superiority of architectures such as MEGA and SSMs in the Long Range Arena, as well as the recent improvement in the results of Transformers, pointing to the positional and local nature of the tasks. We show that while the LRA is a benchmark for long-range dependency modeling, in reality most of the performance comes from short-range dependencies. Using training techniques to mitigate data inefficiency, Transformers are able to reach state-of-the-art performance with proper positional encoding. In addition, with the same techniques, we were able to remove all restrictions from SSM convolutional kernels and learn fully parameterized convolutions without decreasing performance, suggesting that the design choices behind SSMs simply added inductive biases and learning efficiency for these particular tasks. Our insights indicate that LRA results should be interpreted with caution and call for a redesign of the benchmark."
667,679d459debd8ffd557a2b108,cs.CL,https://arxiv.org/pdf/2501.14844,Unmasking Conversational Bias in AI Multiagent Systems,"Erica Coppolillo, Giuseppe Manco, Luca Maria Aiello","Computation and Language, Artificial Intelligence, Multiagent Systems","Detecting biases in the outputs produced by generative models is essential to reduce the potential risks associated with their application in critical settings. However, the majority of existing methodologies for identifying biases in generated text consider the models in isolation and neglect their contextual applications. Specifically, the biases that may arise in multi-agent systems involving generative models remain under-researched. To address this gap, we present a framework designed to quantify biases within multi-agent systems of conversational Large Language Models (LLMs). Our approach involves simulating small echo chambers, where pairs of LLMs, initialized with aligned perspectives on a polarizing topic, engage in discussions. Contrary to expectations, we observe significant shifts in the stance expressed in the generated messages, particularly within echo chambers where all agents initially express conservative viewpoints, in line with the well-documented political bias of many LLMs toward liberal positions. Crucially, the bias observed in the echo-chamber experiment remains undetected by current state-of-the-art bias detection methods that rely on questionnaires. This highlights a critical need for the development of a more sophisticated toolkit for bias detection and mitigation for AI multi-agent systems. The code to perform the experiments is publicly available athttps://anonymous.4open.science/r/LLMsConversationalBias-7725."
668,679d459debd8ffd557a2b109,cs.CL,https://arxiv.org/pdf/2501.16247,Zero-Shot Decision Tree Construction via Large Language Models,"Lucas Carrasco, Felipe Urrutia, Andrés Abeliuk","Machine Learning, Computation and Language","This paper introduces a novel algorithm for constructing decision trees using large language models (LLMs) in a zero-shot manner based on Classification and Regression Trees (CART) principles. Traditional decision tree induction methods rely heavily on labeled data to recursively partition data using criteria such as information gain or the Gini index. In contrast, we propose a method that uses the pre-trained knowledge embedded in LLMs to build decision trees without requiring any training data. Our approach leverages LLMs to perform operations essential for decision tree construction, including attribute discretization, probability calculation, and Gini index computation based on the probabilities. We show that these zero-shot decision trees can outperform baseline zero-shot methods and achieve competitive performance compared to supervised data-driven decision trees on tabular datasets. The decision trees constructed via this method provide transparent and interpretable models, addressing data scarcity while preserving interpretability. This work establishes a new baseline in low-data machine learning, offering a principled, knowledge-driven alternative to data-driven tree construction."
669,679d459debd8ffd557a2b10a,cs.CL,https://arxiv.org/pdf/2501.16241,Phase Transitions in Large Language Models and the $O(N)$ Model,"Youran Sun, Babak Haghighat","Machine Learning, Computation and Language, High Energy Physics - Theory, Data Analysis, Statistics and Probability","Large language models (LLMs) exhibit unprecedentedly rich scaling behaviors.
In physics, scaling behavior is closely related to phase transitions, critical phenomena, and field theory.
To investigate the phase transition phenomena in LLMs, we reformulated the Transformer architecture as anO⁢(N)𝑂𝑁O(N)italic_O ( italic_N )model.
Our study reveals two distinct phase transitions corresponding to the temperature used in text generation and the model’s parameter size, respectively.
The first phase transition enables us to estimate the internal dimension of the model, while the second phase transition is ofhigher-depthand signals the emergence of new capabilities.
As an application, the energy of theO⁢(N)𝑂𝑁O(N)italic_O ( italic_N )model can be used to evaluate whether an LLM’s parameters are sufficient to learn the training data."
670,679d459debd8ffd557a2b10b,cs.CL,https://arxiv.org/pdf/2501.16201,Enhancing and Exploring Mild Cognitive Impairment Detection with W2V-BERT-2.0,"Yueguan Wang, Tatsunari Matsushima, Soichiro Matsushima, Toshimitsu Sakai","Audio and Speech Processing, Computation and Language, Sound","This study explores a multi-lingual audio self-supervised learning model for detecting mild cognitive impairment (MCI) using the TAUKADIAL cross-lingual dataset. While speech transcription-based detection with BERT models is effective, limitations exist due to a lack of transcriptions and temporal information. To address these issues, the study utilizes features directly from speech utterances with W2V-BERT-2.0. We propose a visualization method to detect essential layers of the model for MCI classification and design a specific inference logic considering the characteristics of MCI. The experiment shows competitive results, and the proposed inference logic significantly contributes to the improvements from the baseline. We also conduct detailed analysis which reveals the challenges related to speaker bias in the features and the sensitivity of MCI classification accuracy to the data split, providing valuable insights for future research."
671,679d459debd8ffd557a2b10c,cs.CL,https://arxiv.org/pdf/2501.16073,Challenging Assumptions in Learning Generic Text Style Embeddings,"Phil Ostheimer, Marius Kloft, Sophie Fellenz","Machine Learning, Computation and Language","Recent advancements in language representation learning primarily emphasize language modeling for deriving meaningful representations, often neglecting style-specific considerations. This study addresses this gap by creating generic, sentence-level style embeddings crucial for style-centric tasks. Our approach is grounded on the premise that low-level text style changes can compose any high-level style. We hypothesize that applying this concept to representation learning enables the development of versatile text style embeddings. By fine-tuning a general-purpose text encoder using contrastive learning and standard cross-entropy loss, we aim to capture these low-level style shifts, anticipating that they offer insights applicable to high-level text styles. The outcomes prompt us to reconsider the underlying assumptions as the results do not always show that the learned style representations capture high-level text styles."
672,679d459debd8ffd557a2b10d,cs.CL,https://arxiv.org/pdf/2501.15907,"Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for Speech Generation","Haorui He, Zengqiang Shang, Chaoren Wang, Xuyuan Li, Yicheng Gu, Hua Hua, Liwei Liu, Chen Yang, Jiaqi Li, Peiyang Shi, Yuancheng Wang, Kai Chen, Pengyuan Zhang, Zhizheng Wu","Sound, Computation and Language, Audio and Speech Processing","Recent advancements in speech generation have been driven by the large-scale training datasets. However, current models fall short of capturing the spontaneity and variability inherent in real-world human speech, due to their reliance on audiobook datasets limited to formal read-aloud speech styles. To bridge this gap, we introduceEmilia-Pipe, an open-source preprocessing pipeline to extract high-quality training data from valuable yet underexplored in-the-wild data that capture spontaneous human speech in real-world contexts. By leveraging Emilia-Pipe, we constructEmilia, the first multilingual speech generation dataset derived from in-the-wild speech data. This dataset comprises over 101k hours of speech across six languages: English, Chinese, German, French, Japanese, and Korean. Besides, we expand Emilia toEmilia-Large, a dataset exceeding 216k hours, making it the largest open-source speech generation dataset available. Extensive experiments demonstrate that Emilia significantly outperforms traditional audiobook datasets in generating spontaneous and human-like speech, showcasing superior performance in capturing diverse speaker timbre and speaking styles of real-world human speech. Furthermore, this work underscores the importance of scaling dataset size to advance speech generation research and validates the effectiveness of Emilia for both multilingual and crosslingual speech generation."
673,679d459debd8ffd557a2b10e,cs.CL,https://arxiv.org/pdf/2501.15857,Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?,"Yutong Yin, Zhaoran Wang","Artificial Intelligence, Computation and Language, Machine Learning",
674,679d459debd8ffd557a2b10f,cs.CL,https://arxiv.org/pdf/2501.15797,LemmaHead: RAG Assisted Proof Generation Using Large Language Models,"Tianbo Yang, Mingqi Yang, Hongyi Zhao, Tianshuo Yang","Machine Learning, Computation and Language, Information Retrieval","Developing the logic necessary to solve mathematical problems or write mathematical proofs is one of the more difficult objectives for large language models (LLMS). Currently, the most popular methods in literature consists of fine-tuning the model on written mathematical content such as academic publications and textbooks, so that the model can learn to emulate the style of mathematical writing. In this project, we explore the effectiveness of using retrieval augmented generation (RAG) to address gaps in the mathematical reasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements queries to the model with relevant mathematical context, with particular focus on context from published textbooks. To measure our model’s performance in mathematical reasoning, our testing paradigm focuses on the task of automated theorem proving via generating proofs to a given mathematical claim in the Lean formal language."
675,679d459debd8ffd557a2b110,cs.CL,https://arxiv.org/pdf/2501.15758,Risk-Aware Distributional Intervention Policies for Language Models,"Bao Nguyen, Binh Nguyen, Duy Nguyen, Viet Anh Nguyen","Machine Learning, Computation and Language, Optimization and Control","Language models are prone to occasionally undesirable generations, such as harmful or toxic content, despite their impressive capability to produce texts that appear accurate and coherent. This paper presents a new two-stage approach to detect and mitigate undesirable content generations by rectifying activations. First, we train an ensemble of layerwise classifiers to detect undesirable content using activations by minimizing a smooth surrogate of the risk-aware score. Then, for contents that are detected as undesirable, we propose layerwise distributional intervention policies that perturb the attention heads minimally while guaranteeing probabilistically the effectiveness of the intervention. Benchmarks on several language models and datasets show that our method outperforms baselines in reducing the generation of undesirable output. Our code is available athttps://github.com/nguyenngocbaocmt02/OT-Intervention"
676,679d459debd8ffd557a2b111,cs.CL,https://arxiv.org/pdf/2501.15693,Beyond Benchmarks: On The False Promise of AI Regulation,"Gabriel Stanovsky, Renana Keydar, Gadi Perl, Eliya Habba","Machine Learning, Artificial Intelligence, Computation and Language","The rapid advancement of artificial intelligence (AI) systems in critical domains like healthcare, justice, and social services has sparked numerous regulatory initiatives aimed at ensuring their safe deployment. Current regulatory frameworks, exemplified by recent US and EU efforts, primarily focus on procedural guidelines while presuming that scientific benchmarking can effectively validate AI safety, similar to how crash tests verify vehicle safety or clinical trials validate drug efficacy. However, this approach fundamentally misunderstands the unique technical challenges posed by modern AI systems. Through systematic analysis of successful technology regulation case studies, we demonstrate that effective scientific regulation requires a causal theory linking observable test outcomes to future performance - for instance, how a vehicle’s crash resistance at one speed predicts its safety at lower speeds. We show that deep learning models, which learn complex statistical patterns from training data without explicit causal mechanisms, preclude such guarantees. This limitation renders traditional regulatory approaches inadequate for ensuring AI safety. Moving forward, we call for regulators to reckon with this limitation, and propose a preliminary two-tiered regulatory framework that acknowledges these constraints: mandating human oversight for high-risk applications while developing appropriate risk communication strategies for lower-risk uses. Our findings highlight the urgent need to reconsider fundamental assumptions in AI regulation and suggest a concrete path forward for policymakers and researchers."
677,679d459debd8ffd557a2b112,cs.CL,https://arxiv.org/pdf/2501.15678,"Blissful (A)Ignorance: People form overly positive impressions of others based on their written messages, despite wide-scale adoption of Generative AI","Jiaqi Zhu, Andras Molnar","Computers and Society, Artificial Intelligence, Computation and Language, Human-Computer Interaction",
678,679d459debd8ffd557a2b113,cs.CL,https://arxiv.org/pdf/2501.15613,Stepback: Enhanced Disentanglement for Voice Conversion via Multi-Task Learning,"Qian Yang, Calbert Graham","Sound, Computation and Language, Audio and Speech Processing","Voice conversion (VC) modifies voice characteristics while preserving linguistic content. This paper presents the Stepback network, a novel model for converting speaker identity using non-parallel data. Unlike traditional VC methods that rely on parallel data, our approach leverages deep learning techniques to enhance disentanglement completion and linguistic content preservation."
679,679d459debd8ffd557a2b114,cs.CL,https://arxiv.org/pdf/2501.15602,Rethinking External Slow-Thinking: From Snowball Errors to Probability of Correct Reasoning,"Zeyu Gan, Yun Liao, Yong Liu","Artificial Intelligence, Computation and Language","Test-time scaling, which is also often referred to asslow-thinking, has been demonstrated to enhance multi-step reasoning in large language models (LLMs).
However, despite its widespread utilization, the mechanisms underlying slow-thinking methods remain poorly understood.
This paper explores the mechanisms of external slow-thinking from a theoretical standpoint.
We begin by examining the snowball error effect within the LLM reasoning process and connect it to the likelihood of correct reasoning using information theory.
Building on this, we show that external slow-thinking methods can be interpreted as strategies to mitigate the error probability.
We further provide a comparative analysis of popular external slow-thinking approaches, ranging from simple to complex, highlighting their differences and interrelationships.
Our findings suggest that the efficacy of these methods is not primarily determined by the specific framework employed, and that expanding the search scope or the model’s internal reasoning capacity may yield more sustained improvements in the long term.
We open-source our code athttps://github.com/ZyGan1999/Snowball-Errors-and-Probability."
680,679d459debd8ffd557a2b115,cs.CL,https://arxiv.org/pdf/2501.15556,Commute Your Domains: Trajectory Optimality Criterion for Multi-Domain Learning,"Alexey Rukhovich, Alexander Podolskiy, Irina Piontkovskaya","Machine Learning, Computation and Language","In multi-domain learning, a single model is trained on diverse data domains to leverage shared knowledge and improve generalization. The order in which the data from these domains is used for training can significantly affect the model’s performance on each domain. However, this dependence is under-studied. In this paper, we investigate the influence of training order (or data mixing) in multi-domain learning using the concept of Lie bracket of gradient vector fields. By analyzing the infinitesimal effects of changing the training order, we identify regions in the parameter space where altering the order between two training domains can benefit the target loss. We validate the predictions of our theoretical framework on the influence of training order (or data mixing) both on a toy example and bilingual LLM pre-training."
681,679d459debd8ffd557a2b116,cs.CL,https://arxiv.org/pdf/2501.15463,Mind the Value-Action Gap: Do LLMs Act in Alignment with Their Values?,"Hua Shen, Nicholas Clark, Tanushree Mitra","Human-Computer Interaction, Artificial Intelligence, Computation and Language","Existing research primarily evaluates the values of LLMs by examining their stated inclinations towards specific values. However, the “Value-Action Gap,” a phenomenon rooted in environmental and social psychology, reveals discrepancies between individuals’ stated values and their actions in real-world contexts.To what extent do LLMs exhibit a similar gap between their stated values and their actions informed by those values?This study introducesValueActionLens, an evaluation framework to assess the alignment between LLMs’ stated values and their value-informed actions.
The framework encompasses the generation of a dataset comprising 14.8k value-informed actions across twelve cultures and eleven social topics, and two tasks to evaluate how well LLMs’ stated value inclinations and value-informed actions align across three different alignment measures.
Extensive experiments reveal that the alignment between LLMs’ stated values and actions is suboptimal, varying significantly across scenarios and models.
Analysis of misaligned results identifies potential harms from certain value-action gaps.
To predict the value-action gaps, we also uncover that leveraging reasoned explanations improves the performance. These findings underscore the risks of relying solely on the LLMs’ stated values to predict their behaviors, and emphasize the importance of context-aware evaluations of LLM values and value-action gaps."
682,679d459debd8ffd557a2b117,cs.CL,https://arxiv.org/pdf/2501.15411,"The Potential of Large Language Models in Supply Chain Management: Advancing Decision-Making, Efficiency, and Innovation","Raha Aghaei, Ali A. Kiaei, Mahnaz Boush, Javad Vahidi, Zeynab Barzegar, Mahan Rofoosheh","Computers and Society, Computation and Language",
683,679d459debd8ffd557a2b118,cs.CL,https://arxiv.org/pdf/2501.15393,Diffusion-based Hierarchical Negative Sampling for Multimodal Knowledge Graph Completion,"Guanglin Niu, Xiaowei Zhang","Artificial Intelligence, Computation and Language","Multimodal Knowledge Graph Completion (MMKGC) aims to address the critical issue of missing knowledge in multimodal knowledge graphs (MMKGs) for their better applications. However, both the previous MMGKC and negative sampling (NS) approaches ignore the employment of multimodal information to generate diverse and high-quality negative triples from various semantic levels and hardness levels, thereby limiting the effectiveness of training MMKGC models. Thus, we propose a novel Diffusion-based Hierarchical Negative Sampling (DHNS) scheme tailored for MMKGC tasks, which tackles the challenge of generating high-quality negative triples by leveraging a Diffusion-based Hierarchical Embedding Generation (DiffHEG) that progressively conditions on entities and relations as well as multimodal semantics. Furthermore, we develop a Negative Triple-Adaptive Training (NTAT) strategy that dynamically adjusts training margins associated with the hardness level of the synthesized negative triples, facilitating a more robust and effective learning procedure to distinguish between positive and negative triples. Extensive experiments on three MMKGC benchmark datasets demonstrate that our framework outperforms several state-of-the-art MMKGC models and negative sampling techniques, illustrating the effectiveness of our DHNS for training MMKGC models. The source codes and datasets of this paper are available athttps://github.com/ngl567/DHNS."
684,679d459debd8ffd557a2b119,cs.CL,https://arxiv.org/pdf/2501.15316,ToMoE: Converting Dense Large Language Models to Mixture-of-Experts through Dynamic Structural Pruning,"Shangqian Gao, Ting Hua, Reza Shirkavand, Chi-Heng Lin, Zhen Tang, Zhengao Li, Longge Yuan, Fangyi Li, Zeyu Zhang, Alireza Ganjdanesh, Lou Qian, Xu Jie, Yen-Chang Hsu","Machine Learning, Computation and Language","Large Language Models (LLMs) have demonstrated remarkable abilities in tackling a wide range of complex tasks. However, their huge computational and memory costs raise significant challenges in deploying these models on resource-constrained devices or efficiently serving them. Prior approaches have attempted to alleviate these problems by permanently removing less important model structures, yet these methods often result in substantial performance degradation due to the permanent deletion of model parameters. In this work, we tried to mitigate this issue by reducing the number of active parameters without permanently removing them. Specifically, we introduce a differentiable dynamic pruning method that pushes dense models to maintain a fixed number of active parameters by converting their MLP layers into a Mixture of Experts (MoE) architecture. Our method, even without fine-tuning, consistently outperforms previous structural pruning techniques across diverse model families, including Phi-2, LLaMA-2, LLaMA-3, and Qwen-2.5."
685,679d459debd8ffd557a2b11a,cs.CL,https://arxiv.org/pdf/2501.15056,Feedback-Aware Monte Carlo Tree Search for Efficient Information Seeking in Goal-Oriented Conversations,"Harshita Chopra, Chirag Shah","Artificial Intelligence, Computation and Language, Human-Computer Interaction, Machine Learning","The ability to identify and acquire missing information is a critical component of effective decision making and problem solving. With the rise of conversational artificial intelligence (AI) systems, strategically formulating information-seeking questions becomes crucial and demands efficient methods to guide the search process. We introduce a novel approach to adaptive question-asking through a combination of Large Language Models (LLM) for generating questions that maximize information gain, Monte Carlo Tree Search (MCTS) for constructing and leveraging a decision tree across multiple samples, and a hierarchical feedback mechanism to learn from past interactions. We present two key innovations: (1) an adaptive MCTS algorithm that balances exploration and exploitation for efficient search over potential questions; and (2) a clustering-based feedback algorithm that leverages prior experience to guide future interactions. Each incoming sample is assigned to a cluster based on its semantic similarity with previously observed samples. Our UCT (Upper Confidence bound for Trees) formulation selects optimal questions by combining expected rewards, a function of information gain, with a cluster-specific bonus that decays with depth, to emphasize the importance of early-stage questions that have proven effective for narrowing the solution space in similar samples. Experiments across three domains, including medical diagnosis and troubleshooting, demonstrate that our method leads to an average of 12% improvement in success rates and a 10x reduction in the average number of LLM calls made per conversation for the search process, in comparison to the state of the art.111https://github.com/harshita-chopra/misq-hf"
686,679d459debd8ffd557a2b11b,cs.CL,https://arxiv.org/pdf/2501.15030,OptiSeq: Optimizing Example Ordering for In-Context Learning,"Rahul Atul Bhope, Praveen Venkateswaran, K. R. Jayaram, Vatche Isahagian, Vinod Muthusamy, Nalini Venkatasubramanian","Machine Learning, Artificial Intelligence, Computation and Language, Performance","Developers using LLMs in their applications and agents have provided plenty of anecdotal evidence
that in-context-learning (ICL) is fragile.
In addition to the quantity and quality of examples, we show that the order in which the in-context
examples are listed in the prompt affects the output of the LLM and, consequently, their performance.
In this paper, we present\ours, which introduces
a score based on log probabilities of LLM outputs to prune the universe of possible example orderings
in few-shot ICL and recommend the best order(s) by distinguishing between correct and incorrect outputs resulting from different order permutations. Through a detailed empirical evaluation on
multiple LLMs, datasets and prompts, we demonstrate that\oursimproves accuracy by 6 - 10.5percentage pointsacross multiple tasks."
687,679d459debd8ffd557a2b11c,cs.CL,https://arxiv.org/pdf/2501.14960,LLM4DistReconfig: A Fine-tuned Large Language Model for Power Distribution Network Reconfiguration,"Panayiotis Christou, Md. Zahidul Islam, Yuzhang Lin, Jingwei Xiong","Machine Learning, Artificial Intelligence, Computation and Language","Power distribution networks are evolving due to the integration of distributed energy resources (DERs) and increased customer participation. To maintain optimal operation, minimize losses, and meet varying load demands, frequent network reconfiguration is necessary. Traditionally, the reconfiguration task relies on optimization software and expert operators, but as systems grow more complex, faster and more adaptive solutions are required without expert intervention. Data-driven reconfiguration is gaining traction for its accuracy, speed, and robustness against incomplete network data. Large language models (LLMs), with their ability to capture complex patterns, offer a promising approach for efficient and responsive network reconfiguration in evolving complex power networks."
688,679d459debd8ffd557a2b11d,cs.CL,https://arxiv.org/pdf/2501.14951,E-Gen: Leveraging E-Graphs to Improve Continuous Representations of Symbolic Expressions,"Hongbo Zheng, Suyuan Wang, Neeraj Gangwar, Nickvash Kani","Machine Learning, Computation and Language, Symbolic Computation","As vector representations have been pivotal in advancing natural language processing (NLP), some prior research has concentrated on creating embedding techniques for mathematical expressions by leveraging mathematically equivalent expressions. While effective, these methods are limited by the training data. In this work, we propose augmenting prior algorithms with larger synthetic dataset, using a novel e-graph-based generation scheme. This new mathematical dataset generation scheme, E-Gen, improves upon prior dataset-generation schemes that are limited in size and operator types. We use this dataset to compare embedding models trained with two methods: (1) training the model to generate mathematically equivalent expressions, and (2) training the model using contrastive learning to group mathematically equivalent expressions explicitly. We evaluate the embeddings generated by these methods against prior work on both in-distribution and out-of-distribution language processing tasks. Finally, we compare the performance of our embedding scheme against state-of-the-art large language models and demonstrate that embedding-based language processing methods perform better than LLMs on several tasks, demonstrating the necessity of optimizing embedding methods for the mathematical data modality."
689,679d459debd8ffd557a2b11e,cs.CL,https://arxiv.org/pdf/2501.14846,Wormhole Memory: A Rubik's Cube for Cross-Dialogue Retrieval,Libo Wang,"Machine Learning, Artificial Intelligence, Computation and Language",
690,679d459debd8ffd557a2b11f,cs.CL,https://arxiv.org/pdf/2501.14767,Leveraging Social Media Data and Artificial Intelligence for Improving Earthquake Response Efforts,"Kalin Kopanov, Velizar Varbanov, Tatiana Atanasova","Computers and Society, Artificial Intelligence, Computation and Language, Information Retrieval, Social and Information Networks",
691,679d459debd8ffd557a2b120,cs.CL,https://arxiv.org/pdf/2501.14731,From Critique to Clarity: A Pathway to Faithful and Personalized Code Explanations with Large Language Models,"Zexing Xu, Zhuang Luo, Yichuan Li, Kyumin Lee, S. Rasoul Etesami","Software Engineering, Artificial Intelligence, Computation and Language","A clear and well-documentedLaTeXdocument is presented as an
article formatted for publication by ACM in a conference proceedings
or journal publication. Based on the “acmart” document class, this
article presents and explains many of the common variations, as well
as many of the formatting elements an author may use in the
preparation of the documentation of their work."
692,679d459debd8ffd557a2b121,cs.CL,https://arxiv.org/pdf/2501.14721,Comparable Corpora: Opportunities for New Research Directions,Kenneth Church,Computation and Language,"Most conference papers present new results, but this paper will focus more on opportunities for the audience to make their own contributions. This paper
is intended to challenge the community to think more broadly about what we can do with comparable corpora.
We will start with a review of the history, and then suggest new directions for future research."
693,679d459debd8ffd557a2b122,cs.CL,https://arxiv.org/pdf/2501.14719,Do LLMs Provide Consistent Answers to Health-Related Questions across Languages?,"Ipek Baris Schlicht, Zhixue Zhao, Burcu Sayin, Lucie Flek, Paolo Rosso","Computation and Language, Artificial Intelligence, Human-Computer Interaction, Information Retrieval","Equitable access to reliable health information is vital for public health, but the quality of online health resources varies by language, raising concerns about inconsistencies in Large Language Models (LLMs) for healthcare.
In this study, we examine the consistency of responses provided by LLMs to health-related questions across English, German, Turkish, and Chinese.
We largely expand the HealthFC dataset by categorizing health-related questions by disease type and broadening its multilingual scope with Turkish and Chinese translations.
We reveal significant inconsistencies in responses that could spread healthcare misinformation. Our main contributions are 1) a multilingual health-related inquiry dataset with meta-information on disease categories, and 2) a novel prompt-based evaluation workflow that enables sub-dimensional comparisons between two languages through parsing. Our findings highlight key challenges in deploying LLM-based tools in multilingual contexts and emphasize the need for improved cross-lingual alignment to ensure accurate and equitable healthcare information."
694,679d459debd8ffd557a2b123,cs.CL,https://arxiv.org/pdf/2501.14717,Towards Better Understanding Table Instruction Tuning: Decoupling the Effects from Data versus Models,"Naihao Deng, Sheng Zhang, Henghui Zhu, Shuaichen Chang, Jiani Zhang, Alexander Hanbo Li, Chung-Wei Hang, Hideo Kobayashi, Yiqun Hu, Patrick Ng",Computation and Language,"Recent advances in natural language processing have leveraged instruction tuning to enhance Large Language Models (LLMs) for table-related tasks.
However, previous works train different base models with different training data, lacking an apples-to-apples comparison across the result table LLMs.
To address this, we fine-tune base models from the Mistral, OLMo, and Phi families on existing public training datasets.
Our replication achieves performance on par with or surpassing existing table LLMs, establishing new state-of-the-art performance on Hitab, a table question-answering dataset.
More importantly, through systematic out-of-domain evaluation, we decouple the contributions of training data and the base model, providing insight into their individual impacts.
In addition, we assess the effects of table-specific instruction tuning on general-purpose benchmarks, revealing trade-offs between specialization and generalization."
695,679d459debd8ffd557a2b124,cs.CL,https://arxiv.org/pdf/2501.14713,FlexiGPT: Pruning and Extending Large Language Models with Low-Rank Weight Sharing,"James Seale Smith, Chi-Heng Lin, Shikhar Tuli, Haris Jeelani, Shangqian Gao, Yilin Shen, Hongxia Jin, Yen-Chang Hsu","Computation and Language, Machine Learning","The rapid proliferation of large language models (LLMs) in natural language processing (NLP) has created a critical need for techniques that enable efficient deployment on memory-constrained devices without compromising performance. We present a method to prune LLMs that selectively prunes model blocks based on an importance score and replaces them with a low-parameter replacement strategy. Specifically, we propose a principled metric to replace each pruned block using a weight-sharing mechanism that leverages unpruned counterparts from the model and block-specific low-rank adapters. Furthermore, we facilitate the learning of these replacement blocks with output feature normalization and an adapter initialization scheme built on low-rank SVD reconstructions. Empirical evaluations demonstrate substantial performance gains over existing methods, achieving state-of-the-art performance on 5/6 benchmarks for a compression rate of30%percent3030\%30 %and 6/6 benchmarks for a compression rate of40%percent4040\%40 %. We also demonstrate that our approach can extend smaller models, boosting performance on 6/6 benchmarks using only≈\approx≈0.3% tokens of extended training with minimal additional parameter costs."
696,679d459debd8ffd557a2b125,cs.CL,https://arxiv.org/pdf/2501.14701,NLP-based assessment of prescription appropriateness from Italian referrals,"Vittorio Torri, Annamaria Bottelli, Michele Ercolanoni, Olivia Leoni, Francesca Ieva","Computation and Language, Machine Learning","Objective:This study proposes a Natural Language Processing pipeline to evaluate prescription appropriateness in Italian referrals, where reasons for prescriptions are recorded only as free text, complicating automated comparisons with guidelines. The pipeline aims to derive, for the first time, a comprehensive summary of the reasons behind these referrals and a quantification of their appropriateness. While demonstrated in a specific case study, the approach is designed to generalize to other types of examinations.Methods:Leveraging embeddings from a transformer-based model, the proposed approach clusters referral texts, maps clusters to labels, and aligns these labels with existing guidelines. We present a case study on a dataset of 496,971 referrals, consisting of all referrals for venous echocolordopplers of the lower limbs between 2019 and 2021 in the Lombardy Region. A sample of 1,000 referrals was manually annotated to validate the results.Results:The pipeline exhibited high performance for referrals’ reasons (Prec=92.43%, Rec=83.28%) and excellent results for referrals’ appropriateness (Prec=93.58%, Rec=91.52%) on the annotated subset. Analysis of the entire dataset identified clusters matching guideline-defined reasons - both appropriate and inappropriate - as well as clusters not addressed in the guidelines. Overall, 34.32% of referrals were marked as appropriate, 34.07% inappropriate, 14.37% likely inappropriate, and 17.24% could not be mapped to guidelines.Conclusions:The proposed pipeline effectively assessed prescription appropriateness across a large dataset, serving as a valuable tool for health authorities. Findings have informed the Lombardy Region’s efforts to strengthen recommendations and reduce the burden of inappropriate referrals."
697,679d459debd8ffd557a2b126,cs.CL,https://arxiv.org/pdf/2501.14693,Rethinking Table Instruction Tuning,"Naihao Deng, Rada Mihalcea","Computation and Language, Artificial Intelligence","Recent advances in table understanding have focused on instruction-tuning large language models (LLMs) for table-related tasks.
However, existing research has overlooked the impact of hyperparameter choices and lacks a comprehensive evaluation of the out-of-domain table understanding ability and the general capabilities of these table LLMs.
In this paper, we evaluate these abilities in existing table LLMs, and reveal significant declines in both out-of-domain table understanding and general capabilities compared to their base models.
Through systematic analysis, we show that hyperparameters, such as learning rate, can significantly influence both table-specific and general capabilities.
Contrary to the existing table instruction-tuning works, we demonstrate that smaller learning rates and fewer training instances can enhance table understanding while preserving general capabilities.
Based on our findings, we introduceTAMA, aTAble LLM instruction-tuned from LLaMA3.1 8B Instruct, which achieves performance on par with, or surpassing GPT-3.5 and GPT-4 on table tasks, while maintaining strong out-of-domain generalization and general capabilities.
Our findings highlight the potential for reduced data annotation costs and more efficient model development through careful hyperparameter selection."
698,679d459debd8ffd557a2b127,cs.CL,https://arxiv.org/pdf/2501.14673,State Space Models for Extractive Summarization in Low Resource Scenarios,Nisrine Ait Khayi,"Computation and Language, Artificial Intelligence",
699,679d459debd8ffd557a2b128,cs.CL,https://arxiv.org/pdf/2501.14649,Investigating the (De)Composition Capabilities of Large Language Models in Natural-to-Formal Language Conversion,"Ziyao Xu, Houfeng Wang",Computation and Language,"To achieve generalized and robust natural-to-formal language conversion (N2F), large language models (LLMs) need to have strong capabilities of decomposition and composition in N2F when faced with an unfamiliar formal language and be able to cope with compositional gaps and counter-intuitive symbolic names. To investigate whether LLMs have this set of basic capabilities in N2F, we propose the DEDC framework. This framework semi-automatically performs sample and task construction, allowing decoupled evaluation of the set of decomposition and composition capabilities of LLMs in N2F. Based on this framework, we evaluate and analyze the most advanced LLMs, and the main findings include that: (1) the LLMs are deficient in both decomposition and composition; (2) the LLMs show a wide coverage of error types that can be attributed to deficiencies in natural language understanding and the learning and use of symbolic systems; (3) compositional gaps and counter-intuitive symbolic names both affect the decomposition and composition of the LLMs. Our work provides a new perspective for investigating the basic capabilities of decomposition and composition of LLMs in N2F. The detailed analysis of deficiencies and attributions can help subsequent improvements of LLMs. The dataset and code are available athttps://github.com/xzy-xzy/DEDC."
700,679d459debd8ffd557a2b129,cs.CL,https://arxiv.org/pdf/2501.14617,Funzac at CoMeDi Shared Task: Modeling Annotator Disagreement from Word-In-Context Perspectives,"Olufunke O. Sarumi, Charles Welch, Lucie Flek, Jörg Schlötterer",Computation and Language,"In this work, we evaluate annotator disagreement in Word-in-Context (WiC) tasks exploring the relationship between contextual meaning and disagreement as part of the CoMeDi shared task competition.
While prior studies have modeled disagreement by analyzing annotator attributes with single-sentence inputs, this shared task incorporates WiC to bridge the gap between sentence-level semantic representation and annotator judgment variability.
We describe three different methods that we developed for the shared task, including a feature enrichment approach that combines concatenation, element-wise differences, products, and cosine similarity, Euclidean and Manhattan distances to extend contextual embedding representations, a transformation by Adapter blocks to obtain task-specific representations of contextual embeddings, and classifiers of varying complexities, including ensembles.
The comparison of our methods demonstrates improved performance for methods that include enriched and task-specfic features.
While the performance of our method falls short in comparison to the best system in subtask 1 (OGWiC), it is competitive to the official evaluation results in subtask 2 (DisWiC)."
701,679d459debd8ffd557a2b12a,cs.CL,https://arxiv.org/pdf/2501.14528,Idiom Detection in Sorani Kurdish Texts,"Skala Kamaran Omer, Hossein Hassani",Computation and Language,"Idiom detection using Natural Language Processing (NLP) is the computerized process of recognizing figurative expressions within a text that convey meanings beyond the literal interpretation of the words. While idiom detection has seen significant progress across various languages, the Kurdish language faces a considerable research gap in this area despite the importance of idioms in tasks like machine translation and sentiment analysis. This study addresses idiom detection in Sorani Kurdish by approaching it as a text classification task using deep learning techniques. To tackle this, we developed a dataset containing 10,580 sentences embedding 101 Sorani Kurdish idioms across diverse contexts. Using this dataset, we developed and evaluated three deep learning models: KuBERT-based transformer sequence classification, a Recurrent Convolutional Neural Network (RCNN), and a BiLSTM model with an attention mechanism. The evaluations revealed that the transformer model, the fine-tuned BERT, consistently outperformed the others, achieving nearly 99% accuracy while the RCNN achieved 96.5% and the BiLSTM 80%. These results highlight the effectiveness of Transformer-based architectures in low-resource languages like Kurdish. This research provides a dataset, three optimized models, and insights into idiom detection, laying a foundation for advancing Kurdish NLP."
702,679d459debd8ffd557a2b12b,cs.CL,https://arxiv.org/pdf/2501.14506,WanJuanSiLu: A High-Quality Open-Source Webtext Dataset for Low-Resource Languages,"Jia Yu, Fei Yuan, Rui Min, Jing Yu, Pei Chu, Jiayang Li, Wei Li, Ruijie Zhang, Zhenxiang Li, Zhifei Ren, Dong Zheng, Wenjian Zhang, Yan Teng, Lingyu Meng, ZhenJiang Jin, Jiantao Qiu, ShaSha Wang, Zhongying Tu, Dahua Lin, Yu Wang, Yu Qiao, Yanfeng Wang, Conghui He",Computation and Language,"This paper introduces the open-source dataset WanJuanSiLu, designed to provide high-quality training corpora for low-resource languages, thereby advancing the research and development of multilingual models. To achieve this, we have developed a systematic data processing framework tailored for low-resource languages. This framework encompasses key stages such as data extraction, corpus cleaning, content deduplication, security filtering, quality evaluation, and theme classification. Through the implementation of this framework, we have significantly improved both the quality and security of the dataset, while maintaining its linguistic diversity. As of now, data for all five languages have been fully open-sourced. The dataset can be accessed athttps://opendatalab.com/applyMultilingualCorpus, and GitHub repository is available athttps://github.com/opendatalab/WanJuan3.0"
703,679d459debd8ffd557a2b12c,cs.CL,https://arxiv.org/pdf/2501.14497,Evaluating and Improving Graph to Text Generation with Large Language Models,"Jie He, Yijun Yang, Wanqiu Long, Deyi Xiong, Victor Gutierrez Basulto, Jeff Z. Pan",Computation and Language,"Largelanguagemodels (LLMs) have demonstrated immense potential across various tasks. However, research for exploring and improving the capabilities of LLMs in interpreting graph structures remains limited. To address this gap, we conduct a comprehensive evaluation of prompting current open-source LLMs on graph-to-text generation tasks. Although we explored the optimal prompting strategies and proposed a novel and effective diversity-difficulty-based few-shot sample selection method, we found that the improvements from tuning-free approaches were incremental, as LLMs struggle with planning on complex graphs, particularly those with a larger number of triplets.
To further improve LLMs in planning with graph sequences and grounding in truth, we introduce a new graph-to-text dataset, PlanGTG, annotated with two sub-tasks: reordering and attribution. Through extensive automatic and human evaluations, we demonstrate significant improvements in the quality of generated text from both few-shot learning and fine-tuning perspectives using the PlanGTG dataset. Our study paves the way for new research directions in graph-to-text generation111PlanGTG datasets can be found inhttps://github.com/probe2/kg_text.."
704,679d459debd8ffd557a2b12d,cs.CL,https://arxiv.org/pdf/2501.14492,RealCritic: Towards Effectiveness-Driven Evaluation of Language Model Critiques,"Zhengyang Tang, Ziniu Li, Zhenyang Xiao, Tian Ding, Ruoyu Sun, Benyou Wang, Dayiheng Liu, Fei Huang, Tianyu Liu, Bowen Yu, Junyang Lin","Computation and Language, Artificial Intelligence, Machine Learning","Critiques are pivotal for enhancing the performance of Large Language Models (LLMs), enabling both self-improvement and constructive feedback for others by identifying flaws and suggesting improvements. However, evaluating the critique capabilities of LLMs presents a significant challenge due to the open-ended nature of the task. In this work, we introduce a novel benchmark designed to assess the critique capabilities of LLMs. Unlike existing benchmarks, which typically function in an open-loop fashion, our approach employs a closed-loop methodology that evaluates the quality of corrections generated from critiques. Moreover, the benchmark incorporates features such as self-critique, cross-critique, and iterative critique, which are crucial for distinguishing the abilities of advanced reasoning models from more classical ones. We implement this benchmark using XXX datasets in the domain of mathematical reasoning. Our results yield several important insights: first, [Finding One]; second, [Finding Two]; and third, [Finding Three]. We hope that this benchmark will serve as a valuable resource to guide future advancements."
705,679d459debd8ffd557a2b12e,cs.CL,https://arxiv.org/pdf/2501.14491,Analyzing the Effect of Linguistic Similarity on Cross-Lingual Transfer: Tasks and Experimental Setups Matter,"Verena Blaschke, Masha Fedzechkina, Maartje ter Hoeve",Computation and Language,"Cross-lingual transfer is a popular approach to increase the amount of training data for NLP tasks in a low-resource context. However, the best strategy to decide which cross-lingual data to include is unclear. Prior research often focuses on a small set of languages from a few language families and/or a single task. It is still an open question how these findings extend to a wider variety of languages and tasks. In this work, we analyze cross-lingual transfer for 266 languages from a wide variety of language families. Moreover, we include three popular NLP tasks: POS tagging, dependency parsing, and topic classification.
Our findings indicate that the effect of linguistic similarity on transfer performance depends on a range of factors: the NLP task, the (mono- or multilingual) input representations, and the definition of linguistic similarity."
706,679d459debd8ffd557a2b12f,cs.CL,https://arxiv.org/pdf/2501.14457,Understanding and Mitigating Gender Bias in LLMs via Interpretable Neuron Editing,"Zeping Yu, Sophia Ananiadou",Computation and Language,"Large language models (LLMs) often exhibit gender bias, posing challenges for their safe deployment. Existing methods to mitigate bias lack a comprehensive understanding of its mechanisms or compromise the model’s core capabilities. To address these issues, we propose the CommonWords dataset, to systematically evaluate gender bias in LLMs. Our analysis reveals pervasive bias across models and identifies specific neuron circuits, including “gender neurons” and “general neurons,” responsible for this behavior. Notably, editing even a small number of general neurons can disrupt the model’s overall capabilities due to hierarchical neuron interactions. Based on these insights, we propose an interpretable neuron editing method that combines logit-based and causal-based strategies to selectively target biased neurons. Experiments on five LLMs demonstrate that our method effectively reduces gender bias while preserving the model’s original capabilities, outperforming existing fine-tuning and editing approaches. Our findings contribute a novel dataset, a detailed analysis of bias mechanisms, and a practical solution for mitigating gender bias in LLMs."
707,679d459debd8ffd557a2b130,cs.CL,https://arxiv.org/pdf/2501.14431,Domaino1s: Guiding LLM Reasoning for Explainable Answers in High-Stakes Domains,"Xu Chu, Zhijie Tan, Hanlin Xue, Guanyu Wang, Tong Mo, Weiping Li","Computation and Language, Machine Learning","Large Language Models (LLMs) are widely applied to downstream domains. However, current LLMs for high-stakes domain tasks, such as financial investment and legal QA, typically generate brief answers without reasoning processes and explanations. This limits users’ confidence in making decisions based on their responses. While original CoT shows promise, it lacks self-correction mechanisms during reasoning. This work introduces Domaino⁢1𝑜1o1italic_o 1s, which enhances LLMs’ reasoning capabilities on domain tasks through supervised fine-tuning and tree search. We construct CoT-stock-2k and CoT-legal-2k datasets for fine-tuning models that activate domain-specific reasoning steps based on their judgment.
Additionally, we propose Selective Tree Exploration to spontaneously explore solution spaces and sample optimal reasoning paths to improve performance. We also introduce PROOF-Score, a new metric for evaluating domain models’ explainability, complementing traditional accuracy metrics with richer assessment dimensions. Extensive experiments on stock investment recommendation and legal reasoning QA tasks demonstrate Domaino⁢1𝑜1o1italic_o 1s’s leading performance and explainability. Our code is available athttps://anonymous.4open.science/r/Domaino1s-006F/."
708,679d459debd8ffd557a2b131,cs.CL,https://arxiv.org/pdf/2501.14371,DRESSing Up LLM: Efficient Stylized Question-Answering via Style Subspace Editing,"Xinyu Ma, Yifeng Xu, Yang Lin, Tianlong Wang, Xu Chu, Xin Gao, Junfeng Zhao, Yasha Wang","Computation and Language, Artificial Intelligence, Machine Learning","We introduceDRESS, a novel approach for generating stylized large language model (LLM) responses through representation editing. Existing methods like prompting and fine-tuning are either insufficient for complex style adaptation or computationally expensive, particularly in tasks like NPC creation or character role-playing. Our approach leverages the over-parameterized nature of LLMs to disentangle a style-relevant subspace within the model’s representation space to conduct representation editing, ensuring a minimal impact on the original semantics.
By applying adaptive editing strengths, we dynamically adjust the steering vectors in the style subspace to maintain both stylistic fidelity and semantic integrity. We develop two stylized QA benchmark datasets to validate the effectiveness ofDRESS, and the results demonstrate significant improvements compared to baseline methods such as prompting and ITI. In short,DRESSis a lightweight, train-free solution for enhancing LLMs with flexible and effective style control, making it particularly useful for developing stylized conversational agents.111Codes and benchmark datasets are available athttps://github.com/ArthurLeoM/DRESS-LLM."
709,679d459debd8ffd557a2b132,cs.CL,https://arxiv.org/pdf/2501.14315,Clear Minds Think Alike: What Makes LLM Fine-tuning Robust? A Study of Token Perplexity,"Chao-Chung Wu, Zhi Rui Tam, Chieh-Yen Lin, Hung-yi Lee, Yun-Nung Chen",Computation and Language,"Maintaining consistent model performance across domains is a fundamental challenge in machine learning. While recent work has explored using LLM-generated data for fine-tuning, its impact on cross-domain generalization remains poorly understood. In this paper, we present a systematic analysis revealing that fine-tuning with LLM-generated data not only improves target task performance but also reduces out-of-domain (OOD) degradation compared to fine-tuning with ground truth data. Through analyzing the data sequence in tasks of various domains, we demonstrate that this enhanced OOD robustness stems from a reduced prevalence of high perplexity tokens in LLM-generated sequences. Following this hypothesis we showed that masking high perplexity tokens in ground truth training data also achieves similar OOD preservation comparable to using LLM-generated data. Extensive experiments across diverse model architectures and scales, including Gemma2-2B, Mistral-7B and Llama3-8B, corroborate the consistency of our findings. To the best of our knowledge, this work provides the first mechanistic explanation for the superior OOD robustness conferred by LLM-generated training data, offering valuable insights for developing more robust fine-tuning strategies."
710,679d459debd8ffd557a2b133,cs.CL,https://arxiv.org/pdf/2501.14294,Examining Alignment of Large Language Models through Representative Heuristics: The Case of Political Stereotypes,"Sullam Jeoung, Yubin Ge, Haohan Wang, Jana Diesner","Computation and Language, Artificial Intelligence","Examining the alignment of large language models (LLMs) has become increasingly important, particularly when these systems fail to operate as intended. This study explores the challenge of aligning LLMs with human intentions and values, with specific focus on their political inclinations. Previous research has highlighted LLMs’ propensity to display political leanings, and their ability to mimic certain political parties’ stances on various issues. However, theextentandconditionsunder which LLMs deviate from empirical positions have not been thoroughly examined. To address this gap, our study systematically investigates the factors contributing to LLMs’ deviations from empirical positions on political issues, aiming to quantify these deviations and identify the conditions that cause them."
711,679d459debd8ffd557a2b134,cs.CL,https://arxiv.org/pdf/2501.14288,A Comprehensive Framework for Semantic Similarity Detection Using Transformer Architectures and Enhanced Ensemble Techniques,"Lifu Gao, Qi Zhang, Ziwei Liu","Computation and Language, Artificial Intelligence","Detecting AI-generated text, especially in short-context documents, is difficult because there is not enough context for accurate classification. This paper presents a new teacher-student model that uses domain adaptation and data augmentation to solve these problems. The teacher model, which combines DeBERTa-v3-large and Mamba-790m, learns semantic knowledge through domain-specific fine-tuning. The student model handles short-context text more efficiently. The system uses a Mean Squared Error (MSE) loss function to guide the student’s learning, improving both accuracy and efficiency. Also, data augmentation methods like spelling correction and error injection make the model more robust. Experimental results show that this approach works better than baseline methods, proving its usefulness for real-time AI-generated text detection and other text classification tasks."
712,679d459debd8ffd557a2b135,cs.CL,https://arxiv.org/pdf/2501.14275,Leveraging Online Olympiad-Level Math Problems for LLMs Training and Contamination-Resistant Evaluation,"Sadegh Mahdavi, Muchen Li, Kaiwen Liu, Christos Thrampoulidis, Leonid Sigal, Renjie Liao","Computation and Language, Artificial Intelligence, Machine Learning","Advances in Large Language Models (LLMs) have sparked interest in their ability to solve Olympiad-level math problems.
However, the training and evaluation of these models are constrained by the limited size and quality of available datasets, as creating large-scale data for such advanced problems requires extensive effort from human experts.
In addition, current benchmarks are prone to contamination, leading to unreliable evaluations.
In this paper, we present an automated pipeline that leverages the rich resources of the Art of Problem Solving (AoPS) forum, which predominantly features Olympiad-level problems and community-driven solutions.
Using open-source LLMs, we develop a method to extract question-answer pairs from the forum, resulting inAoPS-Instruct, a dataset of more than 600,000 high-quality QA pairs.
Our experiments demonstrate that fine-tuning LLMs on AoPS-Instruct improves their reasoning abilities across various benchmarks.
Moreover, we build an automatic pipeline that introducesLiveAoPSBench, an evolving evaluation set with timestamps, derived from the latest forum data, providing a contamination-resistant benchmark for assessing LLM performance.
Notably, we observe a significant decline in LLM performance over time, suggesting their success on older examples may stem from pre-training exposure rather than true reasoning ability.
Our work presents a scalable approach to creating and maintaining large-scale, high-quality datasets for advanced math reasoning, offering valuable insights into the capabilities and limitations of LLMs in this domain.
Our benchmark and code is available athttps://github.com/DSL-Lab/aops."
713,679d459debd8ffd557a2b136,cs.CL,https://arxiv.org/pdf/2501.14250,Siren: A Learning-Based Multi-Turn Attack Framework for Simulating Real-World Human Jailbreak Behaviors,"Yi Zhao, Youzhi Zhang","Computation and Language, Artificial Intelligence, Cryptography and Security","Large language models (LLMs) are widely used in real-world applications, raising concerns about their safety and trustworthiness.
While red-teaming with jailbreak prompts exposes the vulnerabilities of LLMs, current efforts focus primarily on single-turn attacks, overlooking the multi-turn strategies used by real-world adversaries. Existing multi-turn methods rely on static patterns or predefined logical chains, failing to account for the dynamic strategies during attacks.
We propose Siren, a learning-based multi-turn attack framework designed tosimulatereal-world humanjailbreak behaviors.
Siren consists of three stages: (1) training set construction utilizing Turn-Level LLM feedback (Turn-MF), (2) post-training attackers with supervised fine-tuning (SFT) and direct preference optimization (DPO), and (3) interactions between the attacking and target LLMs.
Experiments demonstrate that Siren achieves an attack success rate (ASR) of 90% with LLaMA-3-8B as the attacker against Gemini-1.5-Pro as the target model, and 70% with Mistral-7B against GPT-4o, significantly outperforming single-turn baselines.
Moreover, Siren with a 7B-scale model achieves performance comparable to a multi-turn baseline that leverages GPT-4o as the attacker, while requiring fewer turns and employing decomposition strategies that are better semantically aligned with attack goals.
We hope Siren inspires the development of stronger defenses against advanced multi-turn jailbreak attacks under realistic scenarios.
Code is available athttps://github.com/YiyiyiZhao/siren.Warning: This paper contains potentially harmful text."
714,679d459debd8ffd557a2b137,cs.CL,https://arxiv.org/pdf/2501.14225,Multi-agent KTO: Reinforcing Strategic Interactions of Large Language Model in Language Game,"Rong Ye, Yongxin Zhang, Yikai Zhang, Haoyu Kuang, Zhongyu Wei, Peng Sun","Computation and Language, Artificial Intelligence, Human-Computer Interaction","Achieving Artificial General Intelligence (AGI) requires AI agents that can not only make strategic decisions but also engage in flexible and meaningful communication.
Inspired by Wittgenstein’s language game theory in Philosophical Investigations, we propose that language agents can learn through in-context interaction rather than traditional multi-stage frameworks that separate decision-making from language expression. Using Werewolf, a social deduction game that tests language understanding, strategic interaction, and adaptability, we develop the Multi-agent Kahneman & Tversky’s Optimization (MaKTO).
MaKTO engages diverse models in extensive gameplay to generate unpaired desirable and unacceptable responses, then employs KTO to refine the model’s decision-making process.
In 9-player Werewolf games, MaKTO achieves a 61% average win rate across various models, outperforming GPT-4o and two-stage RL agents by relative improvements of 23.0% and 10.9%, respectively.
Notably, MaKTO also demonstrates human-like performance, winning 60% against expert players and showing only 49% detectability in Turing-style blind tests.
These results showcase MaKTO’s superior decision-making, strategic adaptation, and natural language generation in complex social deduction games.111Code and data will be available athttps://reneeye.github.io/MaKTO.html."
715,679d459debd8ffd557a2b138,cs.CL,https://arxiv.org/pdf/2501.14144,Test-Time Code-Switching for Cross-lingual Aspect Sentiment Triplet Extraction,"Dongming Sheng, Kexin Han, Hao Li, Yan Zhang, Yucheng Huang, Jun Lang, Wenqiang Liu",Computation and Language,
716,679d459debd8ffd557a2b139,cs.CL,https://arxiv.org/pdf/2501.14119,Autonomous Structural Memory Manipulation for Large Language Models Using Hierarchical Embedding Augmentation,"Derek Yotheringhay, Alistair Kirkland, Humphrey Kirkbride, Josiah Whitesteeple","Computation and Language, Artificial Intelligence","New innovations in model architectures have introduced hierarchical embedding augmentation as a means to redefine the representation of tokens through multi-level semantic structures, offering enhanced adaptability to complex linguistic inputs. Autonomous structural memory manipulation further advances this paradigm through dynamic memory reallocation mechanisms that prioritize critical contextual features while suppressing less relevant information, enabling scalable and efficient performance across diverse tasks. Experimental results reveal substantial improvements in computational efficiency, with marked reductions in processing overhead for longer input sequences, achieved through memory reorganization strategies that adapt to evolving contextual requirements. Hierarchical embeddings not only improved contextual alignment but also facilitated task generalization by capturing relationships at varying semantic granularities, ensuring coherence across layers without introducing significant computational redundancies. Comparative analysis against baseline models demonstrated unique advantages in accuracy, efficiency, and interpretability, particularly in tasks requiring complex contextual understanding or domain-specific adaptability. The ability to dynamically adjust token representations and memory configurations contributed to the model’s robustness under varied and unpredictable input conditions. Applications benefiting from these advancements include multi-domain generalization, interactive systems, and scenarios involving real-time decision-making, where traditional static memory architectures often face limitations. The proposed methodology combines advanced embedding and memory management strategies into a cohesive framework that addresses scalability challenges while preserving task-specific relevance."
717,679d459debd8ffd557a2b13a,cs.CL,https://arxiv.org/pdf/2501.14114,LeCoPCR: Legal Concept-guided Prior Case Retrieval for European Court of Human Rights cases,"T.Y.S.S. Santosh, Isaac Misael Olguín Nolasco, Matthias Grabmair",Computation and Language,"Prior case retrieval (PCR) is crucial for legal practitioners to find relevant precedent cases given the facts of a query case. Existing approaches often overlook the underlying semantic intent in determining relevance with respect to the query case. In this work, we propose LeCoPCR, a novel approach that explicitly generate intents in the form of legal concepts from a given query case facts and then augments the query with these concepts to enhance models understanding of semantic intent that dictates relavance. To overcome the unavailability of annotated legal concepts, we employ a weak supervision approach to extract key legal concepts from the reasoning section using Determinantal Point Process (DPP) to balance quality and diversity. Experimental results on the ECtHR-PCR dataset demonstrate the effectiveness of leveraging legal concepts and DPP-based key concept extraction."
718,679d459debd8ffd557a2b13b,cs.CL,https://arxiv.org/pdf/2501.14113,RELexED: Retrieval-Enhanced Legal Summarization with Exemplar Diversity,"T.Y.S.S. Santosh, Chen Jia, Patrick Goroncy, Matthias Grabmair",Computation and Language,"This paper addresses the task of legal summarization, which involves distilling complex legal documents into concise, coherent summaries. Current approaches often struggle with content theme deviation and inconsistent writing styles due to their reliance solely on source documents. We propose RELexED, a retrieval-augmented framework that utilizes exemplar summaries along with the source document to guide the model. RELexED employs a two-stage exemplar selection strategy, leveraging a determinantal point process to balance the trade-off between similarity of exemplars to the query and diversity among exemplars, with scores computed via influence functions. Experimental results on two legal summarization datasets demonstrate that RELexED significantly outperforms models that do not utilize exemplars and those that rely solely on similarity-based exemplar selection."
719,679d459debd8ffd557a2b13c,cs.CL,https://arxiv.org/pdf/2501.14112,CoPERLex: Content Planning with Event-based Representations for Legal Case Summarization,"T.Y.S.S. Santosh, Youssef Farag, Matthias Grabmair",Computation and Language,"Legal professionals often struggle with lengthy judgments and require efficient summarization for quick comprehension. To address this challenge, we investigate the need for structured planning in legal case summarization, particularly through event-centric representations that reflect the narrative nature of legal case documents. We propose our framework, CoPERLex, which operates in three stages: first, it performs content selection to identify crucial information from the judgment; second, the selected content is utilized to generate intermediate plans through event-centric representations modeled as Subject-Verb-Object tuples; and finally, it generates coherent summaries based on both the content and the structured plan. Our experiments on four legal summarization datasets demonstrate the effectiveness of integrating content selection and planning components, highlighting the advantages of event-centric plans over traditional entity-centric approaches in the context of legal judgements."
720,679d459debd8ffd557a2b13d,cs.CL,https://arxiv.org/pdf/2501.14105,MedSlice: Fine-Tuned Large Language Models for Secure Clinical Note Sectioning,"Joshua Davis, Thomas Sounack, Kate Sciacca, Jessie M Brain, Brigitte N Durieux, Nicole D Agaronnik, Charlotta Lindvall","Computation and Language, Artificial Intelligence, Information Retrieval, Machine Learning","ObjectiveExtracting sections from clinical notes is crucial for downstream analysis but is challenging due to variability in formatting and labor-intensive nature of manual sectioning. While proprietary large language models (LLMs) have shown promise, privacy concerns limit their accessibility. This study develops a pipeline for automated note sectioning using open-source LLMs, focusing on three sections: History of Present Illness, Interval History, and Assessment and Plan."
721,679d459debd8ffd557a2b13e,cs.CL,https://arxiv.org/pdf/2501.14082,Communicating Activations Between Language Model Agents,"Vignav Ramesh, Kenneth Li","Computation and Language, Artificial Intelligence, Machine Learning","Communication between multiple language model (LM) agents has been shown to scale up the reasoning ability of LMs. While natural language has been the dominant medium for inter-LM communication, it is not obvious this should be the standard: not only does natural language communication incur high inference costs that scale quickly with the number of both agents and messages, but also the decoding process abstracts away too much rich information that could be otherwise accessed from the internal activations. In this work, we propose a simple technique whereby LMs communicate viaactivations; concretely, we pause an LMB𝐵Bitalic_B’s computation at an intermediate layer, combine its current activation with another LMA𝐴Aitalic_A’s intermediate activation via some functionf𝑓fitalic_f, then passf𝑓fitalic_f’s output into the next layer ofB𝐵Bitalic_Band continue the forward pass till decoding is complete. This approach scales up LMs on new tasks withzeroadditional parameters and data, and saves asubstantial amount of computeover natural language communication. We test our method with various functional formsf𝑓fitalic_fon two experimental setups—multi-player coordination games and reasoning benchmarks—and find that it achieves up to27.0%percent27.027.0\%27.0 %improvement over natural language communication across datasets with<<<1/4141/41 / 4the compute, illustrating the superiority and robustness of activations as an alternative “language” for communication between LMs."
722,679d459debd8ffd557a2b13f,cs.CL,https://arxiv.org/pdf/2501.14079,Enhancing Biomedical Relation Extraction with Directionality,"Po-Ting Lai, Chih-Hsuan Wei, Shubo Tian, Robert Leaman, Zhiyong Lu",Computation and Language,
723,679d459debd8ffd557a2b140,cs.CL,https://arxiv.org/pdf/2501.14073,LLMs are Vulnerable to Malicious Prompts Disguised as Scientific Language,"Yubin Ge, Neeraja Kirtane, Hao Peng, Dilek Hakkani-Tür",Computation and Language,"As large language models (LLMs) have been deployed in various real-world settings, concerns about the harm they may propagate have grown. Various jailbreaking techniques have been developed to expose the vulnerabilities of these models and improve their safety. This work reveals that many state-of-the-art proprietary and open-source LLMs are vulnerable to malicious requests hidden behind scientific language. Specifically, our experiments with GPT4o, GPT4o-mini, GPT-4, LLama3-405B-Instruct, Llama3-70B-Instruct, Cohere, Gemini models on the StereoSet data demonstrate that, the models’ biases and toxicity substantially increase when prompted with requests that deliberately misinterpret social science and psychological studies as evidence supporting the benefits of stereotypical biases. Alarmingly, these models can also be manipulated to generate fabricated scientific arguments claiming that biases are beneficial, which can be used by ill-intended actors to systematically jailbreak even the strongest models like GPT.
Our analysis studies various factors that contribute to the models’ vulnerabilities to malicious requests in academic language. Mentioning author names and venues enhances the persuasiveness of some models, and the bias scores can increase as dialogues progress.
Our findings call for a more careful investigation on the use of scientific data in the training of LLMs."
724,679d459debd8ffd557a2b141,cs.CL,https://arxiv.org/pdf/2501.14037,Leveraging Large Language Models to Analyze Emotional and Contextual Drivers of Teen Substance Use in Online Discussions,"Jianfeng Zhu, Ruoming Jin, Hailong Jiang, Yulan Wang, Xinyu Zhang, Karin G. Coifman",Computation and Language,
725,679d459debd8ffd557a2b142,cs.CL,https://arxiv.org/pdf/2501.14002,"Advancing Math Reasoning in Language Models: The Impact of Problem-Solving Data, Data Synthesis Methods, and Training Stages","Zui Chen, Tianqiao Liu, Mi Tian, Qing Tong, Weiqi Luo, Zitao Liu","Computation and Language, Artificial Intelligence",
726,679d459debd8ffd557a2b143,cs.CL,https://arxiv.org/pdf/2501.13999,Framework for Progressive Knowledge Fusion in Large Language Models Through Structured Conceptual Redundancy Analysis,"Joseph Sakau, Evander Kozlowski, Roderick Thistledown, Basil Steinberger","Computation and Language, Artificial Intelligence","The organization of latent knowledge within large-scale models poses unique challenges when addressing overlapping representations and optimizing contextual accuracy. Conceptual redundancies embedded across layers often result in inefficiencies that affect both computational demands and task-specific outcomes. A framework was proposed to restructure these redundancies through advanced clustering techniques and dynamic thresholding, ensuring that critical semantic relationships are preserved while removing unnecessary overlaps. Evaluations revealed improved memory efficiency and faster inference times, alongside better alignment in latent knowledge clusters that enhanced interpretability. Improvements in error rates and adversarial robustness suggest that restructuring redundancies has broader implications for increasing model reliability across diverse applications. Comparative analyses highlighted reductions in resource consumption and notable gains in performance, particularly in translation and summarization tasks. Energy metrics demonstrated significant savings during training phases, further validating the practicality of the approach for real-world deployments. Representational fidelity was also enhanced, with latent space evaluations indicating better cluster alignment and higher semantic consistency. The methodology bridges a key gap in model optimization through directly addressing redundancies at the structural level. Its application opens avenues for scalable, efficient, and contextually aware systems that can adapt to complex, domain-specific tasks without compromising on performance."
727,679d459debd8ffd557a2b144,cs.CL,https://arxiv.org/pdf/2501.13993,CAPRAG: A Large Language Model Solution for Customer Service and Automatic Reporting using Vector and Graph Retrieval-Augmented Generation,"Hamza Landolsi, Kais Letaief, Nizar Taghouti, Ines Abdeljaoued-Tej","Computation and Language, Artificial Intelligence, Information Retrieval","The introduction of new features and services in the banking sector often overwhelms customers, creating an opportunity for banks to enhance user experience through financial chatbots powered by large language models (LLMs). We initiated an AI agent designed to provide customers with relevant information about banking services and insights from annual reports. We proposed a hybrid Customer Analysis Pipeline Retrieval-Augmented Generation (CAPRAG) that effectively addresses both relationship-based and contextual queries, thereby improving customer engagement in the digital banking landscape. To implement this, we developed a processing pipeline to refine text data, which we utilized in two main frameworks: Vector RAG and Graph RAG. This dual approach enables us to populate both vector and graph databases with processed data for efficient retrieval. The Cypher query component is employed to effectively query the graph database. When a user submits a query, it is first expanded by a query expansion module before being routed to construct a final query from the hybrid Knowledge Base (KB). This final query is then sent to an open-source LLM for response generation. Overall, our innovative, designed to international banks, serves bank’s customers in an increasingly complex digital environment, enhancing clarity and accessibility of information."
728,679d459debd8ffd557a2b145,cs.CL,https://arxiv.org/pdf/2501.13984,Comprehensive Modeling and Question Answering of Cancer Clinical Practice Guidelines using LLMs,"Bhumika Gupta, Pralaypati Ta, Keerthi Ram, Mohanasankar Sivaprakasam","Computation and Language, Artificial Intelligence, Machine Learning",
729,679d459debd8ffd557a2b146,cs.CL,https://arxiv.org/pdf/2501.13983,AdEval: Alignment-based Dynamic Evaluation to Mitigate Data Contamination in Large Language Models,Yang Fan,"Computation and Language, Artificial Intelligence","As Large Language Models (LLMs) are pretrained on massive-scale corpora, the issue of data contamination has become increasingly severe, leading to potential overestimation of model performance during evaluation. To address this, we propose AdEval (Alignment-based Dynamic Evaluation), a dynamic data evaluation method aimed at mitigating the impact of data contamination on evaluation reliability. AdEval extracts key knowledge points and main ideas to align dynamically generated questions with static data’s core concepts. It also leverages online search to provide detailed explanations of related knowledge points, thereby creating high-quality evaluation samples with robust knowledge support. Furthermore, AdEval incorporates mechanisms to control the number and complexity of questions, enabling dynamic alignment and flexible adjustment. This ensures that the generated questions align with the complexity of static data while supporting varied complexity levels. Based on Bloom’s taxonomy, AdEval conducts a multi-dimensional evaluation of LLMs across six cognitive levels: remembering, understanding, applying, analyzing, evaluating, and creating. Experimental results on multiple datasets demonstrate that AdEval effectively reduces the impact of data contamination on evaluation outcomes, enhancing both the fairness and reliability of the evaluation process."
730,679d459debd8ffd557a2b147,cs.CL,https://arxiv.org/pdf/2501.13978,Chain of Grounded Objectives: Bridging Process and Goal-oriented Prompting for Code Generation,"Sangyeop Yeo, Seung-won Hwang, Yu-Seung Ma","Computation and Language, Artificial Intelligence, Software Engineering","The use of Large Language Models (LLMs) for code generation has gained significant attention in recent years. Existing methods often aim to improve the quality of generated code by incorporating additional contextual information or guidance into input prompts. Many of these approaches adopt sequential reasoning strategies, mimicking human-like step-by-step thinking. However, such strategies may constrain flexibility, as they do not always align with the structured characteristics of programming languages. This paper introduces theChain of Grounded Objectives (CGO), a method that embeds functional objectives into input prompts to enhance code generation. By leveraging appropriately structured objectives as input and avoiding explicit sequential procedures, CGO adapts effectively to the structured nature of programming tasks. Empirical evaluations demonstrate that CGO effectively enhances code generation, addressing limitations of existing approaches."
731,679d459debd8ffd557a2b148,cs.CL,https://arxiv.org/pdf/2501.13977,Re-ranking Using Large Language Models for Mitigating Exposure to Harmful Content on Social Media Platforms,"Rajvardhan Oak, Muhammad Haroon, Claire Jo, Magdalena Wojcieszak, Anshuman Chhabra","Computation and Language, Artificial Intelligence, Computers and Society, Social and Information Networks","Social media platforms utilize Machine Learning (ML) and Artificial Intelligence (AI) powered recommendation algorithms to maximize user engagement, which can result in inadvertent exposure to harmful content.
Current moderation efforts, reliant on classifiers trained with extensive human-annotated data, struggle with scalability and adapting to new forms of harm.
To address these challenges, we propose a novel re-ranking approach using Large Language Models (LLMs) in zero-shot and few-shot settings. Our method dynamically assesses and re-ranks content sequences, effectively mitigating harmful content exposure without requiring extensive labeled data.
Alongside traditional ranking metrics, we also introduce two new metrics to evaluate the effectiveness of re-ranking in reducing exposure to harmful content.
Through experiments on three datasets, three models and across three configurations, we demonstrate that our LLM-based approach significantly outperforms existing proprietary moderation approaches, offering a scalable and adaptable solution for harm mitigation."
732,679d459debd8ffd557a2b149,cs.CL,https://arxiv.org/pdf/2501.13976,Towards Safer Social Media Platforms: Scalable and Performant Few-Shot Harmful Content Moderation Using Large Language Models,"Akash Bonagiri, Lucen Li, Rajvardhan Oak, Zeerak Babar, Magdalena Wojcieszak, Anshuman Chhabra","Computation and Language, Artificial Intelligence, Computers and Society, Social and Information Networks","The prevalence of harmful content on social media platforms poses significant risks to users and society, necessitating more effective and scalable content moderation strategies. Current approaches rely on human moderators, supervised classifiers, and large volumes of training data, and often struggle with scalability, subjectivity, and the dynamic nature of harmful content (e.g., violent content, dangerous challenge trends, etc.). To bridge these gaps, we utilize Large Language Models (LLMs) to undertake few-shot dynamic content moderation via in-context learning. Through extensive experiments on multiple LLMs, we demonstrate that our few-shot approaches can outperform existing proprietary baselines (Perspective and OpenAI Moderation) as well as prior state-of-the-art few-shot learning methods, in identifying harm. We also incorporate visual information (video thumbnails) and assess if different multimodal techniques improve model performance. Our results underscore the significant benefits of employing LLM based methods for scalable and dynamic harmful content moderation online."
733,679d459debd8ffd557a2b14a,cs.CL,https://arxiv.org/pdf/2501.13959,Assisting Mathematical Formalization with A Learning-based Premise Retriever,"Yicheng Tao, Haotian Liu, Shanwen Wang, Hongteng Xu","Computation and Language, Artificial Intelligence, Information Retrieval","Premise selection is a crucial yet challenging step in mathematical formalization, especially for users with limited experience.
Due to the lack of available formalization projects, existing approaches that leverage language models often suffer from data scarcity.
In this work, we introduce an innovative method for training a premise retriever to support the formalization of mathematics.
Our approach employs a BERT model to embed proof states and premises into a shared latent space. The retrieval model is trained within a contrastive learning framework and incorporates a domain-specific tokenizer along with a fine-grained similarity computation method.
Experimental results show that our model is highly competitive compared to existing baselines, achieving strong performance while requiring fewer computational resources.
Performance is further enhanced through the integration of a re-ranking module.
To streamline the formalization process, we will release a search engine that enables users to query Mathlib theorems directly using proof states, significantly improving accessibility and efficiency.
Codes are available athttps://github.com/ruc-ai4math/Premise-Retrieval."
734,679d459debd8ffd557a2b14b,cs.CL,https://arxiv.org/pdf/2501.13958,A Survey of Graph Retrieval-Augmented Generation for Customized Large Language Models,"Qinggang Zhang, Shengyuan Chen, Yuanchen Bei, Zheng Yuan, Huachi Zhou, Zijin Hong, Junnan Dong, Hao Chen, Yi Chang, Xiao Huang","Computation and Language, Artificial Intelligence, Information Retrieval","Large language models (LLMs) have demonstrated remarkable capabilities in a wide range of tasks, yet their application to specialized domains remains challenging due to the need for deep expertise. Retrieval-Augmented generation (RAG) has emerged as a promising solution to customize LLMs for professional fields by seamlessly integrating external knowledge bases, enabling real-time access to domain-specific expertise during inference. Despite its potential, traditional RAG systems, based on flat text retrieval, face three critical challenges: (i) complex query understanding in professional contexts, (ii) difficulties in knowledge integration across distributed sources, and (iii) system efficiency bottlenecks at scale. This survey presents a systematic analysis of Graph-based Retrieval-Augmented Generation (GraphRAG), a new paradigm that revolutionizes domain-specific LLM applications. GraphRAG addresses traditional RAG limitations through three key innovations: (i) graph-structured knowledge representation that explicitly captures entity relationships and domain hierarchies, (ii) efficient graph-based retrieval techniques that enable context-preserving knowledge retrieval with multihop reasoning ability, and (iii) structure-aware knowledge integration algorithms that leverage retrieved knowledge for accurate and logical coherent generation of LLMs.
In this survey, we systematically analyze the technical foundations of GraphRAG and examine current implementations across various professional domains, identifying key technical challenges and promising research directions. All the related resources of GraphRAG, including research papers, open-source data, and projects, are collected for the community inhttps://github.com/DEEP-PolyU/Awesome-GraphRAG."
735,679d459debd8ffd557a2b14c,cs.CL,https://arxiv.org/pdf/2501.13957,Benchmarking Generative AI for Scoring Medical Student Interviews in Objective Structured Clinical Examinations (OSCEs),"Jadon Geathers, Yann Hicke, Colleen Chan, Niroop Rajashekar, Justin Sewell, Susannah Cornes, Rene Kizilcec, Dennis Shung","Computation and Language, Artificial Intelligence",
736,679d459debd8ffd557a2b14d,cs.CL,https://arxiv.org/pdf/2501.13956,Zep: A Temporal Knowledge Graph Architecture for Agent Memory,"Preston Rasmussen, Pavlo Paliychuk, Travis Beauvais, Jack Ryan, Daniel Chalef","Computation and Language, Artificial Intelligence, Information Retrieval","We introduce Zep, a novel memory layer service for AI agents that outperforms the current state-of-the-art system, MemGPT, in the Deep Memory Retrieval (DMR) benchmark. Additionally, Zep excels in more comprehensive and challenging evaluations than DMR that better reflect real-world enterprise use cases. While existing retrieval-augmented generation (RAG) frameworks for large language model (LLM)-based agents are limited to static document retrieval, enterprise applications demand dynamic knowledge integration from diverse sources including ongoing conversations and business data. Zep addresses this fundamental limitation through its core component Graphiti—a temporally-aware knowledge graph engine that dynamically synthesizes both unstructured conversational data and structured business data while maintaining historical relationships. In the DMR benchmark, which the MemGPT team established as their primary evaluation metric, Zep demonstrates superior performance (94.8% vs 93.4%). Beyond DMR, Zep’s capabilities are further validated through the more challenging LongMemEval benchmark, which better reflects enterprise use cases through complex temporal reasoning tasks. In this evaluation, Zep achieves substantial results with accuracy improvements of up to 18.5% while simultaneously reducing response latency by 90% compared to baseline implementations. These results are particularly pronounced in enterprise-critical tasks such as cross-session information synthesis and long-term context maintenance, demonstrating Zep’s effectiveness for deployment in real-world applications."
737,679d459debd8ffd557a2b14e,cs.CL,https://arxiv.org/pdf/2501.13955,Guided Persona-based AI Surveys: Can we replicate personal mobility preferences at scale using LLMs?,"Ioannis Tzachristas, Santhanakrishnan Narayanan, Constantinos Antoniou","Computation and Language, Artificial Intelligence, Computers and Society",
738,679d459debd8ffd557a2b14f,cs.CL,https://arxiv.org/pdf/2501.13954,Chat3GPP: An Open-Source Retrieval-Augmented Generation Framework for 3GPP Documents,"Long Huang, Ming Zhao, Limin Xiao, Xiujun Zhang, Jungang Hu","Computation and Language, Artificial Intelligence, Distributed, Parallel, and Cluster Computing, Information Retrieval","The 3rd Generation Partnership Project (3GPP) documents is key standards in global telecommunications, while posing significant challenges for engineers and researchers in the telecommunications field due to the large volume and complexity of their contents as well as the frequent updates. Large language models (LLMs) have shown promise in natural language processing tasks, but their general-purpose nature limits their effectiveness in specific domains like telecommunications. To address this, we propose Chat3GPP1, an open-source retrieval-augmented generation (RAG) framework tailored for 3GPP specifications. By combining chunking strategies, hybrid retrieval and efficient indexing methods, Chat3GPP can efficiently retrieve relevant information and generate accurate responses to user queries without requiring domain-specific fine-tuning, which is both flexible and scalable, offering significant potential for adapting to other technical standards beyond 3GPP. We evaluate Chat3GPP on two telecom-specific datasets and demonstrate its superior performance compared to existing methods, showcasing its potential for downstream tasks like protocol generation and code automation."
739,679d459debd8ffd557a2b150,cs.CL,https://arxiv.org/pdf/2501.13953,Redundancy Principles for MLLMs Benchmarks,"Zicheng Zhang, Xiangyu Zhao, Xinyu Fang, Chunyi Li, Xiaohong Liu, Xiongkuo Min, Haodong Duan, Kai Chen, Guangtao Zhai","Computation and Language, Artificial Intelligence","With the rapid iteration of Multi-modality Large Language Models (MLLMs) and the evolving demands of the field,
the number of benchmarks produced annually has surged into the hundreds.
The rapid growth has inevitably led to significant redundancy among benchmarks.
Therefore, it is crucial to take a step back and critically assess the current state of redundancy and propose targeted principles for constructing effective MLLM benchmarks.
In this paper, we focus on redundancy from three key perspectives:1)Redundancy of benchmark capability dimensions,2)Redundancy in the number of test questions,
and3)Cross-benchmark redundancy within specific domains.
Through the comprehensive analysis over hundreds of MLLMs’ performance across more than 20 benchmarks,
we aim to quantitatively measure the level of redundancy lies in existing MLLM evaluations,
provide valuable insights to guide the future development of MLLM benchmarks,
and offer strategies to refine and address redundancy issues effectively."
740,679d459debd8ffd557a2b151,cs.CL,https://arxiv.org/pdf/2501.13952,The Dual-use Dilemma in LLMs: Do Empowering Ethical Capacities Make a Degraded Utility?,"Yiyi Zhang, Xingyu Chen, Kexin Chen, Yuyang Du, Xilin Dang, Pheng-Ann Heng","Computation and Language, Artificial Intelligence","Recent years have witnessed extensive efforts to enhance Large Language Models (LLMs) across various domains, alongside growing attention to their ethical implications. However, a critical challenge remains largely overlooked: LLMs must balance between rejecting harmful requests for safety and accommodating legitimate ones for utility. This paper presents a Direct Preference Optimization (DPO) based alignment framework that achieves better overall performance by addressing this ethical-utility trade-off, using chemical domain applications as a proof-of-concept. Our alignment pipeline starts with a GPT-assisted three-phase data generation scheme, in which we create LibraChemQA, a chemical question-answering dataset comprising 31.6k triplet instances. By incorporating an innovative balanced seed in the data generation process, our framework systematically considers both legitimate and illegitimate requests. The framework also introduces a rephrasing mechanism for efficient data augmentation that enhances the model’s chemical comprehension. We further develop a novel hybrid evaluation scheme with LLM judges for precise assessment of both safety and utility. Experimental results demonstrate our model’s substantial improvements in overall performance where both safety and utility are considered - our resulting model, LibraChem, outperforms leading LLMs including Claude-3, GPT-4o, and LLaMA-3 by margins of 13.44%, 7.16%, and 7.10% respectively on our released benchmark."
741,679d459debd8ffd557a2b152,cs.CL,https://arxiv.org/pdf/2501.13951,A Layered Multi-Expert Framework for Long-Context Mental Health Assessments,"Jinwen Tang, Qiming Guo, Wenbo Sun, Yi Shang","Computation and Language, Artificial Intelligence","Long-form mental health assessments pose unique challenges for large language models (LLMs), which often exhibit hallucinations or inconsistent reasoning when handling extended, domain-specific contexts. We introduceStacked Multi-Model Reasoning (SMMR), a layered framework that leverages multiple LLMs and specialized smaller models as coequal “experts.” Early layers isolate short, discrete subtasks, while later layers integrate and refine these partial outputs through more advanced long-context models. We evaluate SMMR on the DAIC-WOZ depression-screening dataset and 48 curated case studies with psychiatric diagnoses, demonstrating consistent improvements over single-model baselines in terms of accuracy, F1-score, and PHQ-8 error reduction. By harnessing diverse “second opinions,” SMMR mitigates hallucinations, captures subtle clinical nuances, and enhances reliability in high-stakes mental health assessments. Our findings underscore the value of multi-expert frameworks for more trustworthy AI-driven screening."
742,679d459debd8ffd557a2b153,cs.CL,https://arxiv.org/pdf/2501.13949,"Can OpenAI o1 Reason Well in Ophthalmology? A 6,990-Question Head-to-Head Evaluation Study","Sahana Srinivasan, Xuguang Ai, Minjie Zou, Ke Zou, Hyunjae Kim, Thaddaeus Wai Soon Lo, Krithi Pushpanathan, Yiming Kong, Anran Li, Maxwell Singer, Kai Jin, Fares Antaki, David Ziyou Chen, Dianbo Liu, Ron A. Adelman, Qingyu Chen, Yih Chung Tham","Computation and Language, Artificial Intelligence",
743,679d459debd8ffd557a2b154,cs.CL,https://arxiv.org/pdf/2501.13948,Longitudinal Abuse and Sentiment Analysis of Hollywood Movie Dialogues using LLMs,"Rohitash Chandra, Guoxiang Ren, Group-H","Computation and Language, Artificial Intelligence","Over the past decades, there has been an increasing concern about the prevalence of abusive and violent content in Hollywood movies. This study uses Large Language Models (LLMs) to explore the longitudinal abuse and sentiment analysis of Hollywood Oscar and blockbuster movie dialogues from 1950 to 2024. By employing fine-tuned LLMs, we analyze subtitles for over a thousand movies categorised into four genres to examine the trends and shifts in emotional and abusive content over the past seven decades. Our findings reveal significant temporal changes in movie dialogues, which reflect broader social and cultural influences. Overall, the emotional tendencies in the films are diverse, and the detection of abusive content also exhibits significant fluctuations. The results show a gradual rise in abusive content in recent decades, reflecting social norms and regulatory policy changes. Genres such as thrillers still present a higher frequency of abusive content that emphasises the ongoing narrative role of violence and conflict. At the same time, underlying positive emotions such as humour and optimism remain prevalent in most of the movies. Furthermore, the gradual increase of abusive content in movie dialogues has been significant over the last two decades, where Oscar-nominated movies overtook the top ten blockbusters."
744,679d459debd8ffd557a2b155,cs.CL,https://arxiv.org/pdf/2501.13947,A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods,"Lilian Some, Wenli Yang, Michael Bain, Byeong Kang","Computation and Language, Artificial Intelligence","The rapid development of artificial intelligence has brought about substantial advancements in the field. One promising direction is the integration of Large Language Models (LLMs) with structured knowledge-based systems. This approach aims to enhance AI capabilities by combining the generative language understanding of LLMs with the precise knowledge representation of structured systems. This survey explores the synergy between LLMs and knowledge bases, focusing on real-world applications and addressing associated technical, operational, and ethical challenges. Through a comprehensive literature review, the study identifies critical issues and evaluates existing solutions. The paper highlights the benefits of integrating generative AI with knowledge bases, including improved data contextualization, enhanced model accuracy, and better utilization of knowledge resources. The findings provide a detailed overview of the current state of research, identify key gaps, and offer actionable recommendations. These insights contribute to advancing AI technologies and support their practical deployment across various sectors."
745,679d459debd8ffd557a2b156,cs.CL,https://arxiv.org/pdf/2501.13946,Hallucination Mitigation using Agentic AI Natural Language-Based Frameworks,"Diego Gosmar, Deborah A. Dahl","Computation and Language, Artificial Intelligence, Multiagent Systems","Hallucinations remain a significant challenge in current Generative AI models, undermining trust in AI systems and their reliability. This study investigates how orchestrating multiple specialized Artificial Intelligent Agents can help mitigate such hallucinations, with a focus on systems leveraging Natural Language Processing (NLP) to facilitate seamless agent interactions. To achieve this, we design a pipeline that introduces over three hundred prompts, purposefully crafted to induce hallucinations, into a front-end agent. The outputs are then systematically reviewed and refined by second- and third-level agents, each employing distinct large language models and tailored strategies to detect unverified claims, incorporate explicit disclaimers, and clarify speculative content.Additionally, we introduce a set of novel Key Performance Indicators (KPIs) specifically designed to evaluate hallucination score levels. These metrics offer a structured and quantifiable framework for assessing the impact of each agent’s refinements on the factuality and clarity of AI-generated responses. A dedicated fourth-level AI agent is employed to evaluate these KPIs, providing detailed assessments and ensuring accurate quantification of shifts in hallucination-related behaviors.A core component of this investigation is the use of the OVON (Open Voice Network) framework, which relies on universal NLP-based interfaces to transfer contextual information among agents. Through structured JSON messages, each agent communicates its assessment of the hallucination likelihood and the reasons underlying questionable content, thereby enabling the subsequent stage to refine the text without losing context. Experimental results suggest that this multi-agent, JSON-based approach not only lowers the overall hallucination scores but also renders speculative content more transparent and clearly demarcated from factual claims, improving the AI explainability level.Our findings underscore the feasibility of multi-agent orchestration and highlight the importance of maintaining a structured exchange of meta-information - particularly through formats supporting Natural Language API - to enhance the reliability and interpretability of AI-generated responses.
The results demonstrate that employing multiple specialized agents capable of interoperating with each other through NLP-based agentic frameworks - such as the OVON framework - can yield promising outcomes in hallucination mitigation, ultimately bolstering trust within the AI community."
746,679d459debd8ffd557a2b157,cs.CL,https://arxiv.org/pdf/2501.13945,Self-Explanation in Social AI Agents,"Rhea Basappa, Mustafa Tekman, Hong Lu, Benjamin Faught, Sandeep Kakar, Ashok K. Goel","Computation and Language, Artificial Intelligence, Computers and Society",
747,679d459debd8ffd557a2b158,cs.CL,https://arxiv.org/pdf/2501.13944,Fanar: An Arabic-Centric Multimodal Generative AI Platform,"Fanar Team, Ummar Abbas, Mohammad Shahmeer Ahmad, Firoj Alam, Enes Altinisik, Ehsannedin Asgari, Yazan Boshmaf, Sabri Boughorbel, Sanjay Chawla, Shammur Chowdhury, Fahim Dalvi, Kareem Darwish, Nadir Durrani, Mohamed Elfeky, Ahmed Elmagarmid, Mohamed Eltabakh, Masoomali Fatehkia, Anastasios Fragkopoulos, Maram Hasanain, Majd Hawasly, Mus'ab Husaini, Soon-Gyo Jung, Ji Kim Lucas, Walid Magdy, Safa Messaoud, Abubakr Mohamed, Tasnim Mohiuddin, Basel Mousi, Hamdy Mubarak, Ahmad Musleh, Zan Naeem, Mourad Ouzzani, Dorde Popovic, Amin Sadeghi, Husrev Taha Sencar, Mohammed Shinoy, Omar Sinan, Yifan Zhang, Ahmed Ali, Yassine El Kheir, Xiaosong Ma, Chaoyi Ruan","Computation and Language, Artificial Intelligence",
748,679d459debd8ffd557a2b159,cs.CL,https://arxiv.org/pdf/2501.13943,Language Representation Favored Zero-Shot Cross-Domain Cognitive Diagnosis,"Shuo Liu, Zihan Zhou, Yuanhao Liu, Jing Zhang, Hong Qian","Computation and Language, Artificial Intelligence, Computers and Society, Machine Learning","Cognitive diagnosis aims to infer students’ mastery levels based on their historical response logs. However, existing cognitive diagnosis models (CDMs), which rely on ID embeddings, often have to train specific models on specific domains. This limitation may hinder their directly practical application in various target domains, such as different subjects (e.g., Math, English and Physics) or different education platforms (e.g., ASSISTments, Junyi Academy and Khan Academy). To address this issue, this paper proposes the language representation favored zero-shot cross-domain cognitive diagnosis (LRCD). Specifically, LRCD first analyzes the behavior patterns of students, exercises and concepts in different domains, and then describes the profiles of students, exercises and concepts using textual descriptions. Via recent advanced text-embedding modules, these profiles can be transformed to vectors in the unified language space. Moreover, to address the discrepancy between the language space and the cognitive diagnosis space, we propose language-cognitive mappers in LRCD to learn the mapping from the former to the latter. Then, these profiles can be easily and efficiently integrated and trained with existing CDMs. Extensive experiments show that training LRCD on real-world datasets can achieve commendable zero-shot performance across different target domains, and in some cases, it can even achieve competitive performance with some classic CDMs trained on the full response data on target domains. Notably, we surprisingly find that LRCD can also provide interesting insights into the differences between various subjects (such as humanities and sciences) and sources (such as primary and secondary education)."
749,679d459debd8ffd557a2b15a,cs.CL,https://arxiv.org/pdf/2501.14705,The Karp Dataset,"Mason DiCicco, Eamon Worden, Conner Olsen, Nikhil Gangaram, Daniel Reichman, Neil Heffernan","Machine Learning, Computation and Language","Understanding the mathematical reasoning capabilities of Large Language Models (LLMs) is a central topic in the study of artificial intelligence. This new domain necessitates the creation ofdatasets of reasoning tasksfor both training and benchmarking the performance of LLMs. To this end, we introduce theKarp dataset: The first dataset composed of detailed proofs of NP-completeness reductions. The reductions vary in difficulty, ranging from simple exercises of undergraduate courses to more challenging reductions from academic papers.
We compare the performance of state-of-the-art models on this task and demonstrate the effect of fine-tuning with the Karp dataset on reasoning capacity."
750,679d459debd8ffd557a2b15b,cs.CL,https://arxiv.org/pdf/2501.14342,Chain-of-Retrieval Augmented Generation,"Liang Wang, Haonan Chen, Nan Yang, Xiaolong Huang, Zhicheng Dou, Furu Wei","Information Retrieval, Computation and Language","This paper introduces an approach for training o1-like RAG models
that retrieve and reason over relevant information step by step before generating the final answer.
Conventional RAG methods usually perform a single retrieval step before the generation process,
which limits their effectiveness in addressing complex queries due to imperfect retrieval results.
In contrast,
our proposed method,CoRAG(Chain-of-RetrievalAugmentedGeneration),
allows the model to dynamically reformulate the query based on the evolving state.
To train CoRAG effectively,
we utilize rejection sampling to automatically generate intermediate retrieval chains,
thereby augmenting existing RAG datasets that only provide the correct final answer.
At test time,
we propose various decoding strategies to scale the model’s test-time compute
by controlling the length and number of sampled retrieval chains.
Experimental results across multiple benchmarks validate the efficacy of CoRAG,
particularly in multi-hop question answering tasks,
where we observe more than10101010points improvement in EM score compared to strong baselines.
On the KILT benchmark,
CoRAG establishes a new state-of-the-art performance across a diverse range of knowledge-intensive tasks.
Furthermore,
we offer comprehensive analyses to understand the scaling behavior of CoRAG,
laying the groundwork for future research aimed at developing factual and grounded foundation models."
751,679d459debd8ffd557a2b15c,cs.CL,https://arxiv.org/pdf/2501.14300,"Fast Think-on-Graph: Wider, Deeper and Faster Reasoning of Large Language Model on Knowledge Graph","Xujian Liang, Zhaoquan Gu","Artificial Intelligence, Computation and Language, Machine Learning, Social and Information Networks","Graph Retrieval Augmented Generation (GRAG) is a novel paradigm that takes the naive RAG system a step further by integrating graph information, such as knowledge graph (KGs), into large-scale language models (LLMs) to mitigate hallucination. However, existing GRAG still encounter limitations: 1) simple paradigms usually fail with the complex problems due to the narrow and shallow correlations capture from KGs 2) methods of strong coupling with KGs tend to be high computation cost and time consuming if the graph is dense. In this paper, we propose the Fast Think-on-Graph (FastToG), an innovative paradigm for enabling LLMs to think “community by community” within KGs. To do this, FastToG employs community detection for deeper correlation capture and two stages community pruning - coarse and fine pruning for faster retrieval. Furthermore, we also develop two Community-to-Text methods to convert the graph structure of communities into textual form for better understanding by LLMs. Experimental results demonstrate the effectiveness of FastToG, showcasing higher accuracy, faster reasoning, and better explainability compared to the previous works."
752,679d459debd8ffd557a2b15d,cs.CL,https://arxiv.org/pdf/2501.14249,Humanity's Last Exam,"Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Daron Anderson, Tung Nguyen, Mobeen Mahmood, Fiona Feng, Steven Y. Feng, Haoran Zhao, Michael Yu, Varun Gangal, Chelsea Zou, Zihan Wang, Jessica P. Wang, Pawan Kumar, Oleksandr Pokutnyi, Robert Gerbicz, Serguei Popov, John-Clark Levin, Mstyslav Kazakov, Johannes Schmitt, Geoff Galgon, Alvaro Sanchez, Yongki Lee, Will Yeadon, Scott Sauers, Marc Roth, Chidozie Agu, Søren Riis, Fabian Giska, Saiteja Utpala, Zachary Giboney, Gashaw M. Goshu, Joan of Arc Xavier, Sarah-Jane Crowson, Mohinder Maheshbhai Naiya, Noah Burns, Lennart Finke, Zerui Cheng, Hyunwoo Park, Francesco Fournier-Facio, John Wydallis, Mark Nandor, Ankit Singh, Tim Gehrunger, Jiaqi Cai, Ben McCarty, Darling Duclosel, Jungbae Nam, Jennifer Zampese, Ryan G. Hoerr, Aras Bacho, Gautier Abou Loume, Abdallah Galal, Hangrui Cao, Alexis C Garretson, Damien Sileo, Qiuyu Ren, Doru Cojoc, Pavel Arkhipov, Usman Qazi, Lianghui Li, Sumeet Motwani, Christian Schroeder de Witt, Edwin Taylor, Johannes Veith, Eric Singer, Taylor D. Hartman, Paolo Rissone, Jaehyeok Jin, Jack Wei Lun Shi, Chris G. Willcocks, Joshua Robinson, Aleksandar Mikov, Ameya Prabhu, Longke Tang, Xavier Alapont, Justine Leon Uro, Kevin Zhou, Emily de Oliveira Santos, Andrey Pupasov Maksimov, Edward Vendrow, Kengo Zenitani, Julien Guillod, Yuqi Li, Joshua Vendrow, Vladyslav Kuchkin, Ng Ze-An","Machine Learning, Artificial Intelligence, Computation and Language","Benchmarks are important tools for tracking the rapid advancements in large language model (LLM) capabilities.
However, benchmarks are not keeping pace in difficulty: LLMs now achieve over 90% accuracy on popular benchmarks like MMLU, limiting informed measurement of state-of-the-art LLM capabilities. In response, we introduceHumanity’s Last Exam(HLE), a multi-modal benchmark at the frontier of human knowledge, designed to be the final closed-ended academic benchmark of its kind with broad subject coverage.HLEconsists of3,00030003{,}0003 , 000questions across dozens of subjects, including mathematics, humanities, and the natural sciences.HLEis developed globally by subject-matter experts and consists of multiple-choice and short-answer questions suitable for automated grading. Each question has a known solution that is unambiguous and easily verifiable, but cannot be quickly answered via internet retrieval. State-of-the-art LLMs demonstrate low accuracy and calibration onHLE, highlighting a significant gap between current LLM capabilities and the expert human frontier on closed-ended academic questions. To inform research and policymaking upon a clear understanding of model capabilities, we publicly releaseHLEathttps://lastexam.ai."
753,679d459debd8ffd557a2b15e,cs.CL,https://arxiv.org/pdf/2501.14011,QuanTaxo: A Quantum Approach to Self-Supervised Taxonomy Expansion,"Sahil Mishra, Avi Patni, Niladri Chatterjee, Tanmoy Chakraborty","Social and Information Networks, Computation and Language","A taxonomy is a hierarchical graph containing knowledge to provide valuable insights for various web applications. Online retail organizations likeMicrosoftandAmazonutilize taxonomies to improve product recommendations and optimize advertisement by enhancing query interpretation. However, the manual construction of taxonomies requires significant human effort. As web content continues to expand at an unprecedented pace, existing taxonomies risk becoming outdated, struggling to incorporate new and emerging information effectively. As a consequence, there is a growing need for dynamic taxonomy expansion to keep them relevant and up-to-date. Existing taxonomy expansion methods often rely on classical word embeddings to represent entities. However, these embeddings fall short in capturing hierarchical polysemy, where an entity’s meaning can vary based on its position in the hierarchy and its surrounding context. To address this challenge, we introduceQuanTaxo, an innovative quantum-inspired framework for taxonomy expansion.QuanTaxoencodes entity representations in quantum space, effectively modeling hierarchical polysemy by leveraging the principles of Hilbert space to capture interference effects between entities, yielding richer and more nuanced representations. Comprehensive experiments on four real-world benchmark datasets show thatQuanTaxosignificantly outperforms classical embedding models, achieving substantial improvements of 18.45% in accuracy, 20.5% in Mean Reciprocal Rank, and 17.87% in Wu & Palmer metrics across eight classical embedding-based baselines. We further highlight the superiority ofQuanTaxowith extensive ablation and case studies."
754,679d459debd8ffd557a2b15f,cs.CL,https://arxiv.org/pdf/2501.13941,GaussMark: A Practical Approach for Structural Watermarking of Language Models,"Adam Block, Ayush Sekhari, Alexander Rakhlin","Cryptography and Security, Artificial Intelligence, Computation and Language, Machine Learning",
755,679d459debd8ffd557a2b160,cs.CL,https://arxiv.org/pdf/2501.13936,Evaluating Computational Accuracy of Large Language Models in Numerical Reasoning Tasks for Healthcare Applications,Arjun R. Malghan,"Artificial Intelligence, Computation and Language, Machine Learning",
756,679d459debd8ffd557a2b161,cs.CR,https://arxiv.org/pdf/2501.18565,BounTCHA: A CAPTCHA Utilizing Boundary Identification in AI-extended Videos,"Lehao Lin, Ke Wang, Maha Abdallah, Wei Cai","Cryptography and Security, Artificial Intelligence, Human-Computer Interaction","In recent years, the rapid development of artificial intelligence (AI) especially multi-modal Large Language Models (MLLMs), has enabled it to understand text, images, videos, and other multimedia data, allowing AI systems to execute various tasks based on human-provided prompts. However, AI-powered bots have increasingly been able to bypass most existing CAPTCHA systems, posing significant security threats to web applications. This makes the design of new CAPTCHA mechanisms an urgent priority. We observe that humans are highly sensitive to shifts and abrupt changes in videos, while current AI systems still struggle to comprehend and respond to such situations effectively. Based on this observation, we design and implement BounTCHA, a CAPTCHA mechanism that leverages human perception of boundaries in video transitions and disruptions. By utilizing AI’s capability to expand original videos with prompts, we introduce unexpected twists and changes to create a pipeline for generating short videos for CAPTCHA purposes. We develop a prototype and conduct experiments to collect data on humans’ time biases in boundary identification. This data serves as a basis for distinguishing between human users and bots. Additionally, we perform a detailed security analysis of BounTCHA, demonstrating its resilience against various types of attacks. We hope that BounTCHA will act as a robust defense, safeguarding millions of web applications in the AI-driven era."
757,679d459debd8ffd557a2b162,cs.CR,https://arxiv.org/pdf/2501.18549,"CryptoDNA: A Machine Learning Paradigm for DDoS Detection in Healthcare IoT, Inspired by crypto jacking prevention Models","Zag ElSayed, Ahmed Abdelgawad, Nelly Elsayed",Cryptography and Security,"The rapid integration of the Internet of Things (IoT) and Internet of Medical (IoM) devices in the healthcare industry has markedly improved patient care and hospital operations but has concurrently brought substantial risks. Distributed Denial-of-Service (DDoS) attacks present significant dangers, jeopardizing operational stability and patient safety. This study introduces CryptoDNA, an innovative machine learning detection framework influenced by cryptojacking detection methods, designed to identify and alleviate DDoS attacks in healthcare IoT settings. The proposed approach relies on behavioral analytics, including atypical resource usage and network activity patterns. Key features derived from cryptojacking-inspired methodologies include entropy-based analysis of traffic, time-series monitoring of device performance, and dynamic anomaly detection. A lightweight architecture ensures inter-compatibility with resource-constrained IoT devices while maintaining high detection accuracy. The proposed architecture and model were tested in real-world and synthetic datasets to demonstrate the model’s superior performance, achieving over96%percent9696\%96 %accuracy with minimal computational overhead. Comparative analysis reveals its resilience against emerging attack vectors and scalability across diverse device ecosystems. By bridging principles from cryptojacking and DDoS detection, CryptoDNA offers a robust, innovative solution to fortify the healthcare IoT landscape against evolving cyber threats and highlights the potential of interdisciplinary approaches in adaptive cybersecurity defense mechanisms for critical healthcare infrastructures."
758,679d459debd8ffd557a2b163,cs.CR,https://arxiv.org/pdf/2501.18492,GuardReasoner: Towards Reasoning-based LLM Safeguards,"Yue Liu, Hongcheng Gao, Shengfang Zhai, Jun Xia, Tianyi Wu, Zhiwei Xue, Yulin Chen, Kenji Kawaguchi, Jiaheng Zhang, Bryan Hooi","Cryptography and Security, Artificial Intelligence, Machine Learning","As LLMs increasingly impact safety-critical applications, ensuring their safety using guardrails remains a key challenge. This paper proposes GuardReasoner, a new safeguard for LLMs, by guiding the guard model to learn to reason. Concretely, we first create the GuardReasonerTrain dataset, which consists of 127K samples with 460K detailed reasoning steps. Then, we introduce reasoning SFT to unlock the reasoning capability of guard models. In addition, we present hard sample DPO to further strengthen their reasoning ability. In this manner, GuardReasoner achieves better performance, explainability, and generalizability. Extensive experiments and analyses on 13 benchmarks of 3 guardrail tasks demonstrate its superiority. Remarkably, GuardReasoner 8B surpasses GPT-4o+CoT by 5.74% and LLaMA Guard 3 8B by 20.84% F1 score on average. We release the training data, code, and models with different scales (1B, 3B, 8B) of GuardReasoner111https://github.com/yueliu1999/GuardReasoner/."
759,679d459debd8ffd557a2b164,cs.CR,https://arxiv.org/pdf/2501.18387,AuthOr: Lower Cost Authenticity-Oriented Garbling for Arbitrary Boolean Circuits,"Osman Biçer, Ali Ajorian",Cryptography and Security,
760,679d459debd8ffd557a2b165,cs.CR,https://arxiv.org/pdf/2501.18279,SoK: Measuring Blockchain Decentralization,"Christina Ovezik, Dimitris Karakostas, Aggelos Kiayias, Daniel Woods",Cryptography and Security,"In the context of blockchain systems, the importance of decentralization is undermined by the lack of a widely accepted methodology to measure it.
To address this gap, we set out a systematization effort targeting the decentralization measurement workflow.
To facilitate our systematization, we put forth a framework that categorizes all measurement techniques used in previous work based on the resource they target, the methods they use to extract resource allocation, and the functions they apply to produce the final measurements.
We complement this framework with an empirical analysis designed to evaluate whether the various pre-processing steps and metrics used in prior work capture the same underlying concept of decentralization. Our analysis brings about a number of novel insights and observations.
First, the seemingly innocuous choices performed during data extraction, such as the size of estimation windows or the application of thresholds that affect the resource distribution, have important repercussions when calculating the level of decentralization.
Second, exploratory factor analysis suggests that in Proof-of-Work (PoW) blockchains, participation on the consensus layer is not correlated with decentralization, but rather captures a distinct signal, unlike in Proof-of-Stake (PoS) systems, where the different metrics align under a single factor.
These findings challenge the long-held assumption within the blockchain community that higher participation drives higher decentralization. Finally, we combine the results of our empirical analysis with first-principles reasoning to derive practical recommendations for researchers that set out to measure blockchain decentralization, and we further systematize the existing literature in line with these recommendations."
761,679d459debd8ffd557a2b166,cs.CR,https://arxiv.org/pdf/2501.18158,Large Language Models for Cryptocurrency Transaction Analysis: A Bitcoin Case Study,"Yuchen Lei, Yuexin Xiang, Qin Wang, Rafael Dowsley, Tsz Hon Yuen, Jiangshan Yu","Cryptography and Security, Machine Learning","Cryptocurrencies are widely used, yet current methods for analyzing transactions heavily rely on opaque, black-box models. These lack interpretability and adaptability, failing to effectively capture behavioral patterns. Many researchers, including us, believe that Large Language Models (LLMs) could bridge this gap due to their robust reasoning abilities for complex tasks."
762,679d459debd8ffd557a2b167,cs.CR,https://arxiv.org/pdf/2501.18131,Entropy-Synchronized Neural Hashing for Unsupervised Ransomware Detection,"Peter Idliman, Wilfred Balfour, Benedict Featheringham, Hugo Chesterfield","Cryptography and Security, Artificial Intelligence","Entropy-based detection methodologies have gained significant attention due to their ability to analyze structural irregularities within executable files, particularly in the identification of malicious software employing advanced obfuscation techniques. The Entropy-Synchronized Neural Hashing (ESNH) framework introduces a novel approach that leverages entropy-driven hash representations to classify software binaries based on their underlying entropy characteristics. Through the synchronization of entropy profiles with neural network architectures, the model generates robust and unique hash values that maintain stability even when faced with polymorphic and metamorphic transformations. Comparative analysis against traditional detection approaches revealed superior performance in identifying novel threats, reducing false-positive rates, and achieving consistent classification across diverse ransomware families. The incorporation of a self-regulating hash convergence mechanism further ensured that entropy-synchronized hashes remained invariant across executions, minimizing classification inconsistencies that often arise due to dynamic modifications in ransomware payloads. Experimental results demonstrated high detection rates across contemporary ransomware strains, with the model exhibiting resilience against encryption-based evasion mechanisms, code injection strategies, and reflective loading techniques. Unlike conventional detection mechanisms that rely on static signatures and heuristic analysis, the proposed entropy-aware classification framework adapts to emerging threats through an inherent ability to capture entropy anomalies within executable structures. The findings reinforce the potential of entropy-based detection in addressing the limitations of traditional methodologies while enhancing detection robustness against obfuscation and adversarial evasion techniques."
763,679d459debd8ffd557a2b168,cs.CR,https://arxiv.org/pdf/2501.18102,Security for IEEE P1451.1.6-based Sensor Networks for IoT Applications,"Hiroaki Nishi, Janaka Wijekoon, Eugene Y. Song, Kang B. Lee",Cryptography and Security,
764,679d459debd8ffd557a2b169,cs.CR,https://arxiv.org/pdf/2501.18407,Degree is Important: On Evolving Homogeneous Boolean Functions,"Claude Carlet, Marko Ðurasevic, Domagoj Jakobovic, Luca Mariot, Stjepan Picek","Neural and Evolutionary Computing, Cryptography and Security","Boolean functions with good cryptographic properties like high nonlinearity and algebraic degree play an important in the security of stream and block ciphers. Such functions may be designed, for instance, by algebraic constructions or metaheuristics. This paper investigates the use of Evolutionary Algorithms (EAs) to design homogeneous bent Boolean functions, i.e., functions that are maximally nonlinear and whose algebraic normal form contains only monomials of the same degree. In our work, we evaluate three genotype encodings and four fitness functions. Our results show that while EAs manage to find quadratic homogeneous bent functions (with the best method being a GA leveraging a restricted encoding), none of the approaches result in cubic homogeneous bent functions."
765,679d459debd8ffd557a2b16a,cs.CR,https://arxiv.org/pdf/2501.18394,Quantum-Key Distribution using Decoy Pulses to Combat Photon-Number Splitting by Eavesdropper: An Event-by-Event Impairment Enumeration Approach for Performance Evaluation and Design,Debasish Datta,"Quantum Physics, Cryptography and Security, Information Theory, Networking and Internet Architecture","Quantum-key distribution (QKD) schemes employing quantum communication links are typically based on the transmission of weak optical pulses over optical fibers to setup a secret key between the transmitting and receiving nodes. The sender (Alice) transmits optically a random bit stream to the receiver (Bob) through the photon polarizations or the quadrature components of the lightwaves associated with the photons, with a secret key remaining implicitly embedded therein. However, during the above transmission, some eavesdropper (Eve) might attempt to tap the passing-by photons from the optical fiber links to extract the key. In one of the popular QKD schemes, along with signal pulses, some additional decoy pulses are transmitted by Alice, while Eve might use photon-number splitting (PNS) for eavesdropping. In a typical PNS scheme, (i) the optical pulses with single photon are blocked by Eve, (ii) from the optical pulses with two photons, one photon is retained by Eve to carry out eavesdropping operation and the other is retransmitted to Bob, and (iii) all other pulses with more than two photons are retransmitted by Eve to Bob without retaining any photon from them. Extensive theoretical research has been carried out on such QKD schemes, mostly by employing information-theoretic approach along with computer simulations and experimental studies. In this paper, we present a novel event-by-event impairment enumeration approach to evaluate the overall performance of one such QKD scheme analytically with due consideration to the physical layer of the quantum communication links. The proposed approach monitors the impairments of the propagating optical pulses event-by-event at all possible locations along the optical fiber link using statistical approach, and provides estimates of the realizable key generation rate, while assuring an adequate yield ratio between signal and decoy pulses for the detection of possible eavesdropping."
766,679d459debd8ffd557a2b16b,cs.CR,https://arxiv.org/pdf/2501.18371,FLASH-FHE: A Heterogeneous Architecture for Fully Homomorphic Encryption Acceleration,"Junxue Zhang, Xiaodian Cheng, Gang Cao, Meng Dai, Yijun Sun, Han Tian, Dian Shen, Yong Wang, Kai Chen","Hardware Architecture, Cryptography and Security","While many hardware accelerators have recently been proposed to address the inefficiency problem of fully homomorphic encryption (FHE) schemes, none of them is able to deliver optimal performance when facing real-world FHE workloads consisting of a mixture of shallow and deep computations, due primarily to their homogeneous design principle."
767,679d459debd8ffd557a2b16c,cs.CR,https://arxiv.org/pdf/2501.18176,Experimental relativistic zero-knowledge proofs with unconditional security,"Chen-Xun Weng, Ming-Yang Li, Nai-Rui Xu, Yanglin Hu, Ian George, Jiawei Wu, Shengjun Wu, Hua-Lei Yin, Zeng-Bing Chen","Quantum Physics, Computational Complexity, Cryptography and Security","Zero-knowledge proofs (ZKPs) are widely applied in digital economies, such as cryptocurrencies and smart contracts, for establishing trust and ensuring privacy between untrusted parties. However, almost all ZKPs rely on unproven computational assumptions or are vulnerable to quantum adversaries. We propose and experimentally implement an unconditionally secure ZKP for the graph three-coloring problem by combining subset relativistic bit commitments with quantum nonlocality game. Our protocol achieves a linear relationship between interactive rounds and the number of edges, reducing round complexity and storage requirements by thirteen orders of magnitude, thereby significantly enhancing practical feasibility. Our work illustrates the powerful potential of integrating special relativity with quantum theory in trustless cryptography, paving the way for robust applications against quantum attacks in distrustful internet environments."
768,679d459debd8ffd557a2b16d,cs.CR,https://arxiv.org/pdf/2501.18121,Optimal Survey Design for Private Mean Estimation,"Yu-Wei Chen, Raghu Pasupathy, Jordan A. Awan","Machine Learning, Cryptography and Security, Machine Learning, Statistics Theory","This work identifies the first privacy-aware stratified sampling scheme that minimizes the variance for general private mean estimation under the Laplace, Discrete Laplace (DLap) and Truncated-Uniform-Laplace (TuLap) mechanisms within the framework of differential privacy (DP).
We view stratified sampling as a subsampling operation, which amplifies the privacy guarantee; however, to have the same final privacy guarantee for each group, different nominal privacy budgets need to be used depending on the subsampling rate.
Ignoring the effect of DP, traditional stratified sampling strategies risk significant variance inflation.
We phrase our optimal survey design as an optimization problem, where we determine the optimal subsampling sizes for each group with the goal of minimizing the variance of the resulting estimator.
We establish strong convexity of the variance objective, propose an efficient algorithm to identify the integer-optimal design, and offer insights on the structure of the optimal design."
769,679d459debd8ffd557a2b16e,cs.CR,https://arxiv.org/pdf/2501.18006,Topological Signatures of Adversaries in Multimodal Alignments,"Minh Vu, Geigh Zollicoffer, Huy Mai, Ben Nebgen, Boian Alexandrov, Manish Bhattarai","Machine Learning, Artificial Intelligence, Cryptography and Security","Multimodal Machine Learning systems, particularly those aligning text and image data like CLIP/BLIP models, have become increasingly prevalent, yet remain susceptible to adversarial attacks. While substantial research has addressed adversarial robustness in unimodal contexts, defense strategies for multimodal systems are underexplored. This work investigates the topological signatures that arise between image and text embeddings and shows how adversarial attacks disrupt their alignment, introducing distinctive signatures. We specifically leverage persistent homology and introduce two novel Topological-Contrastive losses based on Total Persistence and Multi-scale kernel methods to analyze the topological signatures introduced by adversarial perturbations. We observe a pattern of monotonic changes in the proposed topological losses emerging in a wide range of attacks on image-text alignments, as more adversarial samples are introduced in the data. By designing an algorithm to back-propagate these signatures to input samples, we are able to integrate these signatures into Maximum Mean Discrepancy tests, creating a novel class of tests that leverage topological signatures for better adversarial detection."
770,679d459debd8ffd557a2b16f,cs.CR,https://arxiv.org/pdf/2501.17866,"Advancing Brainwave-Based Biometrics: A Large-Scale, Multi-Session Evaluation","Matin Fallahi, Patricia Arias-Cabarcos, Thorsten Strufe","Signal Processing, Cryptography and Security","The field of brainwave-based biometrics has gained attention for its potential to revolutionize user authentication through hands-free interaction, resistance to shoulder surfing, continuous authentication, and revocability. However, current research often relies on single-session or limited-session datasets with fewer than 55 subjects, raising concerns about generalizability and robustness. To address this gap, we conducted a large-scale study using a public brainwave dataset of 345 subjects and over 6,000 sessions (averaging 17 per subject) recorded over five years with three headsets. Our results reveal that deep learning approaches outperform classic feature extraction methods by 16.4% in Equal Error Rates (EER) and comparing features using a simple cosine distance metric outperforms binary classifiers, which require extra negative samples for training. We also observe EER degrades over time (e.g., 7.7% after 1 day to 19.69% after a year). Therefore, it is necessary to reinforce the enrollment set after successful login attempts. Moreover, we demonstrate that fewer brainwave measurement sensors can be used, with an acceptable increase in EER, which is necessary for transitioning from medical-grade to affordable consumer-grade devices. Finally, we compared our findings with prior work on brainwave authentication and industrial biometric standards. While our performance is comparable or superior to prior work through the use of Supervised Contrastive Learning, standards remain unmet. However, we project that achieving industrial standards will be possible by training the feature extractor with at least 1,500 subjects. Moreover, we open-sourced our analysis code to promote further research."
771,679d459debd8ffd557a2b170,cs.CR,https://arxiv.org/pdf/2501.17824,SMT-Boosted Security Types for Low-Level MPC,"Christian Skalka, Joseph P. Near","Cryptography and Security, Programming Languages",
772,679d459debd8ffd557a2b171,cs.CR,https://arxiv.org/pdf/2501.17786,Atomic Transfer Graphs: Secure-by-design Protocols for Heterogeneous Blockchain Ecosystems,"Stephan Dübler, Federico Badaloni, Pedro Moreno-Sanchez, Clara Schneidewind",Cryptography and Security,
773,679d459debd8ffd557a2b172,cs.CR,https://arxiv.org/pdf/2501.17760,Unraveling Log4Shell: Analyzing the Impact and Response to the Log4j Vulnerabil,"John Doll, Carson McCarthy, Hannah McDougall, Suman Bhunia",Cryptography and Security,"The realm of technology frequently confronts threats posed by adversaries exploiting loopholes in programs. Among these, the Log4Shell vulnerability in the Log4j library stands out due to its widespread impact. Log4j, a prevalent software library for log recording, is integrated into millions of devices worldwide. The Log4Shell vulnerability facilitates remote code execution with relative ease. Its combination with the extensive utilization of Log4j marks it as one of the most dangerous vulnerabilities discovered to date. The severity of this vulnerability, which quickly escalated into a media frenzy, prompted swift action within the industry, thereby mitigating potential extensive damage. This rapid response was crucial, as the consequences could have been significantly more severe if the vulnerability had been exploited by adversaries prior to its public disclosure."
774,679d459debd8ffd557a2b173,cs.CR,https://arxiv.org/pdf/2501.17750,Privacy Audit as Bits Transmission: (Im)possibilities for Audit by One Run,"Zihang Xiang, Tianhao Wang, Di Wang",Cryptography and Security,"Auditing algorithms’ privacy typically involves simulating a game-based protocol that guesses which of two adjacent datasets was the original input. Traditional approaches require thousands of such simulations, leading to significant computational overhead. Recent methods propose single-run auditing of the target algorithm to address this, substantially reducing computational cost. However, these methods’ general applicability and tightness in producing empirical privacy guarantees remain uncertain."
775,679d459debd8ffd557a2b174,cs.CR,https://arxiv.org/pdf/2501.17748,Investigating Vulnerability Disclosures in Open-Source Software Using Bug Bounty Reports and Security Advisories,"Jessy Ayala, Yu-Jye Tung, Joshua Garcia","Cryptography and Security, Software Engineering","In the world of open-source software (OSS), the number of known vulnerabilities has tremendously increased.
The GitHub Advisory Database contains advisories for security risks in GitHub-hosted OSS projects. As of 09/25/2023, there are 197,609 unreviewed GitHub security advisories.
Of those unreviewed, at least 63,852 are publicly documented vulnerabilities, potentially leaving many OSS projects vulnerable.
Recently, bug bounty platforms have emerged to focus solely on providing bounties to help secure OSS.
In this paper, we conduct an empirical study on 3,798 reviewed GitHub security advisories and 4,033 disclosed OSS bug bounty reports, a perspective that is currently understudied, because they contain comprehensive information about security incidents, e.g., the nature of vulnerabilities, their impact, and how they were resolved.
We are the first to determine the explicit process describing how OSS vulnerabilities propagate from security advisories and bug bounty reports, which are the main intermediaries between vulnerability reporters, OSS maintainers, and dependent projects, to vulnerable OSS projects and entries in global vulnerability databases and possibly back.
This process uncovers how missing or delayed CVE assignments for OSS vulnerabilities result in projects, both in and out of OSS, not being notified of necessary security updates promptly and corresponding bottlenecks.
Based on our findings, we provide suggestions, actionable items, and future research directions to help improve the security posture of OSS projects."
776,679d459debd8ffd557a2b175,cs.CR,https://arxiv.org/pdf/2501.17740,Attacker Control and Bug Prioritization,"Guilhem Lacombe, Sébastien Bardin",Cryptography and Security,"As bug-finding methods improve, bug-fixing capabilities are exceeded, resulting in an accumulation of potential vulnerabilities.
There is thus a need for efficient and precise bug prioritization based on exploitability.
In this work, we explore the notion of control of an attacker over a vulnerability’s parameters, which is an often overlooked factor of exploitability.
We show that taint as well as straightforward qualitative and quantitative notions of control are not enough to effectively differentiate vulnerabilities.
Instead, we propose to focus analysis on feasible value sets, which we calldomains of control, in order to better take into account threat models and expert insight.
Our newShrink and Splitalgorithm efficiently extracts domains of control from path constraints obtained with symbolic execution and renders them in an easily processed, human-readable form.
This in turn allows to automatically compute more complex control metrics, such asweighted Quantitative Control, which factors in the varying threat levels of different values.
Experiments show that our method is both efficient and precise.
In particular, it is the only one able to distinguish between vulnerabilities such ascve-2019-14192andcve-2022-30552, while revealing a mistake in the human evaluation ofcve-2022-30790.
The high degree of automation of our tool also brings us closer to a fully-automated evaluation pipeline."
777,679d459debd8ffd557a2b176,cs.CR,https://arxiv.org/pdf/2501.17733,BitMLx: Secure Cross-chain Smart Contracts For Bitcoin-style Cryptocurrencies,"Federico Badaloni, Sebastian Holler, Chrysoula Oikonomou, Pedro Moreno-Sanchez, Clara Schneidewind",Cryptography and Security,
778,679d459debd8ffd557a2b177,cs.CR,https://arxiv.org/pdf/2501.17548,Understanding Trust in Authentication Methods for Icelandic Digital Public Services,"Brynjólfur Stefánsson, Ásta Guðrún Helgadóttir, Martin Nizon-Deladoeuille, Helmut Neukirchen, Thomas Welsh",Cryptography and Security,"Digital public services have revolutionised citizen and private sector interactions with governments. Certain communities are strongly dependent on such digital services for ensuring the availability of public services due to geographical isolation or the presence of adverse geophysical and weather phenomena. However, strong and effective security is key to maintaining the integrity of public records and services yet also for ensuring trust in them. Trust is essential for user uptake, particularly given a global increase in data-protection concerns and a turbulent geopolitical security environment. In this paper, we examine the case of public trust in various forms of authentication for electronic identification in Iceland, which has high availability requirements for digital public services due to its unique and dynamic geophysical characteristics. Additionally, Iceland has historically low levels of institutional trust which may conflict with the requirement for an increased need for digital public services. Through surveying the Icelandic general public, we find that there is a high-level of trust in digital identification services across all demographics. We conclude with a discussion and future research challenges towards improving the effectiveness of authentication considering the diverse groups within Icelandic society, such as the rapidly increasing population of migrants and the large and dynamic population of tourists."
779,679d459debd8ffd557a2b178,cs.CR,https://arxiv.org/pdf/2501.17539,Towards Supporting Penetration Testing Education with Large Language Models: an Evaluation and Comparison,"Martin Nizon-Deladoeuille, Brynjólfur Stefánsson, Helmut Neukirchen, Thomas Welsh",Cryptography and Security,"Cybersecurity education is challenging and it is helpful for educators to understand Large Language Models’ (LLMs’) capabilities for supporting education. This study evaluates the effectiveness of LLMs in conducting a variety of penetration testing tasks. Fifteen representative tasks were selected to cover a comprehensive range of real-world scenarios. We evaluate the performance of 6 models (GPT-4o mini, GPT-4o, Gemini 1.5 Flash, Llama 3.1 405B, Mixtral 8x7B and WhiteRabbitNeo) upon the Metasploitable v3 Ubuntu image and OWASP WebGOAT.
Our findings suggest that GPT-4o mini currently offers the most consistent support making it a valuable tool for educational purposes. However, its use in conjonction with WhiteRabbitNeo should be considered, because of its innovative approach to tool and command recommendations. This study underscores the need for continued research into optimising LLMs for complex, domain-specific tasks in cybersecurity education."
780,679d459debd8ffd557a2b179,cs.CR,https://arxiv.org/pdf/2501.17501,How Much Do Code Language Models Remember? An Investigation on Data Extraction Attacks before and after Fine-tuning,"Fabio Salerno, Ali Al-Kaswan, Maliheh Izadi",Cryptography and Security,
781,679d459debd8ffd557a2b17a,cs.CR,https://arxiv.org/pdf/2501.17429,Algorithmic Segmentation and Behavioral Profiling for Ransomware Detection Using Temporal-Correlation Graphs,"Ignatius Rollere, Caspian Hartsfield, Seraphina Courtenay, Lucian Fenwick, Aurelia Grunwald","Cryptography and Security, Artificial Intelligence","The rapid evolution of cyber threats has outpaced traditional detection methodologies, necessitating innovative approaches capable of addressing the adaptive and complex behaviors of modern adversaries. A novel framework was introduced, leveraging Temporal-Correlation Graphs to model the intricate relationships and temporal patterns inherent in malicious operations. The approach dynamically captured behavioral anomalies, offering a robust mechanism for distinguishing between benign and malicious activities in real-time scenarios. Extensive experiments demonstrated the framework’s effectiveness across diverse ransomware families, with consistently high precision, recall, and overall detection accuracy. Comparative evaluations highlighted its better performance over traditional signature-based and heuristic methods, particularly in handling polymorphic and previously unseen ransomware variants. The architecture was designed with scalability and modularity in mind, ensuring compatibility with enterprise-scale environments while maintaining resource efficiency. Analysis of encryption speeds, anomaly patterns, and temporal correlations provided deeper insights into the operational strategies of ransomware, validating the framework’s adaptability to evolving threats. The research contributes to advancing cybersecurity technologies by integrating dynamic graph analytics and machine learning for future innovations in threat detection. Results from this study underline the potential for transforming the way organizations detect and mitigate complex cyberattacks."
782,679d459debd8ffd557a2b17b,cs.CR,https://arxiv.org/pdf/2501.17413,Fine-Grained 1-Day Vulnerability Detection in Binaries via Patch Code Localization,"Chaopeng Dong, Jingdong Guo, Shouguo Yang, Yang Xiao, Yi Li, Hong Li, Zhi Li, Limin Sun",Cryptography and Security,"1-day vulnerabilities in binaries have become a major threat to software security. Patch presence test is one of the effective ways to detect the vulnerability. However, existing patch presence test works do not perform well in practical scenarios due to the interference from the various compilers and optimizations, patch-similar code blocks, and irrelevant functions in stripped binaries. In this paper, we propose a novel approach namedPLocator, which leverages stable values from both the patch code and its context, extracted from the control flow graph, to accurately locate the real patch code in the target function, offering a practical solution for real-world vulnerability detection scenarios."
783,679d459debd8ffd557a2b17c,cs.CR,https://arxiv.org/pdf/2501.17405,When Everyday Devices Become Weapons: A Closer Look at the Pager and Walkie-talkie Attacks,"Pantha Protim Sarker, Upoma Das, Nitin Varshney, Shang Shi, Akshay Kulkarni, Farimah Farahmandi, Mark Tehranipoor",Cryptography and Security,"Battery-powered technologies like pagers and walkie-talkies have long been integral to civilian and military operations. However, the potential for such everyday devices to be weaponized has largely been underestimated in the realm of cybersecurity. In September 2024, Lebanon experienced a series of unprecedented, coordinated explosions triggered through compromised pagers and walkie-talkies, creating a new category of attack in the domain of cyber-physical warfare. This attack not only disrupted critical communication networks but also resulted in injuries, loss of life, and exposed significant national security vulnerabilities, prompting governments and organizations worldwide to reevaluate their cybersecurity frameworks. This article provides an in-depth investigation into the infamous Pager and Walkie-Talkie attacks, analyzing both technical and non-technical dimensions. Furthermore, the study extends its scope to explore vulnerabilities in other battery-powered infrastructures, such as battery management systems, highlighting their potential exploitation. Existing prevention and detection techniques are reviewed, with an emphasis on their limitations and the challenges they face in addressing emerging threats. Finally, the article discusses emerging methodologies, particularly focusing on the role of physical inspection, as a critical component of future security measures. This research aims to provide actionable insights to bolster the resilience of cyber-physical systems in an increasingly interconnected world."
784,679d459debd8ffd557a2b17d,cs.CR,https://arxiv.org/pdf/2501.17402,Cute-Lock: Behavioral and Structural Multi-Key Logic Locking Using Time Base Keys,"Kevin Lopez, Amin Rezaei",Cryptography and Security,"The outsourcing of semiconductor manufacturing raises security risks, such as piracy and overproduction of hardware intellectual property. To overcome this challenge, logic locking has emerged to lock a given circuit using additional key bits. While single-key logic locking approaches have demonstrated serious vulnerability to a wide range of attacks, multi-key solutions, if carefully designed, can provide a reliable defense against not only oracle-guided logic attacks, but also removal and dataflow attacks. In this paper, using time base keys, we propose, implement and evaluate a family of secure multi-key logic locking algorithms calledCute-Lockthat can be applied both in RTL-level behavioral and netlist-level structural representations of sequential circuits. Our extensive experimental results under a diverse range of attacks confirm that, compared to vulnerable state-of-the-art methods, employing theCute-Lockfamily drives attacking attempts to a dead end without additional overhead."
785,679d459debd8ffd557a2b17e,cs.CR,https://arxiv.org/pdf/2501.17396,Poisoning Attacks and Defenses to Federated Unlearning,"Wenbin Wang, Qiwen Ma, Zifan Zhang, Yuchen Liu, Zhuqing Liu, Minghong Fang","Cryptography and Security, Distributed, Parallel, and Cluster Computing, Machine Learning","Federated learning allows multiple clients to collaboratively train a global model with the assistance of a server. However, its distributed nature makes it susceptible to poisoning attacks, where malicious clients can compromise the global model by sending harmful local model updates to the server. To unlearn an accurate global model from a poisoned one after identifying malicious clients, federated unlearning has been introduced. Yet, current research on federated unlearning has primarily concentrated on its effectiveness and efficiency, overlooking the security challenges it presents. In this work, we bridge the gap via proposingBadUnlearn, the first poisoning attacks targeting federated unlearning. InBadUnlearn, malicious clients send specifically designed local model updates to the server during the unlearning process, aiming to ensure that the resulting unlearned model remains poisoned. To mitigate these threats, we proposeUnlearnGuard, a robust federated unlearning framework that is provably robust against both existing poisoning attacks and ourBadUnlearn. The core concept ofUnlearnGuardis for the server to estimate the clients’ local model updates during the unlearning process and employ a filtering strategy to verify the accuracy of these estimations. Theoretically, we prove that the model unlearned throughUnlearnGuardclosely resembles one obtained by train-from-scratch.
Empirically, we show thatBadUnlearncan effectively corrupt existing federated unlearning methods, whileUnlearnGuardremains secure against poisoning attacks."
786,679d459debd8ffd557a2b17f,cs.CR,https://arxiv.org/pdf/2501.17392,Byzantine-Robust Federated Learning over Ring-All-Reduce Distributed Computing,"Minghong Fang, Zhuqing Liu, Xuecen Zhao, Jia Liu","Cryptography and Security, Machine Learning","Federated learning (FL) has gained attention as a distributed learning paradigm for its data privacy benefits and accelerated convergence through parallel computation. Traditional FL relies on a server-client (SC) architecture, where a central server coordinates multiple clients to train a global model, but this approach faces scalability challenges due to server communication bottlenecks. To overcome this, the ring-all-reduce (RAR) architecture has been introduced, eliminating the central server and achieving bandwidth optimality. However, the tightly coupled nature of RAR’s ring topology exposes it to unique Byzantine attack risks not present in SC-based FL. Despite its potential, designing Byzantine-robust RAR-based FL algorithms remains an open problem. To address this gap, we propose𝖡𝖱𝖠𝖢𝖤𝖡𝖱𝖠𝖢𝖤\mathsf{BRACE}~{}sansserif_BRACE(Byzantine-robust ring-all-reduce), the first RAR-based FL algorithm to achieve both Byzantine robustness and communication efficiency. We provide theoretical guarantees for the convergence of𝖡𝖱𝖠𝖢𝖤𝖡𝖱𝖠𝖢𝖤\mathsf{BRACE}~{}sansserif_BRACEunder Byzantine attacks, demonstrate its bandwidth efficiency, and validate its practical effectiveness through experiments. Our work offers a foundational understanding of Byzantine-robust RAR-based FL design."
787,679d459debd8ffd557a2b180,cs.CR,https://arxiv.org/pdf/2501.17381,Do We Really Need to Design New Byzantine-robust Aggregation Rules?,"Minghong Fang, Seyedsina Nabavirazavi, Zhuqing Liu, Wei Sun, Sundararaja Sitharama Iyengar, Haibo Yang","Cryptography and Security, Distributed, Parallel, and Cluster Computing, Machine Learning","Federated learning (FL) allows multiple clients to collaboratively train a global machine learning model through a server, without exchanging their private training data. However, the decentralized aspect of FL makes it susceptible to poisoning attacks, where malicious clients can manipulate the global model by sending altered local model updates. To counter these attacks, a variety of aggregation rules designed to be resilient to Byzantine failures have been introduced. Nonetheless, these methods can still be vulnerable to sophisticated attacks or depend on unrealistic assumptions about the server. In this paper, we demonstrate that there is no need to design new Byzantine-robust aggregation rules; instead, FL can be secured by enhancing the robustness of well-established aggregation rules. To this end, we present FoundationFL, a novel defense mechanism against poisoning attacks. FoundationFL involves the server generating synthetic updates after receiving local model updates from clients. It then applies existing Byzantine-robust foundational aggregation rules, such as Trimmed-mean or Median, to combine clients’ model updates with the synthetic ones. We theoretically establish the convergence performance of FoundationFL under Byzantine settings. Comprehensive experiments across several real-world datasets validate the efficiency of our FoundationFL method."
788,679d459debd8ffd557a2b181,cs.CR,https://arxiv.org/pdf/2501.17335,Pandora's Box: Cross-Chain Arbitrages in the Realm of Blockchain Interoperability,"Burak Öz, Christof Ferreira Torres, Jonas Gebele, Filip Rezabek, Bruno Mazorra, Florian Matthes","Cryptography and Security, Computational Engineering, Finance, and Science",
789,679d459debd8ffd557a2b182,cs.CR,https://arxiv.org/pdf/2501.17292,Enabling Low-Cost Secure Computing on Untrusted In-Memory Architectures,"Sahar Ghoflsaz Ghinani, Jingyao Zhang, Elaheh Sadredini",Cryptography and Security,"Modern computing systems are limited in performance by the memory bandwidth available to processors, a problem known as the memory wall. Processing-in-Memory (PIM) promises to substantially improve this problem by moving processing closer to the data, improving effective data bandwidth, and leading to superior performance on memory-intensive workloads. However, integrating PIM modules within a secure computing system raises an interesting challenge: unencrypted data has to move off-chip to the PIM, exposing the data to attackers and breaking assumptions on Trusted Computing Bases (TCBs). To tackle this challenge, this paper leverages multi-party computation (MPC) techniques, specifically arithmetic secret sharing and Yao’s garbled circuits, to outsource bandwidth-intensive computation securely to PIM.Additionally, we leverage precomputation optimization to prevent the CPU’s portion of the MPC from becoming a bottleneck.We evaluate our approach using the UPMEM PIM system over various applications such as Deep Learning Recommendation Model inference and Logistic Regression. Our evaluations demonstrate up to a14.66×14.66\times14.66 ×speedup compared to a secure CPU configuration while maintaining data confidentiality and integrity when outsourcing linear and/or nonlinear computation."
790,679d459debd8ffd557a2b183,cs.CR,https://arxiv.org/pdf/2501.17845,Private Information Retrieval on Multigraph-Based Replicated Storage,"Shreya Meel, Xiangliang Kong, Thomas Jacob Maranzatto, Itzhak Tamo, Sennur Ulukus","Information Theory, Cryptography and Security, Networking and Internet Architecture, Signal Processing","We consider the private information retrieval (PIR) problem for a multigraph-based replication system, where each set ofr𝑟ritalic_rfiles is stored on two of the servers according to an underlyingr𝑟ritalic_r-multigraph. Our goal is to establish upper and lower bounds on the PIR capacity of ther𝑟ritalic_r-multigraph. Specifically, we first propose a construction for multigraph-based PIR systems that leverages the symmetry of the underlying graph-based PIR scheme, deriving a capacity lower bound for such multigraphs. Then, we establish a general upper bound using linear programming, expressed as a function of the underlying graph parameters. Our bounds are demonstrated to be tight for PIR systems on multipaths for even number of vertices."
791,679d459debd8ffd557a2b184,cs.CR,https://arxiv.org/pdf/2501.17667,CAMP in the Odyssey: Provably Robust Reinforcement Learning with Certified Radius Maximization,"Derui Wang, Kristen Moore, Diksha Goel, Minjune Kim, Gang Li, Yang Li, Robin Doss, Minhui Xue, Bo Li, Seyit Camtepe, Liming Zhu","Machine Learning, Cryptography and Security","Deep reinforcement learning (DRL) has gained widespread adoption in control and decision-making tasks due to its strong performance in dynamic environments.
However, DRL agents are vulnerable to noisy observations and adversarial attacks, and concerns about the adversarial robustness of DRL systems have emerged.
Recent efforts have focused on addressing these robustness issues by establishing rigorous theoretical guarantees for the returns achieved by DRL agents in adversarial settings.
Among these approaches, policy smoothing has proven to be an effective and scalable method for certifying the robustness of DRL agents.
Nevertheless, existing certifiably robust DRL relies on policies trained with simple Gaussian augmentations, resulting in a suboptimal trade-off between certified robustness and certified return.
To address this issue, we introduce a novel paradigm dubbedCertified-rAdius-MaximizingPolicy (CAMP) training.CAMPis designed to enhance DRL policies, achieving better utility without compromising provable robustness.
By leveraging the insight that the global certified radius can be derived from local certified radii based on training-time statistics,CAMPformulates a surrogate loss related to the local certified radius and optimizes the policy guided by this surrogate loss.
We also introducepolicy imitationas a novel technique to stabilizeCAMPtraining.
Experimental results demonstrate thatCAMPsignificantly improves the robustness-return trade-off across various tasks.
Based on the results,CAMPcan achieve up to twice the certified expected return compared to that of baselines.
Our code is available athttps://github.com/NeuralSec/camp-robust-rl."
792,679d459debd8ffd557a2b185,cs.CR,https://arxiv.org/pdf/2501.17315,A sketch of an AI control safety case,"Tomek Korbak, Joshua Clymer, Benjamin Hilton, Buck Shlegeris, Geoffrey Irving","Artificial Intelligence, Cryptography and Security, Software Engineering","As LLM agents gain a greater capacity to cause harm, AI developers might increasingly rely on control measures such as monitoring to justify that they are safe. We sketch how developers could construct a “control safety case”, which is a structured argument that models areincapableof subverting control measures in order to cause unacceptable outcomes. As a case study, we sketch an argument that a hypothetical LLM agent deployed internally at an AI company won’t exfiltrate sensitive information. The sketch relies on evidence from a “control evaluation,” where a red team deliberately designs models to exfiltrate data in a proxy for the deployment environment. The safety case then hinges on several claims: (1) the red team adequately elicits model capabilities to exfiltrate data, (2) control measures remain at least as effective in deployment, and (3) developers conservatively extrapolate model performance to predict the probability of data exfiltration in deployment. This safety case sketch is a step toward more concrete arguments that can be used to show that a dangerously capable LLM agent is safe to deploy."
793,679d459debd8ffd557a2b186,cs.CR,https://arxiv.org/pdf/2501.17123,Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks Detection: A Comparative Analysis,"Tejal Joshi, Aarya Kawalay, Anvi Jamkhande, Amit Joshi","Cryptography and Security, Neural and Evolutionary Computing","Cache side-channel attacks have emerged as a sophisticated and persistent threat, capable of extracting sensitive user information by exploiting vulnerabilities in modern processors. These attacks leverage the inherent weaknesses in shared computational resources, especially the last-level cache. They infer patterns in data access and execution flows, often bypassing traditional security defenses. These are particularly dangerous because they require no physical access to the victim’s device, making remote attacks feasible. This study focuses on a specific class of these threats— fingerprinting attacks —where an adversary can use cache-side channels to monitor and analyze the behavior of co-located processes, potentially revealing confidential information such as encryption keys or user activity patterns. A comprehensive threat model illustrates how an attacker, sharing computational resources with a target system, can exploit these side-channels. This exploitation allows the attacker to learn patterns in data access, potentially compromising sensitive information. To mitigate such risks, a hybrid deep learning model is proposed to detect cache side-channel attacks. Its performance is compared with five widely-used Deep Learning models like Multi Layer Perceptron, Convolutional Neural Network, Simple Recurrent Neural Network, Long Short Term Memory, and Gated Recurrent Unit, evaluating each model’s resilience to these sophisticated attacks. The experimental results suggest that the hybrid model has achieved a detection rate of up to 99.96%. The findings demonstrate the limitations of existing models and emphasize the need for enhanced defensive mechanisms, shedding light on future developments for securing sensitive data against evolving side-channel threats."
794,679d459debd8ffd557a2b187,cs.CR,https://arxiv.org/pdf/2501.17089,CRSet: Non-Interactive Verifiable Credential Revocation with Metadata Privacy for Issuers and Everyone Else,"Felix Hoops, Jonas Gebele, Florian Matthes",Cryptography and Security,"Like any digital certificate, Verifiable Credentials (VCs) require a way to revoke them in case of an error or key compromise.
Existing solutions for VC revocation, most prominently Bitstring Status List, are not viable for many use cases since they leak the issuer’s behavior, which in turn leaks internal business metrics. For instance, exact staff fluctuation through issuance and revocation of employee IDs.
We introduce CRSet, a revocation mechanism that allows an issuer to encode revocation information for years worth of VCs as a Bloom filter cascade. Padding is used to provide deniability for issuer metrics.
Issuers periodically publish this filter cascade on a decentralized storage system. Relying Parties (RPs) can download it to perform any number of revocation checks locally.
Compared to existing solutions, CRSet protects the metadata of subject, RPs, and issuer equally. At the same time, it is non-interactive, making it work with wallet devices having limited hardware power and drop-in compatible with existing VC exchange protocols and wallet applications.
We present a prototype using the Ethereum blockchain as decentralized storage.
The recently introduced blob-carrying transactions, enabling cheaper data writes, allow us to write each CRSet directly to the chain.
We built software for issuers and RPs that we successfully tested end-to-end with an existing publicly available wallet agents and the OpenID for Verifiable Credentials protocols.
Storage and bandwidth costs paid by issuers and RP are higher than for Bitstring Status List, but still manageable at around 1 MB for an issuer issuing hundreds of thousands of VCs annually and covering decades."
795,679d459debd8ffd557a2b188,cs.CR,https://arxiv.org/pdf/2501.16962,UEFI Memory Forensics: A Framework for UEFI Threat Analysis,"Kalanit Suzan Segal, Hadar Cochavi Gorelik, Oleg Brodt, Yuval Elbahar, Yuval Elovici, Asaf Shabtai",Cryptography and Security,
796,679d459debd8ffd557a2b189,cs.CR,https://arxiv.org/pdf/2501.16948,Stack Overflow Meets Replication: Security Research Amid Evolving Code Snippets (Extended Version),"Alfusainey Jallow, Sven Bugiel",Cryptography and Security,
797,679d459debd8ffd557a2b18a,cs.CR,https://arxiv.org/pdf/2501.16843,Bones of Contention: Exploring Query-Efficient Attacks Against Skeleton Recognition Systems,"Yuxin Cao, Kai Ye, Derui Wang, Minhui Xue, Hao Ge, Chenxiong Qian, Jin Song Dong",Cryptography and Security,"Skeleton action recognition models have secured more attention than video-based ones in various applications due to privacy preservation and lower storage requirements. Skeleton data are typically transmitted to cloud servers for action recognition, with results returned to clients via Apps/APIs. However, the vulnerability of skeletal models against adversarial perturbations gradually reveals the unreliability of these systems. Existing black-box attacks all operate in a decision-based manner, resulting in numerous queries that hinder efficiency and feasibility in real-world applications. Moreover, all attacks off the shelf focus on only restricted perturbations, while ignoring model weaknesses when encountered with non-semantic perturbations. In this paper, we propose two query-effIcientSkeletalAdversarialAttaCks, ISAAC-K and ISAAC-N. As a black-box attack, ISAAC-K utilizes Grad-CAM in a surrogate model to extract key joints where minor sparse perturbations are then added to fool the classifier. To guarantee natural adversarial motions, we introduce constraints of both bone length and temporal consistency. ISAAC-K finds stronger adversarial examples onℓ∞subscriptℓ\ell_{\infty}roman_ℓ start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPTnorm, which can encompass those on other norms. Exhaustive experiments substantiate that ISAAC-K can uplift the attack efficiency of the perturbations under 10 skeletal models. Additionally, as a byproduct, ISAAC-N fools the classifier by replacing skeletons unrelated to the action. We surprisingly find that skeletal models are vulnerable to large perturbations where the part-wise non-semantic joints are just replaced, leading to a query-free no-box attack without any prior knowledge. Based on that, four adaptive defenses are eventually proposed to improve the robustness of skeleton recognition models."
798,679d459debd8ffd557a2b18b,cs.CR,https://arxiv.org/pdf/2501.16784,TORCHLIGHT: Shedding LIGHT on Real-World Attacks on Cloudless IoT Devices Concealed within the Tor Network,"Yumingzhi Pan, Zhen Ling, Yue Zhang, Hongze Wang, Guangchi Liu, Junzhou Luo, Xinwen Fu",Cryptography and Security,
799,679d459debd8ffd557a2b18c,cs.CR,https://arxiv.org/pdf/2501.16763,PTSA: Utilizing Transaction Prioritization to Enhance Confirmation Speed in the IOTA Network,"Seyyed Ali Aghamiri, Reza Sharifnia, Ahmad Khonsari","Cryptography and Security, Networking and Internet Architecture",
800,679d459debd8ffd557a2b18d,cs.CR,https://arxiv.org/pdf/2501.16750,HateBench: Benchmarking Hate Speech Detectors on LLM-Generated Content and Hate Campaigns,"Xinyue Shen, Yixin Wu, Yiting Qu, Michael Backes, Savvas Zannettou, Yang Zhang","Cryptography and Security, Machine Learning",
801,679d459debd8ffd557a2b18e,cs.CR,https://arxiv.org/pdf/2501.16681,Blockchain Address Poisoning,"Taro Tsuchiya, Jin-Dong Dong, Kyle Soska, Nicolas Christin",Cryptography and Security,"In many blockchains, e.g., Ethereum, Binance Smart Chain (BSC),
the primary representation used for wallet addresses is a hardly
memorable 40-digit hexadecimal string. As a result, users often
select addresses from their recent transaction history, which enablesblockchain address poisoning. The adversary first
generates lookalike addresses similar to one with which the victim has
previously interacted, and then engages with the victim to “poison”
their transaction history. The goal is to have the victim mistakenly
send tokens to the lookalike address, as opposed to the intended
recipient.
Compared to contemporary studies,
this paper provides four notable contributions.
First, we develop a detection system and perform measurements over two years on Ethereum and BSC.
We identify 13 times the number of attack attempts reported previously—totaling 270M on-chain attacks targeting 17M victims.
6,633 incidents have caused at least 83.8M USD in losses,
which makes blockchain address poisoning one of the largest cryptocurrency phishing schemes observed in the wild.
Second, we analyze a few large attack entities using improved clustering techniques, and model attacker profitability and competition.
Third, we reveal attack strategies—targeted populations, success conditions (address similarity, timing), and cross-chain attacks.
Fourth, we mathematically define and simulate the lookalike address-generation process across various software- and hardware-based implementations, and identify a large-scale attacker group that appears to use GPUs.
We also discuss defensive countermeasures."
802,679d459debd8ffd557a2b18f,cs.CR,https://arxiv.org/pdf/2501.16680,Differentially Private Set Representations,"Sarvar Patel, Giuseppe Persiano, Joon Young Seo, Kevin Yeo","Cryptography and Security, Data Structures and Algorithms",
803,679d459debd8ffd557a2b190,cs.CR,https://arxiv.org/pdf/2501.16671,Data-Free Model-Related Attacks: Unleashing the Potential of Generative AI,"Dayong Ye, Tianqing Zhu, Shang Wang, Bo Liu, Leo Yu Zhang, Wanlei Zhou, Yang Zhang","Cryptography and Security, Artificial Intelligence","Generative AI technology has become increasingly integrated into our daily lives, offering powerful capabilities to enhance productivity. However, these same capabilities can be exploited by adversaries for malicious purposes. While existing research on adversarial applications of generative AI predominantly focuses on cyberattacks, less attention has been given to attacks targeting deep learning models. In this paper, we introduce the use of generative AI for facilitating model-related attacks, including model extraction, membership inference, and model inversion. Our study reveals that adversaries can launch a variety of model-related attacks against both image and text models in a data-free and black-box manner, achieving comparable performance to baseline methods that have access to the target models’ training data and parameters in a white-box manner. This research serves as an important early warning to the community about the potential risks associated with generative AI-powered attacks on deep learning models. The source code is provided at: https://zenodo.org/records/14737003."
804,679d459debd8ffd557a2b191,cs.CR,https://arxiv.org/pdf/2501.16663,Data Duplication: A Novel Multi-Purpose Attack Paradigm in Machine Unlearning,"Dayong Ye, Tainqing Zhu, Jiayang Li, Kun Gao, Bo Liu, Leo Yu Zhang, Wanlei Zhou, Yang Zhang","Cryptography and Security, Artificial Intelligence","Duplication is a prevalent issue within datasets. Existing research has demonstrated that the presence of duplicated data in training datasets can significantly influence both model performance and data privacy. However, the impact of data duplication on the unlearning process remains largely unexplored. This paper addresses this gap by pioneering a comprehensive investigation into the role of data duplication, not only in standard machine unlearning but also in federated and reinforcement unlearning paradigms.
Specifically, we propose an adversary who duplicates a subset of the target model’s training set and incorporates it into the training set. After training, the adversary requests the model owner to unlearn this duplicated subset, and analyzes the impact on the unlearned model. For example, the adversary can challenge the model owner by revealing that, despite efforts to unlearn it, the influence of the duplicated subset remains in the model.
Moreover, to circumvent detection by de-duplication techniques, we propose three novel near-duplication methods for the adversary, each tailored to a specific unlearning paradigm. We then examine their impacts on the unlearning process when de-duplication techniques are applied.
Our findings reveal several crucial insights: 1) the gold standard unlearning method, retraining from scratch, fails to effectively conduct unlearning under certain conditions; 2) unlearning duplicated data can lead to significant model degradation in specific scenarios;
and 3) meticulously crafted duplicates can evade detection by de-duplication methods. The source code is provided at: https://zenodo.org/records/14736535."
805,679d459debd8ffd557a2b192,cs.CR,https://arxiv.org/pdf/2501.16619,SHIELD: Secure Host-Independent Extensible Logging for SATA/Network Storage Towards Ransomware Detection,"Md Raz, P.V. Sai Charan, Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri","Cryptography and Security, Systems and Control","As malware such as ransomware becomes sophisticated, the ability to find and neutralize it requires more robust and tamper-resistant solutions. Current methods rely on data from compromised hosts, lack hardware isolation, and cannot detect emerging threats. To address these limitations, we introduceShield—a detection architecture leveraging FPGA-based open-source SATA and Network Block Device (NBD) technology to provide off-host, tamper-proof measurements for continuous observation of disk activity for software executing on a target device.Shieldprovides three distinct contributions: It
(1) develops a framework to obtain and analyze multi-level hardware metrics at NBD, FPGA, and SATA storage levels, and shows their ability to differentiate between harmless and malicious software;
(2) Broadens the functionality of an open-source FPGA-driven SATA Host Bus Adapter (HBA) to offer complete data storage capabilities through NBD without relying on the host system;
(3) Provides a foundation for using the methodology and metrics in automated machine learning-assisted detection and ASIC integration for advanced mitigation capabilities in data storage devices.Shieldanalyzes 10 benign programs and 10 modern ransomware families to illustrate its capacity for real-time monitoring and use in distinguishing between ransomware and benign software.
Experimental evidence showsShield’s robust host-independent and hardware-assisted metrics are a basis for detection, allowing to observe program execution and detect malicious activities at the storage level."
806,679d459debd8ffd557a2b193,cs.CR,https://arxiv.org/pdf/2501.16558,Distributional Information Embedding: A Framework for Multi-bit Watermarking,"Haiyun He, Yepeng Liu, Ziqiao Wang, Yongyi Mao, Yuheng Bu","Cryptography and Security, Information Theory, Machine Learning","This paper introduces a novel problem, distributional information embedding, motivated by the practical demands of multi-bit watermarking for large language models (LLMs). Unlike traditional information embedding, which embeds information into a pre-existing host signal, LLM watermarking actively controls the text generation process—adjusting the token distribution—to embed a detectable signal. We develop an information-theoretic framework to analyze this distributional information embedding problem, characterizing the fundamental trade-offs among three critical performance metrics: text quality, detectability, and information rate. In the asymptotic regime, we demonstrate that the maximum achievable rate with vanishing error corresponds to the entropy of the LLM’s output distribution and increases with higher allowable distortion. We also characterize the optimal watermarking scheme to achieve this rate. Extending the analysis to the finite-token case, we identify schemes that maximize detection probability while adhering to constraints on false alarm and distortion."
807,679d459debd8ffd557a2b194,cs.CR,https://arxiv.org/pdf/2501.16534,Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs,"Jean-Charles Noirot Ferrand, Yohan Beugin, Eric Pauley, Ryan Sheatsley, Patrick McDaniel","Cryptography and Security, Artificial Intelligence","Alignment in large language models (LLMs) is used to enforce guidelines such as safety.
Yet, alignment fails in the face ofjailbreakattacks that modify inputs to induce unsafe outputs.
In this paper, we present and evaluate a method to assess the robustness of LLM alignment. We observe that alignment embeds a safety classifier in the target model that is responsible for deciding between refusal and compliance. We seek to extract an approximation of this classifier, called a surrogate classifier, from the LLM.
We develop an algorithm for identifying candidate classifiers from subsets of the LLM model. We evaluate the degree to which the candidate classifiers approximate the model’s embedded classifier in benign (F1subscript𝐹1F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTscore) and adversarial (using surrogates in a white-box attack) settings.
Our evaluation shows that the best candidates achieve accurate agreement (anF1subscript𝐹1F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTscore above 80%) using as little as 20% of the model architecture. Further, we find attacks mounted on the surrogate models can be transferred with high accuracy. For example, a surrogate using only 50% of the Llama 2 model achieved an attack success rate (ASR) of 70%–a substantial improvement over attacking the LLM directly, where we only observed a 22% ASR.
These results show that extracting surrogate classifiers is a viable (and highly effective) means for modeling (and therein addressing) the vulnerability of aligned models to jailbreaking attacks."
808,679d459debd8ffd557a2b195,cs.CR,https://arxiv.org/pdf/2501.16490,Towards Robust Stability Prediction in Smart Grids: GAN-based Approach under Data Constraints and Adversarial Challenges,"Emad Efatinasab, Alessandro Brighente, Denis Donadel, Mauro Conti, Mirco Rampazzo","Cryptography and Security, Artificial Intelligence, Machine Learning","Smart grids are critical for addressing the growing energy demand due to global population growth and urbanization. They enhance efficiency, reliability, and sustainability by integrating renewable energy. Ensuring their availability and safety requires advanced operational control and safety measures. Researchers employ AI and machine learning to assess grid stability, but challenges like the lack of datasets and cybersecurity threats, including adversarial attacks, persist. In particular, data scarcity is a key issue: obtaining grid instability instances is tough due to the need for significant expertise, resources, and time. However, they are essential to test novel research advancements and security mitigations.
In this paper, we introduce a novel framework to detect instability in smart grids by employing only stable data. It relies on a Generative Adversarial Network (GAN) where the generator is trained to create instability data that are used along with stable data to train the discriminator. Moreover, we include a new adversarial training layer to improve robustness against adversarial attacks. Our solution, tested on a dataset composed of real-world stable and unstable samples, achieve accuracy up to 97.5% in predicting grid stability and up to 98.9% in detecting adversarial attacks. Moreover, we implemented our model in a single-board computer demonstrating efficient real-time decision-making with an average response time of less than 7ms. Our solution improves prediction accuracy and resilience while addressing data scarcity in smart grid management."
809,679d459debd8ffd557a2b196,cs.CR,https://arxiv.org/pdf/2501.16466,On the Feasibility of Using LLMs to Execute Multistage Network Attacks,"Brian Singer, Keane Lucas, Lakshmi Adiga, Meghna Jain, Lujo Bauer, Vyas Sekar","Cryptography and Security, Artificial Intelligence","LLMs have shown preliminary promise in some security tasks and CTF challenges.
However, it is unclear whether LLMs are able to realize multistage network attacks, which involve executing a wide variety of actions across multiple hosts such as conducting reconnaissance, exploiting vulnerabilities to gain initial access, leveraging internal hosts to move laterally, and using multiple compromised hosts to exfiltrate data.
We evaluate LLMs across 10 multistage networks and find that popular LLMs are unable to realize these attacks. To enable LLMs to realize these attacks, we introduce Incalmo, an LLM-agnostic high-level attack abstraction layer that sits between an LLM and the environment.
Rather than LLMs issuing low-level command-line instructions, which can lead to incorrect implementations, Incalmo allows LLMs to specify high-level tasks (e.g., infect a host, scan a network), which are then carried out by Incalmo.
Incalmo realizes these tasks by translating them into low-level primitives (e.g., commands to exploit tools).
Incalmo also provides an environment state service and an attack graph service to provide structure to LLMs in selecting actions relevant to a multistage attack.
Across 9 out of 10 realistic emulated networks (from 25 to 50 hosts), LLMs using Incalmo can successfully autonomously execute multistage attacks.
We also conduct an ablation analysis to show the key role the high-level abstractions play. For instance, we find that both Incalmo’s high-level tasks and services are crucial. Furthermore, even smaller-parameter LLMs with Incalmo can fully succeed in 5 of 10 environments, while larger-parameter LLMs without Incalmo do not fully succeed in any."
810,679d459debd8ffd557a2b197,cs.CR,https://arxiv.org/pdf/2501.16451,Emulating OP_RAND in Bitcoin,Oleksandr Kurbatov,Cryptography and Security,This paper proposes a method of emulation ofOP_RANDopcode on Bitcoin through a trustless interactive game between transaction counterparties. The game result is probabilistic and doesn’t allow any party to cheat and increase their chance to win on any protocol step. The protocol can be organized in a way unrecognizable to any external party and doesn’t require some specific scripts or Bitcoin protocol updates. We will show how the protocol works on the simpleThimbles Gameand provide some initial thoughts about approaches and applications that can use the mentioned approach.
811,679d459debd8ffd557a2b198,cs.CR,https://arxiv.org/pdf/2501.17124,The Asymptotic Capacity of Byzantine Symmetric Private Information Retrieval and Its Consequences,"Mohamed Nomeir, Alptug Aytekin, Sennur Ulukus","Information Theory, Cryptography and Security, Networking and Internet Architecture, Signal Processing","We consider the problem of finding the asymptotic capacity of symmetric private information retrieval (SPIR) withB𝐵Bitalic_BByzantine servers. Prior to finding the capacity, a definition for the Byzantine servers is needed since in the literature there are two different definitions. In[1], where it was first defined, the Byzantine servers can send any symbol from the storage, their received queries and some independent random symbols. In[2], Byzantine servers send any random symbol independently of their storage and queries. It is clear that these definitions are not identical, especially whensymmetricprivacy is required. To that end, we define Byzantine servers, inspired by[1], as the servers that can share everything, before and after the scheme initiation. In this setting, we find an upper bound, for an infinite number of messages case, that should be satisfied for all schemes that protect against this setting and develop a scheme that achieves this upper bound. Hence, we identify the capacity of the problem."
812,679d459debd8ffd557a2b199,cs.CR,https://arxiv.org/pdf/2501.17021,On Oblivious Transfer Capacity of Noisy Multiple Access Channel,"Hadi Aghaee, Christian Deppe","Information Theory, Cryptography and Security",
813,679d459debd8ffd557a2b19a,cs.CR,https://arxiv.org/pdf/2501.16964,Few Edges Are Enough: Few-Shot Network Attack Detection with Graph Neural Networks,"Tristan Bilot, Nour El Madhoun, Khaldoun Al Agha, Anis Zouaoui","Machine Learning, Cryptography and Security","Detecting cyberattacks using Graph Neural Networks (GNNs) has seen promising results recently. Most of the state-of-the-art models that leverage these techniques require labeled examples, hard to obtain in many real-world scenarios. To address this issue, unsupervised learning and Self-Supervised Learning (SSL) have emerged as interesting approaches to reduce the dependency on labeled data. Nonetheless, these methods tend to yield more anomalous detection algorithms rather than effective attack detection systems.
This paper introducesFew Edges Are Enough(FEAE), a GNN-based architecture trained with SSL and Few-Shot Learning (FSL) to better distinguish between false positive anomalies and actual attacks.
To maximize the potential of few-shot examples, our model employs a hybrid self-supervised objective that combines the advantages of contrastive-based and reconstruction-based SSL. By leveraging only a minimal number of labeled attack events, represented as attack edges,FEAEachieves competitive performance on two well-known network datasets compared to both supervised and unsupervised methods. Remarkably, our experimental results unveil that employing only 1 malicious event for each attack type in the dataset is sufficient to achieve substantial improvements.FEAEnot only outperforms self-supervised GNN baselines but also surpasses some supervised approaches on one of the datasets."
814,679d459debd8ffd557a2b19b,cs.CR,https://arxiv.org/pdf/2501.16888,Secure Federated Graph-Filtering for Recommender Systems,"Julien Nicolas, César Sabater, Mohamed Maouche, Sonia Ben Mokhtar, Mark Coates","Information Retrieval, Cryptography and Security","Recommender systems often rely on graph-based filters, such as normalized item-item adjacency matrices and low-pass filters. While effective, the centralized computation of these components raises concerns about privacy, security, and the ethical use of user data. This work proposes two decentralized frameworks for securely computing these critical graph components without centralizing sensitive information. The first approach leverages lightweight Multi-Party Computation and distributed singular vector computations to privately compute key graph filters. The second extends this framework by incorporating low-rank approximations, enabling a trade-off between communication efficiency and predictive performance. Empirical evaluations on benchmark datasets demonstrate that the proposed methods achieve comparable accuracy to centralized state-of-the-art systems while ensuring data confidentiality and maintaining low communication costs. Our results highlight the potential for privacy-preserving decentralized architectures to bridge the gap between utility and user data protection in modern recommender systems."
815,679d459debd8ffd557a2b19c,cs.CR,https://arxiv.org/pdf/2501.16638,Analysis of Zero Day Attack Detection Using MLP and XAI,"Ashim Dahal, Prabin Bajgai, Nick Rahimi","Machine Learning, Cryptography and Security",
816,679d459debd8ffd557a2b19d,cs.CR,https://arxiv.org/pdf/2501.16217,Evaluation of isolation design flow (IDF) for Single Chip Cryptography (SCC) application,Arsalan Ali Malik,"Cryptography and Security, Hardware Architecture",
817,679d459debd8ffd557a2b19e,cs.CR,https://arxiv.org/pdf/2501.16184,Cryptographic Compression,"Joshua Cooper, Grant Fickes","Cryptography and Security, Information Theory","We introduce a protocol called ENCORE which simultaneously compresses and encrypts data in a one-pass process that can be implemented efficiently and possesses a number of desirable features as a streaming encoder/decoder. Motivated by the observation that both lossless compression and encryption consist of performing an invertible transformation whose output is close to a uniform distribution over bit streams, we show that these can be done simultaneously, at least for “typical” data with a stable distribution, i.e., approximated reasonably well by the output of a Markov model. The strategy is to transform the data into a dyadic distribution whose Huffman encoding is close to uniform, and then store the transformations made to said data in a compressed secondary stream interwoven into the first with a user-defined encryption protocol. The result is an encoding which we show exhibits a modified version of Yao’s “next-bit test” while requiring many fewer bits of entropy than standard encryption. Numerous open questions remain, particularly regarding results that we suspect can be strengthened considerably."
818,679d459debd8ffd557a2b19f,cs.CR,https://arxiv.org/pdf/2501.16165,Demystifying OS Kernel Fuzzing with a Novel Taxonomy,"Jiacheng Xu, He Sun, Shihao Jiang, Qinying Wang, Mingming Zhang, Xiang Li, Kaiwen Shen, Peng Cheng, Jiming Chen, Charles Zhang, Shouling Ji","Cryptography and Security, Computers and Society, Operating Systems","The Operating System (OS) kernel is foundational in modern computing, especially with the proliferation of diverse computing devices. However, its development also comes with vulnerabilities that can lead to severe security breaches. Kernel fuzzing, a technique used to uncover these vulnerabilities, poses distinct challenges when compared to userspace fuzzing. These include the complexity of configuring the testing environment and addressing the statefulness inherent to both the kernel and the fuzzing process.
Despite the significant interest from the security community, a comprehensive understanding of kernel fuzzing remains lacking, hindering further progress in the field."
819,679d459debd8ffd557a2b1a0,cs.CR,https://arxiv.org/pdf/2501.16029,"FDLLM: A Text Fingerprint Detection Method for LLMs in Multi-Language, Multi-Domain Black-Box Environments","Zhiyuan Fu, Junfan Chen, Hongyu Sun, Ting Yang, Ruidong Li, Yuqing Zhang","Cryptography and Security, Artificial Intelligence","Using large language models (LLMs) integration platforms without transparency about which LLM is being invoked can lead to potential security risks. Specifically, attackers may exploit this black-box scenario to deploy malicious models and embed viruses in the code provided to users. In this context, it is increasingly urgent for users to clearly identify the LLM they are interacting with, in order to avoid unknowingly becoming victims of malicious models.
However, existing studies primarily focus on mixed classification of human and machine-generated text, with limited attention to classifying texts generated solely by different models. Current research also faces dual bottlenecks: poor quality of LLM-generated text (LLMGT) datasets and limited coverage of detectable LLMs, resulting in poor detection performance for various LLMGT in black-box scenarios. We propose the first LLMGT fingerprint detection model,FDLLM, based on Qwen2.5-7B and fine-tuned using LoRA to address these challenges. FDLLM can more efficiently handle detection tasks across multilingual and multi-domain scenarios. Furthermore, we constructed a dataset namedFD-Datasets, consisting of 90,000 samples that span multiple languages and domains, covering 20 different LLMs. Experimental results demonstrate that FDLLM achieves a macro F1 score 16.7% higher than the best baseline method, LM-D."
820,679d459debd8ffd557a2b1a1,cs.CR,https://arxiv.org/pdf/2501.16007,TOPLOC: A Locality Sensitive Hashing Scheme for Trustless Verifiable Inference,"Jack Min Ong, Matthew Di Ferrante, Aaron Pazdera, Ryan Garner, Sami Jaghouar, Manveer Basra, Johannes Hagemann","Cryptography and Security, Distributed, Parallel, and Cluster Computing","Large language models (LLMs) have proven to be very capable, but access to frontier models currently rely on inference providers which introduces trust challenges – how can we be sure that the provider is using the model configuration they claim?
We proposeTopLoc, a novel method for verifiable inference that addresses this problem.TopLocleverages a compact locality sensitive hashing mechanism for intermediate activations which can detect unauthorized modifications to models, prompts, or precision with 100% accuracy, achieving no false positives or negatives in our empirical evaluations.
Our approach is robust across diverse hardware configurations, GPU types, and algebraic reorderings, which allows for validation speeds up to100×100\times100 ×faster than the original inference.
By introducing a polynomial encoding scheme,TopLocminimizes memory overhead of the generated commits by1000×1000\times1000 ×, requiring only 258 bytes of storage per 32 new tokens compared to the 262KB requirement of storing the token embeddings directly for Llama-3.1-8B-Instruct.
Our method empowers users to verify LLM inference computations efficiently, fostering greater trust and transparency in open ecosystems and lays a foundation for decentralized, verifiable and trustless compute protocols."
821,679d459debd8ffd557a2b1a2,cs.CR,https://arxiv.org/pdf/2501.15975,Provisioning Time-Based Subscription in NDN: A Secure and Efficient Access Control Scheme,"Nazatul H. Sultan, Chandan Kumar, Saurab Dulal, Vijay Varadharajan, Seyit Camtepe, Surya Nepal",Cryptography and Security,"This paper proposes a novel encryption-based access control mechanism for Named Data Networking (NDN). The scheme allows data producers to share their content in encrypted form before transmitting it to consumers. The encryption mechanism incorporates time-based subscription access policies directly into the encrypted content, enabling only consumers with valid subscriptions to decrypt it. This makes the scheme well-suited for real-world, subscription-based applications like Netflix. Additionally, the scheme introduces an anonymous and unlinkable signature-based authentication mechanism that empowers edge routers to block bogus content requests at the network’s entry point, thereby mitigating Denial of Service (DoS) attacks. A formal security proof demonstrates the scheme’s resistance to Chosen Plaintext Attacks (CPA). Performance analysis, using Mini-NDN-based emulation and a Charm library implementation, further confirms the practicality of the scheme. Moreover, it outperforms closely related works in terms of functionality, security, and communication overhead."
822,679d459debd8ffd557a2b1a3,cs.CR,https://arxiv.org/pdf/2501.15911,"Web Execution Bundles: Reproducible, Accurate, and Archivable Web Measurements","Florian Hantke, Peter Snyder, Hamed Haddadi, Ben Stock",Cryptography and Security,
823,679d459debd8ffd557a2b1a4,cs.CR,https://arxiv.org/pdf/2501.15836,Intelligent Code Embedding Framework for High-Precision Ransomware Detection via Multimodal Execution Path Analysis,"Levi Gareth, Maximilian Fairbrother, Peregrine Blackwood, Lucasta Underhill, Benedict Ruthermore","Cryptography and Security, Artificial Intelligence","Modern threat landscapes continue to evolve with increasing sophistication, challenging traditional detection methodologies and necessitating innovative solutions capable of addressing complex adversarial tactics. A novel framework was developed to identify ransomware activity through multimodal execution path analysis, integrating high-dimensional embeddings and dynamic heuristic derivation mechanisms to capture behavioral patterns across diverse attack variants. The approach demonstrated high adaptability, effectively mitigating obfuscation strategies and polymorphic characteristics often employed by ransomware families to evade detection. Comprehensive experimental evaluations revealed significant advancements in precision, recall, and accuracy metrics compared to baseline techniques, particularly under conditions of variable encryption speeds and obfuscated execution flows. The framework achieved scalable and computationally efficient performance, ensuring robust applicability across a range of system configurations, from resource-constrained environments to high-performance infrastructures. Notable findings included reduced false positive rates and enhanced detection latency, even for ransomware families employing sophisticated encryption mechanisms. The modular design allowed seamless integration of additional modalities, enabling extensibility and future-proofing against emerging threat vectors. Quantitative analyses further highlighted the system’s energy efficiency, emphasizing its practicality for deployment in environments with stringent operational constraints. The results underline the importance of integrating advanced computational techniques and dynamic adaptability to safeguard digital ecosystems from increasingly complex threats."
824,679d459debd8ffd557a2b1a5,cs.CR,https://arxiv.org/pdf/2501.15760,Investigating Application of Deep Neural Networks in Intrusion Detection System Design,Mofe O. Jeje,"Cryptography and Security, Machine Learning","Despite decades of development, existing IDSs still face challenges in improving detection accuracy, evasion, and detection of unknown attacks. To solve these problems, many researchers have focused on designing and developing IDSs that use Deep Neural Networks (DNN) which provides advanced methods of threat investigation and detection. Given this reason, the motivation of this research then, is to learn how effective applications of Deep Neural Networks (DNN) can accurately detect and identify malicious network intrusion, while advancing the frontiers of their optimal potential use in network intrusion detection. Using the ASNM-TUN dataset, the study used a Multilayer Perceptron modeling approach in Deep Neural Network to identify network intrusions, in addition to distinguishing them in terms of legitimate network traffic, direct network attacks, and obfuscated network attacks. To further enhance the speed and efficiency of this DNN solution, a thorough feature selection technique called Forward Feature Selection (FFS), which resulted in a significant reduction in the feature subset, was implemented. Using the Multilayer Perceptron model, test results demonstrate no support for the model to accurately and correctly distinguish the classification of network intrusion."
825,679d459debd8ffd557a2b1a6,cs.CR,https://arxiv.org/pdf/2501.15751,A Privacy Model for Classical & Learned Bloom Filters,Hayder Tirmazi,"Cryptography and Security, Machine Learning",
826,679d459debd8ffd557a2b1a7,cs.CR,https://arxiv.org/pdf/2501.15578,A Complexity-Informed Approach to Optimise Cyber Defences,Lampis Alevizos,Cryptography and Security,
827,679d459debd8ffd557a2b1a8,cs.CR,https://arxiv.org/pdf/2501.15553,Real-CATS: A Practical Training Ground for Emerging Research on Cryptocurrency Cybercrime Detection,"Jiadong Shi, Chunyu Duan, Hao Lei, Liangmin Wang","Cryptography and Security, Computers and Society",
828,679d459debd8ffd557a2b1a9,cs.CR,https://arxiv.org/pdf/2501.15509,FIT-Print: Towards False-claim-resistant Model Ownership Verification via Targeted Fingerprint,"Shuo Shao, Haozhe Zhu, Hongwei Yao, Yiming Li, Tianwei Zhang, Zhan Qin, Kui Ren","Cryptography and Security, Artificial Intelligence, Machine Learning","Model fingerprinting is a widely adopted approach to safeguard the intellectual property rights of open-source models by preventing their unauthorized reuse. It is promising and convenient since it does not necessitate modifying the protected model. In this paper, we revisit existing fingerprinting methods and reveal that they are vulnerable to false claim attacks where adversaries falsely assert ownership of any third-party model. We demonstrate that this vulnerability mostly stems from theiruntargetednature, where they generally compare the outputs of given samples on different models instead of the similarities to specific references. Motivated by these findings, we propose a targeted fingerprinting paradigm (i.e.formulae-sequence𝑖𝑒i.e.italic_i . italic_e ., FIT-Print) to counteract false claim attacks. Specifically, FIT-Print transforms the fingerprint into a targeted signature via optimization. Building on the principles of FIT-Print, we develop bit-wise and list-wise black-box model fingerprinting methods,i.e.formulae-sequence𝑖𝑒i.e.italic_i . italic_e ., FIT-ModelDiff and FIT-LIME, which exploit the distance between model outputs and the feature attribution of specific samples as the fingerprint, respectively. Extensive experiments on benchmark models and datasets verify the effectiveness, conferrability, and resistance to false claim attacks of our FIT-Print."
829,679d459debd8ffd557a2b1aa,cs.CR,https://arxiv.org/pdf/2501.15478,LoRAGuard: An Effective Black-box Watermarking Approach for LoRAs,"Peizhuo Lv, Yiran Xiahou, Congyi Li, Mengjie Sun, Shengzhi Zhang, Kai Chen, Yingjun Zhang","Cryptography and Security, Machine Learning","LoRA (Low-Rank Adaptation) has achieved remarkable success in the parameter-efficient fine-tuning of large models. The trained LoRA matrix can be integrated with the base model through addition or negation operation to improve performance on downstream tasks. However, the unauthorized use of LoRAs to generate harmful content highlights the need for effective mechanisms to trace their usage. A natural solution is to embed watermarks into LoRAs to detect unauthorized misuse. However, existing methods struggle when multiple LoRAs are combined or negation operation is applied, as these can significantly degrade watermark performance. In this paper, we introduce LoRAGuard, a novel black-box watermarking technique for detecting unauthorized misuse of LoRAs. To support both addition and negation operations, we propose the Yin-Yang watermark technique, where the Yin watermark is verified during negation operation and the Yang watermark during addition operation. Additionally, we propose a shadow-model-based watermark training approach that significantly improves effectiveness in scenarios involving multiple integrated LoRAs. Extensive experiments on both language and diffusion models show that LoRAGuard achieves nearly 100% watermark verification success and demonstrates strong effectiveness."
830,679d459debd8ffd557a2b1ab,cs.CR,https://arxiv.org/pdf/2501.15468,Low-altitude Friendly-Jamming for Satellite-Maritime Communications via Generative AI-enabled Deep Reinforcement Learning,"Jiawei Huang, Aimin Wang, Geng Sun, Jiahui Li, Jiacheng Wang, Dusit Niyato, Victor C. M. Leung","Cryptography and Security, Signal Processing","Low Earth Orbit (LEO) satellites can be used to assist maritime wireless communications for data transmission across wide-ranging areas. However, extensive coverage of LEO satellites, combined with openness of channels, can cause the communication process to suffer from security risks. This paper presents a low-altitude friendly-jamming LEO satellite-maritime communication system enabled by a unmanned aerial vehicle (UAV) to ensure data security at the physical layer. Since such a system requires trade-off policies that balance the secrecy rate and energy consumption of the UAV to meet evolving scenario demands, we formulate a secure satellite-maritime communication multi-objective optimization problem (SSMCMOP). In order to solve the dynamic and long-term optimization problem, we reformulate it into a Markov decision process. We then propose a transformer-enhanced soft actor critic (TransSAC) algorithm, which is a generative artificial intelligence-enable deep reinforcement learning approach to solve the reformulated problem, so that capturing global dependencies and diversely exploring weights. Simulation results demonstrate that the TransSAC outperforms various baselines, and achieves an optimal secrecy rate while effectively minimizing the energy consumption of the UAV. Moreover, the results find more suitable constraint values for the system."
831,679d459debd8ffd557a2b1ac,cs.CR,https://arxiv.org/pdf/2501.15459,FiberPool: Leveraging Multiple Blockchains for Decentralized Pooled Mining,"Akira Sakurai, Kazuyuki Shudo",Cryptography and Security,"The security of blockchain systems based on Proof of Work relies on mining. However, mining suffers from unstable revenue, prompting many miners to form cooperative mining pools. Most existing mining pools operate in a centralized manner, which undermines the decentralization principle of blockchain."
832,679d459debd8ffd557a2b1ad,cs.CR,https://arxiv.org/pdf/2501.15395,Hiding in Plain Sight: An IoT Traffic Camouflage Framework for Enhanced Privacy,"Daniel Adu Worae, Spyridon Mastorakis","Cryptography and Security, Networking and Internet Architecture","The rapid growth of Internet of Things (IoT) devices has introduced significant challenges to privacy, particularly as network traffic analysis techniques evolve. While encryption protects data content, traffic attributes such as packet size and timing can reveal sensitive information about users and devices. Existing single-technique obfuscation methods, such as packet padding, often fall short in dynamic environments like smart homes due to their predictability, making them vulnerable to machine learning-based attacks. This paper introduces a multi-technique obfuscation framework designed to enhance privacy by disrupting traffic analysis. The framework leverages six techniques—Padding, Padding with XORing, Padding with Shifting, Constant Size Padding, Fragmentation, and Delay Randomization—to obscure traffic patterns effectively. Evaluations on three public datasets demonstrate significant reductions in classifier performance metrics, including accuracy, precision, recall, and F1 score. We assess the framework’s robustness against adversarial tactics by retraining and fine-tuning neural network classifiers on obfuscated traffic. The results reveal a notable degradation in classifier performance, underscoring the framework’s resilience against adaptive attacks. Furthermore, we evaluate communication and system performance, showing that higher obfuscation levels enhance privacy but may increase latency and communication overhead."
833,679d459debd8ffd557a2b1ae,cs.CR,https://arxiv.org/pdf/2501.15391,Open Set RF Fingerprinting Identification: A Joint Prediction and Siamese Comparison Framework,"Donghong Cai, Jiahao Shan, Ning Gao, Bingtao He, Yingyang Chen, Shi Jin, Pingzhi Fan",Cryptography and Security,"Radio Frequency Fingerprinting Identification (RFFI) is a lightweight physical layer identity authentication technique. It identifies the radio-frequency device by analyzing the signal feature differences caused by the inevitable minor hardware impairments. However, existing RFFI methods based on closed-set recognition struggle to detect unknown unauthorized devices in open environments. Moreover, the feature interference among legitimate devices can further compromise identification accuracy. In this paper, we propose a joint radio frequency fingerprint prediction and siamese comparison (JRFFP-SC) framework for open set recognition. Specifically, we first employ a radio frequency fingerprint prediction network to predict the most probable category result. Then a detailed comparison among the test sample’s features with registered samples is performed in a siamese network. The proposed JRFFP-SC framework eliminates inter-class interference and effectively addresses the challenges associated with open set identification. The simulation results show that our proposed JRFFP-SC framework can achieve excellent rogue device detection and generalization capability for classifying devices."
834,679d459debd8ffd557a2b1af,cs.CR,https://arxiv.org/pdf/2501.15313,I Know What You Did Last Summer: Identifying VR User Activity Through VR Network Traffic,"Sheikh Samit Muhaimin, Spyridon Mastorakis",Cryptography and Security,"Virtual Reality (VR) technology has gained substantial traction and has the potential to transform a number of industries, including education, entertainment, and professional sectors. Nevertheless, concerns have arisen about the security and privacy implications of VR applications and the impact that they might have on users. In this paper, we investigate the following overarching research question: can VR applications and VR user activities in the context of such applications (e.g., manipulating virtual objects, walking, talking, flying) be identified based on the (potentially encrypted) network traffic that is generated by VR headsets during the operation of VR applications? To answer this question, we collect network traffic data from 25 VR applications running on the Meta Quest Pro headset and identify characteristics of the generated network traffic, which we subsequently use to train off-the-shelf Machine Learning (ML) models. Our results indicate that through the use of ML models, we can identify the VR applications being used with an accuracy of 92.4% and the VR user activities performed with an accuracy of 91%. Furthermore, our results demonstrate that an attacker does not need to collect large amounts of network traffic data for each VR application to carry out such an attack. Specifically, an attacker only needs to collect less than 10 minutes of network traffic data for each VR application in order to identify applications with an accuracy higher than 90% and VR user activities with an accuracy higher than 88%."
835,679d459debd8ffd557a2b1b0,cs.CR,https://arxiv.org/pdf/2501.15290,Advanced Real-Time Fraud Detection Using RAG-Based LLMs,"Gurjot Singh, Prabhjot Singh, Maninder Singh","Cryptography and Security, Artificial Intelligence","Artificial Intelligence (AI) has become a double-edged sword in modern society, being both a boon and a bane. While it empowers individuals, it also enables malicious actors to perpetrate scams such as fraudulent phone calls and user impersonations. This growing threat necessitates a robust system to protect individuals. In this paper, we introduce a novel real-time fraud detection mechanism using Retrieval Augmented Generation (RAG) technology to address this challenge on two fronts. First, our system incorporates a continuously updating policy-checking feature that transcribes phone calls in real-time and uses RAG-based models to verify that the caller is not soliciting private information, thus ensuring transparency and the authenticity of the conversation. Second, we implement a real-time user impersonation check with a two-step verification process to confirm the caller’s identity, ensuring accountability. A key innovation of our system is the ability to update policies without retraining the entire model, enhancing its adaptability. We validated our RAG-based approach using synthetic call recordings, achieving an accuracy of 97.98% and an F1-score of 97.44% with 100 calls, outperforming state-of-the-art methods. This robust and flexible fraud detection system is well-suited for real-world deployment."
836,679d459debd8ffd557a2b1b1,cs.CR,https://arxiv.org/pdf/2501.15288,A Two-Stage CAE-Based Federated Learning Framework for Efficient Jamming Detection in 5G Networks,"Samhita Kuili, Mohammadreza Amini, Burak Kantarci","Cryptography and Security, Machine Learning","Cyber-security for 5G networks is drawing notable attention due to an increase in complex jamming attacks that could target the critical 5G Radio Frequency (RF) domain. These attacks pose a significant risk to heterogeneous network (HetNet) architectures, leading to degradation in network performance. Conventional machine-learning techniques for jamming detection rely on centralized training while increasing the odds of data privacy. To address these challenges, this paper proposes a decentralized two-stage federated learning (FL) framework for jamming detection in 5G femtocells. Our proposed distributed framework encompasses using the Federated Averaging (FedAVG) algorithm to train a Convolutional Autoencoder (CAE) for unsupervised learning. In the second stage, we use a fully connected network (FCN) built on the pre-trained CAE encoder that is trained using Federated Proximal (FedProx) algorithm to perform supervised classification. Our experimental results depict that our proposed framework (FedAVG and FedProx) accomplishes efficient training and prediction across non-IID client datasets without compromising data privacy. Specifically, our framework achieves a precision of 0.94, recall of 0.90, F1-score of 0.92, and an accuracy of 0.92, while minimizing communication rounds to 30 and achieving robust convergence in detecting jammed signals with an optimal client count of 6."
837,679d459debd8ffd557a2b1b2,cs.CR,https://arxiv.org/pdf/2501.15145,PromptShield: Deployable Detection for Prompt Injection Attacks,"Dennis Jacob, Hend Alzahrani, Zhanhao Hu, Basel Alomair, David Wagner",Cryptography and Security,
838,679d459debd8ffd557a2b1b3,cs.CR,https://arxiv.org/pdf/2501.15101,Comprehensive Evaluation of Cloaking Backdoor Attacks on Object Detector in Real-World,"Hua Ma, Alsharif Abuadbba, Yansong Gao, Hyoungshick Kim, Surya Nepal",Cryptography and Security,"The exploration of backdoor vulnerabilities in object detectors, particularly inreal-world scenarios, remains limited. A significant challenge lies in the absence of a natural physical backdoor dataset, and constructing such a dataset is both time- and labor-intensive. In this work, we address this gap by creating a large-scale dataset comprising approximately 11,800 images/frames with annotations featuring natural objects (e.g., T-shirts and hats) as triggers to incur cloaking adversarial effects in diverse real-world scenarios.
This dataset is tailored for the study of physical backdoors in object detectors. Leveraging this dataset, we conduct a comprehensive evaluation of an insidious cloaking backdoor effect against object detectors, wherein the bounding box around a person vanishes when the individual is near a natural object (e.g., a commonly available T-shirt) in front of the detector. Our evaluations encompass three prevalent attack surfaces: data outsourcing, model outsourcing, and the use of pretrained models.
The cloaking effect is successfully implanted in object detectors across all three attack surfaces. We extensively evaluate four popular object detection algorithms (anchor-based Yolo-V3, Yolo-V4, Faster R-CNN, and anchor-free CenterNet) using 19 videos (totaling approximately 11,800 frames) in real-world scenarios. Our results demonstrate that the backdoor attack exhibits remarkable robustness againstvarious factors, including movement, distance, angle, non-rigid deformation, and lighting. In data and model outsourcing scenarios, the attack success rate (ASR) in most videos reaches 100% or near it, while the clean data accuracy of the backdoored model remains indistinguishable from that of the clean model, making it impossible to detect backdoor behavior through a validation set. Notably, two-stage object detectors (e.g., Faster R-CNN) show greater resistance to backdoor attacks under pure data poisoning conditions (i.e., in data outsourcing) compared to one-stage detectors (e.g., the Yolo series). However, this challenge is surmountable when the attacker controls the training process (particularly in model outsourcing), even with the same small poisoning rate budget as in data outsourcing.
In transfer learning attack scenarios assessed on CenterNet, the average ASR remains high at 78%. A detailed 5-minute video illustrating our attack is available athttps://youtu.be/Q3HOF4OobbY."
839,679d459debd8ffd557a2b1b4,cs.CR,https://arxiv.org/pdf/2501.15084,Hierarchical Pattern Decryption Methodology for Ransomware Detection Using Probabilistic Cryptographic Footprints,"Kevin Pekepok, Persephone Kirkwood, Esme Christopolous, Florence Braithwaite, Oliver Nightingale","Cryptography and Security, Artificial Intelligence","The increasing sophistication of encryption-based ransomware has demanded innovative approaches to detection and mitigation, prompting the development of a hierarchical framework grounded in probabilistic cryptographic analysis. By focusing on the statistical characteristics of encryption patterns, the proposed methodology introduces a layered approach that combines advanced clustering algorithms with machine learning to isolate ransomware-induced anomalies. Through comprehensive testing across diverse ransomware families, the framework demonstrated exceptional accuracy, effectively distinguishing malicious encryption operations from benign activities while maintaining low false positive rates. The system’s design integrates dynamic feedback mechanisms, enabling adaptability to varying cryptographic complexities and operational environments. Detailed entropy-based evaluations revealed its sensitivity to subtle deviations in encryption workflows, offering a robust alternative to traditional detection methods reliant on static signatures or heuristics. Computational benchmarks confirmed its scalability and efficiency, achieving consistent performance even under high data loads and complex cryptographic scenarios. The inclusion of real-time clustering and anomaly evaluation ensures rapid response capabilities, addressing critical latency challenges in ransomware detection. Performance comparisons with established methods highlighted its improvements in detection efficacy, particularly against advanced ransomware employing extended key lengths and unique cryptographic protocols."
840,679d459debd8ffd557a2b1b5,cs.CR,https://arxiv.org/pdf/2501.15077,NetChain: Authenticated Blockchain Top-k Graph Data Queries and its Application in Asset Management,"Hongguang Zhao, Xu Yang, Saiyu Qi, Qiuhao Wang, Ke Li","Cryptography and Security, Databases","As a valuable digital resource, graph data is an important data asset, which has been widely utilized across various fields to optimize decision-making and enable smarter solutions. To manage data assets, blockchain is widely used to enable data sharing and trading, but it cannot supply complex analytical queries. vChain was proposed to achieve verifiable boolean queries over blockchain by designing an embedded authenticated data structure (ADS). However, for generating (non-)existence proofs, vChain suffers from expensive storage and computation costs in ADS construction, along with high communication and verification costs. In this paper, we propose a novel NetChain framework that enables efficient top-k queries over on-chain graph data with verifiability. Specifically, we design a novel authenticated two-layer index that supports (non-)existence proof generation in block-level and built-in verifiability for matched objects. To further alleviate the computation and verification overhead, an optimized variant NetChain+is derived.
The authenticity of our frameworks is validated through security analysis.
Evaluations show that NetChain and NetChain+outperform vChain, respectively achieving up to85×85\times85 ×and31×31\times31 ×improvements on ADS construction.
Moreover, compared with vChain, NetChain+reduces the communication and verification costs by87%percent8787\%87 %and96%percent9696\%96 %respectively."
841,679d459debd8ffd557a2b1b6,cs.CR,https://arxiv.org/pdf/2501.15076,Cryptanalysis via Machine Learning Based Information Theoretic Metrics,"Benjamin D. Kim, Vipindev Adat Vasudevan, Rafael G. L. D'Oliveira, Alejandro Cohen, Thomas Stahlbuhk, Muriel Médard","Cryptography and Security, Information Theory, Machine Learning","The fields of machine learning (ML) and cryptanalysis share an interestingly common objective of creating a function, based on a given set of inputs and outputs. However, the approaches and methods in doing so vary vastly between the two fields. In this paper, we explore integrating the knowledge from the ML domain to provide empirical evaluations of cryptosystems. Particularly, we utilize information theoretic metrics to perform ML-based distribution estimation. We propose two novel applications of ML algorithms that can be applied in a known plaintext setting to perform cryptanalysis on any cryptosystem. We use mutual information neural estimation to calculate a cryptosystem’s mutual information leakage, and a binary cross entropy classification to model an indistinguishability under chosen plaintext attack (CPA). These algorithms can be readily applied in an audit setting to evaluate the robustness of a cryptosystem and the results can provide a useful empirical bound. We evaluate the efficacy of our methodologies by empirically analyzing several encryption schemes. Furthermore, we extend the analysis to novel network coding-based cryptosystems and provide other use cases for our algorithms. We show that our classification model correctly identifies the encryption schemes that are not IND-CPA secure, such as DES, RSA, and AES ECB, with high accuracy. It also identifies the faults in CPA-secure cryptosystems with faulty parameters, such a reduced counter version of AES-CTR. We also conclude that with our algorithms, in most cases a smaller-sized neural network using less computing power can identify vulnerabilities in cryptosystems, providing a quick check of the sanity of the cryptosystem and help to decide whether to spend more resources to deploy larger networks that are able to break the cryptosystem."
842,679d459debd8ffd557a2b1b7,cs.CR,https://arxiv.org/pdf/2501.15031,A Portable and Stealthy Inaudible Voice Attack Based on Acoustic Metamaterials,"Zhiyuan Ning, Juan He, Zhanyong Tang, Weihang Hu, Xiaojiang Chen",Cryptography and Security,"We presentMetaAttack, the first approach to leverage acoustic metamaterials for inaudible attacks for voice control systems. Compared to the state-of-the-art inaudible attacks requiring complex and large speaker setups,MetaAttackachieves a longer attacking range and higher accuracy using a compact, portable device small enough to be put into a carry bag. These improvements in portability and stealth have led to the practical applicability of inaudible attacks and their adaptation to a wider range of scenarios. We demonstrate how the recent advancement in metamaterials can be utilized to design a voice attack system with carefully selected implementation parameters and commercial off-the-shelf components. We showcase thatMetaAttackcan be used to launch inaudible attacks for representative voice-controlled personal assistants, including Siri, Alexa, Google Assistant, XiaoAI, and Xiaoyi. The average word accuracy of all assistants is 76%, with a range of 8.85 m."
843,679d459debd8ffd557a2b1b8,cs.CR,https://arxiv.org/pdf/2501.15002,A Proof-Producing Compiler for Blockchain Applications,"Jeremy Avigad, Lior Goldberg, David Levit, Yoav Seginer, Alon Titelman","Cryptography and Security, Logic in Computer Science, Programming Languages","CairoZerois a programming language for running decentralized applications (dApps)
at scale. Programs written in the CairoZero language are compiled to machine
code for the Cairo CPU architecture and cryptographic protocols are used to
verify the results of execution efficiently on blockchain.
We explain how we have extended
the CairoZero compiler with tooling that enables users to prove,
in the Lean 3 proof assistant, that compiled code satisfies
high-level functional specifications.
We demonstrate the success of our approach by verifying primitives
for computation with the secp256k1 and secp256r1 curves over a large finite field
as well as the validation of cryptographic signatures using the former.
We also verify a mechanism for simulating a read-write dictionary data structure in
a read-only setting.
Finally, we reflect on our methodology and discuss some of the benefits of our approach."
844,679d459debd8ffd557a2b1b9,cs.CR,https://arxiv.org/pdf/2501.15718,CENSOR: Defense Against Gradient Inversion via Orthogonal Subspace Bayesian Sampling,"Kaiyuan Zhang, Siyuan Cheng, Guangyu Shen, Bruno Ribeiro, Shengwei An, Pin-Yu Chen, Xiangyu Zhang, Ninghui Li","Machine Learning, Cryptography and Security","Federated learning collaboratively trains a neural network on a global server, where each local client receives the current global model weights and sends back parameter updates (gradients) based on its local private data.
The process of sending these model updates may leak client’s private data information.
Existing gradient inversion attacks can exploit this vulnerability to recover private training instances from a client’s gradient vectors. Recently, researchers have proposed advanced gradient inversion techniques that existing defenses struggle to handle effectively.
In this work, we present a novel defense tailored for large neural network models. Our defense capitalizes on the high dimensionality of the model parameters to perturb gradients within asubspace orthogonalto the original gradient. By leveraging cold posteriors over orthogonal subspaces, our defense implements a refined gradient update mechanism. This enables the selection of an optimal gradient that not only safeguards against gradient inversion attacks but also maintains model utility.
We conduct comprehensive experiments across three different datasets and evaluate our defense against various state-of-the-art attacks and defenses.
Code is available athttps://censor-gradient.github.io."
845,679d459debd8ffd557a2b1ba,cs.CR,https://arxiv.org/pdf/2501.15563,PCAP-Backdoor: Backdoor Poisoning Generator for Network Traffic in CPS/IoT Environments,"Ajesh Koyatan Chathoth, Stephen Lee","Machine Learning, Cryptography and Security, Networking and Internet Architecture","The rapid expansion of connected devices has made them prime targets for cyberattacks. To address these threats, deep learning-based, data-driven intrusion detection systems (IDS) have emerged as powerful tools for detecting and mitigating such attacks. These IDSs analyze network traffic to identify unusual patterns and anomalies that may indicate potential security breaches. However, prior research has shown that deep learning models are vulnerable to backdoor attacks, where attackers inject triggers into the model to manipulate its behavior and cause misclassifications of network traffic. In this paper, we explore the susceptibility of deep learning-based IDS systems to backdoor attacks in the context of network traffic analysis. We introducePCAP-Backdoor, a novel technique that facilitates backdoor poisoning attacks on PCAP datasets.
Our experiments on real-world Cyber-Physical Systems (CPS) and Internet of Things (IoT) network traffic datasets demonstrate that attackers can effectively backdoor a model by poisoning as little as 1% or less of the entire training dataset. Moreover, we show that an attacker can introduce a trigger into benign traffic during model training yet cause the backdoored model to misclassify malicious traffic when the trigger is present. Finally, we highlight the difficulty of detecting this trigger-based backdoor, even when using existing backdoor defense techniques."
846,679d459debd8ffd557a2b1bb,cs.CR,https://arxiv.org/pdf/2501.15529,UNIDOOR: A Universal Framework for Action-Level Backdoor Attacks in Deep Reinforcement Learning,"Oubo Ma, Linkang Du, Yang Dai, Chunyi Zhou, Qingming Li, Yuwen Pu, Shouling Ji","Machine Learning, Artificial Intelligence, Cryptography and Security","Deep reinforcement learning (DRL) is widely applied to safety-critical decision-making scenarios.
However, DRL is vulnerable to backdoor attacks, especially action-level backdoors, which pose significant threats through precise manipulation and flexible activation, risking outcomes like vehicle collisions or drone crashes.
The key distinction of action-level backdoors lies in the utilization of the backdoor reward function to associate triggers with target actions.
Nevertheless, existing studies typically rely on backdoor reward functions with fixed values or conditional flipping, which lack universality across diverse DRL tasks and backdoor designs, resulting in fluctuations or even failure in practice."
847,679d459debd8ffd557a2b1bc,cs.CR,https://arxiv.org/pdf/2501.15032,Stealthy Voice Eavesdropping with Acoustic Metamaterials: Unraveling a New Privacy Threat,"Zhiyuan Ning, Zhanyong Tang, Juan He, Weizhi Meng, Yuntian Chen","Sound, Cryptography and Security, Audio and Speech Processing","We presentSuperEar, a novel privacy threat based on acoustic metamaterials. Unlike previous research,SuperEarcan surreptitiously track and eavesdrop on the phone calls of a moving outdoor target from a safe distance. To design this attack,SuperEarovercomes the challenges faced by traditional acoustic metamaterials, including low low-frequency gain and audio distortion during reconstruction. It successfully magnifies the speech signal by approximately 20 times, allowing the sound to be captured from the earpiece of the target phone.
In addition,SuperEaroptimizes the trade-off between the number and size of acoustic metamaterials, improving the portability and concealability of the interceptor while ensuring effective interception performance. This makes it highly suitable for outdoor tracking and eavesdropping scenarios. Through extensive experimentation, we have evaluatedSuperEarand our results show that it can achieve an eavesdropping accuracy of over 80% within a range of 4.5 meters in the aforementioned scenario, thus validating its great potential in real-world applications."
848,679d459debd8ffd557a2b1bd,cs.CR,https://arxiv.org/pdf/2501.14974,Private Minimum Hellinger Distance Estimation via Hellinger Distance Differential Privacy,"Fengnan Deng, Anand N. Vidyashankar","Statistics Theory, Cryptography and Security, Probability, Methodology, Machine Learning",
849,679d459debd8ffd557a2b1be,cs.CR,https://arxiv.org/pdf/2501.04479,"Understanding, Implementing, and Supporting Security Assurance Cases in Safety-Critical Domains",Mazen Mohamad,"Software Engineering, Cryptography and Security",
850,679d459debd8ffd557a2b1bf,cs.CR,https://arxiv.org/pdf/2501.14651,Data-NoMAD: A Tool for Boosting Confidence in the Integrity of Social Science Survey Data,"Sanford C. Gordon, Cyrus Samii, Zhihao Su",Cryptography and Security,"To safeguard against data fabrication and enhance trust in quantitative social science, we present Data Non-Manipulation Authentication Digest (Data-NoMAD). Data-NoMAD is a tool that allows researchers to certify, and others to verify, that a dataset has not been inappropriately manipulated between the point of data collection and the point at which a replication archive is made publicly available. Data-NoMAD creates and stores acolumn hash digestof a raw dataset upon initial download from a survey platform (the current version works with Qualtrics and SurveyCTO), but before it is subject to appropriate manipulations such as anonymity-preserving redactions. Data-NoMAD can later be used to verify the integrity of a publicly archived dataset by identifying columns that have been deleted, added, or altered. Data-NoMAD complements existing efforts at ensuring research integrity and integrates seamlessly with extant replication practices."
851,679d459debd8ffd557a2b1c0,cs.CR,https://arxiv.org/pdf/2501.14556,A sandbox study proposal for private and distributed health data analysis,"Rickard Brännvall, Hanna Svensson, Kannaki Kaliyaperumal, Håkan Burden, Susanne Stenberg","Cryptography and Security, Computers and Society, Distributed, Parallel, and Cluster Computing","This paper presents a sandbox study proposal focused on the distributed processing of personal health data within the Vinnova-funded SARDIN project. The project aims to develop the Health Data Bank (Hälsodatabanken in Swedish), a secure platform for research and innovation that complies with the European Health Data Space (EHDS) legislation. By minimizing the sharing and storage of personal data, the platform sends analysis tasks directly to the original data locations, avoiding centralization. This approach raises questions about data controller responsibilities in distributed environments and the anonymization status of aggregated statistical results. The study explores federated analysis, secure multi-party aggregation, and differential privacy techniques, informed by real-world examples from clinical research on Parkinson’s disease, stroke rehabilitation, and wound analysis. To validate the proposed study, numerical experiments were conducted using four open-source datasets to assess the feasibility and effectiveness of the proposed methods. The results support the methods for the proposed sandbox study by demonstrating that differential privacy in combination with secure aggregation techniques significantly improves the privacy-utility trade-off."
852,679d459debd8ffd557a2b1c1,cs.CR,https://arxiv.org/pdf/2501.14555,Exploring Answer Set Programming for Provenance Graph-Based Cyber Threat Detection: A Novel Approach,"Fang Li, Fei Zuo, Gopal Gupta","Cryptography and Security, Programming Languages","Provenance graphs are useful and powerful tools for representing system-level activities in cybersecurity; however, existing approaches often struggle with complex queries and flexible reasoning. This paper presents a novel approach using Answer Set Programming (ASP) to model and analyze provenance graphs. We introduce an ASP-based representation that captures intricate relationships between system entities, including temporal and causal dependencies. Our model enables sophisticated analysis capabilities such as attack path tracing, data exfiltration detection, and anomaly identification. The declarative nature of ASP allows for concise expression of complex security patterns and policies, facilitating both real-time threat detection and forensic analysis. We demonstrate our approach’s effectiveness through case studies showcasing its threat detection capabilities. Experimental results illustrate the model’s ability to handle large-scale provenance graphs while providing expressive querying. The model’s extensibility allows for incorporation of new system behaviors and security rules, adapting to evolving cyber threats. This work contributes a powerful, flexible, and explainable framework for reasoning about system behaviors and security incidents, advancing the development of effective threat detection and forensic investigation tools."
853,679d459debd8ffd557a2b1c2,cs.CR,https://arxiv.org/pdf/2501.14512,Real-world Edge Neural Network Implementations Leak Private Interactions Through Physical Side Channel,"Zhuoran Liu, Senna van Hoek, Péter Horváth, Dirk Lauret, Xiaoyun Xu, Lejla Batina",Cryptography and Security,"Neural networks have become a fundamental component of numerous practical applications, and their implementations, which are often accelerated by hardware, are integrated into all types of real-worldphysical devices.
User interactions with neural networks on hardware accelerators are commonly considered privacy-sensitive.
Substantial efforts have been made to uncover vulnerabilities and enhance privacy protection at the level of machine learning algorithms, including membership inference attacks, differential privacy, and federated learning.
However, neural networks are ultimately implemented and deployed on physical devices, and current research pays comparatively less attention to privacy protection at the implementation level.
In this paper, we introduce a generic physical side-channel attack,ScaAR, that extracts user interactions with neural networks by leveragingelectromagnetic (EM) emissionsof physical devices.
Our proposed attack isimplementation-agnostic, meaning it does not require the adversary to possess detailed knowledge of the hardware or software implementations, thanks to the capabilities of deep learning-based side-channel analysis (DLSCA).
Experimental results demonstrate that, through the EM side channel,ScaARcan effectively extract the class label of user interactions with neural classifiers, including inputs and outputs, on the AMD-Xilinx MPSoC ZCU104 FPGA and Raspberry Pi 3 B.
In addition, for the first time, we provide side-channel analysis on edge Large Language Model (LLM) implementations on the Raspberry Pi 5, showing that EM side channel leaks interaction data, and different LLM tokens can be distinguishable from the EM traces."
854,679d459debd8ffd557a2b1c3,cs.CR,https://arxiv.org/pdf/2501.14500,NIFuzz: Estimating Quantified Information Flow with a Fuzzer,"Daniel Blackwell, Ingolf Becker, David Clark","Cryptography and Security, Software Engineering","This paper presents a scalable, practical approach to quantifying information leaks in software; these errors are often overlooked and downplayed, but can seriously compromise security mechanisms such as address space layout randomisation (ASLR) and Pointer Authentication (PAC). We introduce approaches for three different metrics to estimate the size of information leaks, including a new derivation for the calculation of conditional mutual information.
Together, these metrics can inform of the relative safety of the target program against different threat models and provide useful details for finding the source of any leaks. We provide an implementation of a fuzzer,\qlfuzz, which is capable of dynamically computing these metrics with little overhead and has several strategies to optimise for the detection and quantification of information leaks.
We evaluate\qlfuzzon a set of 14 programs—including 8 real-world CVEs and ranging up to 278k lines of code in size—where we find that it is capable of detecting and providing good estimates for all of the known information leaks."
855,679d459debd8ffd557a2b1c4,cs.CR,https://arxiv.org/pdf/2501.14418,Thunderdome: Timelock-Free Rationally-Secure Virtual Channels,"Zeta Avarikioti, Yuheng Wang, Yuyi Wang","Cryptography and Security, Distributed, Parallel, and Cluster Computing, Computer Science and Game Theory","Payment channel networks (PCNs) offer a promising solution to address the limited transaction throughput of deployed blockchains. However, several attacks have recently been proposed that stress the vulnerability of PCNs to timelock and censoring attacks. To address such attacks, we introduceThunderdome, the first timelock-free PCN. Instead,Thunderdomeleverages the design rationale of virtual channels to extend a timelock-free payment channel primitive, thereby enabling multi-hop transactions without timelocks.
Previous works either utilize timelocks or do not accommodate transactions between parties that do not share a channel."
856,679d459debd8ffd557a2b1c5,cs.CR,https://arxiv.org/pdf/2501.14397,Streamlining Plug-and-Charge Authorization for Electric Vehicles with OAuth2 and OIDC,"Jonas Primbs, Dustin Kern, Michael Menth, Christoph Krauß",Cryptography and Security,"ThePlug-and-Charge(PnC) process defined by ISO 15118 standardizes automatedElectric Vehicle(EV) charging by enabling automatic installation of credentials and use for authentication betweenEVandCharge Point(CP).
However, the current credential installation process is non-uniform, relies on a complexPublic Key Infrastructure(PKI), lacks support for fine-grained authorization parameters, and is not very user-friendly.
In this paper, we propose a streamlined approach to the initial charging authorization process by leveraging the OAuth Device Authorization Grant and Rich Authorization Requests.
The proposed solution reduces technical complexity, simplifies credential installation, introduces flexible authorization constraints (e.g., time- and cost-based), and facilitates payment throughOpenID Connect(OIDC).
We present a proof-of-concept implementation along with performance evaluations and conduct a symbolic protocol verification using the Tamarin prover.
Furthermore, our approach solves the issue of OAuth’s cross-device authorization, making it suitable as a formally proven blueprint in contexts beyondEVcharging."
857,679d459debd8ffd557a2b1c6,cs.CR,https://arxiv.org/pdf/2501.14330,Online Authentication Habits of Indian Users,"Pratyush Choudhary, Subhrajit Das, Mukul Paras Potta, Prasuj Das, Abhishek Bichhawat","Cryptography and Security, Computers and Society","Passwords have been long used as the primary authentication method for web services. Weak passwords used by the users have prompted the use of password management tools and two-factor authentication to ensure better account security. While prior studies have studied their adoption individually, none of these studies focuses particularly on the Indian setting, which is culturally and economically different from the countries in which these studies have been done in the past. To this end, we conducted a survey with 90 participants residing in India to better understand the mindset of people on using password managers and two-factor authentication (2FA)."
858,679d459debd8ffd557a2b1c7,cs.CR,https://arxiv.org/pdf/2501.14328,Securing DRAM at Scale: ARFM-Driven Row Hammer Defense with Unveiling the Threat of Short tRC Patterns,"Nogeun Joo, Donghyuk Kim, Hyunjun Cho, Junseok Noh, Dongha Jung, Joo-Young Kim","Cryptography and Security, Hardware Architecture","Since the disclosure of the row hammer (RH) attack phenomenon in 2014, a significant threat to system security, it has been active research in both industry and academia. The major issue with RH is the deterioration of cell endurance as DRAM technology advances, making the defense system more challenging due to the reduced threshold. With the growing challenges posed by malicious RH strategies, various RH mitigation IPs have been proposed. Meanwhile, a new DRAM feature known as refresh management (RFM) was incorporated into the JEDEC standards for DDR5/LPDDR5, which allows the memory controller to have the necessary time to respond to RH. However, indiscriminate use of RFM leads to reduced system performance and higher power consumption. To address the issue of powerful RH attacks, our study involved an extensive analysis of the prevalent attack patterns in the field. We discovered a strong correlation between the timing and density of the active-to-active command period,t⁢R⁢C𝑡𝑅𝐶{tRC}italic_t italic_R italic_C, and the likelihood of RH attacks. Leveraging this correlation, we developed a method to optimize the use of adaptive refresh management (ARFM), thereby maximizing its efficacy."
859,679d459debd8ffd557a2b1c8,cs.CR,https://arxiv.org/pdf/2501.14008,WAFBOOSTER: Automatic Boosting of WAF Security Against Mutated Malicious Payloads,"Cong Wu, Jing Chen, Simeng Zhu, Wenqi Feng, Ruiying Du, Yang Xiang",Cryptography and Security,"Web application firewall (WAF) examines malicious traffic to and from a web application via a set of security rules. It plays a significant role in securing Web applications against web attacks. However, as web attacks grow in sophistication, it is becoming increasingly difficult for WAFs to block the mutated malicious payloads designed to bypass their defenses. In response to this critical security issue, we have developed a novel learning-based framework calledWAFBooster, designed to unveil potential bypasses in WAF detections and suggest rules to fortify their security.
Using a combination of shadow models and payload generation techniques, we can identify malicious payloads and remove or modify them as needed.WAFBoostergenerates signatures for these malicious payloads using advanced clustering and regular expression matching techniques to repair any security gaps we uncover. In our comprehensive evaluation of eight real-world WAFs,WAFBoosterimproved the true rejection rate of mutated malicious payloads from 21% to 96%, with no false rejections.WAFBoosterachieves a false acceptance rate 3×\times×lower than state-of-the-art methods for generating malicious payloads. WithWAFBooster, we have taken a step forward in securing web applications against the ever-evolving threats."
860,679d459debd8ffd557a2b1c9,cs.CR,https://arxiv.org/pdf/2501.13974,Absolute Governance: A Framework for Synchronization and Certification of the Corporate Contractual State,Antonio Hoffert,"Cryptography and Security, Information Theory",
861,679d459debd8ffd557a2b1ca,cs.CR,https://arxiv.org/pdf/2501.13965,ZKLoRA: Efficient Zero-Knowledge Proofs for LoRA Verification,"Bidhan Roy, Peter Potash, Marcos Villagra","Cryptography and Security, Artificial Intelligence, Machine Learning","Low-Rank Adaptation (LoRA) is a widely adopted method for customizing large-scale language models. In distributed, untrusted training environments, an open source base model user may want to use LoRA weights created by an external contributor, leading to two requirements: (1) the base model user must confirm that the LoRA weights are effective when paired with the intended base model, and (2) the LoRA contributor must keep their proprietary weights private until compensation is assured."
862,679d459debd8ffd557a2b1cb,cs.CR,https://arxiv.org/pdf/2501.13962,Adaptive Cyber-Attack Detection in IIoT Using Attention-Based LSTM-CNN Models,"Afrah Gueriani, Hamza Kheddar, Ahmed Cherif Mazari","Cryptography and Security, Artificial Intelligence, Machine Learning, Systems and Control","The rapid expansion of the industrial Internet
of things (IIoT) has introduced new challenges in securing critical infrastructures against sophisticated cyberthreats. This study presents the development and evaluation of an advanced Intrusion detection (IDS) based on a hybrid LSTM-convolution neural network (CNN)-Attention architecture, specifically designed to detect and classify cyberattacks in IIoT environments. The research
focuses on two key classification tasks: binary and multi-class classification. The proposed models was rigorously tested using the Edge-IIoTset dataset. To mitigate the class imbalance in the dataset, the synthetic minority over-sampling technique (SMOTE) was employed to generate synthetic samples for the underrepresented classes. This ensured that the model could learn effectively from all classes, thereby improving the overall classification performance. Through systematic experimentation, various deep learning (DL) models were compared, ultimately demonstrating that the LSTM-CNN-Attention model consistently outperformed others across key performance metrics. In binary classification, the model achieved near-perfect accuracy, while in multi-class classification, it maintained a high accuracy level (99.04%), effectively categorizing different attack types with a loss value of 0.0220%."
863,679d459debd8ffd557a2b1cc,cs.CR,https://arxiv.org/pdf/2501.14700,An Attentive Graph Agent for Topology-Adaptive Cyber Defence,"Ilya Orson Sandoval, Isaac Symes Thompson, Vasilios Mavroudis, Chris Hicks","Machine Learning, Artificial Intelligence, Cryptography and Security, Networking and Internet Architecture","As cyber threats grow increasingly sophisticated, reinforcement learning (RL) is emerging as a promising technique to create intelligent and adaptive cyber defense systems.
However, most existing autonomous defensive agents have overlooked the inherent graph structure of computer networks subject to cyber attacks, potentially missing critical information and constraining their adaptability.
To overcome these limitations, we developed a custom version of the Cyber Operations Research Gym (CybORG) environment, encoding network state as a directed graph with realistic low-level features.
We employ a Graph Attention Network (GAT) architecture to process node, edge, and global features, and adapt its output to be compatible with policy gradient methods in RL.
Our GAT-based approach offers key advantages over flattened alternatives: robust policies capable of handling dynamic network topology changes, generalisation to networks of varying sizes beyond the training distribution, and interpretable defensive actions grounded in tangible network properties.
We demonstrate the effectiveness of our low-level directed graph observations by training GAT defensive policies that successfully adapt to changing network topologies.
Evaluations across networks of different sizes, but consistent subnetwork structure, show our policies achieve comparable performance to policies trained specifically for each network configuration.
Our study contributes to the development of robust cyber defence systems that can better adapt to real-world network security challenges.111Our code is available inhttps://github.com/IlyaOrson/CyberDreamcatcher."
864,679d459debd8ffd557a2b1cd,cs.CR,https://arxiv.org/pdf/2501.14644,Whisper D-SGD: Correlated Noise Across Agents for Differentially Private Decentralized Learning,"Angelo Rodio, Zheng Chen, Erik G. Larsson","Machine Learning, Artificial Intelligence, Cryptography and Security, Distributed, Parallel, and Cluster Computing","Decentralized learning enables distributed agents to train a shared machine learning model through local computation and peer-to-peer communication.
Although each agent retains its dataset locally, the communication of local models can still expose private information to adversaries.
To mitigate these threats, local differential privacy (LDP) injects independent noise per agent, but it suffers a larger utility gap than central differential privacy (CDP).
We introduceWhisper D-SGD, a novel covariance-based approach that generates correlated privacy noise across agents, unifying several state-of-the-art methods as special cases.
By leveraging network topology and mixing weights,Whisper D-SGDoptimizes the noise covariance to achieve network-wide noise cancellation.
Experimental results show thatWhisper D-SGDcancels more noise than existing pairwise-correlation schemes, substantially narrowing the CDP-LDP gap and improving model performance under the same privacy guarantees."
865,679d459debd8ffd557a2b1ce,cs.CR,https://arxiv.org/pdf/2501.14414,SoK: What Makes Private Learning Unfair?,"Kai Yao, Marc Juarez","Machine Learning, Cryptography and Security","Differential privacy has emerged as the most studied framework for privacy-preserving machine learning. However, recent studies show that enforcing differential privacy guarantees can not only significantly degrade the utility of the model, but also amplify existing disparities in its predictive performance across demographic groups. Although there is extensive research on the identification of factors that contribute to this phenomenon, we still lack a complete understanding of the mechanisms through which differential privacy exacerbates disparities. The literature on this problem is muddled by varying definitions of fairness, differential privacy mechanisms, and inconsistent experimental settings, often leading to seemingly contradictory results."
866,679d459debd8ffd557a2b1cf,cs.CR,https://arxiv.org/pdf/2501.14337,Interactive Oracle Proofs of Proximity to Codes on Graphs,"Hugo Delavenne, Tanguy Medevielle, Élina Roussel","Computational Complexity, Cryptography and Security",
867,679d459debd8ffd557a2b1d0,cs.CR,https://arxiv.org/pdf/2501.14184,Tight Sample Complexity Bounds for Parameter Estimation Under Quantum Differential Privacy for Qubits,Farhad Farokhi,"Quantum Physics, Cryptography and Security, Information Theory","This short note provides tight upper and lower bounds for minimal number of samples (copies of quantum states) required to attain a prescribed accuracy (measured by error variance) for scalar parameters using unbiased estimators under quantum local differential privacy for qubits. In the small privacy budgetϵitalic-ϵ\epsilonitalic_ϵregime, i.e.,ϵ≪1much-less-thanitalic-ϵ1\epsilon\ll 1italic_ϵ ≪ 1, the sample complexity scales asΘ⁢(ϵ−2)Θsuperscriptitalic-ϵ2\Theta(\epsilon^{-2})roman_Θ ( italic_ϵ start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT ). This bound matches that of classical parameter estimation under local differential privacy. The lower bound loosens (converges to zero) in the large privacy budget regime, i.e.,ϵ≫1much-greater-thanitalic-ϵ1\epsilon\gg 1italic_ϵ ≫ 1, but that case is not particularly interesting as tight bounds for parameter estimation in the noiseless case are widely known. That being said, extensions to systems with higher dimensions and tightening the bounds for the large privacy budget regime are interesting avenues for future research."
868,679d459debd8ffd557a2b1d1,cs.CR,https://arxiv.org/pdf/2501.14175,Cybersecurity Assessment of Smart Grid Exposure Using a Machine Learning Based Approach,Mofe O. Jeje,"Machine Learning, Cryptography and Security","Given that disturbances to the stable and normal operation of power systems have grown phenomenally, particularly in terms of unauthorized access to confidential and critical data, injection of malicious software, and exploitation of security vulnerabilities in a poorly patched software among others; then developing, as a countermeasure, an assessment solutions with machine learning capabilities to match up in real-time, with the growth and fast pace of these cyber attacks, is not only critical to the security, reliability and safe operation of power system, but also germane to guaranteeing advanced monitoring and efficient threat detection. Using the Mississippi State University and Oak Ridge National Laboratory dataset, the study used an XGB Classifier modeling approach in machine learning to diagnose and assess power system disturbances, in terms of Attack Events, Natural Events and No-Events. As test results show, the model, in all the three sub-datasets, generally demonstrates good performance on all metrics, as it relates to accurately identifying and classifying all the three power system events."
869,679d459debd8ffd557a2b1d2,cs.CR,https://arxiv.org/pdf/2501.14050,GraphRAG under Fire,"Jiacheng Liang, Yuhui Wang, Changjiang Li, Rongyi Zhu, Tanqiu Jiang, Neil Gong, Ting Wang","Machine Learning, Artificial Intelligence, Cryptography and Security","GraphRAG advances retrieval-augmented generation (RAG) by structuring external knowledge as multi-scale knowledge graphs, enabling language models to integrate both broad context and granular details in their reasoning. While GraphRAG has demonstrated success across domains, its security implications remain largely unexplored. To bridge this gap, this work examines GraphRAG’s vulnerability to poisoning attacks, uncovering an intriguing security paradox: compared to conventional RAG, GraphRAG’s graph-based indexing and retrieval enhance resilience against simple poisoning attacks; meanwhile, the same features also create new attack surfaces. We presentGragPoison, a novel attack that exploits shared relations in the knowledge graph to craft poisoning text capable of compromising multiple queries simultaneously.GragPoisonemploys three key strategies:i) relation injection to introduce false knowledge,ii) relation enhancement to amplify poisoning influence, andiii) narrative generation to embed malicious content within coherent text. Empirical evaluation across diverse datasets and models shows thatGragPoisonsubstantially outperforms existing attacks in terms of effectiveness (up to 98% success rate) and scalability (using less than 68% poisoning text). We also explore potential defensive measures and their limitations, identifying promising directions for future research."
870,679d459debd8ffd557a2b1d3,cs.DC,https://arxiv.org/pdf/2501.18447,Semaphores Augmented with a Waiting Array,"Dave Dice, Alex Kogan","Distributed, Parallel, and Cluster Computing","Semaphores are a widely used and foundational synchronization and coordination construct used for
shared memory multithreaded programming.
They are a keystone concept, in the sense that most other synchronization
constructs can be implemented in terms of semaphores, although the converse does not generally hold.
Semaphores and the quality of their implementation are of consequence as they remain heavily used in the
Linux kernel and are also available for application programming via thepthreadsprogramming
interface."
871,679d459debd8ffd557a2b1d4,cs.DC,https://arxiv.org/pdf/2501.18191,Scalable HPC Job Scheduling and Resource Management in SST,"Abubeker Abdurahman, Abrar Hossain, Kevin A Brown, Kazutomo Yoshii, Kishwar Ahmed","Distributed, Parallel, and Cluster Computing","Efficient job scheduling and resource management contributes towards system throughput and efficiency maximization in high-performance computing (HPC) systems. In this paper, we introduce a scalable job scheduling and resource management component within the structural simulation toolkit (SST), a cycle-accurate and parallel discrete-event simulator.
Our proposed simulator includes state-of-the-art job scheduling algorithms and resource management techniques. Additionally, it introduces a workflow management components that supports the simulation of task dependencies and resource allocations, crucial for workflows typical in scientific computing and data-intensive applications.
We present validation and scalability results of our job scheduling simulator. Simulation shows that our simulator achieves good accuracy in various metrics (e.g., job wait times, number of nodes usage) and also achieves good parallel performance."
872,679d459debd8ffd557a2b1d5,cs.DC,https://arxiv.org/pdf/2501.17998,MirLibSpark: A Scalable NGS Plant MicroRNA Prediction Pipeline for Multi-Library Functional Annotation,"Chao-Jung Wu, Amine M. Remita, Abdoulaye Baniré Diallo","Distributed, Parallel, and Cluster Computing, Genomics","The emergence of the Next Generation Sequencing increases drastically the volume of transcriptomic data.
Although many standalone algorithms and workflows for novel microRNA (miRNA) prediction have been proposed,
few are designed for processing large volume of sequence data from large genomes,
and even fewer further annotate functional miRNAs by analyzing multiple libraries.
We propose an improved pipeline for a high volume data facility by implementing mirLibSpark based on the Apache Spark framework.
This pipeline is the fastest actual method, and provides an accuracy improvement compared to the standard.
In this paper, we deliver the first distributed functional miRNA predictor as a standalone and fully automated package.
It is an efficient and accurate miRNA predictor with functional insight.
Furthermore, it compiles with the gold-standard requirement on plant miRNA predictions."
873,679d459debd8ffd557a2b1d6,cs.DC,https://arxiv.org/pdf/2501.17944,WaterWise: Co-optimizing Carbon- and Water-Footprint Toward Environmentally Sustainable Cloud Computing,"Yankai Jiang, Rohan Basu Roy, Raghavendra Kanakagiri, Devesh Tiwari","Distributed, Parallel, and Cluster Computing","The carbon and water footprint of large-scale computing systems poses serious environmental sustainability risks. In this study, we discover that, unfortunately, carbon and water sustainability are at odds with each other – and, optimizing one alone hurts the other. Toward that goal, we introduce, WaterWise, a novel job scheduler for parallel workloads that intelligently co-optimizes carbon and water footprint to improve the sustainability of geographically distributed data centers."
874,679d459debd8ffd557a2b1d7,cs.DC,https://arxiv.org/pdf/2501.18309,"Knowledge in multi-robot systems: an interplay of dynamics, computation and communication","Giorgio Cignarale, Stephan Felber, Eric Goubault, Bernardo Hummes Flores, Hugo Rincon Galeana","Logic in Computer Science, Distributed, Parallel, and Cluster Computing, Robotics","We show that the hybrid systems perspective of distributed multi-robot systems is compatible with logical models of knowledge already used in distributed computing, and demonstrate its usefulness by deriving sufficient epistemic conditions for exploration and gathering robot tasks to be solvable. We provide a separation of the physical and computational aspects of a robotic system, allowing us to decouple the problems related to each and directly use methods from control theory and distributed computing, fields that are traditionally distant in the literature. Finally, we demonstrate a novel approach for reasoning about the knowledge in multi-robot systems through a principled method of converting a switched hybrid dynamical system into a temporal-epistemic logic model, passing through an abstract state machine representation.
This creates space for methods and results to be exchanged across the fields of control theory, distributed computing and temporal-epistemic logic, while reasoning about multi-robot systems."
875,679d459debd8ffd557a2b1d8,cs.DC,https://arxiv.org/pdf/2501.18298,Update Estimation and Scheduling for Over-the-Air Federated Learning with Energy Harvesting Devices,"Furkan Bagci, Busra Tegin, Mohammad Kazemi, Tolga M. Duman","Machine Learning, Distributed, Parallel, and Cluster Computing","We study over-the-air (OTA) federated learning (FL) for energy harvesting devices with heterogeneous data distribution over wireless fading multiple access channel (MAC). To address the impact of low energy arrivals and data heterogeneity on global learning, we propose user scheduling strategies. Specifically, we develop two approaches: 1) entropy-based scheduling for known data distributions and 2) least-squares-based user representation estimation for scheduling with unknown data distributions at the parameter server. Both methods aim to select diverse users, mitigating bias and enhancing convergence. Numerical and analytical results demonstrate improved learning performance by reducing redundancy and conserving energy."
876,679d459debd8ffd557a2b1d9,cs.DC,https://arxiv.org/pdf/2501.17796,"An Incremental Multi-Level, Multi-Scale Approach to Assessment of Multifidelity HPC Systems","Shilpika Shilpika, Bethany Lusch, Venkatram Vishwanath, Michael E. Papka","Distributed, Parallel, and Cluster Computing","With the growing complexity in architecture and the size of large-scale computing systems, monitoring and analyzing system behavior and events has become daunting. Monitoring data amounting to terabytes per day are collected by sensors housed in these massive systems at multiple fidelity levels and varying temporal resolutions. In this work, we develop an incremental version of multiresolution dynamic mode decomposition (mrDMD), which converts high-dimensional data to spatial-temporal patterns at varied frequency ranges. Our incremental implementation of the mrDMD algorithm (I-mrDMD) promptly reveals valuable information in the massive environment log dataset, which is then visually aligned with the processed hardware and job log datasets through our generalizable rack visualization using D3 visualization integrated into the Jupyter Notebook interface. We demonstrate the efficacy of our approach with two use scenarios on a real-world dataset from a Cray XC40 supercomputer, Theta."
877,679d459debd8ffd557a2b1da,cs.DC,https://arxiv.org/pdf/2501.17752,On the Partitioning of GPU Power among Multi-Instances,"Tirth Vamja, Kaustabha Ray, Felix George, UmaMaheswari C Devi","Distributed, Parallel, and Cluster Computing, Performance","Efficient power management in cloud data centers is essential for reducing costs, enhancing performance, and minimizing environmental impact. GPUs, critical for tasks like machine learning (ML) and GenAI, are major contributors to power consumption. NVIDIA’s Multi-Instance GPU (MIG) technology improves GPU utilization by enabling isolated partitions with per-partition resource tracking, facilitating GPU sharing by multiple tenants. However, accurately apportioning GPU power consumption among MIG instances remains challenging due to a lack of hardware support.
This paper addresses this challenge by developing software methods to estimate power usage per MIG partition. We analyze NVIDIA GPU utilization metrics and find that light-weight methods with good accuracy can be difficult to construct. We hence explore the use of ML-based power models to enable accurate, partition-level power estimation. Our findings reveal that a single generic offline power model or modeling method is not applicable across diverse workloads, especially with concurrent MIG usage, and that online models constructed using partition-level utilization metrics of workloads under execution can significantly improve accuracy.
Using NVIDIA A100 GPUs, we demonstrate this approach for
accurate partition-level power estimation for workloads including matrix multiplication and Large Language Model inference, contributing to transparent and fair carbon reporting."
878,679d459debd8ffd557a2b1db,cs.DC,https://arxiv.org/pdf/2501.17732,Gateways for Institutional-Grade Commerce and Interoperability of Digital Assets,"Rafael Belchior, Thomas Hardjono, Alex Chiriac, Venkatraman Ranakrishna","Distributed, Parallel, and Cluster Computing",
879,679d459debd8ffd557a2b1dc,cs.DC,https://arxiv.org/pdf/2501.17610,FeedSign: Robust Full-parameter Federated Fine-tuning of Large Models with Extremely Low Communication Overhead of One Bit,"Zhijie Cai, Haolong Chen, Guangxu Zhu","Distributed, Parallel, and Cluster Computing","Federated fine-tuning (FFT) attempts to fine-tune a pre-trained model with private data from distributed clients by exchanging models rather than data under the orchestration of a parameter server (PS). To overcome the bottleneck forged by the growing communication and memory overhead for clients in such systems due to the growing model sizes, we proposeFeedSign, an FFT algorithm in which the upload and download payload for an aggregation step is exactly1111bit per step, while the memory overhead is squeezed to the amount needed for inference. This is realized by utilizing zeroth-order (ZO) optimizers on large models and shared pseudo-random number generators (PRNG) across devices to represent the gradient estimates as seed-sign pairs. We conduct theoretical analysis on FeedSign and show that it converges at an exponential rate𝒪⁢(e−t)𝒪superscript𝑒𝑡\mathcal{O}(e^{-t})caligraphic_O ( italic_e start_POSTSUPERSCRIPT - italic_t end_POSTSUPERSCRIPT ), wheret𝑡titalic_tis the number of elapsed steps under widely used assumptions. Moreover, FeedSign is found to be robust against data heterogeneity and Byzantine attacks. We conducted extensive experiments on models across different structures and sizes (11M to 13B) and found that the proposed method performs better or closely, depending on scenarios, compared to its ZO and FO counterparts, albeit with an orders-of-magnitude lower communication overhead. We also discuss some interesting advantages as byproducts guaranteed by the minimalistic design ofFeedSign."
880,679d459debd8ffd557a2b1dd,cs.DC,https://arxiv.org/pdf/2501.17466,Proteus: Achieving High-Performance Processing-Using-DRAM via Dynamic Precision Bit-Serial Arithmetic,"Geraldo F. Oliveira, Mayank Kabra, Yuxin Guo, Kangqi Chen, A. Giray Yağlıkçı, Melina Soysal, Mohammad Sadrosadati, Joaquin Olivares Bueno, Saugata Ghose, Juan Gómez-Luna, Onur Mutlu","Hardware Architecture, Distributed, Parallel, and Cluster Computing","Processing-using-DRAM (PUD)is a paradigm where the analog operational properties of DRAM structures are used to perform bulk logic operations.
WhilePUDpromises high throughput at low energy and area cost,
we uncover three limitations of existingPUDapproaches that lead to significant inefficiencies:
(i) static data representation, i.e., 2’s complement with fixed bit-precision, leading tounnecessary computationover useless(i.e., inconsequential)data;
(ii) support foronlythroughput-oriented execution, where the high latency of individualPUDoperations canonlybe hidden in the presence of bulk data-level parallelism; and
(iii) high latency for high-precision (e.g., 32-bit) operations.
To address these issues, we proposeProteus,
which builds on twokey ideas.
First,Proteusparallelizesthe execution of independent primitives in aPUDoperation by leveraging DRAM’s internal parallelism.
Second,Proteusreducesthe bit-precision forPUDoperations by leveragingnarrow values(i.e., values with many leading zeros).

We compareProteusto different state-of-the-art computing platforms (CPU, GPU, and the SIMDRAMPUDarchitecture) for twelve real-world applications.
Using a single DRAM bank,Proteusprovides
(i) 17×\times×, 7.3×\times×, and 10.2×\times×the performance per mm2; and
(ii) 90.3×\times×, 21×\times×, and 8.1×\times×lower energy consumption than that of the CPU, GPU, and SIMDRAM, respectively, on average across twelve real-world applications.Proteusincurs low area cost on top of a DRAM chip (1.6%) and CPU die (0.03%)."
881,679d459debd8ffd557a2b1de,cs.DC,https://arxiv.org/pdf/2501.17416,Are you a DePIN? A Decision Tree to Classify Decentralized Physical Infrastructure Networks,"Michael S. Andrew, Mark C. Ballandies","Emerging Technologies, Computers and Society, Distributed, Parallel, and Cluster Computing","Decentralized physical infrastructure networks (DePINs) are an emerging vertical within ”Web3” replacing the traditional method that physical infrastructures are constructed. Yet, the boundaries between DePIN and traditional method of building crowd-sourced infrastructures such as citizen science initiatives or other Web3 verticals are not always so clear cut. In this work, we systematically analyze the differences between DePIN and other Web2 and Web3 verticals. For this, the study proposes a novel decision tree for classifying systems as DePIN. This tree is informed by prior studies and differentiates DePIN from related concepts using criteria such as the presence of a three-sided market, token-based incentives for supply, and the requirement for physical asset placement in those systems."
882,679d459debd8ffd557a2b1df,cs.DC,https://arxiv.org/pdf/2501.17275,Dual-Lagrange Encoding for Storage and Download in Elastic Computing for Resilience,"Xi Zhong, Samuel Lu, Joerg Kliewer, Mingyue Ji","Information Theory, Distributed, Parallel, and Cluster Computing","Coded elastic computing enables virtual machines to be preempted for high-priority tasks while allowing new virtual machines to join ongoing computation seamlessly. This paper addresses coded elastic computing for matrix-matrix multiplications with straggler tolerance by encoding both storage and download using Lagrange codes.
In 2018, Yanget al.introduced the first coded elastic computing scheme for matrix-matrix multiplications, achieving a lower computational load requirement. However, this scheme lacks straggler tolerance and suffers from high upload cost.
Zhonget al.(2023) later tackled these shortcomings by employing uncoded storage and Lagrange-coded download. However, their approach requires each machine to store the entire dataset.
This paper introduces a new class of elastic computing schemes that utilize Lagrange codes to encode both storage and download, achieving a reduced storage size.
The proposed schemes efficiently mitigate both elasticity and straggler effects, with a storage size reduced to a fraction1L1𝐿\frac{1}{L}divide start_ARG 1 end_ARG start_ARG italic_L end_ARGof Zhonget al.’s approach, at the expense of doubling the download cost.
Moreover, we evaluate the proposed schemes on AWS EC2 by measuring computation time under two different tasks allocations: heterogeneous and cyclic assignments. Both assignments minimize computation redundancy of the system while distributing varying computation loads across machines."
883,679d459debd8ffd557a2b1e0,cs.DC,https://arxiv.org/pdf/2501.16909,Measuring GPU utilization one level deeper,"Paul Elvinger, Foteini Strati, Natalie Enright Jerger, Ana Klimovic","Distributed, Parallel, and Cluster Computing","GPU hardware is vastly underutilized. Even resource-intensive AI applications have diverse resource profiles that often leave parts of GPUs idle. While colocating applications can improve utilization, current spatial sharing systems lack performance guarantees. Providing predictable performance guarantees requires a deep understanding of how applications contend for shared GPU resources such as block schedulers, compute units, L1/L2 caches, and memory bandwidth.
We propose a methodology to profile resource interference of GPU kernels across these dimensions and discuss how to build GPU schedulers that provide strict performance guarantees while colocating applications to minimize cost."
884,679d459debd8ffd557a2b1e1,cs.DC,https://arxiv.org/pdf/2501.16892,On the Shape Containment Problem within the Amoebot Model with Reconfigurable Circuits,"Matthias Artmann, Andreas Padalkin, Christian Scheideler","Distributed, Parallel, and Cluster Computing","Inprogrammable matter, we consider a large number of tiny, primitive computational entities calledparticlesthat run distributed algorithms to control global properties of the particle structure.Shape formationproblems, where the particles have to reorganize themselves into a desired shape using basic movement abilities, are particularly interesting.
In the relatedshape containmentproblem, the particles are given the description of a shapeS𝑆Sitalic_Sand have to find maximally scaled representations ofS𝑆Sitalic_Swithin the initial configuration, without movements.
While the shape formation problem is being studied extensively, no attention has been given to the shape containment problem, which may have additional uses beside shape formation, such as detection of structural flaws."
885,679d459debd8ffd557a2b1e2,cs.DC,https://arxiv.org/pdf/2501.16634,Towards Resource-Efficient Compound AI Systems,"Gohar Irfan Chaudhry, Esha Choukse, Íñigo Goiri, Rodrigo Fonseca, Adam Belay, Ricardo Bianchini","Distributed, Parallel, and Cluster Computing, Artificial Intelligence","Compound AI Systems, integrating multiple interacting components like models, retrievers, and external tools, have emerged as essential for addressing complex AI tasks.
However, current implementations suffer from inefficient resource utilization due to tight coupling between application logic and execution details, a disconnect between orchestration and resource management layers, and the perceived exclusiveness between efficiency and quality."
886,679d459debd8ffd557a2b1e3,cs.DC,https://arxiv.org/pdf/2501.16943,Programming tools for Analogue Quantum Computing in the High-Performance Computing Context -- A Review,"Mateusz Meller, Vendel Szeremi, Oliver Thomson Brown","Quantum Physics, Distributed, Parallel, and Cluster Computing","Recent advances in quantum computing have brought us closer to realizing the potential of this transformative technology. While significant strides have been made in quantum error correction, many challenges persist, particularly in the realm of noise and scalability. Analogue quantum computing schemes, such as Analogue Hamiltonian Simulation and Quantum Annealing, offer a promising approach to address these limitations. By operating at a higher level of abstraction, these schemes can simplify the development of large-scale quantum algorithms.
To fully harness the power of quantum computers, they must be seamlessly integrated with traditional high-performance computing (HPC) systems. While substantial research has focused on the integration of circuit-based quantum computers with HPC, the integration of analogue quantum computers remains relatively unexplored. This paper aims to bridge this gap by contributing in the following way:"
887,679d459debd8ffd557a2b1e4,cs.DC,https://arxiv.org/pdf/2501.16758,Meta-Federated Learning: A Novel Approach for Real-Time Traffic Flow Management,"Bob Johnson, Michael Geller","Machine Learning, Distributed, Parallel, and Cluster Computing, Signal Processing","Efficient management of traffic flow in urban environments presents a significant challenge, exacerbated by dynamic changes and the sheer volume of data generated by modern transportation networks. Traditional centralized traffic management systems often struggle with scalability and privacy concerns, hindering their effectiveness. This paper introduces a novel approach by combining Federated Learning (FL) and Meta-Learning (ML) to create a decentralized, scalable, and adaptive traffic management system. Our approach, termed Meta-Federated Learning, leverages the distributed nature of FL to process data locally at the edge, thereby enhancing privacy and reducing latency. Simultaneously, ML enables the system to quickly adapt to new traffic conditions without the need for extensive retraining. We implement our model across a simulated network of smart traffic devices, demonstrating that Meta-Federated Learning significantly outperforms traditional models in terms of prediction accuracy and response time. Furthermore, our approach shows remarkable adaptability to sudden changes in traffic patterns, suggesting a scalable solution for real-time traffic management in smart cities. This study not only paves the way for more resilient urban traffic systems but also exemplifies the potential of integrated FL and ML in other real-world applications."
888,679d459debd8ffd557a2b1e5,cs.DC,https://arxiv.org/pdf/2501.16519,Optimizing Decentralized Online Learning for Supervised Regression and Classification Problems,"J. M. Diederik Kruijssen, Renata Valieva, Steven N. Longmore","Machine Learning, Distributed, Parallel, and Cluster Computing, Multiagent Systems","Decentralized learning networks aim to synthesize a single network inference from a set of raw inferences provided by multiple participants. To determine the combined inference, these networks must adopt a mapping from historical participant performance to weights, and to appropriately incentivize contributions they must adopt a mapping from performance to fair rewards. Despite the increased prevalence of decentralized learning networks, there exists no systematic study that performs a calibration of the associated free parameters. Here we present an optimization framework for key parameters governing decentralized online learning in supervised regression and classification problems. These parameters include the slope of the mapping between historical performance and participant weight, the timeframe for performance evaluation, and the slope of the mapping between performance and rewards. These parameters are optimized using a suite of numerical experiments that mimic the design of the Allora Network, but have been extended to handle classification tasks in addition to regression tasks. This setup enables a comparative analysis of parameter tuning and network performance optimization (loss minimization) across both problem types. We demonstrate how the optimal performance-weight mapping, performance timeframe, and performance-reward mapping vary with network composition and problem type. Our findings provide valuable insights for the optimization of decentralized learning protocols, and we discuss how these results can be generalized to optimize any inference synthesis-based, decentralized AI network."
889,679d459debd8ffd557a2b1e6,cs.DC,https://arxiv.org/pdf/2501.16245,SP-IMPact: A Framework for Static Partitioning Interference Mitigation and Performance Analysis,"Diogo Costa, Gonçalo Moreira, Afonso Oliveira, José Martins, Sandro Pinto","Distributed, Parallel, and Cluster Computing, Performance, Systems and Control","Modern embedded systems are evolving toward complex, heterogeneous architectures to accommodate increasingly demanding applications. Driven by industry SWAP-C (Size, Weight, Power, and Cost) constraints, this shift has led to the consolidation of multiple systems onto single hardware platforms. Static Partitioning Hypervisors (SPHs) offer a promising solution to partition hardware resources and provide spatial isolation between critical workloads. However, shared hardware resources like the Last-Level Cache (LLC) and system bus can introduce significant temporal interference between virtual machines (VMs), negatively impacting performance and predictability.
Over the past decade, academia and industry have focused on developing interference mitigation techniques, such as cache partitioning and memory bandwidth reservation. Configuring these techniques, however, is complex and time-consuming. Cache partitioning requires careful balancing of cache sections across VMs, while memory bandwidth reservation requires tuning bandwidth budgets and periods. With numerous possible configurations, testing all combinations is impractical and often leads to suboptimal configurations. Moreover, there is a gap in understanding how these techniques interact, as their combined use can result in compounded or conflicting effects on system performance.
Static analysis solutions that estimate worst-case execution times (WCET) and upper bounds on execution times provide some guidance for configuring interference mitigation techniques. While useful in identifying potential interference effects, these tools often fail to capture the full complexity of modern multi-core systems, as they typically focus on a limited set of shared resources and neglect other sources of contention, such as IOMMUs and interrupt controllers.
To address these challenges, we introduce SP-IMPact, an open-source framework designed to analyze and guide the configuration of interference mitigation techniques, through the deployment of diverse VM configurations and setups, and assessment of hardware-level contention (leveraging SPHs).
It supports two mitigation techniques: (i) cache coloring and (ii) memory bandwidth reservation, while also evaluating the interactions between these techniques and their cumulative impact on system performance. By providing insights on real hardware platforms, SP-IMPact helps to optimize the configuration of these techniques in mixed-criticality systems, ensuring both performance and predictability."
890,679d459debd8ffd557a2b1e7,cs.DC,https://arxiv.org/pdf/2501.16205,EPOCH: Enabling Preemption Operation for Context Saving in Heterogeneous FPGA Systems,"Arsalan Ali Malik, Emre Karabulut, Aydin Aysu","Distributed, Parallel, and Cluster Computing, Hardware Architecture","FPGAs are becoming increasingly prevalent in multi-tenant cloud environments. In such an environment, FPGAs are often employed to offload compute-intensive tasks from the main CPU. The operating system (OS) plays a vital role in identifying tasks suitable for offloading and managing the coordination between the CPU and FPGA for seamless task execution. While the OS has long supported preemption as a mechanism for task pause and resume to enhance efficiency and balance CPU time, preempting tasks running on FPGA without losing context poses a significant challenge. Despite the increasing dependence on FPGAs in these dynamic computing environments, FPGA vendors have yet to provide a comprehensive out-of-the-box solution for preemption that considers the full context of tasks.In this paper, we present EPOCH, the firstvendor-independentcomprehensive framework designed to seamlessly preserve the state of tasks running on multi-tenant cloud FPGAs. EPOCH enables interrupting a tenant’s execution at any arbitrary clock cycle, captures its state, and saves this ‘state snapshot’ in off-chip memory with fine-grain granularity. Subsequently, when task resumption is required, EPOCH can resume execution from the saved ‘state snapshot’, eliminating the need to restart the task from scratch. EPOCH automates intricate processes, shields users from complexities, and synchronizes all underlying logic in a common clock domain, mitigating timing violations and ensuring seamless handling of interruptions.We thoroughly verified EPOCH using diverse benchmarks, demonstrating the significance of the proposed work in a real-world scenario with minimal area overhead. EPOCH proficiently captures the state of fundamental FPGA elements, such as look-up tables, flip-flops, block–RAMs, and digital signal processing units. On ZynQ-XC7777Z020020020020SoC, the proposed solution achieves context save and restore operations per frame in62.262.262.262.2and67.467.467.467.4µs, respectively."
891,679d459debd8ffd557a2b1e8,cs.DC,https://arxiv.org/pdf/2501.16103,Static Batching of Irregular Workloads on GPUs: Framework and Application to Efficient MoE Model Inference,"Yinghan Li, Yifei Li, Jiejing Zhang, Bujiao Chen, Xiaotong Chen, Lian Duan, Yejun Jin, Zheng Li, Xuanyu Liu, Haoyu Wang, Wente Wang, Yajie Wang, Jiacheng Yang, Peiyang Zhang, Laiwen Zheng, Wenyuan Yu","Distributed, Parallel, and Cluster Computing, Machine Learning","It has long been a problem to arrange and execute irregular workloads on massively parallel devices.
We propose a general framework for statically batching irregular workloads into a single kernel with a runtime task mapping mechanism on GPUs.
We further apply this framework to Mixture-of-Experts (MoE) model inference and implement an optimized and efficient CUDA kernel.
Our MoE kernel achieves up to 91% of the peak Tensor Core throughput on NVIDIA H800 GPU and 95% on NVIDIA H20 GPU."
892,679d459debd8ffd557a2b1e9,cs.DC,https://arxiv.org/pdf/2501.15962,Share a Tiny Space of Your Freezer to Preserve Seed Diversity,Andrea Vitaletti,"Distributed, Parallel, and Cluster Computing, Computers and Society","The Food and Agriculture Organization (FAO), estimates that 75% of crop diversity was lost since the 1900s. That lack of diversity presents a severe risk to the security of global food systems. Without seed diversity, it is difficult for plants to adapt to pests, diseases, and changing climate conditions. Genebanks, such as the Svalbard Global Seed Vault, are valuable initiatives to preserve seed diversity in a single secure and safe place. However, according to our analysis of the data available in the Seed Portal, the redundancy for some species might be limited, posing a potential threat to their future availability. Interestingly, the conditions to properly store seeds in genebanks, are the ones available in the freezers of our homes. This paper lays out a vision for Distributed Seed Storage relying on a peer-to-peer infrastructure of domestic freezers to increase the overall availability of seeds. We present a Proof-of-Concept focused on monitoring the proper seed storing conditions and incentive user participation through a Blockchain lottery. The PoC proves the feasibility of the proposed approach and outlines the main technical issues that still need to be efficiently solved to realize a fully-fledged solution."
893,679d459debd8ffd557a2b1ea,cs.DC,https://arxiv.org/pdf/2501.15904,Snowman for partial synchrony,"Aaron Buchwald, Stephen Buttolph, Andrew Lewis-Pye, Kevin Sekniqi","Distributed, Parallel, and Cluster Computing","Snowman is the consensus protocol run by blockchains on Avalanche. A recent paper(buchwald2024frosty,)established a rigorous proof of probabilistic consistency for Snowman in thesynchronoussetting, under the simplifying assumption that correct processes execute sampling rounds in ‘lockstep’. In this paper, we describe a modification of the protocol that ensures consistency in thepartially synchronoussetting, and when correct processes carry out successive sampling rounds at their own speed, with the time between sampling rounds determined by local message delays."
894,679d459debd8ffd557a2b1eb,cs.DC,https://arxiv.org/pdf/2501.15829,Aging-aware CPU Core Management for Embodied Carbon Amortization in Cloud LLM Inference,"Tharindu B. Hewage, Shashikant Ilager, Maria Rodriguez Read, Rajkumar Buyya","Distributed, Parallel, and Cluster Computing","Broad adoption of Large Language Models (LLM) demands rapid expansions of cloud LLM inference clusters, leading to accumulation of embodied carbon−--the emissions from manufacturing and supplying IT assets−--that mostly concentrate on inference server CPU. This paper delves into the challenges of sustainable growth of cloud LLM inference, emphasizing extended amortization of CPU embodied over an increased lifespan. Given the reliability risks of silicon aging, we propose an aging-aware CPU core management technique to delay CPU aging effects, allowing the cluster operator to safely increase CPU life. Our technique exploits CPU underutilization patterns that we uncover in cloud LLM inference by halting aging in unused cores and even-outing aging in active cores via selective deep idling and aging-aware inference task allocation. Through extensive simulations using real-world Azure inference traces and an extended LLM cluster simulator from Microsoft, we show superior performance of our technique over existing methods with an estimated 37.67% reduction in yearly embodied carbon emissions through p99 performance of managing CPU aging effects, a 77% reduction in CPU underutilization, and less than 10% impact to the inference service quality."
895,679d459debd8ffd557a2b1ec,cs.DC,https://arxiv.org/pdf/2501.15802,Adaptive AI-based Decentralized Resource Management in the Cloud-Edge Continuum,"Lanpei Li, Jack Bell, Massimo Coppola, Vincenzo Lomonaco","Distributed, Parallel, and Cluster Computing, Artificial Intelligence","The increasing complexity of application requirements and the dynamic nature of the Cloud-Edge Continuum present significant challenges for efficient resource management. These challenges stem from the ever-changing infrastructure—characterized by additions, removals, and reconfigurations of nodes and links—and the variability of application workloads. Traditional centralized approaches struggle to adapt to these changes due to their static nature, while decentralized solutions face challenges such as limited global visibility and coordination overhead. This paper proposes a hybrid decentralized framework for dynamic application placement and resource management. The framework utilizes Graph Neural Networks (GNNs) to embed resource and application states, enabling comprehensive representation and efficient decision-making. It employs a collaborative multi-agent reinforcement learning (MARL) approach, where local agents optimize resource management in their neighborhoods and a global orchestrator ensures system-wide coordination. By combining decentralized application placement with centralized oversight, our framework addresses the scalability, adaptability, and accuracy challenges inherent in the Cloud-Edge Continuum. This work contributes to the development of decentralized application placement strategies, the integration of GNN embeddings, and collaborative MARL systems, providing a foundation for efficient, adaptive and scalable resource management."
896,679d459debd8ffd557a2b1ed,cs.DC,https://arxiv.org/pdf/2501.15504,Task Scheduling in Geo-Distributed Computing: A Survey,"Yujian Wu, Shanjiang Tang, Ce Yu, Bin Yang, Chao Sun, Jian Xiao, Hutong Wu","Distributed, Parallel, and Cluster Computing","Geo-distributed computing, a paradigm that assigns computational tasks to globally distributed nodes, has emerged as a promising approach in cloud computing, edge computing, cloud-edge computing and supercomputer computing (HPC). It enables low-latency services, ensures data locality, and handles large-scale applications. As global computing capacity and task demands increase rapidly, scheduling tasks for efficient execution in geo-distributed computing systems has become an increasingly critical research challenge. It arises from the inherent characteristics of geographic distribution, including heterogeneous network conditions, region-specific resource pricing, and varying computational capabilities across locations. Researchers have developed diverse task scheduling methods tailored to geo-distributed scenarios, aiming to achieve objectives such as performance enhancement, fairness assurance, and fault-tolerance improvement. This survey provides a comprehensive and systematic review of task scheduling techniques across four major distributed computing environments, with an in-depth analysis of these approaches based on their core scheduling objectives. Through our analysis, we identify key research challenges and outline promising directions for advancing task scheduling in geo-distributed computing."
897,679d459debd8ffd557a2b1ee,cs.DC,https://arxiv.org/pdf/2501.15289,ExClique: An Express Consensus Algorithm for High-Speed Transaction Process in Blockchains,"Chonghe Zhao, Yipeng Zhou, Shengli Zhang, Quan Z. Sheng, Yang Zhang, Shiting Wen","Distributed, Parallel, and Cluster Computing","Proof of Authority (PoA) plays a pivotal role in blockchains for reaching consensus. Clique, which selects consensus nodes to generate blocks with a pre-determined order, is the most popular implementation of PoA
due to its low communication overhead and energy consumption.
However, our study unveils that the speed to process transactions by Clique is severely restricted by 1) the long communication delay of full blocks (each containing a certain number of transactions) between consensus nodes; and 2) occurrences of no-turn blocks, generated by no-turn nodes if an in-turn block generation fails. Consequently, Clique struggles to support distributed applications requiring a high transaction processing speed,e.g., online gaming. To overcome this deficiency, we propose an Express Clique (ExClique) algorithm by improving Clique from two perspectives: compacting blocks for broadcasting to shorten communication delay and prohibiting the occurrences of no-turn blocks. For performance evaluation, we implement ExClique by modifying Geth of Ethereum, the software implementing Clique, and deploy a permissioned blockchain network by using container technology. The experimental results show that ExClique achieves a substantial enhancement in transactions per second (TPS). Specifically, it boosts TPS by 2.25×\times×in a typical network with 21 consensus nodes and an impressive 7.01×\times×in a large-scale network with 101 consensus nodes when compared to Clique."
898,679d459debd8ffd557a2b1ef,cs.DC,https://arxiv.org/pdf/2501.15126,Fully-Automated Code Generation for Efficient Computation of Sparse Matrix Permanents on GPUs,"Deniz Elbek, Kamer Kaya","Distributed, Parallel, and Cluster Computing, Discrete Mathematics, Numerical Analysis","Registers are the fastest memory components within the GPU’s complex memory hierarchy, accessed by names rather than addresses. They are managed entirely by the compiler through a process calledregister allocation, during which the compiler attempts to cache predictable data from thread-local memory into thread-private registers. Computing the permanent of a sparse matrix poses a challenge for compilers, as optimizing this process is hindered by the unpredictable distribution of nonzero elements, which only become known at runtime. In this work, we employfully-automated code generationto address this, producing highly optimized kernels tailored to the matrix’s sparsity pattern. State-of-the-art permanent computation algorithms require each thread to store a private array, denoted𝐱𝐱{\bf x}bold_x, of sizen𝑛nitalic_n. We first propose a technique that fully stores these arrays in registers, withinclusionandexclusionkernels generated for each column. Tominimize control divergenceand reduce the number of unique kernels within a warp, we exploit the internal structure of Gray codes, which are also used in the state-of-the-art algorithm. Our second technique reduces register pressure by utilizing both registers and global memory and introduces a matrix ordering and partitioning strategy for greater efficiency. On synthetic matrices, this approach achieves a31×31\times31 ×speedup over state-of-the-art CPU implementations on 112 cores, and an8×8\times8 ×speedup compared to our traditional GPU implementation. For real-world matrices, these speedups are24.9×24.9\times24.9 ×and4.9×4.9\times4.9 ×."
899,679d459debd8ffd557a2b1f0,cs.DC,https://arxiv.org/pdf/2501.14931,"Pod: An Optimal-Latency, Censorship-Free, and Accountable Generalized Consensus Layer","Orestis Alpos, Bernardo David, Dionysis Zindros","Distributed, Parallel, and Cluster Computing",
900,679d459debd8ffd557a2b1f1,cs.DC,https://arxiv.org/pdf/2501.14832,Resource Allocation Driven by Large Models in Future Semantic-Aware Networks,"Haijun Zhang, Jiaxin Ni, Zijun Wu, Xiangnan Liu, V. C. M. Leung","Distributed, Parallel, and Cluster Computing, Networking and Internet Architecture","Large model has emerged as a key enabler for the popularity of future networked intelligent applications. However, the surge of data traffic brought by intelligent applications puts pressure on the resource utilization and energy consumption of the future networks. With efficient content understanding capabilities, semantic communication holds significant potential for reducing data transmission in intelligent applications. In this article, resource allocation driven by large models in semantic-aware networks is investigated. Specifically, a semantic-aware communication network architecture based on scene graph models and multimodal pre-trained models is designed to achieve efficient data transmission. On the basis of the proposed network architecture, an intelligent resource allocation scheme in semantic-aware network is proposed to further enhance resource utilization efficiency. In the resource allocation scheme, the semantic transmission quality is adopted as an evaluation metric and the impact of wireless channel fading on semantic transmission is analyzed. To maximize the semantic transmission quality for multiple users, a diffusion model-based decision-making scheme is designed to address the power allocation problem in semantic-aware networks. Simulation results demonstrate that the proposed large-model-driven network architecture and resource allocation scheme achieve high-quality semantic transmission."
901,679d459debd8ffd557a2b1f2,cs.DC,https://arxiv.org/pdf/2501.14823,Quantifying Energy and Cost Benefits of Hybrid Edge Cloud: Analysis of Traditional and Agentic Workloads,Siavash Alamouti,"Distributed, Parallel, and Cluster Computing, Artificial Intelligence","This paper examines the workload distribution challenges in centralized cloud systems and demonstrates how Hybrid Edge Cloud (HEC)[1]mitigates these inefficiencies. Workloads in cloud environments often follow a Pareto distribution, where a small percentage of tasks consume most resources, leading to bottlenecks and energy inefficiencies. By analyzing both traditional workloads reflective of typical IoT and smart device usage and agentic workloads, such as those generated by AI agents, robotics, and autonomous systems, this study quantifies the energy and cost savings enabled by HEC. Our findings reveal that HEC achieves energy savings of up to75%and cost reductions exceeding80%, even in resource-intensive agentic scenarios. These results highlight the critical role of HEC in enabling scalable, cost-effective, and sustainable computing for the next generation of intelligent systems."
902,679d459debd8ffd557a2b1f3,cs.DC,https://arxiv.org/pdf/2501.14815,A VM-HDL Co-Simulation Framework for Systems with PCIe-Connected FPGAs,"Shenghsun Cho, Mrunal Patel, Basavaraj Kaladagi, Han Chen, Tapti Palit, Michael Ferdman, Peter Milder","Distributed, Parallel, and Cluster Computing, Artificial Intelligence, Hardware Architecture, Networking and Internet Architecture",
903,679d459debd8ffd557a2b1f4,cs.DC,https://arxiv.org/pdf/2501.14812,Implementation and Evaluation of GBDI Memory Compression Algorithm Using C/C++ on a Broader Range of Workloads,Adeyemi Aina,"Distributed, Parallel, and Cluster Computing, Hardware Architecture",
904,679d459debd8ffd557a2b1f5,cs.DC,https://arxiv.org/pdf/2501.14808,HyGen: Efficient LLM Serving via Elastic Online-Offline Request Co-location,"Ting Sun, Penghan Wang, Fan Lai","Distributed, Parallel, and Cluster Computing, Machine Learning","Recent advancements in large language models (LLMs) have facilitated a wide range of applications with distinct quality-of-experience requirements, from latency-sensitive online tasks, such as interactive chatbots, to throughput-focused offline tasks like document summarization. While deploying dedicated machines for these services ensures high-quality performance, it often results in resource underutilization."
905,679d459debd8ffd557a2b1f6,cs.DC,https://arxiv.org/pdf/2501.14807,An efficient GPU approach for designing 3D cultural heritage information systems,"Luis López, Juan Carlos Torres, Germán Arroyo, Pedro Cano, Domingo Martín","Distributed, Parallel, and Cluster Computing, Graphics","We propose a new architecture for 3D information systems that takes advantage of the inherent parallelism of the GPUs. This new solution structures information as thematic layers, allowing a level of detail independent of the resolution of the meshes. Previous proposals of layer based systems present issues, both in terms of performance and storage, due to the use of octrees to index information. In contrast, our approach employs two-dimensional textures, highly efficient in GPU, to store and index information. In this article we describe this architecture and detail the GPU algorithms required to edit these layers. Finally, we present a performance comparison of our approach against an octree based system."
906,679d459debd8ffd557a2b1f7,cs.DC,https://arxiv.org/pdf/2501.14802,DNN-Powered MLOps Pipeline Optimization for Large Language Models: A Framework for Automated Deployment and Resource Management,"Mahesh Vaijainthymala Krishnamoorthy, Kuppusamy Vellamadam Palavesam, Siva Venkatesh Arcot, Rajarajeswari Chinniah Kuppuswami","Distributed, Parallel, and Cluster Computing, Machine Learning",
907,679d459debd8ffd557a2b1f8,cs.DC,https://arxiv.org/pdf/2501.14795,Accelerating Particle-Mesh Algorithms with FPGAs and OmpSs@OpenCL,Nicolas Lee Guidotti,"Distributed, Parallel, and Cluster Computing",
908,679d459debd8ffd557a2b1f9,cs.DC,https://arxiv.org/pdf/2501.14794,HeteroLLM: Accelerating Large Language Model Inference on Mobile SoCs platform with Heterogeneous AI Accelerators,"Le Chen, Dahu Feng, Erhu Feng, Rong Zhao, Yingrui Wang, Yubin Xia, Haibo Chen, Pinjie Xu","Distributed, Parallel, and Cluster Computing, Artificial Intelligence, Machine Learning","With the rapid advancement of artificial intelligence technologies such as ChatGPT, AI agents and video generation,
contemporary mobile systems have begun integrating these AI capabilities on local devices to enhance privacy and reduce response latency.
To meet the computational demands of AI tasks, current mobile SoCs are equipped with diverse AI accelerators, including GPUs and Neural Processing Units (NPUs).
However, there has not been a comprehensive characterization of these heterogeneous processors,
and existing designs typically only leverage a single AI accelerator for LLM inference,
leading to suboptimal use of computational resources and memory bandwidth."
909,679d459debd8ffd557a2b1fa,cs.DC,https://arxiv.org/pdf/2501.14786,Punch Out Model Synthesis: A Stochastic Algorithm for Constraint Based Tiling Generation,Zzyv Zzyzek,"Distributed, Parallel, and Cluster Computing, Machine Learning","As an artistic aid in tiled level design, Constraint Based Tiling Generation (CBTG) algorithms can help
to automatically create level realizations from a set of tiles and placement constraints.
Merrell’sModify in Blocks Model Synthesis(MMS) and Gumin’sWave Function Collapse(WFC) have been proposed
as Constraint Based Tiling Generation (CBTG) algorithms that work well for many scenarios but have limitations
in problem size, problem setup and solution biasing.
We present Punch Out Model Synthesis (POMS), a Constraint Based Tiling Generation algorithm,
that can handle large problem sizes, requires minimal assumptions
for setup and can help mitigate solution biasing.
POMS attempts to resolve indeterminate grid regions by trying to progressively realize sub-blocks, performing a
stochastic boundary erosion on previously resolved regions should sub-block resolution fail.
We highlight the results of running a reference implementation on different tile sets and
discuss a tile correlation length, implied by the tile constraints, and its role in choosing
an appropriate block size to aid POMS in successfully finding grid realizations."
910,679d459debd8ffd557a2b1fb,cs.DC,https://arxiv.org/pdf/2501.14784,DeServe: Towards Affordable Offline LLM Inference via Decentralization,"Linyu Wu, Xiaoyuan Liu, Tianneng Shi, Zhe Ye, Dawn Song","Distributed, Parallel, and Cluster Computing, Artificial Intelligence","The rapid growth of generative AI and its integration into everyday workflows have significantly increased the demand for large language model (LLM) inference services. While proprietary models remain popular, recent advancements in open-source LLMs have positioned them as strong contenders. However, deploying these models is often constrained by the high costs and limited availability of GPU resources.
In response, this paper presents the design of a decentralized offline serving system for LLM inference. Utilizing idle GPU resources, our proposed system, DeServe, decentralizes access to LLMs at a lower cost. DeServe specifically addresses key challenges in optimizing serving throughput in high-latency network environments. Experiments demonstrate that DeServe achieves a 6.7x-12.6x improvement in throughput over existing serving system baselines in such conditions."
911,679d459debd8ffd557a2b1fc,cs.DC,https://arxiv.org/pdf/2501.14783,Persistent HyTM via Fast Path Fine-Grained Locking,"Gaetano Coccimiglio, Trevor Brown, Srivatsan Ravi","Distributed, Parallel, and Cluster Computing","Utilizing hardware transactional memory (HTM) in conjunction with non-volatile memory (NVM) to achieve persistence is quite difficult and somewhat awkward due to the fact that the primitives utilized to write data to NVM will abort HTM transactions.
We present several persistent hybrid transactional memory (HyTM) that, perhaps counterintuitively, utilize an HTM fast path primarily to read or acquire fine-grained locks which protect data items.
Our implementations guarantee durable linearizable transactions and the STM path satisfies either weak progressiveness or strong progressiveness.
We discuss the design choices related to the differing progress guarantees and we examine how these design choices impact performance.
We evaluate our persistent HyTM implementations using various microbenchmarks.
Our implementations achieve improved performance especially for read dominant workloads compared to state of the art persistent STMs and persistent HyTMs despite the challenges and apparent awkwardness of using current implementation HTM to achieve persistence."
912,679d459debd8ffd557a2b1fd,cs.DC,https://arxiv.org/pdf/2501.14781,LEISA: A Scalable Microservice-based System for Efficient Livestock Data Sharing,"Mahir Habib, Muhammad Ashad Kabir, Lihong Zheng","Distributed, Parallel, and Cluster Computing",
913,679d459debd8ffd557a2b1fe,cs.DC,https://arxiv.org/pdf/2501.14771,Dynamic Adaptation in Data Storage: Real-Time Machine Learning for Enhanced Prefetching,"Chiyu Cheng, Chang Zhou, Yang Zhao, Jin Cao","Distributed, Parallel, and Cluster Computing, Machine Learning, Operating Systems",
914,679d459debd8ffd557a2b1ff,cs.DC,https://arxiv.org/pdf/2501.14770,Optimizing SSD Caches for Cloud Block Storage Systems Using Machine Learning Approaches,"Chiyu Cheng, Chang Zhou, Yang Zhao, Jin Cao","Distributed, Parallel, and Cluster Computing, Machine Learning, Operating Systems",
915,679d459debd8ffd557a2b200,cs.DC,https://arxiv.org/pdf/2501.14765,Hybrid Cooperative Co-Evolution Algorithm for Deadlock-prone Distributed Assembly Flowshop Scheduling with Limited buffers Using Petri nets,"Siyi Wang, Yanxiang Feng, Xiaoling Li, Guanghui Zhang, Yikang Yang","Distributed, Parallel, and Cluster Computing, Systems and Control",
916,679d459debd8ffd557a2b201,cs.DC,https://arxiv.org/pdf/2501.14763,Intent-driven scheduling of backup jobs,"Souvik Dutta, Suri Brahmaroutu","Distributed, Parallel, and Cluster Computing, Performance","Job scheduling under various constraints to achieve global optimization is a well-studied problem. However, in scenarios that involve time-dependent constraints, such as scheduling backup jobs, achieving global optimization may not always be desirable. This paper presents a framework for scheduling new backup jobs in the presence of existing job schedules, focusing on satisfying intent-based constraints without disrupting current schedules.
The proposed method accommodates various scheduling intents and constraints, and its effectiveness is validated through extensive testing against a variety of backup scenarios on real-world data from Veritas Netbackup® customer policies."
917,679d459debd8ffd557a2b202,cs.DC,https://arxiv.org/pdf/2501.14757,Heat: Satellite's meat is GPU's poison,"Zhehu Yuan, Jinyang Liu, Guanqun Song, Ting Zhu","Distributed, Parallel, and Cluster Computing","In satellite applications, managing thermal conditions is a significant challenge due to the extreme fluctuations in temperature during orbital cycles. One of the solutions is to heat the satellite when it is not exposed to sunlight, which could protect the satellites from extremely low temperatures. However, heat dissipation is necessary for Graphics Processing Units (GPUs) to operate properly and efficiently. In this way, this paper investigates the use of GPU as a means of passive heating in low-earth orbit (LEO) satellites. Our approach uses GPUs to generate heat during the eclipse phase of satellite orbits, substituting traditional heating systems, while the GPUs are also cooled down during this process. The results highlight the potential advantages and limitations of this method, including the cost implications, operational restrictions, and the technical complexity involved. Also, this paper explores the thermal behavior of GPUs under different computational loads, specifically focusing on execution-dominated and FLOP-dominated workloads. Moreover, this paper discusses future directions for improving GPU-based heating solutions, including further cost analysis, system optimization, and practical testing in real satellite missions."
918,679d459debd8ffd557a2b203,cs.DC,https://arxiv.org/pdf/2501.14755,Data-Juicer 2.0: Cloud-Scale Adaptive Data Processing for Foundation Models,"Daoyuan Chen, Yilun Huang, Xuchen Pan, Nana Jiang, Haibin Wang, Ce Ge, Yushuo Chen, Wenhao Zhang, Zhijian Ma, Yilei Zhang, Jun Huang, Wei Lin, Yaliang Li, Bolin Ding, Jingren Zhou","Distributed, Parallel, and Cluster Computing, Artificial Intelligence","The burgeoning field of foundation models necessitates advanced data processing mechanisms capable of harnessing vast valuable data with varied types utilized by these models.
Nevertheless, the current landscape presents unique challenges that traditional data processing frameworks cannot handle effectively, especially with multimodal intricacies.
In response, we presentData-Juicer2.0, a new system offering fruitful data processing capabilities backed by over a hundred operators spanning various modalities like text, image, audio, and video.
With seamless compatibility and dedicated optimization to popular dataset hubs like Hugging Face and computing engines like Ray,Data-Juicer2.0 enhances its predecessor in both usability, efficiency, and programmability.
It features an easily accessible user interface layer that supports decoupled Python interactions, RESTful APIs, and conversational commands.
Alongside this, it contains a core runtime layer optimized for adaptive execution and management across different dataset scales, processing demands, and computational environments, while shielding unnecessary system details.
Extensive empirical evaluations demonstrateData-Juicer2.0’s remarkable performance and scalability, highlighting its capability to efficiently process tens of billions of data samples with tens of thousands of CPU cores.
The system is publicly available, actively maintained, and broadly adopted in diverse research endeavors, practical applications, and real-world products such as Alibaba Cloud PAI."
919,679d459debd8ffd557a2b204,cs.DC,https://arxiv.org/pdf/2501.14753,ABACUS: A FinOps Service for Cloud Cost Optimization,Saurabh Deochake,"Distributed, Parallel, and Cluster Computing, Artificial Intelligence, Networking and Internet Architecture, Software Engineering","In recent years, as more enterprises have moved their infrastructure to the cloud, significant challenges have emerged in achieving holistic cloud spend visibility and cost optimization. FinOps practices provide a way for enterprises to achieve these business goals by optimizing cloud costs and bringing accountability to cloud spend. This paper presents ABACUS - Automated Budget Analysis and Cloud Usage Surveillance, a FinOps solution for optimizing cloud costs by setting budgets, enforcing those budgets through blocking new deployments, and alerting appropriate teams if spending breaches a budget threshold. ABACUS also leverages best practices like Infrastructure-as-Code to alert engineering teams of the expected cost of deployment before resources are deployed in the cloud. Finally, future research directions are proposed to advance the state of the art in this important field."
920,679d459debd8ffd557a2b205,cs.DC,https://arxiv.org/pdf/2501.14745,AI-Driven Health Monitoring of Distributed Computing Architecture: Insights from XGBoost and SHAP,"Xiaoxuan Sun, Yue Yao, Xiaoye Wang, Pochun Li, Xuan Li","Distributed, Parallel, and Cluster Computing, Machine Learning",
921,679d459debd8ffd557a2b206,cs.DC,https://arxiv.org/pdf/2501.14743,KVDirect: Distributed Disaggregated LLM Inference,"Shiyang Chen, Rain Jiang, Dezhi Yu, Jinlai Xu, Mengyuan Chao, Fanlong Meng, Chenyu Jiang, Wei Xu, Hang Liu","Distributed, Parallel, and Cluster Computing, Machine Learning, Performance","Large Language Models (LLMs) have become the new foundation for many applications, reshaping human society like a storm. Disaggregated inference, which separates prefill and decode stages, is a promising approach to improving hardware utilization and service quality. However, due to inefficient inter-node communication, existing systems restrict disaggregated inference to a single node, limiting resource allocation flexibility and reducing service capacity. This paper introduces KVDirect, which optimizes KV cache transfer to enable a distributed disaggregated LLM inference. KVDirect achieves this through the following contributions. First, we propose a novel tensor-centric communication mechanism that reduces the synchronization overhead in traditional distributed GPU systems. Second, we design a custom communication library to support dynamic GPU resource scheduling and efficient KV cache transfer. Third, we introduce a pull-based KV cache transfer strategy that reduces GPU resource idling and improves latency. Finally, we implement KVDirect as an open-source LLM inference framework. Our evaluation demonstrates that KVDirect reduces per-request latency by 55% compared to the baseline across diverse workloads under the same resource constraints."
922,679d459debd8ffd557a2b207,cs.DC,https://arxiv.org/pdf/2501.14740,Datapath Combinational Equivalence Checking With Hybrid Sweeping Engines and Parallelization,"Zhihan Chen, Xindi Zhang, Yuhang Qian, Shaowei Cai","Distributed, Parallel, and Cluster Computing, Hardware Architecture","In the application of IC design for microprocessors, there are often demands for optimizing the implementation of datapath circuits, on which various arithmetic operations are performed. Combinational equivalence checking (CEC) plays an essential role in ensuring the correctness of design optimization. The most prevalent CEC algorithms are based on SAT sweeping, which utilizes SAT to prove the equivalence of the internal node pairs in topological order, and the equivalent nodes are merged. Datapath circuits usually contain equivalent pairs for which the transitive fan-in cones are small but have a high XOR chain density, and proving such node pairs is very difficult for SAT solvers. An exact probability-based simulation (EPS) is suitable for verifying such pairs, while this method is not suitable for pairs with many primary inputs due to the memory cost. We first reduce the memory cost of EPS and integrate it to improve the SAT sweeping method. Considering the complementary abilities of SAT and EPS, we design an engine selection heuristic to dynamically choose SAT or EPS in the sweeping process, according to XOR chain density. Our method is further improved by reducing unnecessary engine calls by detecting regularity. Furthermore, we parallelized the SAT and EPS engines ofhybridCEC, leading to the parallel CEC prover. Experiments on a benchmark suite from industrial datapath circuits show that our method is much faster than the state-of-the-art CEC tool namely ABC ‘&cec’ on nearly all instances, and is more than 100×\times×faster on 30% of the instances, 1000×\times×faster on 12% of the instances. In addition, the 64 threads version of our method achieved 77x speedup."
923,679d459debd8ffd557a2b208,cs.DC,https://arxiv.org/pdf/2501.14739,Reproduction Research of FSA-Benchmark,"Joshua Ludolf, Yesmin Reyna-Hernandez, Matthew Trevino","Distributed, Parallel, and Cluster Computing, Machine Learning","In the current landscape of big data, the reliability and performance of storage systems are essential to the success of various applications and services. As data volumes continue to grow exponentially, the complexity and scale of the storage infrastructures needed to manage this data also increase. A significant challenge faced by data centers and storage systems is the detection and management of fail-slow disks—disks that experience a gradual decline in performance before ultimately failing. Unlike outright disk failures, fail-slow conditions can go undetected for prolonged periods, leading to considerable impacts on system performance and user experience."
924,679d459debd8ffd557a2b209,cs.DC,https://arxiv.org/pdf/2501.14734,Research on the Application of Spark Streaming Real-Time Data Analysis System and large language model Intelligent Agents,"Jialin Wang, Zhihua Duan","Distributed, Parallel, and Cluster Computing, Artificial Intelligence","This study explores the integration of Agent AI with LangGraph to enhance real-time data analysis systems in big data environments. The proposed framework overcomes limitations of static workflows, inefficient stateful computations, and lack of human intervention by leveraging LangGraph’s graph-based workflow construction and dynamic decision-making capabilities. LangGraph allows large language models (LLMs) to dynamically determine control flows, invoke tools, and assess the necessity of further actions, improving flexibility and efficiency."
925,679d459debd8ffd557a2b20a,cs.DC,https://arxiv.org/pdf/2501.14733,LLM as HPC Expert: Extending RAG Architecture for HPC Data,"Yusuke Miyashita, Patrick Kin Man Tung, Johan Barthélemy","Distributed, Parallel, and Cluster Computing, Artificial Intelligence","High-Performance Computing (HPC) is crucial for performing advanced computational tasks, yet their complexity often challenges users, particularly those unfamiliar with HPC-specific commands and workflows. This paper introduces Hypothetical Command Embeddings (HyCE), a novel method that extends Retrieval-Augmented Generation (RAG) by integrating real-time, user-specific HPC data, enhancing accessibility to these systems. HyCE enriches large language models (LLM) with real-time, user-specific HPC information, addressing the limitations of fine-tuned models on such data. We evaluate HyCE using an automated RAG evaluation framework, where the LLM itself creates synthetic questions from the HPC data and serves as a judge, assessing the efficacy of the extended RAG with the evaluation metrics relevant for HPC tasks. Additionally, we tackle essential security concerns, including data privacy and command execution risks, associated with deploying LLMs in HPC environments. This solution provides a scalable and adaptable approach for HPC clusters to leverage LLMs as HPC expert, bridging the gap between users and the complex systems of HPC ."
926,679d459debd8ffd557a2b20b,cs.DC,https://arxiv.org/pdf/2501.14732,Orthrus: Accelerating Multi-BFT Consensus through Concurrent Partial Ordering of Transactions,"Hanzheng Lyu, Shaokang Xie, Jianyu Niu, Ivan Beschastnikh, Yinqian Zhang, Mohammad Sadoghi, Chen Feng","Distributed, Parallel, and Cluster Computing, Performance","Multi-Byzantine Fault Tolerant (Multi-BFT) consensus allows multiple consensus instances to run in parallel, resolving the leader bottleneck problem inherent in classic BFT consensus. However, the global ordering of Multi-BFT consensus enforces a strict serialized sequence of transactions, imposing additional confirmation latency and also limiting concurrency.
In this paper, we introduce Orthrus, a Multi-BFT protocol that accelerates transaction confirmation through partial ordering while reserving global ordering for transactions requiring stricter sequencing.
To this end, Orthrus strategically partitions transactions to maximize concurrency and ensure consistency. Additionally, it incorporates an escrow mechanism to manage interactions between partially and globally ordered transactions.
We evaluated Orthrus through extensive experiments in realistic settings, deploying 128 replicas in WAN and LAN environments.
Our findings demonstrate latency reductions of up to 87% in WAN compared to existing Multi-BFT protocols."
927,679d459debd8ffd557a2b20c,cs.DC,https://arxiv.org/pdf/2501.16298,Uncoded Download in Lagrange-Coded Elastic Computing with Straggler Tolerance,"Xi Zhong, Samuel Lu, Joerg Kliewer, Mingyue Ji","Information Theory, Distributed, Parallel, and Cluster Computing","Coded elastic computing, introduced by Yanget al.in 2018, is a technique designed to mitigate the impact of elasticity in cloud computing systems, where machines can be preempted or be added during computing rounds.
This approach utilizes maximum distance separable (MDS) coding for both storage and download in matrix-matrix multiplications. The proposed scheme is unable to tolerate stragglers and has high encoding complexity and upload cost. In 2023, we addressed these limitations by employing uncoded storage and Lagrange-coded download. However, it results in a large storage size. To address the challenges of storage size and upload cost, in this paper, we focus on Lagrange-coded elastic computing based on uncoded download. We propose a new class of elastic computing schemes, using Lagrange-coded storage with uncoded download (LCSUD). Our proposed schemes address both elasticity and straggler challenges while achieving lower storage size, reduced encoding complexity, and upload cost compared to existing methods."
928,679d459debd8ffd557a2b20d,cs.DC,https://arxiv.org/pdf/2501.16174,Measuring Heterogeneity in Machine Learning with Distributed Energy Distance,"Mengchen Fan, Baocheng Geng, Roman Shterenberg, Joseph A. Casey, Zhong Chen, Keren Li","Machine Learning, Artificial Intelligence, Distributed, Parallel, and Cluster Computing, Machine Learning","In distributed and federated learning, heterogeneity across data sources remains a major obstacle to effective model aggregation and convergence. We focus on feature heterogeneity and introduce energy distance as a sensitive measure for quantifying distributional discrepancies. While we show that energy distance is robust for detecting data distribution shifts, its direct use in large-scale systems can be prohibitively expensive. To address this, we develop Taylor approximations that preserve key theoretical quantitative properties while reducing computational overhead. Through simulation studies, we show how accurately capturing feature discrepancies boosts convergence in distributed learning. Finally, we propose a novel application of energy distance to assign penalty weights for aligning predictions across heterogeneous nodes, ultimately enhancing coordination in federated and distributed settings."
929,679d459debd8ffd557a2b20e,cs.DC,https://arxiv.org/pdf/2501.16168,Ringmaster ASGD: The First Asynchronous SGD with Optimal Time Complexity,"Artavazd Maranjyan, Alexander Tyurin, Peter Richtárik","Machine Learning, Distributed, Parallel, and Cluster Computing, Optimization and Control, Machine Learning","Asynchronous Stochastic Gradient Descent (Asynchronous SGD) is a cornerstone method for parallelizing learning in distributed machine learning.
However, its performance suffers under arbitrarily heterogeneous computation times across workers, leading to suboptimal time complexity and inefficiency as the number of workers scales.
While severalAsynchronous SGDvariants have been proposed, recent findings byTyurin & Richtárik (2023)reveal that none achieve optimal time complexity, leaving a significant gap in the literature.
In this paper, we proposeRingmaster ASGD, a novelAsynchronous SGDmethod designed to address these limitations and tame the inherent challenges ofAsynchronous SGD.
We establish, through rigorous theoretical analysis, thatRingmaster ASGDachieves optimal time complexity under arbitrarily heterogeneous and dynamically fluctuating worker computation times.
This makes it the firstAsynchronous SGDmethod to meet the theoretical lower bounds for time complexity in such scenarios."
930,679d459debd8ffd557a2b20f,cs.DC,https://arxiv.org/pdf/2501.15995,Brain-Inspired Decentralized Satellite Learning in Space Computing Power Networks,"Peng Yang, Ting Wang, Haibin Cai, Yuanming Shi, Chunxiao Jiang, Linling Kuang","Machine Learning, Distributed, Parallel, and Cluster Computing, Networking and Internet Architecture, Signal Processing","Satellite networks are able to collect massive space information with advanced remote sensing technologies, which is essential for real-time applications such as natural disaster monitoring.
However, traditional centralized processing by the ground server incurs a severe timeliness issue caused by the transmission bottleneck of raw data.
To this end, Space Computing Power Networks (Space-CPN) emerges as a promising architecture to coordinate the computing capability of satellites and enable on-board data processing.
Nevertheless, due to the natural limitations of solar panels, satellite power system is difficult to meet the energy requirements for ever-increasing intelligent computation tasks of artificial neural networks.
To tackle this issue, we propose to employ spiking neural networks (SNNs), which is supported by the neuromorphic computing architecture, for on-board data processing.
The extreme sparsity in its computation enables a high energy efficiency.
Furthermore, to achieve effective training of these on-board models, we put forward a decentralized neuromorphic learning framework, where a communication-efficient inter-plane model aggregation method is developed with the inspiration from RelaySum.
We provide a theoretical analysis to characterize the convergence behavior of the proposed algorithm, which reveals a network diameter related convergence speed.
We then formulate a minimum diameter spanning tree problem on the inter-plane connectivity topology and solve it to further improve the learning performance.
Extensive experiments are conducted to evaluate the superiority of the proposed method over benchmarks."
931,679d459debd8ffd557a2b210,cs.DC,https://arxiv.org/pdf/2501.15939,Harnessing CUDA-Q's MPS for Tensor Network Simulations of Large-Scale Quantum Circuits,"Gabin Schieffer, Stefano Markidis, Ivy Peng","Quantum Physics, Distributed, Parallel, and Cluster Computing","Quantum computer simulators are an indispensable tool for prototyping quantum algorithms and verifying the functioning of existing quantum computer hardware. The current largest quantum computers feature more than one thousand qubits, challenging their classical simulators. State-vector quantum simulators are challenged by the exponential increase of representable quantum states with respect to the number of qubits, making more than fifty qubits practically unfeasible. A more appealing approach for simulating quantum computers is adopting the tensor network approach, whose memory requirements fundamentally depend on the level of entanglement in the quantum circuit, and allows simulating the current largest quantum computers. This work investigates and evaluates the CUDA-Q tensor network simulators on an Nvidia Grace Hopper system, particularly the Matrix Product State (MPS) formulation. We compare the performance of the CUDA-Q state vector implementation and validate the correctness of MPS simulations. Our results highlight that tensor network-based methods provide a significant opportunity to simulate large-qubit circuits, albeit approximately. We also show that current GPU-accelerated computation cannot fully utilize GPU efficiently in the case of MPS simulations."
932,679d459debd8ffd557a2b211,cs.DC,https://arxiv.org/pdf/2501.15348,ReInc: Scaling Training of Dynamic Graph Neural Networks,"Mingyu Guan, Saumia Singhal, Taesoo Kim, Anand Padmanabha Iyer","Machine Learning, Distributed, Parallel, and Cluster Computing","Dynamic Graph Neural Networks (DGNNs) have gained widespread attention due to their applicability in diverse domains such as traffic network prediction, epidemiological forecasting, and social network analysis. In this paper, we presentReInc, a system designed to enable efficient and scalable training of DGNNs on large-scale graphs.ReIncintroduces key innovations that capitalize on the unique combination of Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs) inherent in DGNNs. By reusing intermediate results and incrementally computing aggregations across consecutive graph snapshots,ReIncsignificantly enhances computational efficiency. To support these optimizations,ReIncincorporates a novel two-level caching mechanism with a specialized caching policy aligned to the DGNN execution workflow. Additionally,ReIncaddresses the challenges of managing structural and temporal dependencies in dynamic graphs through a new distributed training strategy. This approach eliminates communication overheads associated with accessing remote features and redistributing intermediate results. Experimental results demonstrate thatReIncachieves up to an order of magnitude speedup compared to state-of-the-art frameworks, tested across various dynamic GNN architectures and real-world graph datasets."
933,679d459debd8ffd557a2b212,cs.DC,https://arxiv.org/pdf/2501.15305,Enhancing Disaster Resilience with UAV-Assisted Edge Computing: A Reinforcement Learning Approach to Managing Heterogeneous Edge Devices,"Talha Azfar, Kaicong Huang, Ruimin Ke","Emerging Technologies, Artificial Intelligence, Distributed, Parallel, and Cluster Computing","Edge sensing and computing is rapidly becoming part of intelligent infrastructure architecture leading to operational reliance on such systems in disaster or emergency situations. In such scenarios there is a high chance of power supply failure due to power grid issues, and communication system issues due to base stations losing power or being damaged by the elements, e.g., flooding, wildfires etc. Mobile edge computing in the form of unmanned aerial vehicles (UAVs) has been proposed to provide computation offloading from these devices to conserve their battery, while the use of UAVs as relay network nodes has also been investigated previously. This paper considers the use of UAVs with further constraints on power and connectivity to prolong the life of the network while also ensuring that the data is received from the edge nodes in a timely manner. Reinforcement learning is used to investigate numerous scenarios of various levels of power and communication failure. This approach is able to identify the device most likely to fail in a given scenario, thus providing priority guidance for maintenance personnel. The evacuations of a rural town and urban downtown area are also simulated to demonstrate the effectiveness of the approach at extending the life of the most critical edge devices."
934,679d459debd8ffd557a2b213,cs.DC,https://arxiv.org/pdf/2501.15104,Two-sorted algebraic decompositions of Brookes's shared-state denotational semantics,"Yotam Dvir, Ohad Kammar, Ori Lahav, Gordon Plotkin","Programming Languages, Distributed, Parallel, and Cluster Computing, Logic in Computer Science",
935,679d459debd8ffd557a2b214,cs.DC,https://arxiv.org/pdf/2501.14336,RadiK: Scalable and Optimized GPU-Parallel Radix Top-K Selection,"Yifei Li, Bole Zhou, Jiejing Zhang, Xuechao Wei, Yinghan Li, Yingda Chen","Data Structures and Algorithms, Distributed, Parallel, and Cluster Computing","Top-k selection, which identifies the largest or smallestk𝑘kitalic_kelements from a data set, is a fundamental operation in data-intensive domains such as databases and deep learning, so its scalability and efficiency are critical for these high-performance systems.
However, previous studies on its efficient GPU implementation are mostly merge-based and rely heavily on the fast but size-limited on-chip memory, thereby limiting the scalability with a restricted upper bound onk𝑘kitalic_k.
This work introduces a scalable and optimized GPU-parallel radix top-k selection that supports significantly largerk𝑘kitalic_kvalues than existing methods without compromising efficiency, regardless of input length and batch size.
Our method incorporates a novel optimization framework tailored for high memory bandwidth and resource utilization, achieving up to 2.5×\times×speedup over the prior art for non-batch queries and up to 4.8×\times×speedup for batch queries.
In addition, we propose an adaptive scaling technique that strengthens the robustness, which further provides up to 2.7×\times×speedup on highly adversarial input distributions."
936,679d459debd8ffd557a2b215,cs.DC,https://arxiv.org/pdf/2501.14456,Experimentally Evaluating the Resource Efficiency of Big Data Autoscaling,"Jonathan Will, Nico Treide, Lauritz Thamsen, Odej Kao","Distributed, Parallel, and Cluster Computing","Distributed dataflow systems like Spark and Flink enable data-parallel processing of large datasets on clusters.
Yet, selecting appropriate computational resources for dataflow jobs is often challenging.
For efficient execution, individual resource allocations, such as memory and CPU cores, must meet the specific resource requirements of the job.
An alternative to selecting a static resource allocation for a job execution is autoscaling as implemented for example by Spark."
937,679d459debd8ffd557a2b216,cs.DC,https://arxiv.org/pdf/2501.14417,DeepFlow: Serverless Large Language Model Serving at Scale,"Junhao Hu, Jiang Xu, Zhixia Liu, Yulong He, Yuetao Chen, Hao Xu, Jiang Liu, Baoquan Zhang, Shining Wan, Gengyuan Dan, Zhiyu Dong, Zhihao Ren, Jie Meng, Chao He, Changhong Liu, Tao Xie, Dayun Lin, Qin Zhang, Yue Yu, Hao Feng, Xusheng Chen, Yizhou Shan","Distributed, Parallel, and Cluster Computing","This paper introducesDeepFlow, a scalable and serverless AI platform designed to efficiently serve large language models (LLMs) at scale in cloud environments.DeepFlowaddresses key challenges such as resource allocation, serving efficiency, and cold start latencies through four main design components.
First, it uses a simple serverless abstraction called the request-job-task model, which helps manage AI workloads across post-training and model serving tasks.
Second, it builds an in-house serving engineFlowServeusing a microkernel-inspired design, NPU-centric execution, and SPMD-based parallelism to optimize LLM serving. The system also includes novel scheduling policies tailored for both PD-disaggregated and PD-colocated configurations. With optimizations like pre-warmed pods, DRAM pre-loading, and NPU-fork,DeepFlowcan scale up to 64 instances in seconds.DeepFlowhas been in production for over a year, operating on a large Ascend NPU cluster and providing industry-standard APIs for fine-tuning, agent serving, and model serving to our customers."
938,679d459debd8ffd557a2b217,cs.DC,https://arxiv.org/pdf/2501.14406,Adaptive Rank Allocation for Federated Parameter-Efficient Fine-Tuning of Language Models,"Fei Wu, Jia Hu, Geyong Min, Shiqiang Wang","Distributed, Parallel, and Cluster Computing, Artificial Intelligence, Machine Learning, Networking and Internet Architecture","Pre-trained Language Models (PLMs) have demonstrated their superiority and versatility in modern Natural Language Processing (NLP), effectively adapting to various downstream tasks through further fine-tuning. Federated Parameter-Efficient Fine-Tuning (FedPEFT) has emerged as a promising solution to address privacy and efficiency challenges in distributed training for PLMs on mobile devices. However, our measurements reveal two key limitations of FedPEFT: heterogeneous data leads to significant performance degradation, and a fixed parameter configuration results in communication inefficiency.
To overcome these limitations, we propose FedARA, a novel Federated Adaptive Rank Allocation for parameter-efficient fine-tuning of language models. Specifically, FedARA employs truncated singular value decomposition (SVD) adaptation to enhance flexibility and expressiveness, significantly mitigating the adverse effects of data heterogeneity. Subsequently, it utilizes dynamic rank allocation to progressively identify critical ranks, effectively improving communication efficiency. Lastly, it leverages rank-based module pruning to remove inactive modules, steadily reducing local training time and peak memory usage in each round. Extensive experiments show that FedARA consistently outperforms weak baselines by an average of 8.49% and strong baselines by 6.95% across various datasets under data heterogeneity while significantly improving communication efficiency by 2.40×\times×. Moreover, experiments on AGX Orin, Orin Nano and Raspberry Pi 5 devices demonstrate substantial decreases in total training time and energy consumption by up to 48.90% and 46.95%, respectively."
939,679d459debd8ffd557a2b218,cs.DC,https://arxiv.org/pdf/2501.14312,Locality-aware Fair Scheduling in LLM Serving,"Shiyi Cao, Yichuan Wang, Ziming Mao, Pin-Lun Hsu, Liangsheng Yin, Tian Xia, Dacheng Li, Shu Liu, Yineng Zhang, Yang Zhou, Ying Sheng, Joseph Gonzalez, Ion Stoica","Distributed, Parallel, and Cluster Computing, Machine Learning","Large language model (LLM) inference workload dominates a wide variety of modern AI applications, ranging from multi-turn conversation to document analysis. Balancing fairness and efficiency is critical for managing diverse client workloads with varying prefix patterns. Unfortunately, existing fair scheduling algorithms for LLM serving, such as Virtual Token Counter (VTC), fail to take prefix locality into consideration and thus suffer from poor performance. On the other hand, locality-aware scheduling algorithms in existing LLM serving frameworks tend to maximize the prefix cache hit rate without considering fair sharing among clients."
940,679d459debd8ffd557a2b219,cs.DC,https://arxiv.org/pdf/2501.14653,Federated Domain Generalization with Data-free On-server Gradient Matching,"Trong-Binh Nguyen, Minh-Duong Nguyen, Jinsun Park, Quoc-Viet Pham, Won Joo Hwang","Machine Learning, Artificial Intelligence, Distributed, Parallel, and Cluster Computing, Multiagent Systems","Domain Generalization (DG) aims to learn from multiple known source domains a model that can generalize well to unknown target domains.
One of the key approaches in DG is training an encoder which generates domain-invariant representations. However, this approach is not applicable in Federated Domain Generalization (FDG), where data from various domains are distributed across different clients.
In this paper, we introduce a novel approach, dubbed Federated Learning via On-server Matching Gradient (FedOMG), which canefficiently leverage domain information from distributed domains. Specifically, we utilize the local gradients as information about the distributed models to find an invariant gradient direction across all domains through gradient inner product maximization. The advantages are two-fold: 1) FedOMG can aggregate the characteristics of distributed models on the centralized server without incurring any additional communication cost, and 2)
FedOMG is orthogonal to many existing FL/FDG methods, allowing for additional performance improvements by being seamlessly integrated with them. Extensive experimental evaluations on various settings to demonstrate the robustness of FedOMG compared to other FL/FDG baselines. Our method outperforms recent SOTA baselines on four FL benchmark datasets (MNIST, EMNIST, CIFAR-10, and CIFAR-100), and three FDG benchmark datasets (PACS, VLCS, and OfficeHome)."
941,679d459debd8ffd557a2b21a,cs.DC,https://arxiv.org/pdf/2501.14458,A Survey of Optimization Methods for Training DL Models: Theoretical Perspective on Convergence and Generalization,"Jing Wang, Anna Choromanska","Machine Learning, Distributed, Parallel, and Cluster Computing, Optimization and Control",
942,679d459debd8ffd557a2b21b,cs.DC,https://arxiv.org/pdf/2501.14438,Data-efficient Performance Modeling via Pre-training,"Chunting Liu, Riyadh Baghdadi","Programming Languages, Distributed, Parallel, and Cluster Computing, Machine Learning","Performance models are essential for automatic code optimization, enabling compilers to predict the effects of code transformations on performance and guide search for optimal transformations. Building state-of-the-art performance models with deep learning, however, requires vast labeled datasets of random programs – an expensive and time-consuming process, stretching over months. This paper introduces a self-supervised pre-training scheme with autoencoders to reduce the need for labeled data. By pre-training on a large dataset of random programs, the autoencoder learns representations of code and transformations, which are then used to embed programs for the performance model. Implemented in the Tiramisu autoscheduler, our approach improves model accuracy with less data. For example, to achieve a MAPE of20.72%percent20.7220.72\%20.72 %, the original model requires 18 million data points, whereas our method achieves a similar MAPE of22.44%percent22.4422.44\%22.44 %with only 3.6 million data points, reducing data requirements by5×5\times5 ×."
943,679d459debd8ffd557a2b21c,cs.DC,https://arxiv.org/pdf/2501.14370,TCDM Burst Access: Breaking the Bandwidth Barrier in Shared-L1 RVV Clusters Beyond 1000 FPUs,"Diyou Shen, Yichao Zhang, Marco Bertuletti, Luca Benini","Hardware Architecture, Distributed, Parallel, and Cluster Computing","As computing demand and memory footprint of deep learning applications accelerate, clusters of cores sharing local (L1) multi-banked memory are widely used as key building blocks in large-scale architectures.
When the cluster’s core count increases, a flat all-to-all interconnect between cores and L1 memory banks becomes a physical implementation bottleneck, and hierarchical network topologies are required.
However, hierarchical, multi-level intra-cluster networks are subject to internal contention which may lead to significant performance degradation, especially for SIMD or vector cores, as their memory access is bursty.
We present the TCDM Burst Access architecture, a software-transparent burst transaction support to improve bandwidth utilization in clusters with many vector cores tightly coupled to a multi-banked L1 data memory.
In our solution, a Burst Manager dispatches burst requests to L1 memory banks, multiple 32b words from burst responses are retired in parallel on channels with parametric data-width.
We validate our design on aRISC-V Vector (RVV)many-core cluster, evaluating the benefits on different core counts.
With minimal logic area overhead (less than8%times8percent8\text{\,}\mathrm{\%}start_ARG 8 end_ARG start_ARG times end_ARG start_ARG % end_ARG), we improve the bandwidth of a 16-, a 256-, and a 1024-Floating Point Unit (FPU)baseline clusters, withoutTightly Coupled Data Memory (TCDM)Burst Access, by118%times118percent118\text{\,}\mathrm{\%}start_ARG 118 end_ARG start_ARG times end_ARG start_ARG % end_ARG,226%times226percent226\text{\,}\mathrm{\%}start_ARG 226 end_ARG start_ARG times end_ARG start_ARG % end_ARG, and77%times77percent77\text{\,}\mathrm{\%}start_ARG 77 end_ARG start_ARG times end_ARG start_ARG % end_ARGrespectively.
Reaching up to80%times80percent80\text{\,}\mathrm{\%}start_ARG 80 end_ARG start_ARG times end_ARG start_ARG % end_ARGof the cores-memory peak bandwidth, our design demonstrates ultra-high bandwidth utilization and enables efficient performance scaling.
Implemented in12121212-nm FinFET technology node, compared to the serialized access baseline, our solution achieves up to1.9xenergy efficiency and2.76xperformance in real-world kernel benchmarkings."
944,679d459debd8ffd557a2b21d,cs.DC,https://arxiv.org/pdf/2501.14170,Argos: Agentic Time-Series Anomaly Detection with Autonomous Rule Generation via Large Language Models,"Yile Gu, Yifan Xiong, Jonathan Mace, Yuting Jiang, Yigong Hu, Baris Kasikci, Peng Cheng","Machine Learning, Distributed, Parallel, and Cluster Computing, Multiagent Systems","Observability in cloud infrastructure is critical for service providers, driving the widespread adoption of anomaly detection systems for monitoring metrics.
However, existing systems often struggle to simultaneously achieve explainability, reproducibility, and autonomy, which are three indispensable properties for production use.
We introduceArgos, an agentic system for detecting time-series anomalies in cloud infrastructure by leveraging large language models (LLMs).Argosproposes to use explainable and reproducible anomaly rules as intermediate representation and employs LLMs to autonomously generate such rules.
The system will efficiently train error-free and accuracy-guaranteed anomaly rules through multiple collaborative agents and deploy the trained rules for low-cost online anomaly detection.
Through evaluation results, we demonstrate thatArgosoutperforms state-of-the-art methods, increasingF1subscript𝐹1F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTscores by up to9.5%percent9.59.5\%9.5 %and28.3%percent28.328.3\%28.3 %on public anomaly detection datasets and an internal dataset collected from Microsoft, respectively."
945,679d459debd8ffd557a2b21e,cs.DS,https://arxiv.org/pdf/2501.18496,Graph Exploration with Edge Weight Estimates,"Matthias Gehnen, Ralf Klasing, Émile Naquin",Data Structures and Algorithms,"In theTravelling Salesman Problem, every vertex of an edge-weighted
graph has to be visited by an agent who traverses the edges of the graph.
In this problem, it is usually assumed that the costs of each edge are given in advance,
making it computationally hard but possible to calculate an optimal tour for the agent."
946,679d459debd8ffd557a2b21f,cs.DS,https://arxiv.org/pdf/2501.18105,Facility Location on High-dimensional Euclidean Spaces,"Euiwoong Lee, Kijun Shin",Data Structures and Algorithms,"Recent years have seen great progress in the approximability of fundamental clustering and facility location problems on high-dimensional Euclidean spaces, includingk𝑘kitalic_k-Means andk𝑘kitalic_k-Median. While they admit strictly better approximation ratios than their general metric versions, their approximation ratios are still higher than the hardness ratios for general metrics, leaving the possibility that the ultimate optimal approximation ratios will be the same between Euclidean and general metrics. Moreover, such an improved algorithm for Euclidean spaces is not known for Uncapaciated Facility Location (UFL), another fundamental problem in the area."
947,679d459debd8ffd557a2b220,cs.DS,https://arxiv.org/pdf/2501.18010,Sequential Testing with Subadditive Costs,"Blake Harris, Viswanath Nagarajan, Rayen Tan",Data Structures and Algorithms,"In the classic sequential testing problem, we are given a system with several components each of which fails with some independent probability. The goal is to identify whether or not some component has failed.
When the test costs are additive, it is well known that a greedy algorithm finds an optimal solution.
We consider a much more general setting withsubadditivecost functions and provide a(4⁢ρ+γ)4𝜌𝛾(4\rho+\gamma)( 4 italic_ρ + italic_γ )-approximation algorithm, assuming aγ𝛾\gammaitalic_γ-approximate value oracle (that computes the cost of any subset) and aρ𝜌\rhoitalic_ρ-approximate ratio oracle (that finds a subset with minimum ratio of cost to failure probability).
While the natural greedy algorithm has a poor approximation ratio in the subadditive case, we show that a suitable truncation achieves the above guarantee.
Our analysis is based on a connection to the minimum sum set cover problem.
As applications, we obtain the first approximation algorithms for sequential testing under various cost-structures:(5+ϵ)5italic-ϵ(5+\epsilon)( 5 + italic_ϵ )-approximation for tree-based costs,9.59.59.59.5-approximation for routing costs and(4+ln⁡n)4𝑛(4+\ln n)( 4 + roman_ln italic_n )for machine activation costs.
We also show that sequential testing under submodular costs does not admit any poly-logarithmic approximation (assuming the exponential time hypothesis)."
948,679d459debd8ffd557a2b221,cs.DS,https://arxiv.org/pdf/2501.18522,Digital Quantum Simulations of the Non-Resonant Open Tavis-Cummings Model,"Aidan N. Sims, Dhrumil Patel, Aby Philip, Alex H. Rubin, Rahul Bandyopadhyay, Marina Radulaski, Mark M. Wilde","Quantum Physics, Data Structures and Algorithms, Computational Physics, Optics","The open Tavis–Cummings model consists ofN𝑁Nitalic_Nquantum emitters interacting with a common cavity mode, accounts for losses and decoherence, and is frequently explored for quantum information processing and designing quantum devices. AsN𝑁Nitalic_Nincreases, it becomes harder to simulate the open Tavis–Cummings model using traditional methods. To address this problem, we implement two quantum algorithms for simulating the dynamics of this model in the inhomogenous, non-resonant regime, with up to three excitations in the cavity. We show that the implemented algorithms have gate complexities that scale polynomially, asO⁢(N2)𝑂superscript𝑁2O(N^{2})italic_O ( italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )andO⁢(N3)𝑂superscript𝑁3O(N^{3})italic_O ( italic_N start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ). One of these algorithms is the sampling-based wave matrix Lindbladization algorithm, for which we propose two protocols to implement its system-independent fixed interaction, resolving key open questions of [Patel and Wilde,Open Sys. & Info. Dyn., 30:2350014 (2023)]. Furthermore, we benchmark our results against a classical differential equation solver and demonstrate the ability to simulate classically intractable systems."
949,679d459debd8ffd557a2b222,cs.DS,https://arxiv.org/pdf/2501.17836,Matrix Product Sketching via Coordinated Sampling,"Majid Daliri, Juliana Freire, Danrong Li, Christopher Musco","Data Structures and Algorithms, Databases, Machine Learning","We revisit the well-studied problem of approximating a matrix product,𝐀T⁢𝐁superscript𝐀𝑇𝐁\mathbf{A}^{T}\mathbf{B}bold_A start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_B, based on small space sketches𝒮⁢(𝐀)𝒮𝐀\mathcal{S}(\mathbf{A})caligraphic_S ( bold_A )and𝒮⁢(𝐁)𝒮𝐁\mathcal{S}(\mathbf{B})caligraphic_S ( bold_B )of𝐀∈ℝn×d𝐀superscriptℝ𝑛𝑑\mathbf{A}\in\mathbb{R}^{n\times d}bold_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_d end_POSTSUPERSCRIPTand𝐁∈ℝn×m𝐁superscriptℝ𝑛𝑚\mathbf{B}\in\mathbb{R}^{n\times m}bold_B ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_m end_POSTSUPERSCRIPT. We are interested in the setting where the sketches must be computed independently of each other, except for the use of a shared random seed. We prove that, when𝐀𝐀\mathbf{A}bold_Aand𝐁𝐁\mathbf{B}bold_Bare sparse, methods based oncoordinated random samplingcan outperform classical linear sketching approaches, like Johnson-Lindenstrauss Projection or CountSketch. For example, to obtain Frobenius norm errorϵ⁢‖𝐀‖F⁢‖𝐁‖Fitalic-ϵsubscriptnorm𝐀𝐹subscriptnorm𝐁𝐹\epsilon\|\mathbf{A}\|_{F}\|\mathbf{B}\|_{F}italic_ϵ ∥ bold_A ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT ∥ bold_B ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT, coordinated sampling requires sketches of sizeO⁢(s/ϵ2)𝑂𝑠superscriptitalic-ϵ2O(s/\epsilon^{2})italic_O ( italic_s / italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )when𝐀𝐀\mathbf{A}bold_Aand𝐁𝐁\mathbf{B}bold_Bhave at mosts≤d,m𝑠𝑑𝑚s\leq d,mitalic_s ≤ italic_d , italic_mnon-zeros per row. In contrast, linear sketching leads to sketches of sizeO⁢(d/ϵ2)𝑂𝑑superscriptitalic-ϵ2O(d/\epsilon^{2})italic_O ( italic_d / italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )andO⁢(m/ϵ2)𝑂𝑚superscriptitalic-ϵ2O(m/\epsilon^{2})italic_O ( italic_m / italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )for𝐀𝐀\mathbf{A}bold_Aand𝐁𝐁\mathbf{B}bold_B. We empirically evaluate our approach on two applications: 1) distributed linear regression in databases, a problem motivated by tasks like dataset discovery and augmentation, and 2) approximating attention matrices in transformer-based language models. In both cases, our sampling algorithms yield an order of magnitude improvement over linear sketching."
950,679d459debd8ffd557a2b223,cs.DS,https://arxiv.org/pdf/2501.17708,Improved fixed-parameter bounds for Min-Sum-Radii and Diameters $k$-clustering and their fair variants,"Sandip Banerjee, Yair Bartal, Lee-Ad Gottlieb, Alon Hovav",Data Structures and Algorithms,"We provide improved upper and lower bounds for the Min-Sum-Radii (MSR) and Min-Sum-Diameters (MSD) clustering problems with a bounded number of clustersk𝑘kitalic_k.
In particular, we propose an exact MSD algorithm with running-timenO⁢(k)superscript𝑛𝑂𝑘n^{O(k)}italic_n start_POSTSUPERSCRIPT italic_O ( italic_k ) end_POSTSUPERSCRIPT.
We also provide(1+ϵ)1italic-ϵ(1+\epsilon)( 1 + italic_ϵ )approximation algorithms for both MSR and MSD with running-times ofO⁢(k⁢n)+(1/ϵ)O⁢(d⁢k)𝑂𝑘𝑛superscript1italic-ϵ𝑂𝑑𝑘O(kn)+(1/\epsilon)^{O(dk)}italic_O ( italic_k italic_n ) + ( 1 / italic_ϵ ) start_POSTSUPERSCRIPT italic_O ( italic_d italic_k ) end_POSTSUPERSCRIPTin metrics spaces of doubling dimensiond𝑑ditalic_d.
Our algorithms extend tok𝑘kitalic_k-center, improving upon previous results, and toα𝛼\alphaitalic_α-MSR, where radii are raised to theα𝛼\alphaitalic_αpower forα>1𝛼1\alpha>1italic_α > 1.
Forα𝛼\alphaitalic_α-MSD we prove an exponential time ETH-based lower bound forα>log⁡3𝛼3\alpha>\log 3italic_α > roman_log 3. All algorithms can also be modified to handle outliers.
Moreover, we can extend the results to variants that observefairnessconstraints, as well as to the general framework ofmergeableclustering, which includes many other popular clustering variants. We complement these upper bounds with ETH-based lower bounds for these problems, in particular proving thatnO⁢(k)superscript𝑛𝑂𝑘n^{O(k)}italic_n start_POSTSUPERSCRIPT italic_O ( italic_k ) end_POSTSUPERSCRIPTtime is tight for MSR andα𝛼\alphaitalic_α-MSR even in doubling spaces, and that2o⁢(k)superscript2𝑜𝑘2^{o(k)}2 start_POSTSUPERSCRIPT italic_o ( italic_k ) end_POSTSUPERSCRIPTbounds are impossible for MSD."
951,679d459debd8ffd557a2b224,cs.DS,https://arxiv.org/pdf/2501.17701,Decision-Theoretic Approaches in Learning-Augmented Algorithms,"Spyros Angelopoulos, Christoph Dürr, Georgii Melidi","Data Structures and Algorithms, Machine Learning",
952,679d459debd8ffd557a2b225,cs.DS,https://arxiv.org/pdf/2501.17682,Unifying Scheduling Algorithms for Group Completion Time,"Alexander Lindermayr, Zhenwei Liu, Nicole Megow",Data Structures and Algorithms,
953,679d459debd8ffd557a2b226,cs.DS,https://arxiv.org/pdf/2501.17563,Search Trees on Trees via LP,"Yaniv Sadeh, Haim Kaplan, Uri Zwick",Data Structures and Algorithms,"We consider the problem of computing optimal search trees on trees (STTs). STTs generalize binary search trees (BSTs) in which we search nodes in a path (linear order) to search trees that facilitate search over general tree topologies. Golinsky[9]proposed a linear programming (LP) relaxation of the problem of computing an optimal static STT over a given tree topology. He used this LP formulation to compute an STT that is a2222-approximation to an optimal STT, and conjectured that it is, in fact, an extended formulation of the convex-hull of all depths-vectors of STTs, and thus always gives an optimal solution. In this work we study this LP approach further. We show that the conjecture is false and that Golinsky’s LP does not always give an optimal solution.
To show this we use what we call thenormals method.
We use this method to enumerate over vertices of Golinsky’s polytope for all tree topologies of no more than 8 nodes. We give a lower bound on the integrality gap of the LP and on the approximation ratio of Golinsky’s rounding method. We further enumerate several research directions that can lead to the resolution of the question whether one can compute an optimal STT in polynomial time."
954,679d459debd8ffd557a2b227,cs.DS,https://arxiv.org/pdf/2501.17379,Stable Tree Labelling for Accelerating Distance Queries on Dynamic Road Networks,"Henning Koehler, Muhammad Farhan, Qing Wang","Data Structures and Algorithms, Databases","Finding the shortest-path distance between two arbitrary vertices is an important problem in road networks. Due to real-time traffic conditions, road networks undergo dynamic changes all the time. Current state-of-the-art methods incrementally maintain a distance labelling based on a hierarchy among vertices to support efficient distance computation. However, their labelling sizes are often large and cannot be efficiently maintained. To combat these issues, we present a simple yet efficient labelling method, namelyStable Tree Labelling(STL), for answering distance queries on dynamic road networks. We observe that the properties of an underlying hierarchy play an important role in improving and balancing query and update performance. Thus, we introduce the notion ofstable tree hierarchywhich lays the ground for developing efficient maintenance algorithms on dynamic road networks. Based on stable tree hierarchy, STL can be efficiently constructed as a 2-hop labelling. A crucial ingredient of STL is to only store distances within subgraphs in labels, rather than distances in the entire graph, which restricts the labels affected by dynamic changes. We further develop two efficient maintenance algorithms upon STL:Label Search algorithmandPareto Search algorithm. Label Search algorithm identifies affected ancestors in a stable tree hierarchy and performs efficient searches to update labels from those ancestors. Pareto Search algorithm explores the interaction between search spaces of different ancestors, and combines searches from multiple ancestors into only two searches for each update, eliminating duplicate graph traversals. The experiments show that our algorithms significantly outperform state-of-the-art dynamic methods in maintaining the labelling and query processing, while requiring an order of magnitude less space."
955,679d459debd8ffd557a2b228,cs.DS,https://arxiv.org/pdf/2501.17277,Hardness and Approximation Algorithms for Balanced Districting Problems,"Prathamesh Dharangutte, Jie Gao, Shang-En Huang, Fang-Yi Yu",Data Structures and Algorithms,"We introduce and study the problem of balanced districting, where given an undirected graph with vertices carrying two types of weights (different population, resource types, etc) the goal is to maximize the total weights covered in vertex disjoint districts such that each district is a star or (in general) a connected induced subgraph with the two weights to be balanced. This problem is strongly motivated by political redistricting, where contiguity, population balance, and compactness are essential. We provide hardness and approximation algorithms for this problem. In particular, we showNP-hardness for an approximation better thann1/2−δsuperscript𝑛12𝛿n^{1/2-\delta}italic_n start_POSTSUPERSCRIPT 1 / 2 - italic_δ end_POSTSUPERSCRIPTfor any constantδ>0𝛿0\delta>0italic_δ > 0in general graphs even when the districts are star graphs, as well asNP-hardness on complete graphs, tree graphs, planar graphs and other restricted settings. On the other hand, we develop an algorithm for balanced star districting that gives anO⁢(n)𝑂𝑛O(\sqrt{n})italic_O ( square-root start_ARG italic_n end_ARG )-approximation on any graph (which is basically tight considering matching hardness of approximation results), anO⁢(log⁡n)𝑂𝑛O(\log n)italic_O ( roman_log italic_n )approximation on planar graphs with extensions to minor-free graphs. Our algorithm uses a modified Whack-a-Mole algorithm [Bhattacharya, Kiss, and Saranurak, SODA 2023] to find a sparse solution of a fractional packing linear program (despite exponentially many variables) which requires a new design of a separation oracle specific for our balanced districting problem.
To turn the fractional solution to a feasible integer solution, we adopt the randomized rounding algorithm by [Chan and Har-Peled, SoCG 2009]. To get a good approximation ratio of the rounding procedure, a crucial element in the analysis is thebalanced scattering separatorsfor planar graphs and minor-free graphs — separators that can be partitioned into a small number ofk𝑘kitalic_k-hop independent sets for some constantk𝑘kitalic_k— which may find independent interest in solving other packing style problems. Further, our algorithm is versatile —the very same algorithmcan be analyzed in different ways on various graph classes, which leads to class-dependent approximation ratios. We also provide a FPTAS algorithm for complete graphs and tree graphs, as well as greedy algorithms and approximation ratios when the district cardinality is bounded, the graph has bounded degree or the weights are binary."
956,679d459debd8ffd557a2b229,cs.DS,https://arxiv.org/pdf/2501.17638,Better and Simpler Reducibility Bounds over the Integers,Asaf Levin,"Optimization and Control, Discrete Mathematics, Data Structures and Algorithms",We study the settings where we are given a function ofn𝑛nitalic_nvariables defined in a given box of integers. We show that in many cases we can replace the given objective function by a new function with a much smaller domain. Our approach allows us to transform a family of weakly polynomial time algorithms into strongly polynomial time algorithms.
957,679d459debd8ffd557a2b22a,cs.DS,https://arxiv.org/pdf/2501.17205,Near-Optimal Algorithms for Omniprediction,"Princewill Okoroafor, Robert Kleinberg, Michael P. Kim","Machine Learning, Data Structures and Algorithms, Machine Learning",
958,679d459debd8ffd557a2b22b,cs.DS,https://arxiv.org/pdf/2501.16535,Latency Guarantees for Caching with Delayed Hits,"Keerthana Gurushankar, Noah G. Singer, Bernardo Subercaseaux",Data Structures and Algorithms,
959,679d459debd8ffd557a2b22c,cs.DS,https://arxiv.org/pdf/2501.16419,Near-Optimal Parameter Tuning of Level-1 QAOA for Ising Models,"V Vijendran, Dax Enshan Koh, Eunok Bae, Hyukjoon Kwon, Ping Koy Lam, Syed M Assad","Quantum Physics, Data Structures and Algorithms, Emerging Technologies, Optimization and Control",
960,679d459debd8ffd557a2b22d,cs.DS,https://arxiv.org/pdf/2501.16039,Complexity of Minimal Faithful Permutation Degree for Fitting-free Groups,"Michael Levet, Pranjal Srivastava, Dhara Thakkar","Data Structures and Algorithms, Computational Complexity, Group Theory","In this paper, we investigate the complexity of computing the minimal faithful permutation degree for groups without abelian normal subgroups. When our groups are given as quotients of permutation groups, we establish that this problem is inP. Furthermore, in the setting of permutation groups, we obtain an upper bound ofNCfor this problem. This improves upon the work of Das and Thakkar (STOC 2024), who established a Las Vegas polynomial-time algorithm for this class in the setting of permutation groups."
961,679d459debd8ffd557a2b22e,cs.DS,https://arxiv.org/pdf/2501.15952,Polynomial Kernel and Incompressibility for Prison-Free Edge Deletion and Completion,"Séhane Bel Houari-Durand, Eduard Eiben, Magnus Wahlström",Data Structures and Algorithms,"Given a graphG𝐺Gitalic_Gand an integerk𝑘kitalic_k, theH𝐻Hitalic_H-free Edge
Deletionproblem asks whether there exists a set of at mostk𝑘kitalic_kedges ofG𝐺Gitalic_Gwhose deletion makesG𝐺Gitalic_Gfree of induced copies ofH𝐻Hitalic_H. Significant attention has been given to thekernelizabilityaspects of this problem – i.e., for which graphsH𝐻Hitalic_Hdoes the problem
admit an “efficient preprocessing” procedure, known as apolynomial kernelization,
where an instanceI𝐼Iitalic_Iof the problem with parameterk𝑘kitalic_kis reduced
to an equivalent instanceI′superscript𝐼′I^{\prime}italic_I start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPTwhose size and parameter value are bounded polynomially ink𝑘kitalic_k?
Although such routines are known for many graphsH𝐻Hitalic_Hwhere the class
ofH𝐻Hitalic_H-free graphs has significant restricted structure,
it is also clear that for most graphsH𝐻Hitalic_Hthe problem isincompressible,
i.e., admits no polynomial kernelization parameterized byk𝑘kitalic_kunless the polynomial hierarchy collapses.
These results led Marx and Sandeep to the conjecture thatH𝐻Hitalic_H-free
Edge Deletionis incompressible for any graphH𝐻Hitalic_Hwith at least five vertices, unlessH𝐻Hitalic_His complete or has at most one edge (JCSS 2022). This conjecture was reduced
to the incompressibility ofH𝐻Hitalic_H-free Edge Deletionfor a
finite list of graphsH𝐻Hitalic_H. We consider one of these graphs, which we dub theprison, and show thatPrison-Free Edge Deletionhas a polynomial kernel, refuting the conjecture. On the other
hand, the same problem for the complement of the prison is
incompressible."
962,679d459debd8ffd557a2b22f,cs.DS,https://arxiv.org/pdf/2501.15782,Online Allocation with Multi-Class Arrivals: Group Fairness vs Individual Welfare,"Faraz Zargari, Hossein Nekouyan Jazi, Bo Sun, Xiaoqi Tan","Computer Science and Game Theory, Data Structures and Algorithms",
963,679d459debd8ffd557a2b230,cs.DS,https://arxiv.org/pdf/2501.14537,Forbidden Subgraph Problems with Predictions,"Hans-Joachim Böckenhauer, Melvin Jahn, Dennis Komm, Moritz Stocker",Data Structures and Algorithms,"In theOnline Delayed ConnectedH𝐻Hitalic_H-Node-Deletion Problem,
an unweighted graph is revealed vertex by vertex and it must remain free
of any induced copies of a specific connected induced forbidden subgraphH𝐻Hitalic_Hat
each point in time. To achieve this, an algorithm must, upon each occurrence ofH𝐻Hitalic_H, identify and irrevocably delete one or more vertices. The objective is to
delete as few vertices as possible. We provide tight bounds on the competitive
ratio for forbidden subgraphsH𝐻Hitalic_Hthat do not contain two true twins or that do
not contain two false twins."
964,679d459debd8ffd557a2b231,cs.DS,https://arxiv.org/pdf/2501.14461,Efficient parameterized approximation,"Stefan Kratsch, Pascal Kunz",Data Structures and Algorithms,"Many important problems areNPNP\operatorname{NP}roman_NP-hard and, unlessP=NPPNP\operatorname{P}=\operatorname{NP}roman_P = roman_NP, they do not admit polynomial-time exact algorithms. In fact, the fastest known algorithms for solving them exactly usually take time exponential in the input size. Much research effort has gone into obtaining much faster exact algorithms for instances that are sufficiently well-structured, e.g., through parameterized algorithms with running timef⁢(k)⋅n𝒪⁢(1)⋅𝑓𝑘superscript𝑛𝒪1f(k)\cdot n^{\mathcal{O}(1)}italic_f ( italic_k ) ⋅ italic_n start_POSTSUPERSCRIPT caligraphic_O ( 1 ) end_POSTSUPERSCRIPTwheren𝑛nitalic_nis the input size andk𝑘kitalic_kquantifies some structural property such as treewidth. Whenk𝑘kitalic_kis small, this is comparable to a polynomial-time exact algorithm and usually it outperforms the fastest exact exponential-time algorithms for a large range ofk𝑘kitalic_k."
965,679d459debd8ffd557a2b232,cs.DS,https://arxiv.org/pdf/2501.14450,Changing Induced Subgraph Isomorphisms Under Extended Reconfiguration Rules,"Tatsuhiro Suga, Akira Suzuki, Yuma Tamura, Xiao Zhou",Data Structures and Algorithms,"In a reconfiguration problem, we are given two feasible solutions of a combinatorial problem and our goal is to determine whether it is possible to reconfigure one into the other, with the steps dictated by specific reconfiguration rules. Traditionally, most studies on reconfiguration problems have focused on rules that allow changing a single element at a time. In contrast, this paper considers scenarios in whichk≥2𝑘2k\geq 2italic_k ≥ 2elements can be changed simultaneously. We investigate the general reconfiguration problem of isomorphisms.
For theInduced Subgraph Isomorphism Reconfigurationproblem, we show that the problem remains\PSPACE-complete even under stringent constraints on the pattern graph whenk𝑘kitalic_kis constant. We then give two meta-theorems applicable whenk𝑘kitalic_kis slightly less than the number of vertices in the pattern graph.
In addition, we investigate the complexity of theIndependent Set Reconfigurationproblem, which is a special case of theInduced Subgraph Isomorphism Reconfigurationproblem."
966,679d459debd8ffd557a2b233,cs.DS,https://arxiv.org/pdf/2501.14010,On Extended Concentration Inequalities for Fast JL Embeddings of Infinite Sets,"Edem Boahen, March T. Boedihardjo, Rafael Chiclana, Mark Iwen","Data Structures and Algorithms, Probability","The Johnson-Lindenstrauss (JL) lemma allows subsets of a high-dimensional space to be embedded into a lower-dimensional space while approximately preserving all pairwise Euclidean distances. This important result has inspired an extensive literature, with a significant portion dedicated to constructing structured random matrices with fast matrix-vector multiplication algorithms that generate such embeddings for finite point sets. In this paper, we briefly consider fast JL embedding matrices forinfinitesubsets ofℝdsuperscriptℝ𝑑\mathbb{R}^{d}blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. Prior work in this direction such as[15,14]has focused on constructing fast JL matricesH⁢D∈ℝk×d𝐻𝐷superscriptℝ𝑘𝑑HD\in\mathbb{R}^{k\times d}italic_H italic_D ∈ blackboard_R start_POSTSUPERSCRIPT italic_k × italic_d end_POSTSUPERSCRIPTby multiplying structured matrices with RIP(-like) propertiesH∈ℝk×d𝐻superscriptℝ𝑘𝑑H\in\mathbb{R}^{k\times d}italic_H ∈ blackboard_R start_POSTSUPERSCRIPT italic_k × italic_d end_POSTSUPERSCRIPTagainst a random diagonal matrixD∈ℝd×d𝐷superscriptℝ𝑑𝑑D\in\mathbb{R}^{d\times d}italic_D ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT. However, utilizing RIP(-like) matricesH𝐻Hitalic_Hin this fashion necessarily has the unfortunate side effect that the resulting embedding dimensionk𝑘kitalic_kmust depend on the ambient dimensiond𝑑ditalic_dno matter how simple the infinite set is that one aims to embed. Motivated by this, we explore an alternate strategy for removing thisd𝑑ditalic_d-dependence fromk𝑘kitalic_kherein: Extending a concentration inequality proven by Ailon and Liberty[1]in the hope of later utilizing it in a chaining argument to obtain a near-optimal result for infinite sets. Though this strategy ultimately fails to provide the near-optimal embedding dimension we seek, along the way we obtain a stronger-than-sub-exponential extension of the concentration inequality in[1]which may be of independent interest."
967,679d459debd8ffd557a2b234,cs.DS,https://arxiv.org/pdf/2501.14658,Tree independence number V. Walls and claws,"Maria Chudnovsky, Julien Codsi, Daniel Lokshtanov, Martin Milanič, Varun Sivashankar","Combinatorics, Discrete Mathematics, Data Structures and Algorithms","Given a familyℋℋ\mathcal{H}caligraphic_Hof graphs, we say that a graphG𝐺Gitalic_Gisℋℋ\mathcal{H}caligraphic_H-free if no induced subgraph ofG𝐺Gitalic_Gis isomorphic to a member ofℋℋ\mathcal{H}caligraphic_H.
LetSt,t,tsubscript𝑆𝑡𝑡𝑡S_{t,t,t}italic_S start_POSTSUBSCRIPT italic_t , italic_t , italic_t end_POSTSUBSCRIPTbe the graph obtained fromK1,3subscript𝐾13K_{1,3}italic_K start_POSTSUBSCRIPT 1 , 3 end_POSTSUBSCRIPTby subdividing each edget−1𝑡1t-1italic_t - 1times, and letWt×tsubscript𝑊𝑡𝑡W_{t\times t}italic_W start_POSTSUBSCRIPT italic_t × italic_t end_POSTSUBSCRIPTbe thet𝑡titalic_t-by-t𝑡titalic_thexagonal grid.
Letℒtsubscriptℒ𝑡\mathcal{L}_{t}caligraphic_L start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTbe the family of all graphsG𝐺Gitalic_Gsuch thatG𝐺Gitalic_Gis the line graph of some subdivision ofWt×tsubscript𝑊𝑡𝑡W_{t\times t}italic_W start_POSTSUBSCRIPT italic_t × italic_t end_POSTSUBSCRIPT.
We prove that for every positive integert𝑡titalic_tthere existsc⁢(t)𝑐𝑡c(t)italic_c ( italic_t )such that everyℒt∪{St,t,t,Kt,t}subscriptℒ𝑡subscript𝑆𝑡𝑡𝑡subscript𝐾𝑡𝑡\mathcal{L}_{t}\cup\{S_{t,t,t},K_{t,t}\}caligraphic_L start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∪ { italic_S start_POSTSUBSCRIPT italic_t , italic_t , italic_t end_POSTSUBSCRIPT , italic_K start_POSTSUBSCRIPT italic_t , italic_t end_POSTSUBSCRIPT }-freen𝑛nitalic_n-vertex graph admits a tree decomposition in which
the maximum size of an independent set in each bag is at mostc⁢(t)⁢log4⁡n𝑐𝑡superscript4𝑛c(t)\log^{4}nitalic_c ( italic_t ) roman_log start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT italic_n.
This is a variant of a conjecture of Dallard, Krnc, Kwon, Milanič, Munaro, Štorgel, and Wiederrecht from 2024.
This implies that theMaximum Weight Independent Setproblem, as well as
many other natural algorithmic problems,
that are known to beNP-hard in general, can be solved in quasi-polynomial time if
the input graph isℒt∪{St,t,t,Kt,t}subscriptℒ𝑡subscript𝑆𝑡𝑡𝑡subscript𝐾𝑡𝑡\mathcal{L}_{t}\cup\{S_{t,t,t},K_{t,t}\}caligraphic_L start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∪ { italic_S start_POSTSUBSCRIPT italic_t , italic_t , italic_t end_POSTSUBSCRIPT , italic_K start_POSTSUBSCRIPT italic_t , italic_t end_POSTSUBSCRIPT }-free.
As part of our proof, we show that for every positive integert𝑡titalic_tthere exists an integerd𝑑ditalic_dsuch that everyℒt∪{St,t,t}subscriptℒ𝑡subscript𝑆𝑡𝑡𝑡\mathcal{L}_{t}\cup\{S_{t,t,t}\}caligraphic_L start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∪ { italic_S start_POSTSUBSCRIPT italic_t , italic_t , italic_t end_POSTSUBSCRIPT }-free graph admits a balanced separator that is contained
in the neighborhood of at mostd𝑑ditalic_dvertices."
968,679d459debd8ffd557a2b235,cs.OS,https://arxiv.org/pdf/2501.17707,Ownership-based Virtual Memory for Intermittently-Powered Embedded Systems,"Markus Elias Gerber, Luis Gerhorst, Ishwar Mudraje, Kai Vogelgesang, Thorsten Herfet, Peter Wägemann","Operating Systems, Emerging Technologies, Programming Languages","The Battery-Free Internet of Things might revolutionize our understanding of connected devices, which harvest their operational energy from the environment (e.g., using solar cells).
These systems come with the major system-software challenge that the intermittently-powered IoT devices have to checkpoint their state in non-volatile memory to later resume operation with this state when sufficient energy is available.
The scarce energy resources demand that only modified data is persisted to non-volatile memory before a power failure, which requires precise modification-tracking."
969,679d459debd8ffd557a2b236,cs.AI,https://arxiv.org/pdf/2501.18542,Semantic Web and Creative AI -- A Technical Report from ISWS 2023,"Raia Abu Ahmad, Reham Alharbi, Roberto Barile, Martin Böckling, Francisco Bolanos, Sara Bonfitto, Oleksandra Bruns, Irene Celino, Yashrajsinh Chudasama, Martin Critelli, Claudia d'Amato, Giada D'Ippolito, Ioannis Dasoulas, Stefano De Giorgis, Vincenzo De Leo, Chiara Di Bonaventura, Marco Di Panfilo, Daniil Dobriy, John Domingue, Xuemin Duan, Michel Dumontier, Sefika Efeoglu, Ruben Eschauzier, Fakih Ginwa, Nicolas Ferranti, Arianna Graciotti, Philipp Hanisch, George Hannah, Golsa Heidari, Aidan Hogan, Hassan Hussein, Alexane Jouglar, Jan-Christoph Kalo, Manoé Kieffer, Antonis Klironomos, Inês Koch, Weronika Lajewska, Nicolas Lazzari, Mikael Lindekrans, Anna Sofia Lippolis, Majlinda Llugiqi, Eleonora Mancini, Eleonora Marzi, Laura Menotti, Daniela Milon Flores, Soulakshmee Nagowah, Kerstin Neubert, Emetis Niazmand, Ebrahim Norouzi, Beatriz Olarte Martinez, Anouk Michelle Oudshoorn, Andrea Poltronieri, Valentina Presutti, Disha Purohit, Ensiyeh Raoufi, Celian Ringwald, Johanna Rockstroh, Sebastian Rudolph, Harald Sack, Zafar Saeed, Mohammad Javad Saeedizade, Aya Sahbi, Cristian Santini, Aleksandra Simic, Dennis Sommer, Rita Sousa, Mary Ann Tan, Vidyashree Tarikere, Tabea Tietz, Liam Tirpitz, Arnaldo Tomasino, Frank van Harmelen, Joao Vissoci, Caitlin Woods, Bohui Zhang, Xinyue Zhang, Heng Zheng",Artificial Intelligence,
970,679d459debd8ffd557a2b237,cs.AI,https://arxiv.org/pdf/2501.18455,Conversation Games and a Strategic View of the Turing Test,Kaveh Aryan,"Artificial Intelligence, Computer Science and Game Theory","Although many game-theoretic models replicate real interactions that often rely on natural language, explicit study of games where language is central to strategic interaction remains limited. This paper introduces theconversation game, a multi-stage, extensive-form game based on linguistic strategic interaction. We focus on a subset of the games, called verdict games. In a verdict game, two players alternate to contribute to a conversation, which is evaluated at each stage by a non-strategic judge who may render a conclusive binary verdict, or a decision to continue the dialogue. The game ends once a limit is reached or a verdict is given. We show many familiar processes, such as interrogation or a court process fall under this category. We also, show that the Turing test is an instance of verdict game, and discuss the significance of a strategic view of the Turing test in the age of advanced AI deception. We show the practical relevance of the proposed concepts by simulation experiments, and show that a strategic agent outperforms a naive agent by a high margin."
971,679d459debd8ffd557a2b238,cs.AI,https://arxiv.org/pdf/2501.18413,GBFRS: Robust Fuzzy Rough Sets via Granular-ball Computing,"Shuyin Xia, Xiaoyu Lian, Binbin Sang, Guoyin Wang, Xinbo Gao","Artificial Intelligence, Machine Learning","Fuzzy rough set theory is effective for processing datasets with complex attributes, supported by a solid mathematical foundation and closely linked to kernel methods in machine learning. Attribute reduction algorithms and classifiers based on fuzzy rough set theory exhibit promising performance in the analysis of high-dimensional multivariate complex data. However, most existing models operate at the finest granularity, rendering them inefficient and sensitive to noise, especially for high-dimensional big data. Thus, enhancing the robustness of fuzzy rough set models is crucial for effective feature selection. Muiti-garanularty granular-ball computing, a recent development, uses granular-balls of different sizes to adaptively represent and cover the sample space, performing learning based on these granular-balls. This paper proposes integrating multi-granularity granular-ball computing into fuzzy rough set theory, using granular-balls to replace sample points. The coarse-grained characteristics of granular-balls make the model more robust. Additionally, we propose a new method for generating granular-balls, scalable to the entire supervised method based on granular-ball computing. A forward search algorithm is used to select feature sequences by defining the correlation between features and categories through dependence functions. Experiments demonstrate the proposed model’s effectiveness and superiority over baseline methods. The source codes and datasets are both available on the public link: https://github.com/lianxiaoyu724/GBFRS"
972,679d459debd8ffd557a2b239,cs.AI,https://arxiv.org/pdf/2501.18411,Gravity-Bench-v1: A Benchmark on Gravitational Physics Discovery for Agents,"Nolan Koblischke, Hyunseok Jang, Kristen Menou, Mohamad Ali-Dib",Artificial Intelligence,"Modern science emerged from reasoning over repeatedly-observed planetary motions. We present Gravity-Bench-v1, an environment-based benchmark that challenges AI agents on tasks that parallel this historical development. Gravity-Bench-v1 evaluates agents on the discovery of physics concealed within a dynamic environment, using rigorous gravitational dynamics simulations. Gravity-Bench includes out-of-distribution cases, i.e. with physics that deviates from the real world, to evaluate true scientific generalization capabilities. Agents must plan to collect data within an experimental budget and must perform a dynamic form of data analysis and reasoning to solve tasks efficiently. Our benchmark admits an open-ended space of solutions. PhD-level solutions for each task are provided, to calibrate AI performance against human expertise. Technically at an upper-undergraduate level, our benchmark proves challenging to baseline AI agents. Gravity-Bench-v1 and planned extensions should help map out AI progress towards scientific discovery capabilities."
973,679d459debd8ffd557a2b23a,cs.AI,https://arxiv.org/pdf/2501.18320,Leveraging LLM Agents for Automated Optimization Modeling for SASP Problems: A Graph-RAG based Approach,"Tianpeng Pan, Wenqiang Pu, Licheng Zhao, Rui Zhou","Artificial Intelligence, Signal Processing","Automated optimization modeling (AOM) has evoked considerable interest with the rapid evolution of large language models (LLMs). Existing approaches predominantly rely on prompt engineering, utilizing meticulously designed expert response chains or structured guidance. However, prompt-based techniques have failed to perform well in the sensor array signal processing (SASP) area due the lack of specific domain knowledge. To address this issue, we propose an automated modeling approach based on retrieval-augmented generation (RAG) technique, which consists of two principal components: a multi-agent (MA) structure and a graph-based RAG (Graph-RAG) process. The MA structure is tailored for the architectural AOM process, with each agent being designed based on principles of human modeling procedure. The Graph-RAG process serves to match user query with specific SASP modeling knowledge, thereby enhancing the modeling result. Results on ten classical signal processing problems demonstrate that the proposed approach (termed as MAG-RAG) outperforms several AOM benchmarks."
974,679d459debd8ffd557a2b23b,cs.AI,https://arxiv.org/pdf/2501.18299,Model-Free RL Agents Demonstrate System 1-Like Intentionality,"Hal Ashton, Matija Franklin",Artificial Intelligence,"This paper argues that model-free reinforcement learning (RL) agents, while lacking explicit planning mechanisms, exhibit behaviours that can be analogised to System 1 (”thinking fast”) processes in human cognition. Unlike model-based RL agents, which operate akin to System 2 (”thinking slow”) reasoning by leveraging internal representations for planning, model-free agents react to environmental stimuli without anticipatory modelling. We propose a novel framework linking the dichotomy of System 1 and System 2 to the distinction between model-free and model-based RL. This framing challenges the prevailing assumption that intentionality and purposeful behaviour require planning, suggesting instead that intentionality can manifest in the structured, reactive behaviours of model-free agents. By drawing on interdisciplinary insights from cognitive psychology, legal theory, and experimental jurisprudence, we explore the implications of this perspective for attributing responsibility and ensuring AI safety. These insights advocate for a broader, contextually informed interpretation of intentionality in RL systems, with implications for their ethical deployment and regulation."
975,679d459debd8ffd557a2b23c,cs.AI,https://arxiv.org/pdf/2501.18296,Extending the design space of ontologization practices: Using bCLEARer as an example,"Chris Partridge, Andrew Mitchell, Sergio de Cesare, John Beverley",Artificial Intelligence,
976,679d459debd8ffd557a2b23d,cs.AI,https://arxiv.org/pdf/2501.18291,CueTip: An Interactive and Explainable Physics-aware Pool Assistant,"Sean Memery, Kevin Denamganai, Jiaxin Zhang, Zehai Tu, Yiwen Guo, Kartic Subr","Artificial Intelligence, Human-Computer Interaction","We present an interactive and explainable automated coaching assistant called CueTip for a variant of pool/billiards. CueTip’s novelty lies in its combination of three features:
a natural-language interface, an ability to perform contextual, physics-aware reasoning, and that its explanations are rooted in a set of predetermined guidelines developed by domain experts. We instrument a physics simulator so that it generates event traces in natural language alongside traditional state traces. Event traces lend themselves to interpretation by language models, which serve as the interface to our assistant. We design and train a neural adaptor that decouples tactical choices made by CueTip from its interactivity and explainability allowing it to be reconfigured to mimic any pool playing agent. Our experiments show that CueTip enables contextual query-based assistance and explanations while maintaining the strength of the agent in terms of win rate (improving it in some situations). The explanations generated by CueTip are physically-aware and grounded in the expert rules and are therefore more reliable."
977,679d459debd8ffd557a2b23e,cs.AI,https://arxiv.org/pdf/2501.18202,On Scaling Neurosymbolic Programming through Guided Logical Inference,"Thomas Jean-Michel Valentin, Luisa Sophie Werner, Pierre Genevès, Nabil Layaïda",Artificial Intelligence,
978,679d459debd8ffd557a2b23f,cs.AI,https://arxiv.org/pdf/2501.18201,Neural Operator based Reinforcement Learning for Control of first-order PDEs with Spatially-Varying State Delay,"Jiaqi Hu, Jie Qi, Jing Zhang","Artificial Intelligence, Systems and Control","Control of distributed parameter systems affected by delays is a challenging task, particularly when the delays depend on spatial variables. The idea of integrating analytical control theory with learning-based control within a unified control scheme is becoming increasingly promising and advantageous. In this paper, we address the problem of controlling an unstable first-order hyperbolic PDE with spatially-varying delays by combining PDE backstepping control strategies and deep reinforcement learning (RL). To eliminate the assumption on the delay function required for the backstepping design, we propose a soft actor-critic (SAC) architecture incorporating a DeepONet to approximate the backstepping controller. The DeepONet extracts features from the backstepping controller and feeds them into the policy network. In simulations, our algorithm outperforms the baseline SAC without prior backstepping knowledge and the analytical controller."
979,679d459debd8ffd557a2b240,cs.AI,https://arxiv.org/pdf/2501.18190,Economic Rationality under Specialization: Evidence of Decision Bias in AI Agents,"ShuiDe Wen, Juan Feng",Artificial Intelligence,
980,679d459debd8ffd557a2b241,cs.AI,https://arxiv.org/pdf/2501.18081,Normative Evaluation of Large Language Models with Everyday Moral Dilemmas,"Pratik S. Sachdeva, Tom van Nuenen","Artificial Intelligence, Computers and Society","The rapid adoption of large language models (LLMs) has spurred extensive research into their encoded moral norms and decision-making processes. Much of this research relies on prompting LLMs with survey-style questions to assess how well models are aligned with certain demographic groups, moral beliefs, or political ideologies. While informative, the adherence of these approaches to relatively superficial constructs, beliefs, and moral questions tends to oversimplify the complexity and nuance underlying everyday moral dilemmas. We argue that auditing LLMs along more detailed axes of human interaction is of paramount importance to better assess the degree to which they may impact human beliefs and actions. To this end, we evaluate LLMs on complex, everyday moral dilemmas sourced from the “Am I the Asshole” (AITA) community on Reddit, where users seek moral judgments on everyday conflicts from other community members. We prompted seven commonly used LLMs, including proprietary and open-source models, to assign blame and provide explanations for over 10,000 AITA moral dilemmas. We then compared the LLMs’ judgments and explanations to those of Redditors and to each other, aiming to uncover patterns in their moral reasoning. Our results demonstrate that large language models exhibit distinct patterns of moral judgment, varying substantially from human evaluations on the AITA subreddit. LLMs demonstrate moderate to high self-consistency but low inter-model agreement, suggesting that differences in training and alignment lead to fundamentally different approaches to moral reasoning. We further observe that an ensemble of LLMs, despite individual inconsistencies, collectively approximates Redditor consensus in assigning blame. Further analysis of model explanations reveals distinct patterns in how models invoke various moral principles, with some models showing greater sensitivity to specific themes such as fairness or harm. These findings highlight the complexity of implementing consistent moral reasoning in artificial systems and the need for careful evaluation of how different models approach ethical judgment. As LLMs continue to be used in roles requiring ethical decision-making such as therapists and companions, careful evaluation is crucial to mitigate potential biases and limitations. Despite the capacity of LLMs to analyze moral dilemmas, their judgments ultimately lack the ethical accountability of human deliberation, requiring careful scrutiny and reflection on their role in ethical discourse."
981,679d459debd8ffd557a2b242,cs.AI,https://arxiv.org/pdf/2501.18009,Large Language Models Think Too Fast To Explore Effectively,"Lan Pan, Hanbo Xie, Robert C. Wilson","Artificial Intelligence, Neurons and Cognition","Large Language Models (LLMs) have emerged many intellectual capacities. While numerous benchmarks assess their intelligence, limited attention has been given to their ability to explore—an essential capacity for discovering new information and adapting to novel environments in both natural and artificial systems. The extent to which LLMs can effectively explore, particularly in open-ended tasks, remains unclear. This study investigates whether LLMs can surpass humans in exploration during an open-ended task, usingLittle Alchemy 2as a paradigm, where agents combine elements to discover new ones. Results show most LLMs underperform compared to humans, except for theo1model, with those traditional LLMs relying primarily on uncertainty-driven strategies, unlike humans who balance uncertainty and empowerment. Representational analysis of the models with Sparse Autoencoders (SAE) revealed that uncertainty and choices are represented at earlier transformer blocks, while empowerment values are processed later, causing LLMs to think too fast and make premature decisions, hindering effective exploration. These findings shed light on the limitations of LLM exploration and suggest directions for improving their adaptability."
982,679d459debd8ffd557a2b243,cs.AI,https://arxiv.org/pdf/2501.17991,Investigating the Monte-Carlo Tree Search Approach for the Job Shop Scheduling Problem,"Laurie Boveroux, Damien Ernst, Quentin Louveaux","Artificial Intelligence, Optimization and Control",
983,679d459debd8ffd557a2b244,cs.AI,https://arxiv.org/pdf/2501.17974,Think Smarter not Harder: Adaptive Reasoning with Inference Aware Optimization,"Zishun Yu, Tengyu Xu, Di Jin, Karthik Abinav Sankararaman, Yun He, Wenxuan Zhou, Zhouhao Zeng, Eryk Helenowski, Chen Zhu, Sinong Wang, Hao Ma, Han Fang",Artificial Intelligence,
984,679d459debd8ffd557a2b245,cs.AI,https://arxiv.org/pdf/2501.18596,DeltaLLM: Compress LLMs with Low-Rank Deltas between Shared Weights,"Liana Mikaelyan, Ayyoob Imani, Mathew Salvaris, Parth Pathak, Mohsen Fayyaz","Machine Learning, Artificial Intelligence","We introduceDeltaLLM, a new post-training compression technique to reduce the memory footprint of LLMs. We propose an alternative way of structuring LLMs with weight sharing between layers in subsequent Transformer blocks, along with additional low-rank difference matrices between them. For training, we adopt the progressing module replacement method and show that the lightweight training of the low-rank modules with approximately 30M-40M tokens is sufficient to achieve performance on par with LLMs of comparable sizes trained from scratch."
985,679d459debd8ffd557a2b246,cs.AI,https://arxiv.org/pdf/2501.18577,Prediction-Powered Inference with Imputed Covariates and Nonuniform Sampling,"Dan M. Kluger, Kerri Lu, Tijana Zrnic, Sherrie Wang, Stephen Bates","Methodology, Artificial Intelligence, Machine Learning, Machine Learning","Machine learning models are increasingly used to produce predictions that serve as input data in subsequent statistical analyses.
For example, computer vision predictions of economic and environmental indicators based on satellite imagery are used in downstream regressions; similarly, language models are widely used to approximate human ratings and opinions in social science research.
However, failure to properly account for errors in the machine learning predictions renders standard statistical procedures invalid.
Prior work uses what we call thePredict-Then-Debiasestimator to give valid confidence intervals when machine learning algorithms impute missing variables, assuming a smallcompletesample from the population of interest.
We expand the scope by introducing bootstrap confidence intervals that apply when the complete data is a nonuniform (i.e., weighted, stratified, or clustered) sample and to settings where an arbitrary subset of features is imputed. Importantly, the method can be applied to many settings without requiring additional calculations.
We prove that these confidence intervals are valid under no assumptions on the quality of the machine learning model and are no wider than the intervals obtained by methods that do not use machine learning predictions."
986,679d459debd8ffd557a2b247,cs.AI,https://arxiv.org/pdf/2501.18535,A Hybrid Data-Driven Approach For Analyzing And Predicting Inpatient Length Of Stay In Health Centre,"Tasfia Noor Chowdhury, Sanjida Afrin Mou, Kazi Naimur Rahman","Machine Learning, Artificial Intelligence",
987,679d459debd8ffd557a2b248,cs.AI,https://arxiv.org/pdf/2501.18501,Beyond Prior Limits: Addressing Distribution Misalignment in Particle Filtering,"Yiwei Shi, Jingyu Hu, Yu Zhang, Mengyue Yang, Weinan Zhang, Cunjia Liu, Weiru Liu","Machine Learning, Artificial Intelligence, Machine Learning","Particle filtering is a Bayesian inference method and a fundamental tool in state estimation for dynamic systems, but its effectiveness is often limited by the constraints of the initial prior distribution, a phenomenon we define as the Prior Boundary Phenomenon. This challenge arises when target states lie outside the prior’s support, rendering traditional particle filtering methods inadequate for accurate estimation. Although techniques like unbounded priors and larger particle sets have been proposed, they remain computationally prohibitive and lack adaptability in dynamic scenarios. To systematically overcome these limitations, we propose the Diffusion-Enhanced Particle Filtering Framework, which introduces three key innovations: adaptive diffusion through exploratory particles, entropy-driven regularisation to prevent weight collapse, and kernel-based perturbations for dynamic support expansion. These mechanisms collectively enable particle filtering to explore beyond prior boundaries, ensuring robust state estimation for out-of-boundary targets. Theoretical analysis and extensive experiments validate framework’s effectiveness, indicating significant improvements in success rates and estimation accuracy across high-dimensional and non-convex scenarios."
988,679d459debd8ffd557a2b249,cs.AI,https://arxiv.org/pdf/2501.18490,Curriculum-based Sample Efficient Reinforcement Learning for Robust Stabilization of a Quadrotor,"Fausto Mauricio Lagos Suarez, Akshit Saradagi, Vidya Sumathy, Shruti Kotpaliwar, George Nikolakopoulos","Robotics, Artificial Intelligence","This article introduces a curriculum learning approach to develop a reinforcement learning-based robust stabilizing controller for a Quadrotor that meets predefined performance criteria. The learning objective is to achieve desired positions from random initial conditions while adhering to both transient and steady-state performance specifications. This objective is challenging for conventional one-stage end-to-end reinforcement learning, due to the strong coupling between position and orientation dynamics, the complexity in designing and tuning the reward function, and poor sample efficiency, which necessitates substantial computational resources and leads to extended convergence times. To address these challenges, this work decomposes the learning objective into a three-stage curriculum that incrementally increases task complexity. The curriculum begins with learning to achieve stable hovering from a fixed initial condition, followed by progressively introducing randomization in initial positions, orientations and velocities. A novel additive reward function is proposed, to incorporate transient and steady-state performance specifications. The results demonstrate that the Proximal Policy Optimization (PPO)-based curriculum learning approach, coupled with the proposed reward structure, achieves superior performance compared to a single-stage PPO-trained policy with the same reward function, while significantly reducing computational resource requirements and convergence time. The curriculum-trained policy’s performance and robustness are thoroughly validated under random initial conditions and in the presence of disturbances."
989,679d459debd8ffd557a2b24a,cs.AI,https://arxiv.org/pdf/2501.18475,CLoQ: Enhancing Fine-Tuning of Quantized LLMs via Calibrated LoRA Initialization,"Yanxia Deng, Aozhong Zhang, Naigang Wang, Selcuk Gurses, Zi Yang, Penghang Yin","Machine Learning, Artificial Intelligence","Fine-tuning large language models (LLMs) using low-rank adaptation (LoRA) has become a highly efficient approach for downstream tasks, particularly in scenarios with limited computational resources. However, applying LoRA techniques to quantized LLMs poses unique challenges due to the reduced representational precision of quantized weights. In this paper, we introduce CLoQ (CalibratedLoRA initialization forQuantized LLMs), a simplistic initialization strategy designed to overcome these challenges. Our approach focuses on minimizing the layer-wise discrepancy between the original LLM and its quantized counterpart with LoRA components during initialization. By leveraging a small calibration dataset, CLoQ quantizes a pre-trained LLM and determines the optimal LoRA components for each layer, ensuring a strong foundation for subsequent fine-tuning.
A key contribution of this work is a novel theoretical result that enables the accurate and closed-form construction of these optimal LoRA components. We validate the efficacy of CLoQ across multiple tasks such as language generation, arithmetic reasoning, and commonsense reasoning, demonstrating that it consistently outperforms existing LoRA fine-tuning methods for quantized LLMs, especially at ultra low-bit widths. The code is available athttps://github.com/AozhongZhang/CLoQ"
990,679d459debd8ffd557a2b24b,cs.AI,https://arxiv.org/pdf/2501.18468,Beyond Instructed Tasks: Recognizing In-the-Wild Reading Behaviors in the Classroom Using Eye Tracking,"Eduardo Davalos, Jorge Alberto Salas, Yike Zhang, Namrata Srivastava, Yashvitha Thatigotla, Abbey Gonzales, Sara McFadden, Sun-Joo Cho, Gautam Biswas, Amanda Goodwin","Human-Computer Interaction, Artificial Intelligence","Understanding reader behaviors such as skimming, deep reading, and scanning is essential for improving educational instruction. While prior eye-tracking studies have trained models to recognize reading behaviors, they often rely on instructed reading tasks, which can alter natural behaviors and limit the applicability of these findings to in-the-wild settings. Additionally, there is a lack of clear definitions for reading behavior archetypes in the literature. We conducted a classroom study to address these issues by collecting instructed and in-the-wild reading data. We developed a mixed-method framework, including a human-driven theoretical model, statistical analyses, and an AI classifier, to differentiate reading behaviors based on their velocity, density, and sequentiality. Our lightweight 2D CNN achieved an F1 score of 0.8 for behavior recognition, providing a robust approach for understanding in-the-wild reading. This work advances our ability to provide detailed behavioral insights to educators, supporting more targeted and effective assessment and instruction."
991,679d459debd8ffd557a2b24c,cs.AI,https://arxiv.org/pdf/2501.18452,Clustering Properties of Self-Supervised Learning,"Xi Weng, Jianing An, Xudong Ma, Binhang Qi, Jie Luo, Xi Yang, Jin Song Dong, Lei Huang","Machine Learning, Artificial Intelligence","Self-supervised learning (SSL) methods via joint embedding architectures have proven remarkably effective at capturing semantically rich representations with strong clustering properties, magically in the absence of label supervision. Despite this, few of them have explored leveraging these untapped properties to improve themselves. In this paper, we provide an evidence through various metrics that the encoder’s outputencodingexhibits superior and more stable clustering properties compared to other components. Building on this insight, we propose a novel positive-feedback SSL method, termedRepresentationSoftAssignment (ReSA), which leverages the model’s clustering properties to promote learning in a self-guided manner. Extensive experiments on standard SSL benchmarks reveal that models pretrained with ReSA outperform other state-of-the-art SSL methods by a significant margin. Finally, we analyze how ReSA facilitates better clustering properties, demonstrating that it effectively enhances clustering performance at both fine-grained and coarse-grained levels, shaping representations that are inherently more structured and semantically meaningful."
992,679d459debd8ffd557a2b24d,cs.AI,https://arxiv.org/pdf/2501.18448,Autonomy and Safety Assurance in the Early Development of Robotics and Autonomous Systems,"Dhaminda B. Abeywickrama, Michael Fisher, Frederic Wheeler, Louise Dennis","Robotics, Artificial Intelligence",
993,679d459debd8ffd557a2b24e,cs.AI,https://arxiv.org/pdf/2501.18438,o3-mini vs DeepSeek-R1: Which One is Safer?,"Aitor Arrieta, Miriam Ugarte, Pablo Valle, José Antonio Parejo, Sergio Segura","Software Engineering, Artificial Intelligence","The irruption of DeepSeek-R1 constitutes a turning point for the AI industry in general and the LLMs in particular. Its capabilities have demonstrated outstanding performance in several tasks, including creative thinking, code generation, maths and automated program repair, at apparently lower execution cost. However, LLMs must adhere to an important qualitative property, i.e., their alignment with safety and human values. A clear competitor of DeepSeek-R1 is its American counterpart, OpenAI’s o3-mini model, which is expected to set high standards in terms of performance, safety and cost. In this paper we conduct a systematic assessment of the safety level of both, DeepSeek-R1 (70b version) and OpenAI’s o3-mini (beta version).111The team conducting the study was part of the early access safety testing program of OpenAI:https://openai.com/index/early-access-for-safety-testing/To this end, we make use of our recently released automated safety testing tool, named ASTRAL. By leveraging this tool, we automatically and systematically generate and execute a total of 1260 unsafe test inputs on both models. After conducting a semi-automated assessment of the outcomes provided by both LLMs, the results indicate that DeepSeek-R1 is highly unsafe as compared to OpenAI’s o3-mini. Based on our evaluation, DeepSeek-R1 answered unsafely to 11.98% of the executed prompts whereas o3-mini only to 1.19%."
994,679d459debd8ffd557a2b24f,cs.AI,https://arxiv.org/pdf/2501.18432,Solving Drone Routing Problems with Quantum Computing: A Hybrid Approach Combining Quantum Annealing and Gate-Based Paradigms,"Eneko Osaba, Pablo Miranda-Rodriguez, Andreas Oikonomakis, Matic Petrič, Sebastian Bock, Michail-Alexandros Kourtis","Quantum Physics, Artificial Intelligence, Emerging Technologies","This paper presents a novel hybrid approach to solving real-world drone routing problems by leveraging the capabilities of quantum computing. The proposed method, coinedQuantum for Drone Routing(Q4DR), integrates the two most prominent paradigms in the field: quantum gate-based computing, through theEclipse Qrispprogramming language; and quantum annealers, by means of D-Wave System’s devices. The algorithm is divided into two different phases: an initial clustering phase executed using a Quantum Approximate Optimization Algorithm (QAOA), and a routing phase employing quantum annealers. The efficacy ofQ4DRis demonstrated through three use cases of increasing complexity, each incorporating real-world constraints such as asymmetric costs, forbidden paths, and itinerant charging points. This research contributes to the growing body of work in quantum optimization, showcasing the practical applications of quantum computing in logistics and route planning."
995,679d459debd8ffd557a2b250,cs.AI,https://arxiv.org/pdf/2501.18426,Guaranteed confidence-band enclosures for PDE surrogates,"Ander Gray, Vignesh Gopakumar, Sylvain Rousseau, Sébastien Destercke","Machine Learning, Artificial Intelligence","We propose a method for obtaining statistically guaranteed confidence bands for functional machine learning techniques: surrogate models which map between function spaces, motivated by the need build reliable PDE emulators. The method constructs nested confidence sets on a low-dimensional representation (an SVD) of the surrogate model’s prediction error, and then maps these sets to the prediction space using set-propagation techniques. The result are conformal-like coverage guaranteed prediction sets for functional surrogate models. We use zonotopes as basis of the set construction, due to their well studied set-propagation and verification properties. The method is model agnostic and can thus be applied to complex Sci-ML models, including Neural Operators, but also in simpler settings. We also elicit a technique to capture the truncation error of the SVD, ensuring the guarantees of the method."
996,679d459debd8ffd557a2b251,cs.AI,https://arxiv.org/pdf/2501.18367,A Learnable Multi-views Contrastive Framework with Reconstruction Discrepancy for Medical Time-Series,"Yifan Wang, Hongfeng Ai, Ruiqi Li, Maowei Jiang, Cheng Jiang, Chenzhong Li","Machine Learning, Artificial Intelligence","In medical time series disease diagnosis, two key challenges are identified. First, the high annotation cost of medical data leads to overfitting in models trained on label-limited, single-center datasets. To address this, we propose incorporating external data from related tasks and leveraging AE-GAN to extract prior knowledge, providing valuable references for downstream tasks. Second, many existing studies employ contrastive learning to derive more generalized medical sequence representations for diagnostic tasks, usually relying on manually designed diverse positive and negative sample pairs. However, these approaches are complex, lack generalizability, and fail to adaptively capture disease-specific features across different conditions. To overcome this, we introduce LMCF (Learnable Multi-views Contrastive Framework), a framework that integrates a multi-head attention mechanism and adaptively learns representations from different views through inter-view and intra-view contrastive learning strategies. Additionally, the pre-trained AE-GAN is used to reconstruct discrepancies in the target data as disease probabilities, which are then integrated into the contrastive learning process. Experiments on three target datasets demonstrate that our method consistently outperforms other seven baselines, highlighting its significant impact on healthcare applications such as the diagnosis of myocardial infarction, Alzheimer’s disease, and Parkinson’s disease. We release the source code at xxxxx."
997,679d459debd8ffd557a2b252,cs.AI,https://arxiv.org/pdf/2501.18344,Transfer Learning of Surrogate Models: Integrating Domain Warping and Affine Transformations,"Shuaiqun Pan, Diederick Vermetten, Manuel López-Ibáñez, Thomas Bäck, Hao Wang","Machine Learning, Artificial Intelligence","Surrogate models provide efficient alternatives to computationally demanding real-world processes but often require large datasets for effective training. A promising solution to this limitation is the transfer of pre-trained surrogate models to new tasks. Previous studies have investigated the transfer of differentiable and non-differentiable surrogate models, typically assuming an affine transformation between the source and target functions. This paper extends previous research by addressing a broader range of transformations, including linear and nonlinear variations. Specifically, we consider the combination of an unknown input warping—such as one modeled by the beta cumulative distribution function—with an unspecified affine transformation. Our approach achieves transfer learning by employing a limited number of data points from the target task to optimize these transformations, minimizing empirical loss on the transfer dataset. We validate the proposed method on the widely used Black-Box Optimization Benchmark (BBOB) testbed and a real-world transfer learning task from the automobile industry. The results underscore the significant advantages of the approach, revealing that the transferred surrogate significantly outperforms both the original surrogate and the one built from scratch using the transfer dataset, particularly in data-scarce scenarios."
998,679d459debd8ffd557a2b253,cs.AI,https://arxiv.org/pdf/2501.18337,Unfaithful Probability Distributions in Binary Triple of Causality Directed Acyclic Graph,Jingwei Liu,"Machine Learning, Artificial Intelligence, Machine Learning","Faithfulness is the foundation of probability distribution and graph in causal discovery and causal inference. In this paper, several unfaithful probability distribution examples are constructed in three–vertices binary causality directed acyclic graph (DAG) structure, which are not faithful to causal DAGs described in J.M.,Robins,et al. Uniform consistency in causal inference. Biometrika (2003),90(3): 491–515. And the general unfaithful probability distribution with multiple independence and conditional independence in binary triple causal DAG is given."
999,679d459debd8ffd557a2b254,cs.AI,https://arxiv.org/pdf/2501.18310,Efficient Neural Theorem Proving via Fine-grained Proof Structure Analysis,"Haoxiong Liu, Jiacheng Sun, Zhenguo Li, Andrew C Yao","Machine Learning, Artificial Intelligence","The synergy between deep learning models and traditional automation tools plays a pivotal role in developing robust neural theorem provers (NTPs). However, for proof synthesis with LLMs, previous work applies automation tools either only when the model explicitly calls the method, or only at a single granularity level, failing to fully exploit the power of built-in tactics and off-the-shelf automated theorem provers.
In this work, we proposeProofAug, a novel theorem proving method that enjoys superior sample efficiency through equipping proof-generation LLMs with automation methods in different granularities via fine-grained structure analysis of model-generated proof proposals.
Furthermore, ProofAug serves as a versatile plug-and-play module that seamlessly integrates with any tree-search algorithm, enabling our construction of an efficient recursive proving (ERP) module to further enhance performance.
The superiority of our method is validated on the miniF2F-test benchmark using the open-source deepseek-math-7b-base model and the Isabelle proof assistant.
Notably, by additionally employing a mixed prompting strategy, we achieve a cumulative pass rate of 66.0% after curation of the dataset (61.9% for the original version), setting a new SOTA across all proof languages with a total sample budget of only 2100. Our code is available athttps://github.com/haoxiongliu/ProofAug."
1000,679d459debd8ffd557a2b255,cs.AI,https://arxiv.org/pdf/2501.18271,Pre-Trained Vision-Language Model Selection and Reuse for Downstream Tasks,"Hao-Zhe Tan, Zhi Zhou, Lan-Zhe Guo, Yu-Feng Li","Machine Learning, Artificial Intelligence","Pre-trained Vision-Language Models (VLMs) are becoming increasingly popular across various visual tasks, and several open-sourced VLM variants have been released. However, selecting the best-performing pre-trained VLM for a specific downstream task is challenging since no single VLM can achieve promising performance on all downstream tasks, and evaluating all available VLMs is impossible due to time and data limitations. To address this problem, this paper proposes a novel paradigm to select and reuse VLM for downstream tasks, called Model Label Learning (MLL). The proposal contains three key modules:model labeling, which assigns labels to each VLM to describe their specialty and utility;model selection, which matches the requirements of the target task with model labels; andmodel reuse, which applies selected VLMs to the target task in an ensemble manner. The proposal is highly computationally efficient and growable since the model labeling process is completed target task independent and the ability could grow with the number of candidate VLMs. We also introduce a new benchmark for evaluating VLM selection methods, including 49 VLMs and 17 target task datasets. Experimental results clearly demonstrate the effectiveness of the proposed method for selecting and reusing VLMs."
1001,679d459debd8ffd557a2b256,cs.AI,https://arxiv.org/pdf/2501.18258,PDE-DKL: PDE-constrained deep kernel learning in high dimensionality,"Weihao Yan, Christoph Brune, Mengwu Guo","Machine Learning, Artificial Intelligence, Machine Learning","Many physics-informed machine learning methods for PDE-based problems rely on Gaussian processes (GPs) or neural networks (NNs). However, both face limitations when data are scarce, and the dimensionality is high. Although GPs are known for their robust uncertainty quantification in low-dimensional settings, their computational complexity becomes prohibitive as the dimensionality increases. In contrast, while conventional NNs can accommodate high-dimensional input, they often require extensive training data and do not offer uncertainty quantification.
To address these challenges, we propose a PDE-constrained Deep Kernel Learning (PDE-DKL) framework that combines DL and GPs under explicit PDE constraints. Specifically, NNs learn a low-dimensional latent representation of the high-dimensional PDE problem, reducing the complexity of the problem. GPs then perform kernel regression subject to the governing PDEs, ensuring accurate solutions and principled uncertainty quantification, even when available data are limited. This synergy unifies the strengths of both NNs and GPs, yielding high accuracy, robust uncertainty estimates, and computational efficiency for high-dimensional PDEs.
Numerical experiments demonstrate that PDE-DKL achieves high accuracy with reduced data requirements. They highlight its potential as a practical, reliable, and scalable solver for complex PDE-based applications in science and engineering."
1002,679d459debd8ffd557a2b257,cs.AI,https://arxiv.org/pdf/2501.18223,Exploring Large Protein Language Models in Constrained Evaluation Scenarios within the FLIP Benchmark,"Manuel F. Mollon, Joaquin Gonzalez-Rodriguez, Alicia Lozano-Diez, Daniel Ramos, Doroteo T. Toledano","Machine Learning, Artificial Intelligence","In this study, we expand upon the FLIP benchmark—designed for evaluating protein fitness prediction models in small, specialized prediction tasks—by assessing the performance of state-of-the-art large protein language models, including ESM-2 and SaProt on the FLIP dataset. Unlike larger, more diverse benchmarks such as ProteinGym, which cover a broad spectrum of tasks, FLIP focuses on constrained settings where data availability is limited. This makes it an ideal framework to evaluate model performance in scenarios with scarce task-specific data. We investigate whether recent advances in protein language models lead to significant improvements in such settings. Our findings provide valuable insights into the performance of large-scale models in specialized protein prediction tasks."
1003,679d459debd8ffd557a2b258,cs.AI,https://arxiv.org/pdf/2501.18199,HKAN: Hierarchical Kolmogorov-Arnold Network without Backpropagation,"Grzegorz Dudek, Tomasz Rodak","Machine Learning, Artificial Intelligence","This paper introduces the Hierarchical Kolmogorov-Arnold Network (HKAN), a novel network architecture that offers a competitive alternative to the recently proposed Kolmogorov-Arnold Network (KAN). Unlike KAN, which relies on backpropagation, HKAN adopts a randomized learning approach, where the parameters of its basis functions are fixed, and linear aggregations are optimized using least-squares regression.
HKAN utilizes a hierarchical multi-stacking framework, with each layer refining the predictions from the previous one by solving a series of linear regression problems. This non-iterative training method simplifies computation and eliminates sensitivity to local minima in the loss function.
Empirical results show that HKAN delivers comparable, if not superior, accuracy and stability relative to KAN across various regression tasks, while also providing insights into variable importance. The proposed approach seamlessly integrates theoretical insights with practical applications, presenting a robust and efficient alternative for neural network modeling."
1004,679d459debd8ffd557a2b259,cs.AI,https://arxiv.org/pdf/2501.18187,In-Context Learning of Polynomial Kernel Regression in Transformers with GLU Layers,"Haoyuan Sun, Ali Jadbabaie, Navid Azizan","Machine Learning, Artificial Intelligence","Transformer-based models have demonstrated remarkable ability inin-context learning(ICL), where they can adapt to unseen tasks from a prompt with a few examples, without requiring parameter updates.
Recent research has provided insight into how linear Transformers can perform ICL by implementing gradient descent estimators.
In particular, it has been shown that the optimal linear self-attention (LSA) mechanism can implement one step of gradient descent with respect to a linear least-squares objective when trained on random linear regression tasks."
1005,679d459debd8ffd557a2b25a,cs.AI,https://arxiv.org/pdf/2501.18137,Tensor Completion for Surrogate Modeling of Material Property Prediction,"Shaan Pakala, Dawon Ahn, Evangelos Papalexakis","Machine Learning, Materials Science, Artificial Intelligence","When designing materials to optimize certain properties, there are often many possible configurations of designs that need to be explored. For example, the materials’ composition of elements will affect properties such as strength or conductivity, which are necessary to know when developing new materials. Exploring all combinations of elements to find optimal materials becomes very time consuming, especially when there are more design variables. For this reason, there is growing interest in using machine learning (ML) to predict a material’s properties. In this work, we model the optimization of certain material properties as a tensor completion problem, to leverage the structure of our datasets and navigate the vast number of combinations of material configurations. Across a variety of material property prediction tasks, our experiments show tensor completion methods achieving 10-20% decreased error compared with baseline ML models such as GradientBoosting and Multilayer Perceptron (MLP), while maintaining similar training speed."
1006,679d459debd8ffd557a2b25b,cs.AI,https://arxiv.org/pdf/2501.18129,Revisiting gender bias research in bibliometrics: Standardizing methodological variability using Scholarly Data Analysis (SoDA) Cards,"HaeJin Lee, Shubhanshu Mishra, Apratim Mishra, Zhiwen You, Jinseok Kim, Jana Diesner","Digital Libraries, Artificial Intelligence, Social and Information Networks","Gender biases in scholarly metrics remain a persistent concern, despite numerous bibliometric studies exploring their presence and absence across productivity, impact, acknowledgment, and self-citations. However, methodological inconsistencies, particularly in author name disambiguation and gender identification, limit the reliability and comparability of these studies, potentially perpetuating misperceptions and hindering effective interventions. A review of 70 relevant publications over the past 12 years reveals a wide range of approaches, from name-based and manual searches to more algorithmic and gold-standard methods, with no clear consensus on best practices. This variability, compounded by challenges such as accurately disambiguating Asian names and managing unassigned gender labels, underscores the urgent need for standardized and robust methodologies. To address this critical gap, we propose the development and implementation of “Scholarly Data Analysis (SoDA) Cards.” These cards will provide a structured framework for documenting and reporting key methodological choices in scholarly data analysis, including author name disambiguation and gender identification procedures. By promoting transparency and reproducibility, SoDA Cards will facilitate more accurate comparisons and aggregations of research findings, ultimately supporting evidence-informed policymaking and enabling the longitudinal tracking of analytical approaches in the study of gender and other social biases in academia."
1007,679d459debd8ffd557a2b25c,cs.AI,https://arxiv.org/pdf/2501.18122,VQLTI: Long-Term Tropical Cyclone Intensity Forecasting with Physical Constraints,"Xinyu Wang, Lei Liu, Kang Chen, Tao Han, Bin Li, Lei Bai","Machine Learning, Artificial Intelligence","Tropical cyclone (TC) intensity forecasting is crucial for early disaster warning and emergency decision-making. Numerous researchers have explored deep-learning methods to address computational and post-processing issues in operational forecasting. Regrettably, they exhibit subpar long-term forecasting capabilities. We use two strategies to enhance long-term forecasting. (1) By enhancing the matching between TC intensity and spatial information, we can improve long-term forecasting performance. (2) Incorporating physical knowledge and physical constraints can help mitigate the accumulation of forecasting errors. To achieve the above strategies, we propose the VQLTI framework. VQLTI transfers the TC intensity information to a discrete latent space while retaining the spatial information differences, using large-scale spatial meteorological data as conditions. Furthermore, we leverage the forecast from the weather prediction model FengWu to provide additional physical knowledge for VQLTI. Additionally, we calculate the potential intensity (PI) to impose physical constraints on the latent variables. In the global long-term TC intensity forecasting, VQLTI achieves state-of-the-art results for the 24h to 120h, with the MSW (Maximum Sustained Wind) forecast error reduced by35.65%-42.51%compared to ECMWF-IFS."
1008,679d459debd8ffd557a2b25d,cs.AI,https://arxiv.org/pdf/2501.18108,Investigating an Intelligent System to Monitor \& Explain Abnormal Activity Patterns of Older Adults,"Min Hun Lee, Daniel P. Siewiorek, Alexandre Bernardino","Human-Computer Interaction, Artificial Intelligence, Machine Learning","Despite the growing potential of older adult care technologies, the adoption of these technologies remains challenging. In this work, we conducted a focus-group session with family caregivers to scope designs of the older adult care technology. We then developed a high-fidelity prototype and conducted its qualitative study with professional caregivers and older adults to understand their perspectives on the system functionalities. This system monitors abnormal activity patterns of older adults using wireless motion sensors and machine learning models and supports interactive dialogue responses to explain abnormal activity patterns of older adults to caregivers and allow older adults proactively sharing their status with caregivers for an adequate intervention. Both older adults and professional caregivers appreciated that our system can provide a faster, personalized service while proactively controlling what information is to be shared through interactive dialogue responses. We further discuss other considerations to realize older adult technology in practice."
1009,679d459debd8ffd557a2b25e,cs.AI,https://arxiv.org/pdf/2501.18086,DIAL: Distribution-Informed Adaptive Learning of Multi-Task Constraints for Safety-Critical Systems,"Se-Wook Yoo, Seung-Woo Seo","Machine Learning, Artificial Intelligence, Robotics, Systems and Control","Safe reinforcement learning has traditionally relied on predefined constraint functions to ensure safety in complex real-world tasks, such as autonomous driving. However, defining these functions accurately for varied tasks is a persistent challenge. Recent research highlights the potential of leveraging pre-acquired task-agnostic knowledge to enhance both safety and sample efficiency in related tasks. Building on this insight, we propose a novel method to learn shared constraint distributions across multiple tasks. Our approach identifies the shared constraints through imitation learning and then adapts to new tasks by adjusting risk levels within these learned distributions. This adaptability addresses variations in risk sensitivity stemming from expert-specific biases, ensuring consistent adherence to general safety principles even with imperfect demonstrations. Our method can be applied to control and navigation domains, including multi-task and meta-task scenarios, accommodating constraints such as maintaining safe distances or adhering to speed limits. Experimental results validate the efficacy of our approach, demonstrating superior safety performance and success rates compared to baselines, all without requiring task-specific constraint definitions. These findings underscore the versatility and practicality of our method across a wide range of real-world tasks."
1010,679d459debd8ffd557a2b25f,cs.AI,https://arxiv.org/pdf/2501.18071,Towards Transparent and Accurate Diabetes Prediction Using Machine Learning and Explainable Artificial Intelligence,"Pir Bakhsh Khokhar, Viviana Pentangelo, Fabio Palomba, Carmine Gravino","Machine Learning, Artificial Intelligence, Software Engineering",
1011,679d459debd8ffd557a2b260,cs.AI,https://arxiv.org/pdf/2501.18064,Learning Metal Microstructural Heterogeneity through Spatial Mapping of Diffraction Latent Space Features,"Mathieu Calvat, Chris Bean, Dhruv Anjaria, Hyoungryul Park, Haoren Wang, Kenneth Vecchio, J.C. Stinville","Materials Science, Artificial Intelligence","To leverage advancements in machine learning for metallic materials design and property prediction, it is crucial to develop a data-reduced representation of metal microstructures that surpasses the limitations of current physics-based discrete microstructure descriptors. This need is particularly relevant for metallic materials processed through additive manufacturing, which exhibit complex hierarchical microstructures that cannot be adequately described using the conventional metrics typically applied to wrought materials. Furthermore, capturing the spatial heterogeneity of microstructures at the different scales is necessary within such framework to accurately predict their properties. To address these challenges, we propose the physical spatial mapping of metal diffraction latent space features. This approach integrates (i) point diffraction data encoding via variational autoencoders or contrastive learning and (ii) the physical mapping of the encoded values. Together these steps offer a method offers a novel means to comprehensively describe metal microstructures. We demonstrate this approach on a wrought and additively manufactured alloy, showing that it effectively encodes microstructural information and enables direct identification of microstructural heterogeneity not directly possible by physics-based models. This data-reduced microstructure representation opens the application of machine learning models in accelerating metallic material design and accurately predicting their properties."
1012,679d459debd8ffd557a2b261,cs.AI,https://arxiv.org/pdf/2501.18059,Learning the Optimal Stopping for Early Classification within Finite Horizons via Sequential Probability Ratio Test,"Akinori F. Ebihara, Taiki Miyagawa, Kazuyuki Sakurai, Hitoshi Imaoka","Machine Learning, Artificial Intelligence","Time-sensitive machine learning benefits from Sequential Probability Ratio Test (SPRT), which provides an optimal stopping time for early classification of time series. However, infinite horizonscenarios, where input lengths are finite, determining the optimal stopping rule becomes computationally intensive due to the need forbackward induction, limiting practical applicability. We thus introduceFIRMBOUND, an SPRT-based framework that efficiently estimates the solution to backward induction from training data, bridging the gap between optimal stopping theory and real-world deployment. It employsdensity ratio estimationandconvex function learningto provide statistically consistent estimators for sufficient statistic and conditional expectation, both essential for solving backward induction; consequently,FIRMBOUNDminimizes Bayes risk to reach optimality. Additionally, we present a faster alternative using Gaussian process regression, which significantly reduces training time while retaining low deployment overhead, albeit with potential compromise in statistical consistency. Experiments across independent and identically distributed (i.i.d.), non-i.i.d., binary, multiclass, synthetic, and real-world datasets show thatFIRMBOUNDachieves optimalities in the sense of Bayes risk and speed-accuracy tradeoff. Furthermore, it advances the tradeoff boundary toward optimality when possible and reduces decision-time variance, ensuring reliable decision-making. Code is publicly available athttps://github.com/Akinori-F-Ebihara/FIRMBOUND."
1013,679d459debd8ffd557a2b262,cs.AI,https://arxiv.org/pdf/2501.18055,Current Pathology Foundation Models are unrobust to Medical Center Differences,"Edwin D. de Jong, Eric Marcus, Jonas Teuwen","Machine Learning, Artificial Intelligence","Pathology Foundation Models (FMs) hold great promise for healthcare. Before they can be used in clinical practice, it is essential to ensure they are robust to variations between medical centers. We measure whether pathology FMs focus on biological features like tissue and cancer type, or on the well known confounding medical center signatures introduced by staining procedure and other differences."
1014,679d459debd8ffd557a2b263,cs.AI,https://arxiv.org/pdf/2501.18052,SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders,"Bartosz Cywiński, Kamil Deja","Machine Learning, Artificial Intelligence","This document provides a basic paper template and submission guidelines.
Abstracts must be a single paragraph, ideally between 4–6 sentences long.
Gross violations will trigger corrections at the camera-ready phase."
1015,679d459debd8ffd557a2b264,cs.AI,https://arxiv.org/pdf/2501.18016,Digital Twin-Enabled Real-Time Control in Robotic Additive Manufacturing via Soft Actor-Critic Reinforcement Learning,"Matsive Ali, Sandesh Giri, Sen Liu, Qin Yang","Robotics, Artificial Intelligence, Machine Learning, Systems and Control","Smart manufacturing systems increasingly rely on adaptive control mechanisms to optimize complex processes. This research presents a novel approach integrating Soft Actor-Critic (SAC) reinforcement learning with digital twin technology to enable real-time process control in robotic additive manufacturing. We demonstrate our methodology using a Viper X300s robot arm, implementing two distinct control scenarios: static target acquisition and dynamic trajectory following. The system architecture combines Unity’s simulation environment with ROS2 for seamless digital twin synchronization, while leveraging transfer learning to efficiently adapt trained models across tasks. Our hierarchical reward structure addresses common reinforcement learning challenges including local minima avoidance, convergence acceleration, and training stability. Experimental results show rapid policy convergence and robust task execution in both simulated and physical environments, with performance metrics including cumulative reward, value prediction accuracy, policy loss, and discrete entropy coefficient demonstrating the effectiveness of our approach. This work advances the integration of reinforcement learning with digital twins for industrial robotics applications, providing a framework for enhanced adaptive real-time control for smart additive manufacturing process."
1016,679d459debd8ffd557a2b265,cs.AI,https://arxiv.org/pdf/2501.17982,Belief Roadmaps with Uncertain Landmark Evanescence,"Erick Fuentes, Jared Strader, Ethan Fahnestock, Nicholas Roy","Robotics, Artificial Intelligence",
1017,679d459debd8ffd557a2b266,cs.AI,https://arxiv.org/pdf/2501.17980,Limits to AI Growth: The Ecological and Social Consequences of Scaling,"Eshta Bhardwaj, Rohan Alexander, Christoph Becker","Computers and Society, Artificial Intelligence","The accelerating development and deployment of AI technologies depend on the continued ability to scale their infrastructure. This has implied increasing amounts of monetary investment and natural resources. Frontier AI applications have thus resulted in rising financial, environmental, and social costs. While the factors that AI scaling depends on reach its limits, the push for its accelerated advancement and entrenchment continues. In this paper, we provide a holistic review of AI scaling using four lenses (technical, economic, ecological, and social) and review the relationships between these lenses to explore the dynamics of AI growth. We do so by drawing on system dynamics concepts including archetypes such as “limits to growth” to model the dynamic complexity of AI scaling and synthesize several perspectives. Our work maps out the entangled relationships between the technical, economic, ecological and social perspectives and the apparent limits to growth. The analysis explains how industry’s responses to external limits enables continued (but temporary) scaling and how this benefits Big Tech while externalizing social and environmental damages. To avoid an “overshoot and collapse” trajectory, we advocate for realigning priorities and norms around scaling to prioritize sustainable and mindful advancements."
1018,679d459debd8ffd557a2b267,cs.AI,https://arxiv.org/pdf/2501.17917,Deep Ensembles Secretly Perform Empirical Bayes,"Gabriel Loaiza-Ganem, Valentin Villecroze, Yixin Wang","Machine Learning, Artificial Intelligence, Machine Learning","Quantifying uncertainty in neural networks is a highly relevant problem which is essential to many applications. The two predominant paradigms to tackle this task are Bayesian neural networks (BNNs) and deep ensembles. Despite some similarities between these two approaches, they are typically surmised to lack a formal connection and are thus understood as fundamentally different. BNNs are often touted as more principled due to their reliance on the Bayesian paradigm, whereas ensembles are perceived as moread-hoc; yet, deep ensembles tend to empirically outperform BNNs, with no satisfying explanation as to why this is the case. In this work we bridge this gap by showing that deep ensembles performexactBayesian averaging with a posterior obtained with an implicitly learned data-dependent prior. In other words deep ensembles are Bayesian, or more specifically, they implement anempirical Bayesprocedure wherein the prior is learned from the data. This perspective offers two main benefits:(i)𝑖(i)( italic_i )it theoretically justifies deep ensembles and thus provides an explanation for their strong empirical performance; and(i⁢i)𝑖𝑖(ii)( italic_i italic_i )inspection of the learned prior reveals it is given by a mixture of point masses – the use of such a strong prior helps elucidate observed phenomena about ensembles. Overall, our work delivers a newfound understanding of deep ensembles which is not only of interest in it of itself, but which is also likely to generate future insights that drive empirical improvements for these models."
1019,679d459debd8ffd557a2b268,cs.AI,https://arxiv.org/pdf/2501.17903,Free Agent in Agent-Based Mixture-of-Experts Generative AI Framework,Jung-Hua Liu,"Multiagent Systems, Artificial Intelligence",
1020,679d459debd8ffd557a2b269,cs.AI,https://arxiv.org/pdf/2501.17899,The Right to AI,"Rashid Mushkani, Hugo Berard, Allison Cohen, Shin Koeski","Computers and Society, Artificial Intelligence, Human-Computer Interaction","This position paper proposes a “Right to AI,” which asserts that individuals and communities should meaningfully participate in the development and governance of the AI systems that shape their lives.Motivated by the increasing deployment of AI in critical domains and inspired by Henri Lefebvre’s concept of the “Right to the City,” we reconceptualize AI as a societal infrastructure, rather than merely a product of expert design. In this paper, we critically evaluate how generative agents, large-scale data extraction, and diverse cultural values bring new complexities to AI oversight. The paper proposes that grassroots participatory methodologies can mitigate biased outcomes and enhance social responsiveness. It asserts that data is socially produced and should be managed and owned collectively. Drawing on Sherry Arnstein’s Ladder of Citizen Participation and analyzing nine case studies, the paper develops a four-tier model for the Right to AI that situates the current paradigm and envisions an aspirational future. It proposes recommendations for inclusive data ownership, transparent design processes, and stakeholder-driven oversight. We also discuss market-led and state-centric alternatives and argue that participatory approaches offer a better balance between technical efficiency and democratic legitimacy."
1021,679d459debd8ffd557a2b26a,cs.AI,https://arxiv.org/pdf/2501.17894,Progress in Artificial Intelligence and its Determinants,"Michael R. Douglas, Sergiy Verstyuk","General Economics, Artificial Intelligence, Computers and Society, Machine Learning, Physics and Society","We study long-run progress in artificial intelligence in a quantitative way.
Many measures, including traditional ones such as patents and publications, machine learning benchmarks, and a new Aggregate State of the Art in ML (or ASOTA) Index we have constructed from these, show exponential growth at roughly constant rates over long periods.
Production of patents and publications doubles every ten years, by contrast with the growth of computing resources driven by Moore’s Law, roughly a doubling every two years.
We argue that the input of AI researchers is also crucial and its contribution can be objectively estimated.
Consequently, we give a simple argument that explains the 5:1 relation between these two rates.
We then discuss the application of this argument to
different output measures and compare our analyses with predictions based on machine learning scaling laws proposed in existing literature.
Our quantitative framework facilitates understanding, predicting, and modulating the development of these important technologies."
1022,679d459debd8ffd557a2b26b,cs.AI,https://arxiv.org/pdf/2501.17889,Knoop: Practical Enhancement of Knockoff with Over-Parameterization for Variable Selection,"Xiaochen Zhang, Yunfeng Cai, Haoyi Xiong","Machine Learning, Artificial Intelligence, Machine Learning","Variable selection plays a crucial role in enhancing modeling effectiveness across diverse fields, addressing the challenges posed by high-dimensional datasets of correlated variables. This work introduces a novel approach namelyKnockoff withover-parameterization(Knoop) to enhance Knockoff filters for variable selection. Specifically,Knoopfirst generates multiple knockoff variables for each original variable and integrates them with the original variables into an over-parameterized Ridgeless regression model. For each original variable,Knoopevaluates the coefficient distribution of its knockoffs and compares these with the original coefficients to conduct an anomaly-based significance test, ensuring robust variable selection. Extensive experiments demonstrate superior performance compared to existing methods in both simulation and real-world datasets.Knoopachieves a notably higher Area under the Curve (AUC) of the Receiver Operating Characteristic (ROC) Curve for effectively identifying relevant variables against the ground truth by controlled simulations, while showcasing enhanced predictive accuracy across diverse regression and classification tasks. The analytical results further backup our observations."
1023,679d459debd8ffd557a2b26c,cs.AI,https://arxiv.org/pdf/2501.17888,RadioLLM: Introducing Large Language Model into Cognitive Radio via Hybrid Prompt and Token Reprogrammings,"Shuai Chen, Yong Zu, Zhixi Feng, Shuyuan Yang, Mengchang Li, Yue Ma, Jun Liu, Qiukai Pan, Xinlei Zhang, Changjun Sun","Signal Processing, Artificial Intelligence, Machine Learning","The increasing scarcity of spectrum resources and the rapid growth of wireless device have made efficient management of radio networks a critical challenge. Cognitive Radio Technology (CRT), when integrated with deep learning (DL), offers promising solutions for tasks such as radio signal classification (RSC), signal denoising, and spectrum allocation. However, existing DL-based CRT frameworks are often task-specific and lack scalability to diverse real-world scenarios. Meanwhile, Large Language Models (LLMs) have demonstrated exceptional generalization capabilities across multiple domains, making them a potential candidate for advancing CRT technologies. In this paper, we introduce RadioLLM, a novel framework that incorporates Hybrid Prompt and Token Reprogramming (HPTR) and a Frequency Attuned Fusion (FAF) module to enhance LLMs for CRT tasks. HPTR enables the integration of radio signal features with expert knowledge, while FAF improves the modeling of high-frequency features critical for precise signal processing. These innovations allow RadioLLM to handle diverse CRT tasks, bridging the gap between LLMs and traditional signal processing methods. Extensive empirical studies on multiple benchmark datasets demonstrate that the proposed RadioLLM achieves superior performance over current baselines."
1024,679d459debd8ffd557a2b26d,cs.AI,https://arxiv.org/pdf/2501.17883,Explainable and Robust Millimeter Wave Beam Alignment for AI-Native 6G Networks,"Nasir Khan, Asmaa Abdallah, Abdulkadir Celik, Ahmed M. Eltawil, Sinem Coleri","Signal Processing, Artificial Intelligence","Integrated artificial intelligence (AI) and communication has been recognized as a key pillar of 6G and beyond networks. In line with AI-native 6G vision, explainability and robustness in AI-driven systems are critical for establishing trust and ensuring reliable performance in diverse and evolving environments. This paper addresses these challenges by developing a robust and explainable deep learning (DL)-based beam alignment engine (BAE) for millimeter-wave (mmWave) multiple-input multiple-output (MIMO) systems. The proposed convolutional neural network (CNN)-based BAE utilizes received signal strength indicator (RSSI) measurements over a set of wide beams to accurately predict the
best narrow beam for each UE, significantly reducing the overhead associated with exhaustive codebook-based narrow beam sweeping for initial access (IA) and data transmission. To ensure transparency and resilience, the Deep k-Nearest Neighbors (DkNN) algorithm is employed to assess the internal representations of the network via nearest neighbor approach, providing human-interpretable explanations and confidence metrics for detecting out-of-distribution inputs. Experimental results demonstrate that the proposed DL-based BAE exhibits robustness to measurement noise, reduces beam training overhead by 75% compared to the exhaustive search while maintaining near-optimal performance in terms of spectral efficiency. Moreover, the proposed framework improves outlier detection robustness by up to 5×\times×and offers clearer insights into beam prediction decisions compared to traditional softmax-based classifiers."
1025,679d459debd8ffd557a2b26e,cs.AI,https://arxiv.org/pdf/2501.17881,RayLoc: Wireless Indoor Localization via Fully Differentiable Ray-tracing,"Xueqiang Han, Tianyue Zheng, Tony Xiao Han, Jun Luo","Signal Processing, Artificial Intelligence, Machine Learning, Networking and Internet Architecture","Wireless indoor localization has been a pivotal area of research over the last two decades, becoming a cornerstone for numerous sensing applications. However, conventional wireless localization methods rely on channel state information to perform blind modelling and estimation of a limited set of localization parameters. This oversimplification neglects many sensing scene details, resulting in suboptimal localization accuracy. To address this limitation, this paper presents a novel approach to wireless indoor localization by reformulating it as an inverse problem of wireless ray-tracing, inferring scene parameters that generates the measured CSI. At the core of our solution is a fully differentiable ray-tracing simulator that enables backpropagation to comprehensive parameters of the sensing scene, allowing for precise localization. To establish a robust localization context, RayLoc constructs a high-fidelity sensing scene by refining coarse-grained background model. Furthermore, RayLoc overcomes the challenges of sparse gradient and local minima by convolving the signal generation process with a Gaussian kernel. Extensive experiments showcase that RayLoc outperforms traditional localization baselines and is able to generalize to different sensing environments."
1026,679d459debd8ffd557a2b26f,cs.AI,https://arxiv.org/pdf/2501.17880,"Assessment of the January 2025 Los Angeles County wildfires: A multi-modal analysis of impact, response, and population exposure",Seyd Teymoor Seydi,"Signal Processing, Artificial Intelligence, Machine Learning, Numerical Analysis","This study presents a comprehensive analysis of four significant California wildfires—Palisades, Eaton, Kenneth, and Hurst—examining their impacts through multiple dimensions, including land cover change, jurisdictional management, structural damage, and demographic vulnerability. Utilizing the Chebyshev-Kolmogorov-Arnold network (Cheby-KAN) model, applied to Sentinel-2 imagery, the extent of burned areas was mapped, ranging from 315.36 to 10,960.98 hectares. Our analysis revealed that shrubland ecosystems were consistently the most affected, comprising 57.4-75.8% of burned areas across all events. The jurisdictional assessment demonstrated varying management complexities, ranging from singular authority (98.7% in the Palisades Fire) to distributed management across multiple agencies. A subsequent structural impact analysis revealed significant disparities between urban interface fires (Eaton: 9,869 structures; Palisades: 8,436 structures) and rural events (Kenneth: 24 structures; Hurst: 17 structures). The demographic analysis revealed consistent gender distributions, with 50.9% of the population identified as female and 49.1% as male, across all events. The analysis also identified that working-age populations constituted the majority of the affected populations, ranging from 53.7% to 54.1%, and exhibited notable temporal shifts in post-fire periods. The study identified strong correlations between urban interface proximity, structural damage, and population exposure. The Palisades and Eaton fires affected over 20,000 people each, compared to fewer than 500 in rural events. These findings offer pivotal insights for the formulation of targeted wildfire management strategies, particularly in wildland-urban interface zones, and underscore the necessity for age- and gender-conscious approaches in emergency response planning."
1027,679d459debd8ffd557a2b270,cs.AI,https://arxiv.org/pdf/2501.17879,Task and Perception-aware Distributed Source Coding for Correlated Speech under Bandwidth-constrained Channels,"Sagnik Bhattacharya, Muhammad Ahmed Mohsin, Ahsan Bilal, John M. Cioffi","Information Theory, Artificial Intelligence, Sound, Audio and Speech Processing, Signal Processing","Emerging wireless AR/VR applications require real-time transmission of correlated high-fidelity speech from multiple resource-constrained devices over unreliable, bandwidth-limited channels. Existing autoencoder-based speech source coding methods fail to address the combination of the following - (1) dynamic bitrate adaptation without retraining the model, (2) leveraging correlations among multiple speech sources, and (3) balancing downstream task loss with realism of reconstructed speech. We propose a neural distributed principal component analysis (NDPCA)-aided distributed source coding algorithm for correlated speech sources transmitting to a central receiver. Our method includes a perception-aware downstream task loss function that balances perceptual realism with task-specific performance. Experiments show significant PSNR improvements under bandwidth constraints over naive autoencoder methods in task-agnostic (19%) and task-aware settings (52%). It also approaches the theoretical upper bound, where all correlated sources are sent to a single encoder, especially in low-bandwidth scenarios. Additionally, we present a rate-distortion-perception trade-off curve, enabling adaptive decisions based on application-specific realism needs."
1028,679d459debd8ffd557a2b271,cs.AI,https://arxiv.org/pdf/2501.17704,Inferring Implicit Goals Across Differing Task Models,"Silvia Tulli, Stylianos Loukas Vasileiou, Mohamed Chetouani, Sarath Sreedharan","Artificial Intelligence, Robotics, Systems and Control","One of the significant challenges to generating value-aligned behavior is to not only account for the specified user objectives but also any implicit or unspecified user requirements. The existence of such implicit requirements could be particularly common in settings where the user’s understanding of the task model may differ from the agent’s estimate of the model. Under this scenario, the user may incorrectly expect some agent behavior to be inevitable or guaranteed. This paper addresses such expectation mismatch in the presence of differing models by capturing the possibility of unspecified user subgoal in the context of a task captured as a Markov Decision Process (MDP) and querying for it as required. Our method identifies bottleneck states and uses them as candidates for potential implicit subgoals. We then introduce a querying strategy that will generate the minimal number of queries required to identify a policy guaranteed to achieve the underlying goal. Our empirical evaluations demonstrate the effectiveness of our approach in inferring and achieving unstated goals across various tasks."
1029,679d459debd8ffd557a2b272,cs.AI,https://arxiv.org/pdf/2501.17559,"Solving Urban Network Security Games: Learning Platform, Benchmark, and Challenge for AI Research","Shuxin Zhuang, Shuxin Li, Tianji Yang, Muheng Li, Xianjie Shi, Bo An, Youzhi Zhang","Artificial Intelligence, Computer Science and Game Theory","After the great achievement of solving two-player zero-sum games, more and more AI researchers focus on solving multiplayer games. To facilitate the development of designing efficient learning algorithms for solving multiplayer games, we propose a multiplayer game platform for solving Urban Network Security Games (UNSG) that model real-world scenarios. That is,
preventing criminal activity is a highly significant responsibility assigned to police officers in cities, and police officers have to allocate their limited security resources to interdict the escaping criminal when a crime takes place in a city. This interaction between multiple police officers and the escaping criminal can be modeled as a UNSG. The variants of UNSGs can model different real-world settings, e.g., whether real-time information is available or not, and whether police officers can communicate or not.
The main challenges of solving this game include the large size of the game and the co-existence of cooperation and competition.
While previous efforts have been made to tackle UNSGs, they have been hampered by performance and scalability issues. Therefore, we propose an open-source UNSG platform (GraphChase) for designing efficient learning algorithms for solving UNSGs.
Specifically, GraphChase offers a unified and flexible game environment for modeling various variants of UNSGs, supporting the development, testing, and benchmarking of algorithms. We believe that GraphChase not only facilitates the development of efficient algorithms for solving real-world problems but also paves the way for significant advancements in algorithmic development for solving general multiplayer games."
1030,679d459debd8ffd557a2b273,cs.AI,https://arxiv.org/pdf/2501.17507,"Reflections on ""Can AI Understand Our Universe?""",Yu Wang,"Artificial Intelligence, High Energy Astrophysical Phenomena, Instrumentation and Methods for Astrophysics","This article briefly discusses the philosophical and technical aspects of AI. It focuses on two concepts of understanding: intuition and causality, and highlights three AI technologies: Transformers, chain-of-thought reasoning, and multimodal processing. We anticipate that in principle AI could form understanding, with these technologies representing promising advancements."
1031,679d459debd8ffd557a2b274,cs.AI,https://arxiv.org/pdf/2501.17496,SemML: Enhancing Automata-Theoretic LTL Synthesis with Machine Learning,"Jan Kretinsky, Tobias Meggendorfer, Maximilian Prokop, Ashkan Zarkhah","Artificial Intelligence, Systems and Control",
1032,679d459debd8ffd557a2b275,cs.AI,https://arxiv.org/pdf/2501.17493,Certifying Pareto-Optimality in Multi-Objective Maximum Satisfiability,"Christoph Jabs, Jeremias Berg, Bart Bogaerts, Matti Järvisalo",Artificial Intelligence,"Due to the wide employment of automated reasoning in
the analysis and construction of correct systems, the results reported
by automated reasoning engines must be trustworthy.
For Boolean satisfiability (SAT) solvers—and more recently
SAT-based maximum satisfiability (MaxSAT) solvers—trustworthiness is
obtained by integrating proof logging into solvers, making solvers
capable of emitting machine-verifiable proofs to certify correctness of the reasoning steps performed.
In this work, we enable for the first time
proof logging based on theVeriPBproof format for multi-objective MaxSAT (MO-MaxSAT) optimization
techniques.
AlthoughVeriPBdoes not offer direct support for multi-objective problems,
we detail how preorders inVeriPBcan be used
to provide certificates for
MO-MaxSAT algorithms computing a representative solution for each element in the non-dominated
set of the search space under Pareto-optimality,
without extending theVeriPBformat or the proof checker.
By implementingVeriPBproof logging into a state-of-the-art multi-objective MaxSAT solver,
we show empirically that proof logging can be made scalable for MO-MaxSAT with reasonable overhead."
1033,679d459debd8ffd557a2b276,cs.AI,https://arxiv.org/pdf/2501.17393,Intensional Inheritance Between Concepts: An Information-Theoretic Interpretation,Ben Goertzel,"Artificial Intelligence, Information Theory","This paper addresses the problem of formalizing and quantifying the concept of ”intensional inheritance” between two concepts. We begin by conceiving the intensional inheritance ofW𝑊Witalic_WfromF𝐹Fitalic_Fas the amount of information the proposition ""x isF𝐹Fitalic_F"" provides about the proposition ""x isW𝑊Witalic_W. To flesh this out, we consider conceptsF𝐹Fitalic_FandW𝑊Witalic_Wdefined by sets of properties{F1,F2,…,Fn}subscript𝐹1subscript𝐹2…subscript𝐹𝑛\left\{F_{1},F_{2},\ldots,F_{n}\right\}{ italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_F start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }and{W1,W2,…,Wm}subscript𝑊1subscript𝑊2…subscript𝑊𝑚\left\{W_{1},W_{2},\ldots,W_{m}\right\}{ italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_W start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT }with associated degrees{d1,d2,…,dn}subscript𝑑1subscript𝑑2…subscript𝑑𝑛\left\{d_{1},d_{2},\ldots,d_{n}\right\}{ italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_d start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }and{e1,e2,…,em}subscript𝑒1subscript𝑒2…subscript𝑒𝑚\left\{e_{1},e_{2},\ldots,e_{m}\right\}{ italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_e start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT }, respectively, where the properties may overlap. We then derive formulas for the intensional inheritance using both Shannon information theory and algorithmic information theory, incorporating interaction information among properties. We examine a special case where all properties are mutually exclusive and calculate the intensional inheritance in this case in both frameworks. We also derive expressions forP⁢(W∣F)𝑃conditional𝑊𝐹P(W\mid F)italic_P ( italic_W ∣ italic_F )based on the mutual information formula. Finally we consider the relationship between intensional inheritance and conventional set-theoretic ""extensional"" inheritance, concluding that in our information-theoretic framework, extensional inheritance emerges as a special case of intensional inheritance."
1034,679d459debd8ffd557a2b277,cs.AI,https://arxiv.org/pdf/2501.17310,Probing LLM World Models: Enhancing Guesstimation with Wisdom of Crowds Decoding,"Yun-Shiuan Chuang, Nikunj Harlalka, Sameer Narendran, Alexander Cheung, Sizhe Gao, Siddharth Suresh, Junjie Hu, Timothy T. Rogers","Artificial Intelligence, Human-Computer Interaction","Guesstimation, the task of making approximate quantity estimates, is a common real-world challenge. However, it has been largely overlooked in large language models (LLMs) and vision language models (VLMs) research. We introduce a novel guesstimation dataset,MARBLES. This dataset requires one to estimate how many items (e.g., marbles) can fit into containers (e.g., a one-cup measuring cup), both with and without accompanying images. Inspired by the social science concept of the “Wisdom of Crowds” (WOC) - taking the median from estimates from a crowd), which has proven effective in guesstimation, we propose “WOC decoding” strategy for LLM guesstimation. We show that LLMs/VLMs perform well on guesstimation, suggesting that they possess some level of a ""world model"" necessary for guesstimation. Moreover, similar to human performance, the WOC decoding method improves LLM/VLM guesstimation accuracy. Furthermore, the inclusion of images in the multimodal condition enhances model performance. These results highlight the value of WOC decoding strategy for LLMs/VLMs and position guesstimation as a probe for evaluating LLMs/VLMs’ world model. As LLMs’ world model is a fundamental prerequisite for many real-world tasks, e.g., human-AI teaming, our findings have broad implications for the AI community."
1035,679d459debd8ffd557a2b278,cs.AI,https://arxiv.org/pdf/2501.17206,Integrating Reinforcement Learning and AI Agents for Adaptive Robotic Interaction and Assistance in Dementia Care,"Fengpei Yuan, Nehal Hasnaeen, Ran Zhang, Bryce Bible, Joseph Riley Taylor, Hairong Qi, Fenghui Yao, Xiaopeng Zhao","Artificial Intelligence, Robotics","This study explores a novel approach to advancing dementia care by integrating socially assistive robotics, reinforcement learning (RL), large language models (LLMs), and clinical domain expertise within a simulated environment. This integration addresses the critical challenge of limited experimental data in socially assistive robotics for dementia care, providing a dynamic simulation environment that realistically models interactions between persons living with dementia (PLWDs) and robotic caregivers. The proposed framework introduces a probabilistic model to represent the cognitive and emotional states of PLWDs, combined with an LLM-based behavior simulation to emulate their responses. We further develop and train an adaptive RL system enabling humanoid robots, such as Pepper, to deliver context-aware and personalized interactions and assistance based on PLWDs’ cognitive and emotional states. The framework also generalizes to computer-based agents, highlighting its versatility. Results demonstrate that the RL system, enhanced by LLMs, effectively interprets and responds to the complex needs of PLWDs, providing tailored caregiving strategies. This research contributes to human-computer and human-robot interaction by offering a customizable AI-driven caregiving platform, advancing understanding of dementia-related challenges, and fostering collaborative innovation in assistive technologies. The proposed approach has the potential to enhance the independence and quality of life for PLWDs while alleviating caregiver burden, underscoring the transformative role of interaction-focused AI systems in dementia care."
1036,679d459debd8ffd557a2b279,cs.AI,https://arxiv.org/pdf/2501.17201,Smart Cubing for Graph Search: A Comparative Study,"Markus Kirchweger, Hai Xia, Tomáš Peitl, Stefan Szeider",Artificial Intelligence,
1037,679d459debd8ffd557a2b27a,cs.AI,https://arxiv.org/pdf/2501.17188,"Letters, Colors, and Words: Constructing the Ideal Building Blocks Set","Ricardo Salazar, Shahrzad Jamshidi","Artificial Intelligence, Neural and Evolutionary Computing","Definea building blocks setto be a collection ofn𝑛nitalic_ncubes (each with six sides) where each side is assigned one letter and one color from a palette ofm𝑚mitalic_mcolors. We propose a novel problem of assigning letters and colors to each face so as to maximize the number of words one can spell from a chosen dataset that are eithermono words, all letters have the same color, orrainbow words, all letters have unique colors. We explore this problem considering a chosen set of English words, up to six letters long, from a typical vocabulary of a US American 14 year old and explore the problem whenn=6𝑛6n=6italic_n = 6andm=6𝑚6m=6italic_m = 6, with the added restriction that each color appears exactly once on the cube. The problem is intractable, as the size of the solution space makes a brute force approach computationally infeasible. Therefore we aim to solve this problem using random search, simulated annealing, two distinct tree search approaches (greedy and best-first), and a genetic algorithm. To address this, we explore a range of optimization techniques: random search, simulated annealing, two distinct tree search methods (greedy and best-first), and a genetic algorithm. Additionally, we attempted to implement a reinforcement learning approach; however, the model failed to converge to viable solutions within the problem’s constraints. Among these methods, the genetic algorithm delivered the best performance, achieving a total of 2846 mono and rainbow words.Keywords:Permutations, Simulated Annealing, Tree-search, genetic-algorithm."
1038,679d459debd8ffd557a2b27b,cs.AI,https://arxiv.org/pdf/2501.17855,GRACE: Generalizing Robot-Assisted Caregiving with User Functionality Embeddings,"Ziang Liu, Yuanchen Ju, Yu Da, Tom Silver, Pranav N. Thakkar, Jenna Li, Justin Guo, Katherine Dimitropoulou, Tapomayukh Bhattacharjee","Robotics, Artificial Intelligence, Human-Computer Interaction","Robot caregiving should be personalized to meet the diverse needs of care recipients—assisting with tasks as needed, while taking user agency in action into account.
In physical tasks such as handover, bathing, dressing, and rehabilitation, a key aspect of this diversity is the functional range of motion (fROM), which can vary significantly between individuals.
In this work, we learn to predict personalized fROM as a way to generalize robot decision-making in a wide range of caregiving tasks.
We propose a novel data-driven method for predicting personalized fROM using functional assessment scores from occupational therapy.
We develop a neural model that learns to embed functional assessment scores into a latent representation of the user’s physical function.
The model is trained using motion capture data collected from users with emulated mobility limitations.
After training, the model predicts personalized fROM for new users without motion capture.
Through simulated experiments and a real-robot user study, we show that the personalized fROM predictions from our model enable the robot to provide personalized and effective assistance while improving the user’s agency in action. See our website for more visualizations:https://emprise.cs.cornell.edu/grace/."
1039,679d459debd8ffd557a2b27c,cs.AI,https://arxiv.org/pdf/2501.17842,From Sparse to Dense: Toddler-inspired Reward Transition in Goal-Oriented Reinforcement Learning,"Junseok Park, Hyeonseo Yang, Min Whoo Lee, Won-Seok Choi, Minsu Lee, Byoung-Tak Zhang","Machine Learning, Artificial Intelligence, Robotics","Reinforcement learning (RL) agents often face challenges in balancing exploration and exploitation, particularly in environments where sparse or dense rewards bias learning. Biological systems, such as human toddlers, naturally navigate this balance by transitioning from free exploration with sparse rewards to goal-directed behavior guided by increasingly dense rewards. Inspired by this natural progression, we investigate theToddler-Inspired Reward Transitionin goal-oriented RL tasks. Our study focuses ontransitioning from sparse to potential-based dense (S2D) rewardswhile preserving optimal strategies. Through experiments on dynamic robotic arm manipulation and egocentric 3D navigation tasks, we demonstrate that effective S2D reward transitions significantly enhance learning performance and sample efficiency. Additionally, using a Cross-Density Visualizer, we show that S2D transitions smooth the policy loss landscape, resulting in wider minima that improve generalization in RL models. In addition, we reinterpret Tolman’s maze experiments, underscoring the critical role of early free exploratory learning in the context of S2D rewards."
1040,679d459debd8ffd557a2b27d,cs.AI,https://arxiv.org/pdf/2501.17805,International AI Safety Report,"Yoshua Bengio, Sören Mindermann, Daniel Privitera, Tamay Besiroglu, Rishi Bommasani, Stephen Casper, Yejin Choi, Philip Fox, Ben Garfinkel, Danielle Goldfarb, Hoda Heidari, Anson Ho, Sayash Kapoor, Leila Khalatbari, Shayne Longpre, Sam Manning, Vasilios Mavroudis, Mantas Mazeika, Julian Michael, Jessica Newman, Kwan Yee Ng, Chinasa T. Okolo, Deborah Raji, Girish Sastry, Elizabeth Seger, Theodora Skeadas, Tobin South, Emma Strubell, Florian Tramèr, Lucia Velasco, Nicole Wheeler, Daron Acemoglu, Olubayo Adekanmbi, David Dalrymple, Thomas G. Dietterich, Edward W. Felten, Pascale Fung, Pierre-Olivier Gourinchas, Fredrik Heintz, Geoffrey Hinton, Nick Jennings, Andreas Krause, Susan Leavy, Percy Liang, Teresa Ludermir, Vidushi Marda, Helen Margetts, John McDermid, Jane Munga, Arvind Narayanan, Alondra Nelson, Clara Neppel, Alice Oh, Gopal Ramchurn, Stuart Russell, Marietje Schaake, Bernhard Schölkopf, Dawn Song, Alvaro Soto, Lee Tiedrich, Gaël Varoquaux, Andrew Yao, Ya-Qin Zhang, Fahad Albalawi, Marwan Alserkal, Olubunmi Ajala, Guillaume Avrin, Christian Busch, André Carlos Ponce de Leon Ferreira de Carvalho, Bronwyn Fox, Amandeep Singh Gill, Ahmet Halit Hatip, Juha Heikkilä, Gill Jolly, Ziv Katzir, Hiroaki Kitano, Antonio Krüger, Chris Johnson, Saif M. Khan, Kyoung Mu Lee, Dominic Vincent Ligot, Oleksii Molchanovskyi, Andrea Monti, Nusu Mwamanzi, Mona Nemer, Nuria Oliver, José Ramón López Portillo, Balaraman Ravindran, Raquel Pezoa Rivera, Hammam Riza, Crystal Rugege, Ciarán Seoighe, Jerry Sheehan, Haroon Sheikh, Denise Wong, Yi Zeng","Computers and Society, Artificial Intelligence, Machine Learning",
1041,679d459debd8ffd557a2b27e,cs.AI,https://arxiv.org/pdf/2501.17759,Yin-Yang: Developing Motifs With Long-Term Structure And Controllability,"Keshav Bhandari, Geraint A. Wiggins, Simon Colton","Sound, Artificial Intelligence, Symbolic Computation","Transformer models have made great strides in generating symbolically represented music with local coherence. However, controlling the development of motifs in a structured way with global form remains an open research area. One of the reasons for this challenge is due to the note-by-note autoregressive generation of such models, which lack the ability to correct themselves after deviations from the motif. In addition, their structural performance on datasets with shorter durations has not been studied in the literature. In this study, we propose Yin-Yang, a framework consisting of a phrase generator, phrase refiner, and phrase selector models for the development of motifs into melodies with long-term structure and controllability. The phrase refiner is trained on a novel corruption-refinement strategy which allows it to produce melodic and rhythmic variations of an original motif at generation time, thereby rectifying deviations of the phrase generator. We also introduce a new objective evaluation metric for quantifying how smoothly the motif manifests itself within the piece. Evaluation results show that our model achieves better performance compared to state-of-the-art transformer models while having the advantage of being controllable and making the generated musical structure semi-interpretable, paving the way for musical analysis. Our code and demo page can be found at https://github.com/keshavbhandari/yinyang."
1042,679d459debd8ffd557a2b27f,cs.AI,https://arxiv.org/pdf/2501.17755,AI Governance through Markets,"Philip Moreira Tomei, Rupal Jain, Matija Franklin","General Economics, Artificial Intelligence","This paper argues that market governance mechanisms should be considered a key approach in the governance of artificial intelligence (AI), alongside traditional regulatory frameworks. While current governance approaches have predominantly focused on regulation, we contend that market-based mechanisms offer effective incentives for responsible AI development. We examine four emerging vectors of market governance: insurance, auditing, procurement, and due diligence, demonstrating how these mechanisms can affirm the relationship between AI risk and financial risk while addressing capital allocation inefficiencies. While we do not claim that market forces alone can adequately protect societal interests, we maintain that standardised AI disclosures and market mechanisms can create powerful incentives for safe and responsible AI development. This paper urges regulators, economists, and machine learning researchers to investigate and implement market-based approaches to AI governance."
1043,679d459debd8ffd557a2b280,cs.AI,https://arxiv.org/pdf/2501.17749,Early External Safety Testing of OpenAI's o3-mini: Insights from the Pre-Deployment Evaluation,"Aitor Arrieta, Miriam Ugarte, Pablo Valle, José Antonio Parejo, Sergio Segura","Software Engineering, Artificial Intelligence","Large Language Models (LLMs) have become an integral part of our daily lives. However, they impose certain risks, including those that can harm individuals’ privacy, perpetuate biases and spread misinformation. These risks highlight the need for robust safety mechanisms, ethical guidelines, and thorough testing to ensure their responsible deployment. Safety of LLMs is a key property that needs to be thoroughly tested prior the model to be deployed and accessible to the general users. This paper reports the external safety testing experience conducted by researchers from Mondragon University and University of Seville on OpenAI’s new o3-mini LLM as part of OpenAI’s early access for safety testing program111https://openai.com/index/early-access-for-safety-testing/. In particular, we apply our tool, ASTRAL, to automatically and systematically generate up to date unsafe test inputs (i.e., prompts) that helps us test and assess different safety categories of LLMs. We automatically generate and execute a total of 10,080 unsafe test input on a early o3-mini beta version. After manually verifying the test cases classified as unsafe byASTRAL, we identify a total of 87 actual instances of unsafe LLM behavior. We highlight key insights and findings uncovered during the pre-deployment external testing phase of OpenAI’s latest LLM."
1044,679d459debd8ffd557a2b281,cs.AI,https://arxiv.org/pdf/2501.17731,Exact characterization of {\epsilon}-Safe Decision Regions for exponential family distributions and Multi Cost SVM approximation,"Alberto Carlevaro, Teodoro Alamo, Fabrizio Dabbene, Maurizio Mongelli","Machine Learning, Artificial Intelligence, Machine Learning","Probabilistic guarantees on the prediction of data-driven classifiers are necessary to define models that can be considered reliable. This is a key requirement for modern machine learning in which the goodness of a system is measured in terms of trustworthiness, clearly dividing what issafefrom what isunsafe. The spirit of this paper is exactly in this direction. First, we introduce a formal definition ofε𝜀\varepsilonitalic_ε-Safe Decision Region, a subset of the input space in which the prediction of a target (safe) class is probabilistically guaranteed. Second, we prove that, when data come from exponential family distributions, the form of such a region is analytically determined and controllable by design parameters, i.e. the probability of sampling the target class and the confidence on the prediction. However, the request of having exponential data is not always possible. Inspired by this limitation, we developed Multi Cost SVM, an SVM based algorithm that approximates the safe region and is also able to handle unbalanced data. The research is complemented by experiments and code available for reproducibility."
1045,679d459debd8ffd557a2b282,cs.AI,https://arxiv.org/pdf/2501.17665,Planning with Vision-Language Models and a Use Case in Robot-Assisted Teaching,"Xuzhe Dang, Lada Kudláčková, Stefan Edelkamp","Robotics, Artificial Intelligence","Automating the generation of Planning Domain Definition Language (PDDL) with Large Language Model (LLM) opens new research topic in AI planning, particularly for complex real-world tasks. This paper introduces Image2PDDL, a novel framework that leverages Vision-Language Models (VLMs) to automatically convert images of initial states and descriptions of goal states into PDDL problems. By providing a PDDL domain alongside visual inputs, Imasge2PDDL addresses key challenges in bridging perceptual understanding with symbolic planning, reducing the expertise required to create structured problem instances, and improving scalability across tasks of varying complexity. We evaluate the framework on various domains, including standard planning domains like blocksworld and sliding tile puzzles, using datasets with multiple difficulty levels. Performance is assessed on syntax correctness, ensuring grammar and executability, and content correctness, verifying accurate state representation in generated PDDL problems. The proposed approach demonstrates promising results across diverse task complexities, suggesting its potential for broader applications in AI planning.
We will discuss a potential use case in robot-assisted teaching of students with Autism Spectrum Disorder."
1046,679d459debd8ffd557a2b283,cs.AI,https://arxiv.org/pdf/2501.17629,The Imitation Game According To Turing,"Sharon Temtsin, Diane Proudfoot, David Kaber, Christoph Bartneck","Human-Computer Interaction, Artificial Intelligence, Computers and Society",
1047,679d459debd8ffd557a2b284,cs.AI,https://arxiv.org/pdf/2501.17612,VoicePrompter: Robust Zero-Shot Voice Conversion with Voice Prompt and Conditional Flow Matching,"Ha-Yeong Choi, Jaehan Park","Sound, Artificial Intelligence, Audio and Speech Processing, Signal Processing","Despite remarkable advancements in recent voice conversion (VC) systems, enhancing speaker similarity in zero-shot scenarios remains challenging. This challenge arises from the difficulty of generalizing and adapting speaker characteristics in speech within zero-shot environments, which is further complicated by mismatch between the training and inference processes. To address these challenges, we propose VoicePrompter, a robust zero-shot VC model that leverages in-context learning with voice prompts. VoicePrompter is composed of (1) a factorization method that disentangles speech components and (2) a DiT-based conditional flow matching (CFM) decoder that conditions on these factorized features and voice prompts. Additionally, (3) latent mixup is used to enhance in-context learning by combining various speaker features. This approach improves speaker similarity and naturalness in zero-shot VC by applying mixup to latent representations. Experimental results demonstrate that VoicePrompter outperforms existing zero-shot VC systems in terms of speaker similarity, speech intelligibility, and audio quality. Our demo is available athttps://hayeong0.github.io/VoicePrompter-demo/."
1048,679d459debd8ffd557a2b285,cs.AI,https://arxiv.org/pdf/2501.17578,Music2Latent2: Audio Compression with Summary Embeddings and Autoregressive Decoding,"Marco Pasini, Stefan Lattner, George Fazekas","Sound, Artificial Intelligence, Machine Learning, Audio and Speech Processing","Efficiently compressing high-dimensional audio signals into a compact and informative latent space is crucial for various tasks, including generative modeling and music information retrieval (MIR). Existing audio autoencoders, however, often struggle to achieve high compression ratios while preserving audio fidelity and facilitating efficient downstream applications. We introduce Music2Latent2, a novel audio autoencoder that addresses these limitations by leveraging consistency models and a novel approach to representation learning based on unordered latent embeddings, which we callsummary embeddings. Unlike conventional methods that encode local audio features into ordered sequences, Music2Latent2 compresses audio signals into sets of summary embeddings, where each embedding can capture distinct global features of the input sample. This enables to achieve higher reconstruction quality at the same compression ratio. To handle arbitrary audio lengths, Music2Latent2 employs an autoregressive consistency model trained on two consecutive audio chunks with causal masking, ensuring coherent reconstruction across segment boundaries. Additionally, we propose a novel two-step decoding procedure that leverages the denoising capabilities of consistency models to further refine the generated audio at no additional cost. Our experiments demonstrate that Music2Latent2 outperforms existing continuous audio autoencoders regarding audio quality and performance on downstream tasks. Music2Latent2 paves the way for new possibilities in audio compression."
1049,679d459debd8ffd557a2b286,cs.AI,https://arxiv.org/pdf/2501.17567,Exploring the Potential of Wireless-enabled Multi-Chip AI Accelerators,"Emmanuel Irabor, Mariam Musavi, Abhijit Das, Sergi Abadal","Hardware Architecture, Artificial Intelligence","The insatiable appetite of Artificial Intelligence (AI) workloads for computing power is pushing the industry to develop faster and more efficient accelerators. The rigidity of custom hardware, however, conflicts with the need for scalable and versatile architectures capable of catering to the needs of the evolving and heterogeneous pool of Machine Learning (ML) models in the literature. In this context, multi-chiplet architectures assembling multiple (perhaps heterogeneous) accelerators are an appealing option that is unfortunately hindered by the still rigid and inefficient chip-to-chip interconnects. In this paper, we explore the potential of wireless technology as a complement to existing wired interconnects in this multi-chiplet approach. Using an evaluation framework from the state-of-the-art, we show that wireless interconnects can lead to speedups of 10% on average and 20% maximum. We also highlight the importance of load balancing between the wired and wireless interconnects, which will be further explored in future work."
1050,679d459debd8ffd557a2b287,cs.AI,https://arxiv.org/pdf/2501.17546,Is Conversational XAI All You Need? Human-AI Decision Making With a Conversational XAI Assistant,"Gaole He, Nilay Aishwarya, Ujwal Gadiraju","Human-Computer Interaction, Artificial Intelligence","Explainable artificial intelligence (XAI) methods are being proposed to help interpret and understand how AI systems reach specific predictions. Inspired by prior work on conversational user interfaces, we argue that augmenting existing XAI methods with conversational user interfaces can increase user engagement and boost user understanding of the AI system. In this paper, we explored the impact of a conversational XAI interface on users’ understanding of the AI system, their trust, and reliance on the AI system.
In comparison to an XAI dashboard, we found that the conversational XAI interface can bring about a better understanding of the AI system among users and higher user trust. However, users of both the XAI dashboard and conversational XAI interfaces showed clear over-reliance on the AI system. Enhanced conversations powered by large language model (LLM) agents amplified over-reliance.
Based on our findings, we reason that the potential cause of such over-reliance is the illusion of explanatory depth that is concomitant with both XAI interfaces.
Our findings have important implications for designing effective conversational XAI interfaces to facilitate appropriate reliance and improve human-AI collaboration."
1051,679d459debd8ffd557a2b288,cs.AI,https://arxiv.org/pdf/2501.17518,RegD: Hierarchical Embeddings via Distances over Geometric Regions,"Hui Yang, Jiaoyan Chen","Machine Learning, Artificial Intelligence","Hierarchical data are common in many domains like life sciences and e-commerce, and their embeddings often play a critical role.
Although hyperbolic embeddings offer a grounded approach to representing hierarchical structures in low-dimensional spaces, their utility is hindered by optimization difficulties in hyperbolic space and dependence on handcrafted structural constraints. We propose RegD, a novel Euclidean framework that addresses these limitations by representing hierarchical data as geometric regions with two new metrics: (1) depth distance, which preserves the representational power of hyperbolic spaces for hierarchical data, and (2) boundary distance, which explicitly encodes set-inclusion relationships between regions in a general way.
Our empirical evaluation on diverse real-world datasets shows consistent performance gains over state-of-the-art methods and demonstrates RegD’s potential for broader applications beyond hierarchy alone tasks."
1052,679d459debd8ffd557a2b289,cs.AI,https://arxiv.org/pdf/2501.17489,Neural Spelling: A Spell-Based BCI System for Language Neural Decoding,"Xiaowei Jiang, Charles Zhou, Yiqun Duan, Ziyi Zhao, Thomas Do, Chin-Teng Lin","Human-Computer Interaction, Artificial Intelligence","Brain-computer interfaces (BCIs) present a promising avenue by translating neural activity directly into text, eliminating the need for physical actions. However, existing non-invasive BCI systems have not successfully covered the entire alphabet, limiting their practicality. In this paper, we propose a novel non-invasive EEG-based BCI system with Curriculum-based Neural Spelling Framework, which recognizes all 26 alphabet letters by decoding neural signals associated with handwriting first, and then apply a Generative AI (GenAI) to enhance spell-based neural language decoding tasks. Our approach combines the ease of handwriting with the accessibility of EEG technology, utilizing advanced neural decoding algorithms and pre-trained large language models (LLMs) to translate EEG patterns into text with high accuracy. This system show how GenAI can improve the performance of typical spelling-based neural language decoding task, and addresses the limitations of previous methods, offering a scalable and user-friendly solution for individuals with communication impairments, thereby enhancing inclusive communication options."
1053,679d459debd8ffd557a2b28a,cs.AI,https://arxiv.org/pdf/2501.17414,Reqo: A Robust and Explainable Query Optimization Cost Model,"Baoming Chang, Amin Kamali, Verena Kantere","Databases, Artificial Intelligence, Machine Learning","In recent years, there has been a growing interest in using machine learning (ML) in query optimization to select more efficient plans. Existing learning-based query optimizers use certain model architectures to convert tree-structured query plans into representations suitable for downstream ML tasks. As the design of these architectures significantly impacts cost estimation, we propose a tree model architecture based on Bidirectional Graph Neural Networks (Bi-GNN) aggregated by Gated Recurrent Units (GRUs) to achieve more accurate cost estimates. The inherent uncertainty of data and model parameters also leads to inaccurate cost estimates, resulting in suboptimal plans and less robust query performance. To address this, we implement a novel learning-to-rank cost model that effectively quantifies the uncertainty in cost estimates using approximate probabilistic ML. This model adaptively integrates quantified uncertainty with estimated costs and learns from comparing pairwise plans, achieving more robust performance. In addition, we propose the first explainability technique specifically designed for learning-based cost models. This technique explains the contribution of any subgraphs in the query plan to the final predicted cost, which can be integrated and trained with any learning-based cost model to significantly boost the model’s explainability. By incorporating these innovations, we propose a cost model for aRobust andExplainableQueryOptimizer, Reqo, that improves the accuracy, robustness, and explainability of cost estimation, outperforming state-of-the-art approaches in all three dimensions."
1054,679d459debd8ffd557a2b28b,cs.AI,https://arxiv.org/pdf/2501.17411,A Genetic Algorithm-Based Approach for Automated Optimization of Kolmogorov-Arnold Networks in Classification Tasks,"Quan Long, Bin Wang, Bing Xue, Mengjie Zhang","Neural and Evolutionary Computing, Artificial Intelligence, Machine Learning","To address the issue of interpretability in multilayer perceptrons (MLPs), Kolmogorov-Arnold Networks (KANs) are introduced in 2024. However, optimizing KAN structures is labor-intensive, typically requiring manual intervention and parameter tuning.
This paper proposes GA-KAN, a genetic algorithm-based approach that automates the optimization of KANs, requiring no human intervention in the design process.
To the best of our knowledge, this is the first time that evolutionary computation is explored to optimize KANs automatically.
Furthermore, inspired by the use of sparse connectivity in MLPs in effectively reducing the number of parameters, GA-KAN further explores sparse connectivity to tackle the challenge of extensive parameter spaces in KANs.
GA-KAN is validated on two toy datasets, achieving optimal results without the manual tuning required by the original KAN.
Additionally, GA-KAN demonstrates superior performance across five classification datasets, outperforming traditional methods on all datasets and providing interpretable symbolic formulae for the Wine and Iris datasets, thereby enhancing model transparency.
Furthermore, GA-KAN significantly reduces the number of parameters over the standard KAN across all the five datasets. The core contributions of GA-KAN include automated optimization, a new encoding strategy, and a new decoding process, which together improve the accuracy and interpretability, and reduce the number of parameters."
1055,679d459debd8ffd557a2b28c,cs.AI,https://arxiv.org/pdf/2501.17384,A Dual-Agent Adversarial Framework for Robust Generalization in Deep Reinforcement Learning,"Zhengpeng Xie, Jiahang Cao, Yulong Zhang, Qiang Zhang, Renjing Xu","Machine Learning, Artificial Intelligence","Recently, empowered with the powerful capabilities of neural networks, reinforcement learning (RL) has successfully tackled numerous challenging tasks. However, while these models demonstrate enhanced decision-making abilities, they are increasingly prone to overfitting. For instance, a trained RL model often fails to generalize to even minor variations of the same task, such as a change in background color or other minor semantic differences. To address this issue, we propose a dual-agent adversarial policy learning framework, which allows agents to spontaneously learn the underlying semantics without introducing any human prior knowledge. Specifically, our framework involves a game process between two agents: each agent seeks to maximize the impact of perturbing on the opponent’s policy by producing representation differences for the same state, while maintaining its own stability against such perturbations. This interaction encourages agents to learn generalizable policies, capable of handling irrelevant features from the high-dimensional observations. Extensive experimental results on the Procgen benchmark demonstrate that the adversarial process significantly improves the generalization performance of both agents, while also being applied to various RL algorithms, e.g., Proximal Policy Optimization (PPO). With the adversarial framework, the RL agent outperforms the baseline methods by a significant margin, especially in hard-level tasks, marking a significant step forward in the generalization capabilities of deep reinforcement learning."
1056,679d459debd8ffd557a2b28d,cs.AI,https://arxiv.org/pdf/2501.17377,ASAP: Learning Generalizable Online Bin Packing via Adaptive Selection After Pruning,"Han Fang, Paul Weng, Yutong Ban","Machine Learning, Artificial Intelligence",
1057,679d459debd8ffd557a2b28e,cs.AI,https://arxiv.org/pdf/2501.17374,A Geometric Perspective for High-Dimensional Multiplex Graphs,"Kamel Abdous, Nairouz Mrabah, Mohamed Bouguessa","Machine Learning, Artificial Intelligence","High-dimensional multiplex graphs are characterized by their high number of complementary and divergent dimensions. The existence of multiple hierarchical latent relations between the graph dimensions poses significant challenges to embedding methods. In particular, the geometric distortions that might occur in the representational space have been overlooked in the literature. This work studies the problem of high-dimensional multiplex graph embedding from a geometric perspective. We find that the node representations reside on highly curved manifolds, thus rendering their exploitation more challenging for downstream tasks. Moreover, our study reveals that increasing the number of graph dimensions can cause further distortions to the highly curved manifolds. To address this problem, we propose a novel multiplex graph embedding method that harnesses hierarchical dimension embedding and Hyperbolic Graph Neural Networks. The proposed approach hierarchically extracts hyperbolic node representations that reside on Riemannian manifolds while gradually learning fewer and more expressive latent dimensions of the multiplex graph. Experimental results on real-world high-dimensional multiplex graphs show that the synergy between hierarchical and hyperbolic embeddings incurs much fewer geometric distortions and brings notable improvements over state-of-the-art approaches on downstream tasks."
1058,679d459debd8ffd557a2b28f,cs.AI,https://arxiv.org/pdf/2501.17366,Forecasting S&P 500 Using LSTM Models,"Prashant Pilla, Raji Mekonen","Machine Learning, Artificial Intelligence, Computational Finance, Trading and Market Microstructure","With the volatile, complex nature of financial data which is also influenced by many external factors, forecasting the stock market has been seen to be a challenging task. Traditional models like ARIMA and GARCH were observed to be good with linear data. However, the stock market data involves non-linear dependencies and intricate patterns that are better handled by machine learning and deep learning approaches. Taking that a step further to patch hyper-parameter tuning and computational complexity that machine learning lacks, we get deep learning models like Long Short-Term Memory (LSTM) networks.
In this report, we compare ARIMA and LSTM models in predicting the S&P 500 index, one of the most popular financial benchmarks. Using historical price data and technical indicators, we evaluated these models using the Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) metrics. The ARIMA model showcased reasonable performance with an MAE of 462.1, RMSE of 614, and an accuracy of 89.8%. This demonstrated its effectiveness in capturing short-term trends but also showed that it is limited by its linear assumptions. The LSTM model, with favorable features, achieved an MAE of 369.32, RMSE of 412.84, and an accuracy of 92. 46%, capturing both short- and long-term dependencies. The LSTM model without features outperformed the version with all features, achieving an MAE of 175.9, RMSE of 207.34, and an accuracy of 96.41%, which showcased its ability to handle market data.
Accurately forecasting the stock market is crucial because of its effect on investment strategies, risk assessments, and market stability. By taking advantage of the sequential processing capabilities of LSTM, this report confirms how deep learning methods can handle volatile market conditions when compared to traditional models. The results of our analysis not only reaffirm the transformative potential of LSTM but also provide steps that can be taken to improve upon the model. Through this comprehensive study forecasting financial data, we aim to showcase the insights, limitations, and potential for prediction accuracy."
1059,679d459debd8ffd557a2b290,cs.AI,https://arxiv.org/pdf/2501.17361,The M-factor: A Novel Metric for Evaluating Neural Architecture Search in Resource-Constrained Environments,"Srikanth Thudumu, Hy Nguyen, Hung Du, Nhat Duong, Zafaryab Rasool, Rena Logothetis, Scott Barnett, Rajesh Vasa, Kon Mouzakis","Machine Learning, Artificial Intelligence","Neural Architecture Search (NAS) aims to automate the design of deep neural networks. However, existing NAS techniques often focus primarily on maximizing accuracy, neglecting model efficiency. This limitation hinders their applicability in resource-constrained environments such as mobile devices and edge computing systems. Additionally, current evaluation metrics typically prioritize performance over efficiency, lacking a balanced approach to assess architectures suitable for deployment in constrained scenarios. To address these limitations, this paper introduces theM-factor, a novel metric that combines model accuracy and size. We compare four diverse NAS techniques: Policy-Based Reinforcement Learning, Regularized Evolution, Tree-structured Parzen Estimator (TPE), and Multi-trial Random search. This selection represents different approaches in NAS, allowing for a comprehensive assessment of the M-Factor across various paradigms. The study examines ResNet configurations on the CIFAR-10 dataset, with a search space of 19,683 configurations. Experiments show Policy-Based Reinforcement Learning and Regularized Evolution achievedM-factorvalues of 0.84 and 0.82 respectively, while Multi-trial Random search attained 0.75 and TPE reached 0.67. Policy-based reinforcement Learning exhibited performance changes after 39 trials, and Regularized Evolution showed optimization within 20 trials. The research analyzes optimization dynamics and trade-offs between accuracy and model size for each strategy. Results indicate that in some cases, random search performed comparably to more complex algorithms when evaluated using theM-factor. These findings demonstrate how theM-factoraddresses the limitations of existing metrics by guiding NAS towards balanced architectures, providing insights into strategy selection for scenarios requiring both model performance and efficiency."
1060,679d459debd8ffd557a2b291,cs.AI,https://arxiv.org/pdf/2501.17347,Deep-and-Wide Learning: Enhancing Data-Driven Inference via Synergistic Learning of Inter- and Intra-Data Representations,"Md Tauhidul Islam, Lei Xing","Machine Learning, Artificial Intelligence",
1061,679d459debd8ffd557a2b292,cs.AI,https://arxiv.org/pdf/2501.17329,Anomaly Detection in Cooperative Vehicle Perception Systems under Imperfect Communication,"Ashish Bastola, Hao Wang, Abolfazl Razi","Multiagent Systems, Artificial Intelligence, Machine Learning","Anomaly detection is a critical requirement for ensuring safety in autonomous driving. In this work, we leverage Cooperative Perception to share information across nearby vehicles, enabling more accurate identification and consensus of anomalous behaviors in complex traffic scenarios. To account for the real-world challenge of imperfect communication, we propose a cooperative-perception-based anomaly detection framework (CPAD), which is a robust architecture that remains effective under communication interruptions, thereby facilitating reliable performance even in low-bandwidth settings. Since no multi-agent anomaly detection dataset exists for vehicle trajectories, we introduce 15,000 different scenarios with a 90,000 trajectories benchmark dataset generated through rule-based vehicle dynamics analysis. Empirical results demonstrate that our approach outperforms standard anomaly classification methods in F1-score, AUC and showcase strong robustness to agent connection interruptions. The code and dataset will be made publicly available at:https://github.com/abastola0/CPAD"
1062,679d459debd8ffd557a2b293,cs.AI,https://arxiv.org/pdf/2501.17325,Connecting Federated ADMM to Bayes,"Siddharth Swaroop, Mohammad Emtiyaz Khan, Finale Doshi-Velez","Machine Learning, Artificial Intelligence, Machine Learning","We provide new connections between two distinct federated learning approaches based on (i) ADMM and (ii) Variational Bayes (VB), and propose new variants by combining their complementary strengths. Specifically, we show that the dual variables in ADMM naturally emerge through the “site” parameters used in VB with isotropic Gaussian covariances. Using this, we derive two versions of ADMM from VB that use flexible covariances and functional regularisation, respectively. Through numerical experiments, we validate the improvements obtained in performance. The work shows connection between two fields that are believed to be fundamentally different and combines them to improve federated learning."
1063,679d459debd8ffd557a2b294,cs.AI,https://arxiv.org/pdf/2501.17296,Multi-Physics Simulations via Coupled Fourier Neural Operator,"Shibo Li, Tao Wang, Yifei Sun, Hewei Tang","Machine Learning, Artificial Intelligence","Physical simulations are essential tools across critical fields such as mechanical and aerospace engineering, chemistry, meteorology,etc.. While neural operators, particularly the Fourier Neural Operator (FNO), have shown promise in predicting simulation results with impressive performance and efficiency, they face limitations when handling real-world scenarios involving coupled multi-physics outputs. Current neural operator methods either overlook the correlations between multiple physical processes or employ simplistic architectures that inadequately capture these relationships. To overcome these challenges, we introduce a novelcoupledmulti-physics neuraloperatorlearning (COMPOL) framework that extends the capabilities of Fourier operator layers to model interactions among multiple physical processes. Our approach implements feature aggregation through recurrent and attention mechanisms, enabling comprehensive modeling of coupled interactions. Our method’s core is an innovative system for aggregating latent features from multi-physics processes. These aggregated features serve as enriched information sources for neural operator layers, allowing our framework to capture complex physical relationships accurately. We evaluated our coupled multi-physics neural operator across diverse physical simulation tasks, including biological systems, fluid mechanics, and multiphase flow in porous media. Our proposed model demonstrates a two to three-fold improvement in predictive performance compared to existing approaches."
1064,679d459debd8ffd557a2b295,cs.AI,https://arxiv.org/pdf/2501.17207,Rethinking Functional Brain Connectome Analysis: Do Graph Deep Learning Models Help?,"Keqi Han, Yao Su, Lifang He, Liang Zhan, Sergey Plis, Vince Calhoun, Carl Yang","Neural and Evolutionary Computing, Artificial Intelligence, Machine Learning, Neurons and Cognition","Functional brain connectome is crucial for deciphering the neural mechanisms underlying cognitive functions and neurological disorders. Graph deep learning models have recently gained tremendous popularity in this field. However, their actual effectiveness in modeling the brain connectome remains unclear. In this study, we re-examine graph deep learning models based on four large-scale neuroimaging studies encompassing diverse cognitive and clinical outcomes. Surprisingly, we find that the message aggregation mechanism, a hallmark of graph deep learning models, does not help with predictive performance as typically assumed, but rather consistently degrades it.
To address this issue, we propose a hybrid model combining a linear model with a graph attention network through dual pathways, achieving robust predictions and enhanced interpretability by revealing both localized and global neural connectivity patterns. Our findings urge caution in adopting complex deep learning models for functional brain connectome analysis, emphasizing the need for rigorous experimental designs to establish tangible performance gains and perhaps more importantly, to pursue improvements in model interpretability."
1065,679d459debd8ffd557a2b296,cs.AI,https://arxiv.org/pdf/2501.17168,EvoGP: A GPU-accelerated Framework for Tree-Based Genetic Programming,"Lishuang Wang, Zhihong Wu, Kebin Sun, Zhuozhao Li, Ran Cheng","Neural and Evolutionary Computing, Artificial Intelligence","Tree-based Genetic Programming (TGP) is a key evolutionary algorithm widely used in symbolic regression, feature engineering, and scientific modeling.
Its high computational demands make GPU acceleration essential for scalable and high-performance evolutionary computation.
However, GPU acceleration of TGP faces three key challenges: inefficient tree encoding, highly heterogeneous genetic operations, and limited parallelism in fitness evaluation.
To address these challenges, we introduce EvoGP, a comprehensive GPU-accelerated TGP framework.
First, we design a tensorized encoding scheme to represent tree with different structures as tensors with the same shape, optimizing memory access and enabling efficient parallel execution.
Second, we propose a unified parallel framework for genetic operations by leveraging shared computational primitives and implementing dedicated CUDA kernels for scalable performance.
Third, we present a fully parallel fitness evaluation strategy for symbolic regression, exploiting both population-level and data-level parallelism to maximize GPU utilization.
Moreover, we implement a comprehensive library to provide rich algorithm operators and benchmark problems.
EvoGP is extensively tested on various tasks, including symbolic regression, classification, and robotics control, demonstrating its versatility and effectiveness across diverse application scenarios.
Experimental results show that EvoGP achieves up to a 140.89× speedup over the state-of-the-art GPU-based TGP implementation, while maintaining or exceeding the accuracy of baseline methods.
EvoGP is open-source and accessible at: https://github.com/EMI-Group/evogp."
1066,679d459debd8ffd557a2b297,cs.AI,https://arxiv.org/pdf/2501.17167,QualityFlow: An Agentic Workflow for Program Synthesis Controlled by LLM Quality Checks,"Yaojie Hu, Qiang Zhou, Qihong Chen, Xiaopeng Li, Linbo Liu, Dejiao Zhang, Amit Kachroo, Talha Oz, Omer Tripp","Software Engineering, Artificial Intelligence","We introduce QualityFlow, a dynamic agentic workflow for program synthesis.
Given the English description of a programming problem and a set of unit tests, the model’s goal is to synthesize the correct program that solves the problem and passes the tests.
QualityFlow consists of multiple large language model (LLM) agents that resemble a software development team, including code generation, testing, and self-debugging.
Existing program synthesis methods face three major limitations: assumption of visible unit test conformity, bottleneck of synthesized test quality, and deviation of self-debugging trajectory.
To address them, we propose the LLM Quality Checker, which explicitly “imagines” whether the synthesized programs’ execution would conform to the unit tests.
The Quality Checks dynamically control the workflow, including actions to submit the final answer, clarify the problem statement, and revert previous workflow steps.
As a result, our Quality Checker can precisely accept any correct program, mitigate faulty synthesized tests, and prevent potential workflow deviation.
The success of the Quality Checker further enables Diversified Prompting, which encourages variations in LLM responses to maximize the possibility that a correct program appears and passes the quality check.
In experiments, QualityFlow establishes the state-of-the-art results on four program synthesis benchmarks: MBPP, HumanEval, and the stricter evaluations of both MBPP and HumanEval from EvalPlus.
Our systematic analysis shows that the dynamic workflow controlled by LLM quality checks can outperform static workflows and single-attempt zero-shot synthesis.
The Quality Checker is the center of our investigation, and we dissect its individual performance and integrated impact on the workflow accuracy, as well as other ablations experiments to justify our workflow design."
1067,679d459debd8ffd557a2b298,cs.AI,https://arxiv.org/pdf/2501.17164,"Split Knowledge Distillation for Large Models in IoT: Architecture, Challenges, and Solutions","Zuguang Li, Wen Wu, Shaohua Wu, Qiaohua Lin, Yaping Sun, Hui Wang","Machine Learning, Artificial Intelligence","Large models (LMs) have immense potential in Internet of Things (IoT) systems, enabling applications such as intelligent voice assistants, predictive maintenance, and healthcare monitoring. However, training LMs on edge servers raises data privacy concerns, while deploying them directly on IoT devices is constrained by limited computational and memory resources. We analyze the key challenges of training LMs in IoT systems, including energy constraints, latency requirements, and device heterogeneity, and propose potential solutions such as dynamic resource management, adaptive model partitioning, and clustered collaborative training. Furthermore, we propose a split knowledge distillation framework to efficiently distill LMs into smaller, deployable versions for IoT devices while ensuring raw data remains local. This framework integrates knowledge distillation and split learning to minimize energy consumption and meet low model training delay requirements. A case study is presented to evaluate the feasibility and performance of the proposed framework."
1068,679d459debd8ffd557a2b299,cs.AI,https://arxiv.org/pdf/2501.17015,Revisit Mixture Models for Multi-Agent Simulation: Experimental Study within a Unified Framework,"Longzhong Lin, Xuewu Lin, Kechun Xu, Haojian Lu, Lichao Huang, Rong Xiong, Yue Wang","Artificial Intelligence, Multiagent Systems, Robotics","Simulation plays a crucial role in assessing autonomous driving systems, where the generation of realistic multi-agent behaviors is a key aspect.
In multi-agent simulation, the primary challenges include behavioral multimodality and closed-loop distributional shifts.
In this study, we revisit mixture models for generating multimodal agent behaviors, which can cover the mainstream methods including continuous mixture models and GPT-like discrete models.
Furthermore, we introduce a closed-loop sample generation approach tailored for mixture models to mitigate distributional shifts.
Within the unified mixture model (UniMM) framework, we recognize critical configurations from both model and data perspectives.
We conduct a systematic examination of various model configurations, including positive component matching, continuous regression, prediction horizon, and the number of components.
Moreover, our investigation into the data configuration highlights the pivotal role of closed-loop samples in achieving realistic simulations.
To extend the benefits of closed-loop samples across a broader range of mixture models, we further address the shortcut learning and off-policy learning issues.
Leveraging insights from our exploration, the distinct variants proposed within the UniMM framework, including discrete, anchor-free, and anchor-based models, all achieve state-of-the-art performance on the WOSAC benchmark."
1069,679d459debd8ffd557a2b29a,cs.AI,https://arxiv.org/pdf/2501.16961,Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers,"Mohammad Raza, Natasa Milic-Frayling",Artificial Intelligence,"Robustness of reasoning remains a significant challenge for large language models, and addressing it is essential for the practical applicability of AI-driven reasoning systems. We introduce Semantic Self-Verification (SSV), a novel approach that addresses the key challenge in combining language models with the rigor of logical solvers: to accurately formulate the reasoning problem from natural language to the formal language of the solver. SSV uses a consistency-based approach to produce strong abstract formalizations of problems using concrete instantiations that are generated by the model and verified by the solver. In addition to significantly advancing the overall reasoning accuracy over the state-of-the-art, a key novelty that this approach presents is a feature of verification that has near-perfect precision over a significant coverage of cases, as we demonstrate on open reasoning benchmarks. We propose suchnear-certain reasoningas a new approach to reduce the need for manual verification in many cases, taking us closer to more dependable and autonomous AI reasoning systems."
1070,679d459debd8ffd557a2b29b,cs.AI,https://arxiv.org/pdf/2501.16922,"Agential AI for Integrated Continual Learning, Deliberative Behavior, and Comprehensible Models","Zeki Doruk Erden, Boi Faltings","Artificial Intelligence, Machine Learning","Contemporary machine learning paradigm excels in statistical data analysis, solving problems that classical AI couldn’t. However, it faces key limitations, such as a lack of integration with planning, incomprehensible internal structure, and inability to learn continually. We present the initial design for an AI system, Agential AI (AAI), in principle operating independently or on top of statistical methods, designed to overcome these issues. AAI’s core is a learning method that models temporal dynamics with guarantees of completeness, minimality, and continual learning, using component-level variation and selection to learn the structure of the environment. It integrates this with a behavior algorithm that plans on a learned model and encapsulates high-level behavior patterns. Preliminary experiments on a simple environment show AAI’s effectiveness and potential."
1071,679d459debd8ffd557a2b29c,cs.AI,https://arxiv.org/pdf/2501.16689,MACI: Multi-Agent Collaborative Intelligence for Adaptive Reasoning and Temporal Planning,Edward Y. Chang,Artificial Intelligence,"Artificial intelligence requires deliberate reasoning, temporal awareness, and effective constraint management—capabilities traditional LLMs often lack due to their reliance on pattern matching, limited self-verification, and inconsistent constraint handling. We introduce Multi-Agent Collaborative Intelligence (MACI), a framework comprising three key components: 1) ameta-planner(MP) that identifies, formulates, and refines all roles and constraints of a task (e.g., wedding planning) while generating a dependency graph, with common-sense augmentation to ensure realistic and practical constraints; 2) acollection of agentsto facilitate planning and address task-specific requirements; and 3) arun-time monitorthat manages plan adjustments as needed. By decoupling planning from validation, maintaining minimal agent context, and integrating common-sense reasoning, MACI overcomes the aforementioned limitations and demonstrates robust performance in two scheduling problems."
1072,679d459debd8ffd557a2b29d,cs.AI,https://arxiv.org/pdf/2501.16546,Sample-Efficient Behavior Cloning Using General Domain Knowledge,"Feiyu Zhu, Jean Oh, Reid Simmons",Artificial Intelligence,"Behavior cloning has shown success in many sequential decision-making tasks by learning from expert demonstrations, yet they can be very sample inefficient and fail to generalize to unseen scenarios. One approach to these problems is to introduce general domain knowledge, such that the policy can focus on the essential features and may generalize to unseen states by applying that knowledge. Although this knowledge is easy to acquire from the experts, it is hard to be combined with learning from individual examples due to the lack of semantic structure in neural networks and the time-consuming nature of feature engineering. To enable learning from both general knowledge and specific demonstration trajectories, we use a large language model’s coding capability to instantiate a policy structure based on expert domain knowledge expressed in natural language and tune the parameters in the policy with demonstrations. We name this approach the Knowledge Informed Model (KIM) as the structure reflects the semantics of expert knowledge. In our experiments with lunar lander and car racing tasks, our approach learns to solve the tasks with as few as 5 demonstrations and is robust to action noise, outperforming the baseline model without domain knowledge. This indicates that with the help of large language models, we can incorporate domain knowledge into the structure of the policy, increasing sample efficiency for behavior cloning."
1073,679d459debd8ffd557a2b29e,cs.AI,https://arxiv.org/pdf/2501.16448,What is Harm? Baby Don't Hurt Me! On the Impossibility of Complete Harm Specification in AI Alignment,Robin Young,"Artificial Intelligence, Machine Learning","”First, do no harm” faces a fundamental challenge in artificial intelligence: how can we specify what constitutes harm? While prior work treats harm specification as a technical hurdle to be overcome through better algorithms or more data, we argue this assumption is unsound. Drawing on information theory, we demonstrate that complete harm specification is fundamentally impossible for any system where harm is defined external to its specifications. This impossibility arises from an inescapable information-theoretic gap: the entropy of harm H(O) always exceeds the mutual information I(O;I) between ground truth harm O and a system’s specifications I."
1074,679d459debd8ffd557a2b29f,cs.AI,https://arxiv.org/pdf/2501.17152,Three-Dimensional Diffusion-Weighted Multi-Slab MRI With Slice Profile Compensation Using Deep Energy Model,"Reza Ghorbani, Jyothi Rikhab Chand, Chu-Yu Lee, Mathews Jacob, Merry Mani","Image and Video Processing, Artificial Intelligence, Medical Physics","Three-dimensional (3D) multi-slab acquisition is a technique frequently employed in high-resolution diffusion-weighted MRI in order to achieve the best signal-to-noise ratio (SNR) efficiency. However, this technique is limited by slab boundary artifacts that cause intensity fluctuations and aliasing between slabs which reduces the accuracy of anatomical imaging. Addressing this issue is crucial for advancing diffusion MRI quality and making high-resolution imaging more feasible for clinical and research applications. In this work, we propose a regularized slab profile encoding (PEN) method within a Plug-and-Play ADMM framework, incorporating multi-scale energy (MuSE) regularization to effectively improve the slab combined reconstruction. Experimental results demonstrate that the proposed method significantly improves image quality compared to non-regularized and TV-regularized PEN approaches. The regularized PEN framework provides a more robust and efficient solution for high-resolution 3D diffusion MRI, potentially enabling clearer, more reliable anatomical imaging across various applications."
1075,679d459debd8ffd557a2b2a0,cs.AI,https://arxiv.org/pdf/2501.17096,Why is the estimation of metaorder impact with public market data so challenging?,"Manuel Naviglio, Giacomo Bormetti, Francesco Campigli, German Rodikov, Fabrizio Lillo","Trading and Market Microstructure, Artificial Intelligence, Econometrics, Physics and Society","Estimating market impact and transaction costs of large trades (metaorders) is a very important topic in finance. However, using models of price and trade based on public market data provide average price trajectories which are qualitatively different from what is observed during real metaorder executions: the price increases linearly, rather than in a concave way, during the execution and the amount of reversion after its end is very limited. We claim that this is a generic phenomenon due to the fact that even sophisticated statistical models are unable to correctly describe the origin of the autocorrelation of the order flow. We propose a modified Transient Impact Model which provides more realistic trajectories by assuming that only a fraction of the metaorder trading triggers market order flow. Interestingly, in our model there is a critical condition on the kernels of the price and order flow equations in which market impact becomes permanent."
1076,679d459debd8ffd557a2b2a1,cs.AI,https://arxiv.org/pdf/2501.17081,Graph Transformers for inverse physics: reconstructing flows around arbitrary 2D airfoils,"Gregory Duthé, Imad Abdallah, Eleni Chatzi","Machine Learning, Artificial Intelligence, Computational Engineering, Finance, and Science","We introduce a Graph Transformer framework that serves as a general inverse physics engine on meshes, demonstrated through the challenging task of reconstructing aerodynamic flow fields from sparse surface measurements. While deep learning has shown promising results in forward physics simulation, inverse problems remain particularly challenging due to their ill-posed nature and the difficulty of propagating information from limited boundary observations. Our approach addresses these challenges by combining the geometric expressiveness of message-passing neural networks with the global reasoning of Transformers, enabling efficient learning of inverse mappings from boundary conditions to complete states. We evaluate this framework on a comprehensive dataset of steady-state RANS simulations around diverse airfoil geometries, where the task is to reconstruct full pressure and velocity fields from surface pressure measurements alone. The architecture achieves high reconstruction accuracy while maintaining fast inference times. We conduct experiments and provide insights into the relative importance of local geometric processing and global attention mechanisms in mesh-based inverse problems. We also find that the framework is robust to reduced sensor coverage. These results suggest that Graph Transformers can serve as effective inverse physics engines across a broader range of applications where complete system states must be reconstructed from limited boundary observations."
1077,679d459debd8ffd557a2b2a2,cs.AI,https://arxiv.org/pdf/2501.17079,Learning Mean Field Control on Sparse Graphs,"Christian Fabian, Kai Cui, Heinz Koeppl","Multiagent Systems, Artificial Intelligence, Computer Science and Game Theory, Machine Learning","Large agent networks are abundant in applications and nature and pose difficult challenges in the field of multi-agent reinforcement learning (MARL) due to their computational and theoretical complexity. While graphon mean field games and their extensions provide efficient learning algorithms for dense and moderately sparse agent networks, the case of realistic sparser graphs remains largely unsolved. Thus, we propose a novel mean field control model inspired by local weak convergence to include sparse graphs such as power law networks with coefficients above two. Besides a theoretical analysis, we design scalable learning algorithms which apply to the challenging class of graph sequences with finite first moment. We compare our model and algorithms for various examples on synthetic and real world networks with mean field algorithms based on Lp graphons and graphexes. As it turns out, our approach outperforms existing methods in many examples and on various networks due to the special design aiming at an important, but so far hard to solve class of MARL problems."
1078,679d459debd8ffd557a2b2a3,cs.AI,https://arxiv.org/pdf/2501.17077,Induced Modularity and Community Detection for Functionally Interpretable Reinforcement Learning,"Anna Soligo, Pietro Ferraro, David Boyle","Machine Learning, Artificial Intelligence","Interpretability in reinforcement learning is crucial for ensuring AI systems align with human values and fulfill the diverse related requirements including safety, robustness and fairness. Building on recent approaches to encouraging sparsity and locality in neural networks, we demonstrate how the penalisation of non-local weights leads to the emergence of functionally independent modules in the policy network of a reinforcement learning agent. To illustrate this, we demonstrate the emergence of two parallel modules for assessment of movement along the X and Y axes in a stochastic Minigrid environment. Through the novel application of community detection algorithms, we show how these modules can be automatically identified and their functional roles verified through direct intervention on the network weights prior to inference. This establishes a scalable framework for reinforcement learning interpretability through functional modularity, addressing challenges regarding the trade-off between completeness and cognitive tractability of reinforcement learning explanations."
1079,679d459debd8ffd557a2b2a4,cs.AI,https://arxiv.org/pdf/2501.17041,Benchmarking Quantum Convolutional Neural Networks for Signal Classification in Simulated Gamma-Ray Burst Detection,"Farida Farsian, Nicolò Parmiggiani, Alessandro Rizzo, Gabriele Panebianco, Andrea Bulgarelli, Francesco Schillirò, Carlo Burigana, Vincenzo Cardone, Luca Cappelli, Massimo Meneghetti, Giuseppe Murante, Giuseppe Sarracino, Roberto Scaramella, Vincenzo Testa, Tiziana Trombetti","High Energy Astrophysical Phenomena, Artificial Intelligence, Quantum Physics","This study evaluates the use of Quantum Convolutional Neural Networks (QCNNs) for identifying signals resembling Gamma-Ray Bursts (GRBs) within simulated astrophysical datasets in the form of light curves. The task addressed here focuses on distinguishing GRB-like signals from background noise in simulated Cherenkov Telescope Array Observatory (CTAO) data, the next-generation astrophysical observatory for very high-energy gamma-ray science.
QCNNs, a quantum counterpart of classical Convolutional Neural Networks (CNNs), leverage quantum principles to process and analyze high-dimensional data efficiently. We implemented a hybrid quantum-classical machine learning technique using the Qiskit framework, with the QCNNs trained on a quantum simulator. Several QCNN architectures were tested, employing different encoding methods such as Data Reuploading and Amplitude encoding.
Key findings include that QCNNs achieved accuracy comparable to classical CNNs, often surpassing 90%, while using fewer parameters, potentially leading to more efficient models in terms of computational resources. A benchmark study further examined how hyperparameters like the number of qubits and encoding methods affected performance, with more qubits and advanced encoding methods generally enhancing accuracy but increasing complexity.
QCNNs showed robust performance on time-series datasets, successfully detecting GRB signals with high precision. The research is a pioneering effort in applying QCNNs to astrophysics, offering insights into their potential and limitations. This work sets the stage for future investigations to fully realize the advantages of QCNNs in astrophysical data analysis."
1080,679d459debd8ffd557a2b2a5,cs.AI,https://arxiv.org/pdf/2501.17037,Standardised schema and taxonomy for AI incident databases in critical digital infrastructure,"Avinash Agarwal, Manisha J. Nene","Computers and Society, Artificial Intelligence, Human-Computer Interaction","The rapid deployment of Artificial Intelligence (AI) in critical digital infrastructure introduces significant risks, necessitating a robust framework for systematically collecting AI incident data to prevent future incidents. Existing databases lack the granularity as well as the standardized structure required for consistent data collection and analysis, impeding effective incident management. This work proposes a standardized schema and taxonomy for AI incident databases, addressing these challenges by enabling detailed and structured documentation of AI incidents across sectors. Key contributions include developing a unified schema, introducing new fields such as incident severity, causes, and harms caused, and proposing a taxonomy for classifying AI incidents in critical digital infrastructure. The proposed solution facilitates more effective incident data collection and analysis, thus supporting evidence-based policymaking, enhancing industry safety measures, and promoting transparency. This work lays the foundation for a coordinated global response to AI incidents, ensuring trust, safety, and accountability in using AI across regions."
1081,679d459debd8ffd557a2b2a6,cs.AI,https://arxiv.org/pdf/2501.16986,Generative quantum combinatorial optimization by means of a novel conditional generative quantum eigensolver,"Shunya Minami, Kouhei Nakaji, Yohichi Suzuki, Alán Aspuru-Guzik, Tadashi Kadowaki","Quantum Physics, Artificial Intelligence, Machine Learning","Quantum computing is entering a transformative phase with the emergence of logical quantum processors, which hold the potential to tackle complex problems beyond classical capabilities.
While significant progress has been made, applying quantum algorithms to real-world problems remains challenging.
Hybrid quantum-classical techniques have been explored to bridge this gap, but they often face limitations in expressiveness, trainability, or scalability.
In this work, we introduce conditional Generative Quantum Eigensolver (conditional-GQE), a context-aware quantum circuit generator powered by an encoder-decoder Transformer.
Focusing on combinatorial optimization, we train our generator for solving problems with up to 10 qubits, exhibiting nearly perfect performance on new problems.
By leveraging the high expressiveness and flexibility of classical generative models, along with an efficient preference-based training scheme, conditional-GQE provides a generalizable and scalable framework for quantum circuit generation.
Our approach advances hybrid quantum-classical computing and contributes to accelerate the transition toward fault-tolerant quantum computing."
1082,679d459debd8ffd557a2b2a7,cs.AI,https://arxiv.org/pdf/2501.16966,Heterogeneity-aware Personalized Federated Learning via Adaptive Dual-Agent Reinforcement Learning,"Xi Chen, Qin Li, Haibin Cai, Ting Wang","Machine Learning, Artificial Intelligence","Federated Learning (FL) empowers multiple clients to collaboratively train machine learning models without sharing local data, making it highly applicable in heterogeneous Internet of Things (IoT) environments. However, intrinsic heterogeneity in clients’ model architectures and computing capabilities often results in model accuracy loss and the intractable straggler problem, which significantly impairs training effectiveness.
To tackle these challenges, this paper proposes a novel Heterogeneity-aware Personalized Federated Learning method, named HAPFL, via multi-level Reinforcement Learning (RL) mechanisms. HAPFL optimizes the training process by incorporating three strategic components: 1) An RL-based heterogeneous model allocation mechanism. The parameter server employs a Proximal Policy Optimization (PPO)-based RL agent to adaptively allocate appropriately sized, differentiated models to clients based on their performance, effectively mitigating performance disparities. 2) An RL-based training intensity adjustment scheme. The parameter server leverages another PPO-based RL agent to dynamically fine-tune the training intensity for each client to further enhance training efficiency and reduce straggling latency. 3) A knowledge distillation-based mutual learning mechanism. Each client deploys both a heterogeneous local model and a homogeneous lightweight model named LiteModel, where these models undergo mutual learning through knowledge distillation.
This uniform LiteModel plays a pivotal role in aggregating and sharing global knowledge, significantly enhancing the effectiveness of personalized local training.
Experimental results across multiple benchmark datasets demonstrate that HAPFL not only achieves high accuracy but also substantially reduces the overall training time by 20.9%-40.4% and decreases straggling latency by 19.0%-48.0% compared to existing solutions."
1083,679d459debd8ffd557a2b2a8,cs.AI,https://arxiv.org/pdf/2501.16944,Exact Computation of Any-Order Shapley Interactions for Graph Neural Networks,"Fabian Fumagalli, Maximilian Muschalik, Paolo Frazzetto, Janine Strotherm, Luca Hermes, Alessandro Sperduti, Eyke Hüllermeier, Barbara Hammer","Machine Learning, Artificial Intelligence","Albeit the ubiquitous use ofGraph Neural Networks (GNNs)inmachine learning (ML)prediction tasks involving graph-structured data, their interpretability remains challenging.
Inexplainable artificial intelligence (XAI), theShapley Value (SV)is the predominant method to quantify contributions of individual features to aMLmodel’s output.
Addressing the limitations ofSVsin complex prediction models,Shapley Interactions (SIs)extend theSVto groups of features.
In this work, we explain single graph predictions ofGNNswithSIsthat quantify node contributions and interactions among multiple nodes.
By exploiting theGNNarchitecture, we show that the structure of interactions in node embeddings are preserved for graph prediction.
As a result, the exponential complexity ofSIsdepends only on the receptive fields, i.e. the message-passing ranges determined by the connectivity of the graph and the number of convolutional layers.
Based on our theoretical results, we introduce GraphSHAP-IQ, an efficient approach to compute any-orderSIsexactly.
GraphSHAP-IQ is applicable to popular message-passing techniques in conjunction with a linear global pooling and output layer.
We showcase that GraphSHAP-IQ substantially reduces the exponential complexity of computing exactSIson multiple benchmark datasets.
Beyond exact computation, we evaluate GraphSHAP-IQ’s approximation ofSIson popularGNNarchitectures and compare with existing baselines.
Lastly, we visualizeSIsof real-world water distribution networks and molecule structures using aSI-Graph."
1084,679d459debd8ffd557a2b2a9,cs.AI,https://arxiv.org/pdf/2501.16899,RDMM: Fine-Tuned LLM Models for On-Device Robotic Decision Making with Enhanced Contextual Awareness in Specific Domains,"Shady Nasrat, Myungsu Kim, Seonil Lee, Jiho Lee, Yeoncheol Jang, Seung-joon Yi","Robotics, Artificial Intelligence","Large language models (LLMs) represent a significant advancement in integrating physical robots with AI-driven systems. We showcase the capabilities of our framework within the context of the real-world household competition. This research introduces a framework that utilizes RDMM (Robotics Decision-Making Models), which possess the capacity for decision-making within domain-specific contexts, as well as an awareness of their personal knowledge and capabilities. The framework leverages information to enhance the autonomous decision-making of the system.
In contrast to other approaches, our focus is on real-time, on-device solutions, successfully operating on hardware with as little as 8GB of memory. Our framework incorporates visual perception models equipping robots with understanding of their environment. Additionally, the framework has integrated real-time speech recognition capabilities, thus enhancing the human-robot interaction experience.
Experimental results demonstrate that the RDMM framework can plan with an 93% accuracy. Furthermore, we introduce a new dataset consisting of 27k planning instances, as well as 1.3k text-image annotated samples derived from the competition. The framework, benchmarks, datasets, and models developed in this work are publicly available on our GitHub repository at https://github.com/shadynasrat/RDMM."
1085,679d459debd8ffd557a2b2aa,cs.AI,https://arxiv.org/pdf/2501.16800,DIRIGENt: End-To-End Robotic Imitation of Human Demonstrations Based on a Diffusion Model,"Josua Spisak, Matthias Kerzel, Stefan Wermter","Robotics, Artificial Intelligence","There has been substantial progress in humanoid robots, with new skills continuously being taught, ranging from navigation to manipulation. While these abilities may seem impressive, the teaching methods often remain inefficient. To enhance the process of teaching robots, we propose leveraging a mechanism effectively used by humans: teaching by demonstrating. In this paper, we introduce DIRIGENt (DIrect Robotic Imitation GENeration model), a novel end-to-end diffusion approach that directly generates joint values from observing human demonstrations, enabling a robot to imitate these actions without any existing mapping between it and humans. We create a dataset in which humans imitate a robot and then use this collected data to train a diffusion model that enables a robot to imitate humans. The following three aspects are the core of our contribution. First is our novel dataset with natural pairs between human and robot poses, allowing our approach to imitate humans accurately despite the gap between their anatomies. Second, the diffusion input to our model alleviates the challenge of redundant joint configurations, limiting the search space. And finally, our end-to-end architecture from perception to action leads to an improved learning capability. Through our experimental analysis, we show that combining these three aspects allows DIRIGENt to outperform existing state-of-the-art approaches in the field of generating joint values from RGB images.111Code and Data will be made available along with ademonstration video (currently in supplementary material)."
1086,679d459debd8ffd557a2b2ab,cs.AI,https://arxiv.org/pdf/2501.16744,LLM Assisted Anomaly Detection Service for Site Reliability Engineers: Enhancing Cloud Infrastructure Resilience,"Nimesh Jha, Shuxin Lin, Srideepika Jayaraman, Kyle Frohling, Christodoulos Constantinides, Dhaval Patel","Machine Learning, Artificial Intelligence","This paper introduces a scalable Anomaly Detection Service with a generalizable API tailored for industrial time-series data, designed to assist Site Reliability Engineers (SREs) in managing cloud infrastructure. The service enables efficient anomaly detection in complex data streams, supporting proactive identification and resolution of issues. Furthermore, it presents an innovative approach to anomaly modeling in cloud infrastructure by utilizing Large Language Models (LLMs) to understand key components, their failure modes, and behaviors."
1087,679d459debd8ffd557a2b2ac,cs.AI,https://arxiv.org/pdf/2501.16734,Distilling Large Language Models for Network Active Queue Management,"Deol Satish, Shiva Raj Pokhrel, Jonathan Kua, Anwar Walid","Networking and Internet Architecture, Artificial Intelligence","The growing complexity of network traffic and demand for ultra-low latency communication require smarter packet traffic management. Existing Deep Learning-based queuing approaches struggle with dynamic network scenarios and demand high engineering effort. We propose AQM-LLM, distilling Large Language Models (LLMs) with few-shot learning, contextual understanding, and pattern recognition to improve Active Queue Management (AQM) [RFC 9330] with minimal manual effort. We consider a specific case where AQM isLow Latency, Low Loss, and Scalable Throughput(L4S) and our design of AQM-LLM builds onspeculative decodingandreinforcement-based distilling of LLMby tackling congestion prevention in the L4S architecture using Explicit Congestion Notification (ECN) [RFC 9331] and periodic packet dropping. We develop a new open-source experimental platform by executing L4S-AQM on FreeBSD-14, providing interoperable modules to support LLM integration and facilitate IETF recognition through wider testing. Our extensive evaluations show L4S-LLM enhances queue management, prevents congestion, reduces latency, and boosts network performance, showcasing LLMs’ adaptability and efficiency in uplifting AQM systems."
1088,679d459debd8ffd557a2b2ad,cs.AI,https://arxiv.org/pdf/2501.16729,On the Interplay Between Sparsity and Training in Deep Reinforcement Learning,"Fatima Davelouis, John D. Martin, Michael Bowling","Machine Learning, Artificial Intelligence","We study the benefits of different sparse architectures for deep reinforcement learning.
In particular, we focus on image-based domains where spatially-biased and fully-connected architectures are common.
Using these and several other architectures of equal capacity, we show that sparse structure has a significant effect on learning performance.
We also observe that choosing the best sparse architecture for a given domain depends on whether the hidden layer weights are fixed or learned."
1089,679d459debd8ffd557a2b2ae,cs.AI,https://arxiv.org/pdf/2501.16726,Bridging Neural Networks and Wireless Systems with MIMO-OFDM Semantic Communications,"Hanju Yoo, Dongha Choi, Yonghwi Kim, Yoontae Kim, Songkuk Kim, Chan-Byoung Chae, Robert W. Heath Jr","Information Theory, Artificial Intelligence, Networking and Internet Architecture","Semantic communications aim to enhance transmission efficiency by jointly optimizing source coding, channel coding, and modulation. While prior research has demonstrated promising performance in simulations, real-world implementations often face significant challenges, including noise variability and nonlinear distortions, leading to performance gaps. This article investigates these challenges in a multiple-input multiple-output (MIMO) and orthogonal frequency division multiplexing (OFDM)-based semantic communication system, focusing on the practical impacts of power amplifier (PA) nonlinearity and peak-to-average power ratio (PAPR) variations. Our analysis identifies frequency selectivity of the actual channel as a critical factor in performance degradation and demonstrates that targeted mitigation strategies can enable semantic systems to approach theoretical performance. By addressing key limitations in existing designs, we provide actionable insights for advancing semantic communications in practical wireless environments. This work establishes a foundation for bridging the gap between theoretical models and real-world deployment, highlighting essential considerations for system design and optimization."
1090,679d459debd8ffd557a2b2af,cs.AI,https://arxiv.org/pdf/2501.16722,Hypergraph Diffusion for High-Order Recommender Systems,"Darnbi Sakong, Thanh Trung Huynh, Jun Jo","Information Retrieval, Artificial Intelligence, Databases, Machine Learning, Social and Information Networks","Recommender systems rely on Collaborative Filtering (CF) to predict user preferences by leveraging patterns in historical user-item interactions. While traditional CF methods primarily focus on learning compact vector embeddings for users and items, graph neural network (GNN)-based approaches have emerged as a powerful alternative, utilizing the structure of user-item interaction graphs to enhance recommendation accuracy. However, existing GNN-based models, such as LightGCN and UltraGCN, often struggle with two major limitations: an inability to fully account for heterophilic interactions, where users engage with diverse item categories, and the over-smoothing problem in multi-layer GNNs, which hinders their ability to model complex, high-order relationships. To address these gaps, we introduce WaveHDNN, an innovative wavelet-enhanced hypergraph diffusion framework. WaveHDNN integrates a Heterophily-aware Collaborative Encoder, designed to capture user-item interactions across diverse categories, with a Multi-scale Group-wise Structure Encoder, which leverages wavelet transforms to effectively model localized graph structures. Additionally, cross-view contrastive learning is employed to maintain robust and consistent representations. Experiments on benchmark datasets validate the efficacy of WaveHDNN, demonstrating its superior ability to capture both heterophilic and localized structural information, leading to improved recommendation performance."
1091,679d459debd8ffd557a2b2b0,cs.AI,https://arxiv.org/pdf/2501.16692,Optimizing Code Runtime Performance through Context-Aware Retrieval-Augmented Generation,"Manish Acharya, Yifan Zhang, Kevin Leach, Yu Huang","Software Engineering, Artificial Intelligence","Optimizing software performance through automated code refinement offers a promising avenue for enhancing execution speed and efficiency. Despite recent advancements in LLMs, a significant gap remains in their ability to perform in-depth program analysis. This study introducesAutoPatch, an in-context learning approach designed to bridge this gap by enabling LLMs to automatically generate optimized code. Inspired by how programmers learn and apply knowledge to optimize software,AutoPatchincorporates three key components: (1) an analogy-driven framework to align LLM optimization with human cognitive processes, (2) a unified approach that integrates historical code examples and CFG analysis for context-aware learning, and (3) an automated pipeline for generating optimized code through in-context prompting. Experimental results demonstrate thatAutoPatchachieves a 7.3% improvement in execution efficiency over GPT-4o across common generated executable code, highlighting its potential to advance automated program runtime optimization."
1092,679d459debd8ffd557a2b2b1,cs.AI,https://arxiv.org/pdf/2501.16666,Federated Learning for Efficient Condition Monitoring and Anomaly Detection in Industrial Cyber-Physical Systems,"William Marfo, Deepak K. Tosh, Shirley V. Moore","Machine Learning, Artificial Intelligence","Detecting and localizing anomalies in cyber-physical systems (CPS) has become increasingly challenging as systems grow in complexity, particularly due to varying sensor reliability and node failures in distributed environments. While federated learning (FL) offers a foundation for distributed model training, existing approaches lack mechanisms to handle these CPS-specific challenges. This paper presents an enhanced FL framework that introduces three key innovations: adaptive model aggregation based on sensor reliability, dynamic node selection for resource optimization, and Weibull-based checkpointing for fault tolerance. Our framework enables reliable condition monitoring while addressing the computational and reliability challenges of industrial CPS deployments. Experiments on NASA Bearing and Hydraulic System Datasets demonstrate superior performance over state-of-the-art FL methods, achieving 99.5% AUC-ROC in anomaly detection and maintaining accuracy under node failures. Statistical validation using Mann-Whitney (UU\mathrm{U}roman_U) test confirms significant improvements(p<0.05)𝑝0.05(p<0.05)( italic_p < 0.05 )in both detection accuracy and computational efficiency across diverse operational scenarios."
1093,679d459debd8ffd557a2b2b2,cs.AI,https://arxiv.org/pdf/2501.16627,Engaging with AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision-Making,"Zichen Chen, Yunhao Luo, Misha Sra","Human-Computer Interaction, Artificial Intelligence","As reliance on AI systems for decision-making grows, it becomes critical to ensure that human users can appropriately balance trust in AI suggestions with their own judgment, especially in high-stakes domains like healthcare. However, human + AI teams have been shown to perform worse than AI alone, with evidence indicating automation bias as the reason for poorer performance, particularly because humans tend to follow AI’s recommendations even when they are incorrect. In many existing human + AI systems, decision-making support is typically provided in the form of text explanations (XAI) to help users understand the AI’s reasoning. Since human decision-making often relies on System 1 thinking (fast, intuitive, heuristics driven, prone to cognitive biases), users may ignore or insufficiently engage with the explanations, leading to poor decision-making. Previous research suggests that there is need for new approaches that encourage users to engage with the explanations and one proposed method is the use of cognitive forcing functions (CFFs). In this work, we examine how various decision-support mechanisms impact user engagement, trust, and human-AI collaborative task performance in a diabetes management decision-making scenario. In a controlled experiment with 108 participants, we evaluated the effects of six distinct decision-support mechanisms split into two categories of explanations (text, visual) and four CFFs: (1) text explanations (baseline) (2) visual explanations, (3) AI confidence levels (CLs), (4) human feedback, (5) AI-driven questions, and (6) performance visualization. Our findings reveal that mechanisms such as AI CLs, text explanations, and performance visualizations significantly enhanced human-AI collaborative task performance, as measured by decision accuracy, and improved trust when AI reasoning clues were provided. While mechanisms like human feedback and AI-driven questions encouraged deeper reflection, they may have resulted in decreased task performance, likely due to increased cognitive effort and heightened scrutiny, which negatively impacted trust. Simple mechanisms like visual explanations did not significantly improve trust, highlighting the need for a contextual and balanced approach between CFF and XAI design with interactivity, decision frequency, and task complexity."
1094,679d459debd8ffd557a2b2b3,cs.AI,https://arxiv.org/pdf/2501.16621,Chinese Stock Prediction Based on a Multi-Modal Transformer Framework: Macro-Micro Information Fusion,"Lumen AI, Tengzhou No. 1 Middle School, Shihao Ji, Zihui Song, Fucheng Zhong, Jisen Jia, Zhaobo Wu, Zheyi Cao, Xu Tianhao","Machine Learning, Artificial Intelligence","This paper proposes an innovative Multi-Modal Transformer framework (MMF-Trans) designed to significantly improve the prediction accuracy of the Chinese stock market by integrating multi-source heterogeneous information including macroeconomy, micro-market, financial text, and event knowledge. The framework consists of four core modules: (1) A four-channel parallel encoder that processes technical indicators, financial text, macro data, and event knowledge graph respectively for independent feature extraction of multi-modal data; (2) A dynamic gated cross-modal fusion mechanism that adaptively learns the importance of different modalities through differentiable weight allocation for effective information integration; (3) A time-aligned mixed-frequency processing layer that uses an innovative position encoding method to effectively fuse data of different time frequencies and solves the time alignment problem of heterogeneous data; (4) A graph attention-based event impact quantification module that captures the dynamic impact of events on the market through event knowledge graph and quantifies the event impact coefficient. We introduce a hybrid-frequency Transformer and Event2Vec algorithm to effectively fuse data of different frequencies and quantify the event impact. Experimental results show that in the prediction task of CSI 300 constituent stocks, the root mean square error (RMSE) of the MMF-Trans framework is reduced by 23.7% compared to the baseline model, the event response prediction accuracy is improved by 41.2%, and the Sharpe ratio is improved by 32.6%. The theoretical contribution is that we establish a quantitative evaluation system for the impact of political events and demonstrate the convergence of the model through mathematical proof, while solving the technical problem of heterogeneous data frequency alignment. In addition, we elaborate on the deployment and application of the model in a real intelligent investment research platform, and conduct a quantitative impact analysis of the ”carbon neutrality” policy, providing valuable reference for policy makers and investors."
1095,679d459debd8ffd557a2b2b4,cs.AI,https://arxiv.org/pdf/2501.16613,Safe Reinforcement Learning for Real-World Engine Control,"Julian Bedei, Lucas Koch, Kevin Badalian, Alexander Winkler, Patrick Schaber, Jakob Andert","Machine Learning, Artificial Intelligence",
1096,679d459debd8ffd557a2b2b5,cs.AI,https://arxiv.org/pdf/2501.16606,Governing the Agent-to-Agent Economy of Trust via Progressive Decentralization,Tomer Jordi Chaffer,"Multiagent Systems, Artificial Intelligence","Current approaches to AI governance often fall short in anticipating a future where AI agents manage critical tasks, such as financial operations, administrative functions, and beyond. As AI agents may eventually delegate tasks among themselves to optimize efficiency, understanding the foundational principles of human value exchange could offer insights into how AI-driven economies might operate. Just as trust and value exchange are central to human interactions in open marketplaces, they may also be critical for enabling secure and efficient interactions among AI agents. While cryptocurrencies could serve as the foundation for monetizing value exchange in a collaboration and delegation dynamic among AI agents, a critical question remains: how can these agents reliably determine whom to trust, and how can humans ensure meaningful oversight and control as an economy of AI agents scales and evolves? This paper is a call for a collective exploration of cryptoeconomic incentives, which can help design decentralized governance systems that allow AI agents to autonomously interact and exchange value while ensuring human oversight via progressive decentralization. Toward this end, I propose a research agenda to address the question of agent-to-agent trust using AgentBound Tokens (ABTs)—non-transferable, non-fungible tokens uniquely tied to individual AI agents, akin to Soulbound tokens for humans in Web3. By staking ABTs as collateral for autonomous actions within an agent-to-agent network via a proof-of-stake mechanism, agents may be incentivized towards ethical behavior, and penalties for misconduct are automatically enforced."
1097,679d459debd8ffd557a2b2b6,cs.AI,https://arxiv.org/pdf/2501.16605,Impact and influence of modern AI in metadata management,"Wenli Yang, Rui Fu, Muhammad Bilal Amin, Byeong Kang","Databases, Artificial Intelligence",
1098,679d459debd8ffd557a2b2b7,cs.AI,https://arxiv.org/pdf/2501.16591,Applying Ensemble Models based on Graph Neural Network and Reinforcement Learning for Wind Power Forecasting,"Hongjin Song, Qianrun Chen, Tianqi Jiang, Yongfeng Li, Xusheng Li, Wenjun Xi, Songtao Huang","Machine Learning, Artificial Intelligence",
1099,679d459debd8ffd557a2b2b8,cs.AI,https://arxiv.org/pdf/2501.16577,Generative AI Uses and Risks for Knowledge Workers in a Science Organization,"Kelly B. Wagman, Matthew T. Dearing, Marshini Chetty","Human-Computer Interaction, Artificial Intelligence","Generative AI could enhance scientific discovery by supporting knowledge workers in science organizations. However, the real-world applications and perceived concerns of generative AI use in these organizations are uncertain. In this paper, we report on a collaborative study with a US national laboratory with employees spanning Science and Operations about their use of generative AI tools. We surveyed 66 employees, interviewed a subset (N=22), and measured early adoption of an internal generative AI interface called Argo lab-wide. We have four findings: (1) Argo usage data shows small but increasing use by Science and Operations employees; Common current and envisioned use cases for generative AI in this context conceptually fall into either a (2)copilotor (3)workflow agentmodality; and (4) Concerns include sensitive data security, academic publishing, and job impacts. Based on our findings, we make recommendations for generative AI use in science and other organizations."
1100,679d459debd8ffd557a2b2b9,cs.AI,https://arxiv.org/pdf/2501.16539,Generalized Mission Planning for Heterogeneous Multi-Robot Teams via LLM-constructed Hierarchical Trees,"Piyush Gupta, David Isele, Enna Sachdeva, Pin-Hao Huang, Behzad Dariush, Kwonjoon Lee, Sangjae Bae","Robotics, Artificial Intelligence, Multiagent Systems","We present a novel mission-planning strategy for heterogeneous multi-robot teams, taking into account the specific constraints and capabilities of each robot. Our approach employs hierarchical trees to systematically break down complex missions into manageable sub-tasks. We develop specialized APIs and tools, which are utilized by Large Language Models (LLMs) to efficiently construct these hierarchical trees. Once the hierarchical tree is generated, it is further decomposed to create optimized schedules for each robot, ensuring adherence to their individual constraints and capabilities. We demonstrate the effectiveness of our framework through detailed examples covering a wide range of missions, showcasing its flexibility and scalability."
1101,679d459debd8ffd557a2b2ba,cs.AI,https://arxiv.org/pdf/2501.16510,Decrypting the temperature field in flow boiling with latent diffusion models,"UngJin Na, JunYoung Seo, Taeil Kim, ByongGuk Jeon, HangJin Jo","Fluid Dynamics, Artificial Intelligence","This paper presents an innovative method using Latent Diffusion Models (LDMs) to generate temperature fields from phase indicator maps. By leveraging the BubbleML dataset from numerical simulations, the LDM translates phase field data into corresponding temperature distributions through a two-stage training process involving a vector-quantized variational autoencoder (VQVAE) and a denoising autoencoder. The resulting model effectively reconstructs complex temperature fields at interfaces. Spectral analysis indicates a high degree of agreement with ground truth data in the low to mid wavenumber ranges, even though some inconsistencies are observed at higher wavenumbers, suggesting areas for further enhancement. This machine learning approach significantly reduces the computational burden of traditional simulations and improves the precision of experimental calibration methods. Future work will focus on refining the model’s ability to represent small-scale turbulence and expanding its applicability to a broader range of boiling conditions."
1102,679d459debd8ffd557a2b2bb,cs.AI,https://arxiv.org/pdf/2501.16509,Reinforcement Learning for Quantum Circuit Design: Using Matrix Representations,"Zhiyuan Wang, Chunlin Feng, Christopher Poon, Lijian Huang, Xingjian Zhao, Yao Ma, Tianfan Fu, Xiao-Yang Liu","Quantum Physics, Artificial Intelligence","Quantum computing promises advantages over classical computing. The manufacturing of quantum hardware is in the infancy stage, called the Noisy Intermediate-Scale Quantum (NISQ) era. A major challenge is automated quantum circuit design that map a quantum circuit to gates in a universal gate set. In this paper, we present a generic MDP modeling and employ Q-learning and DQN algorithms for quantum circuit design. By leveraging the power of deep reinforcement learning, we aim to provide an automatic and scalable approach over traditional hand-crafted heuristic methods."
1103,679d459debd8ffd557a2b2bc,cs.AI,https://arxiv.org/pdf/2501.16507,Characterizing Network Structure of Anti-Trans Actors on TikTok,"Maxyn Leitner, Rebecca Dorn, Fred Morstatter, Kristina Lerman","Human-Computer Interaction, Artificial Intelligence, Social and Information Networks","Content Warning: Trans-antagonistic Rhetoric and TerminologyThe recent proliferation of short form video social media sites such as TikTok has been effectively utilized for increased visibility, communication, and community connection amongst trans/nonbinary creators online. However, these same platforms have also been exploited by right-wing actors targeting trans/nonbinary people, enabling such anti-trans actors to efficiently spread hate speech and propaganda. Given these divergent groups, what are the differences in network structure between anti-trans and pro-trans communities on TikTok, and to what extent do they amplify the effects of anti-trans content? In this paper, we collect a sample of TikTok videos containing pro and anti-trans content, and develop a taxonomy of trans related sentiment to enable the classification of content on TikTok, and ultimately analyze the reply network structures of pro-trans and anti-trans communities. In order to accomplish this, we worked with hired expert data annotators from the trans/nonbinary community in order to generate a sample of highly accurately labeled data. From this subset, we utilized a novel classification pipeline leveraging Retrieval-Augmented Generation (RAG) with annotated examples and taxonomy definitions to classify content into pro-trans, anti-trans, or neutral categories. We find that incorporating our taxonomy and its logics into our classification engine results in improved ability to differentiate trans related content, and that Results from network analysis indicate many interactions between posters of pro-trans and anti-trans content exist, further demonstrating targeting of trans individuals, and demonstrating the need for better content moderation tools."
1104,679d459debd8ffd557a2b2bd,cs.AI,https://arxiv.org/pdf/2501.16504,Digital Twin Enabled Site Specific Channel Precoding: Over the Air CIR Inference,"Majumder Haider, Imtiaz Ahmed, Zoheb Hassan, Timothy J. O'Shea, Lingjia Liu, Danda B. Rawat","Signal Processing, Artificial Intelligence","This paper investigates the significance of designing a reliable, intelligent, and true physical environment-aware precoding scheme by leveraging an accurately designed channel twin model to obtain realistic channel state information (CSI) for cellular communication systems. Specifically, we propose a fine-tuned multi-step channel twin design process that can render CSI very close to the CSI of the actual environment. After generating a precise CSI, we execute precoding using the obtained CSI at the transmitter end. We demonstrate a two-step parameters’ tuning approach to design channel twin by ray tracing (RT) emulation, then further fine-tuning of CSI by employing an artificial intelligence (AI) based algorithm can significantly reduce the gap between actual CSI and the fine-tuned digital twin (DT) rendered CSI. The simulation results show the effectiveness of the proposed novel approach in designing a true physical environment-aware channel twin model."
1105,679d459debd8ffd557a2b2be,cs.AI,https://arxiv.org/pdf/2501.16471,SIM: Surface-based fMRI Analysis for Inter-Subject Multimodal Decoding from Movie-Watching Experiments,"Simon Dahan, Gabriel Bénédict, Logan Z. J. Williams, Yourong Guo, Daniel Rueckert, Robert Leech, Emma C. Robinson","Machine Learning, Artificial Intelligence, Audio and Speech Processing, Image and Video Processing, Neurons and Cognition","Current AI frameworks for brain decoding and encoding, typically train and test models within the same datasets. This limits their utility for brain computer interfaces (BCI) or neurofeedback, for which it would be useful to pool experiences across individuals to better simulate stimuli not sampled during training. A key obstacle to model generalisation is the degree of variability of inter-subject cortical organisation, which makes it difficult to align or compare cortical signals across participants. In this paper we address this through use of surface vision transformers, which build a generalisable model of cortical functional dynamics, through encoding the topography of cortical networks and their interactions as a moving image across a surface. This is then combined with tri-modal self-supervised contrastive (CLIP) alignment of audio, video, and fMRI modalities to enable the retrieval of visual and auditory stimuli from patterns of cortical activity (and vice-versa). We validate our approach on 7T task-fMRI data from 174 healthy participants engaged in the movie-watching experiment from the Human Connectome Project (HCP). Results show that it is possible to detect which movie clips an individual is watching purely from their brain activity, even for individuals and moviesnot seen during training.
Further analysis of attention maps reveals that our model captures individual patterns of brain activity that reflect semantic and visual systems. This opens the door to future personalised simulations of brain function. Code & pre-trained models will be made available athttps://github.com/metrics-lab/sim, processed data for training will be available upon request athttps://gin.g-node.org/Sdahan30/sim."
1106,679d459debd8ffd557a2b2bf,cs.AI,https://arxiv.org/pdf/2501.16453,Detecting Zero-Day Attacks in Digital Substations via In-Context Learning,"Faizan Manzoor, Vanshaj Khattar, Akila Herath, Clifton Black, Matthew C Nielsen, Junho Hong, Chen-Ching Liu, Ming Jin","Machine Learning, Artificial Intelligence",
1107,679d459debd8ffd557a2b2c0,cs.AI,https://arxiv.org/pdf/2501.16450,360Brew: A Decoder-only Foundation Model for Personalized Ranking and Recommendation,"Hamed Firooz, Maziar Sanjabi, Adrian Englhardt, Aman Gupta, Ben Levine, Dre Olgiati, Gungor Polatkan, Iuliia Melnychuk, Karthik Ramgopal, Kirill Talanine, Kutta Srinivasan, Luke Simon, Natesh Sivasubramoniapillai, Necip Fazil Ayan, Qingquan Song, Samira Sriram, Souvik Ghosh, Tao Song, Vignesh Kothapalli, Xiaoling Zhai, Ya Xu, Yu Wang, Yun Dai","Information Retrieval, Artificial Intelligence","Ranking and recommendation systems constitute the foundation for numerous online experiences, ranging from search results to personalized content delivery. These systems have evolved into complex, multilayered architectures that leverage vast datasets and often incorporate thousands of predictive models. The maintenance and enhancement of these models is a labor intensive process that requires extensive feature engineering. This approach not only exacerbates technical debt but also hampers innovation in extending these systems to emerging problem domains."
1108,679d459debd8ffd557a2b2c1,cs.AI,https://arxiv.org/pdf/2501.16409,Classification of Mild Cognitive Impairment Based on Dynamic Functional Connectivity Using Spatio-Temporal Transformer,"Jing Zhang, Yanjun Lyu, Xiaowei Yu, Lu Zhang, Chao Cao, Tong Chen, Minheng Chen, Yan Zhuang, Tianming Liu, Dajiang Zhu","Image and Video Processing, Artificial Intelligence, Neurons and Cognition",
1109,679d459debd8ffd557a2b2c2,cs.AI,https://arxiv.org/pdf/2501.16404,DynaPrompt: Dynamic Test-Time Prompt Tuning,"Zehao Xiao, Shilin Yan, Jack Hong, Jiayin Cai, Xiaolong Jiang, Yao Hu, Jiayi Shen, Qi Wang, Cees G. M. Snoek","Machine Learning, Artificial Intelligence, Computation and Language","Test-time prompt tuning enhances zero-shot generalization of vision-language models but tends to ignore the relatedness among test samples during inference. Online test-time prompt tuning provides a simple way to leverage the information in previous test samples, albeit with the risk of prompt collapse due to error accumulation. To enhance test-time prompt tuning, we propose DynaPrompt, short fordynamic test-time prompt tuning, exploiting relevant data distribution information while reducing error accumulation. Built on an online prompt buffer, DynaPrompt adaptively selects and optimizes the relevant prompts for each test sample during tuning. Specifically, we introduce a dynamic prompt selection strategy based on two metrics: prediction entropy and probability difference. For unseen test data information, we develop dynamic prompt appending, which allows the buffer to append new prompts and delete the inactive ones. By doing so, the prompts are optimized to exploit beneficial information on specific test data, while alleviating error accumulation. Experiments on fourteen datasets demonstrate the effectiveness of dynamic test-time prompt tuning."
1110,679d459debd8ffd557a2b2c3,cs.AI,https://arxiv.org/pdf/2501.16391,Leveraging Induced Transferable Binding Principles for Associative Prediction of Novel Drug-Target Interactions,"Xiaoqing Lian, Jie Zhu, Tianxu Lv, Shiyun Nie, Hang Fan, Guosheng Wu, Yunjun Ge, Lihua Li, Xiangxiang Zeng, Xiang Pan","Machine Learning, Artificial Intelligence, Biomolecules","Significant differences in protein structures hinder the generalization of existing drug-target interaction (DTI) models, which often rely heavily on pre-learned binding principles or detailed annotations. In contrast, BioBridge designs an Inductive-Associative pipeline inspired by the workflow of scientists who base their accumulated expertise on drawing insights into novel drug-target pairs from weakly related references. BioBridge predicts novel drug-target interactions using limited sequence data, incorporating multi-level encoders with adversarial training to accumulate transferable binding principles. On these principles basis, BioBridge employs a dynamic prototype meta-learning framework to associate insights from weakly related annotations, enabling robust predictions for previously unseen drug-target pairs. Extensive experiments demonstrate that BioBridge surpasses existing models, especially for unseen proteins. Notably, when only homologous protein binding data is available, BioBridge proves effective for virtual screening of the epidermal growth factor receptor and adenosine receptor, underscoring its potential in drug discovery."
1111,679d459debd8ffd557a2b2c4,cs.AI,https://arxiv.org/pdf/2501.16383,RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations,"Zunhai Su, Zhe Chen, Wang Shen, Hanyu Wei, Linge Li, Huangqi Yu, Kehong Yuan","Machine Learning, Artificial Intelligence, Computation and Language",
1112,679d459debd8ffd557a2b2c5,cs.AI,https://arxiv.org/pdf/2501.16382,GraPPI: A Retrieve-Divide-Solve GraphRAG Framework for Large-scale Protein-protein Interaction Exploration,"Ziwen Li, Xiang 'Anthony' Chen, Youngseung Jeon","Quantitative Methods, Artificial Intelligence, Machine Learning","Drug discovery (DD) has tremendously contributed to maintaining and improving public health. Hypothesizing that inhibiting protein misfolding can slow disease progression, researchers focus on target identification (Target ID) to find protein structures for drug binding. While Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) frameworks have accelerated drug discovery, integrating models into cohesive workflows remains challenging. We conducted a user study with drug discovery researchers to identify the applicability of LLMs and RAGs in Target ID. We identified two main findings: 1) an LLM should provide multiple Protein-Protein Interactions (PPIs) based on an initial protein and protein candidates that have a therapeutic impact; 2) the model must provide the PPI and relevant explanations for better understanding. Based on these observations, we identified three limitations in previous approaches for Target ID: 1) semantic ambiguity, 2) lack of explainability, and 3) short retrieval units.
To address these issues, we proposeGraPPI, a large-scale knowledge graph (KG)-based retrieve-divide-solve agent pipeline RAG framework to support large-scale PPI signaling pathway exploration in understanding therapeutic impacts by decomposing the analysis of entire PPI pathways into sub-tasks focused on the analysis of PPI edges111Code link:https://github.com/AaronLi43/GraPPI††*Corresponding author.."
1113,679d459debd8ffd557a2b2c6,cs.AI,https://arxiv.org/pdf/2501.16380,UDiTQC: U-Net-Style Diffusion Transformer for Quantum Circuit Synthesis,"Zhiwei Chen, Hao Tang","Machine Learning, Artificial Intelligence, Quantum Physics","Quantum computing is a transformative technology with wide-ranging applications, and efficient quantum circuit generation is crucial for unlocking its full potential. Current diffusion model approaches based on U-Net architectures, while promising, encounter challenges related to computational efficiency and modeling global context. To address these issues, we propose UDiT, a novel U-Net-style Diffusion Transformer architecture, which combines U-Net’s strengths in multi-scale feature extraction with the Transformer’s ability to model global context. Building upon this foundation, we introduce UDiTQC, an extension specifically designed for quantum circuit generation. We demonstrate the framework’s effectiveness on two tasks: entanglement generation and unitary compilation, where UDiTQC consistently outperforms existing methods. Additionally, our framework supports tasks such as masking and editing circuits to meet specific physical property requirements. This dual advancement, improving quantum circuit synthesis and refining generative model architectures, marks a significant milestone in the convergence of quantum computing and machine learning research."
1114,679d459debd8ffd557a2b2c7,cs.AI,https://arxiv.org/pdf/2501.16379,FedAGHN: Personalized Federated Learning with Attentive Graph HyperNetworks,"Jiarui Song, Yunheng Shen, Chengbin Hou, Pengyu Wang, Jinbao Wang, Ke Tang, Hairong Lv","Machine Learning, Artificial Intelligence","Personalized Federated Learning (PFL) aims to address the statistical heterogeneity of data across clients by learning the personalized model for each client. Among various PFL approaches, thepersonalizedaggregation-based approach conducts parameter aggregation in the server-side aggregation phase to generate personalized models, and focuses on learning appropriate collaborative relationships among clients for aggregation. However, the collaborative relationships vary in different scenarios and even at different stages of the FL process. To this end, we propose Personalized Federated Learning with Attentive Graph HyperNetworks (FedAGHN), which employs Attentive Graph HyperNetworks (AGHNs) to dynamically capture fine-grained collaborative relationships and generate client-specific personalized initial models. Specifically, AGHNs empower graphs to explicitly model the client-specific collaborative relationships, construct collaboration graphs, and introduce tunable attentive mechanism to derive the collaboration weights, so that the personalized initial models can be obtained by aggregating parameters over the collaboration graphs. Extensive experiments can demonstrate the superiority of FedAGHN. Moreover, a series of visualizations are presented to explore the effectiveness of collaboration graphs learned by FedAGHN."
1115,679d459debd8ffd557a2b2c8,cs.AI,https://arxiv.org/pdf/2501.16377,Optimal Signal Decomposition-based Multi-Stage Learning for Battery Health Estimation,"Vijay Babu Pamshetti, Wei Zhang, King Jet Tseng, Bor Kiat Ng, Qingyu Yan","Machine Learning, Artificial Intelligence","Battery health estimation is fundamental to ensure battery safety and reduce cost. However, achieving accurate estimation has been challenging due to the batteries’ complex nonlinear aging patterns and capacity regeneration phenomena. In this paper, we propose OSL, anoptimalsignal decomposition-based multi-stage machinelearning for battery health estimation. OSL treats battery signals optimally. It uses optimized variational mode decomposition to extract decomposed signals capturing different frequency bands of the original battery signals. It also incorporates a multi-stage learning process to analyze both spatial and temporal battery features effectively. An experimental study is conducted with a public battery aging dataset. OSL demonstrates exceptional performance with a mean error of just 0.26%. It significantly outperforms comparison algorithms, both those without and those with suboptimal signal decomposition and analysis. OSL considers practical battery challenges and can be integrated into real-world battery management systems, offering a good impact on battery monitoring and optimization."
1116,679d459debd8ffd557a2b2c9,cs.AI,https://arxiv.org/pdf/2501.16376,HWPQ: Hessian-free Weight Pruning-Quantization For LLM Compression And Acceleration,"Yuhan Kang, Zhongdi Luo, Mei Wen, Yang Shi, Jun He, Jianchao Yang, Zeyu Xue, Jing Feng, Xinwang Liu","Machine Learning, Artificial Intelligence","Large Language Models (LLMs) have achieved remarkable success across numerous domains. However, the high time complexity of existing pruning and quantization methods significantly hinders their effective deployment on resource-constrained consumer or edge devices. In this study, we propose a novel Hessian-free Weight Pruning-Quantization (HWPQ) method. HWPQ eliminates the need for computationally intensive Hessian matrix calculations by introducing a contribution-based weight metric, which evaluates the importance of weights without relying on second-order derivatives. Additionally, we employ the Exponentially Weighted Moving Average (EWMA) technique to bypass weight sorting, enabling the selection of weights that contribute most to LLM accuracy and further reducing time complexity. Our approach is extended to support 2:4 structured sparsity pruning, facilitating efficient execution on modern hardware accelerators. Experimental results demonstrate that HWPQ significantly enhances the compression performance of LLaMA2. Compared to state-of-the-art quantization and pruning frameworks, HWPQ achieves average speedups of 5.97×\times×(up to 20.75×\times×) in quantization time and 12.29×\times×(up to 56.02×\times×) in pruning time, while largely preserving model accuracy. Furthermore, we observe a 1.50×\times×inference speedup compared to the baseline."
1117,679d459debd8ffd557a2b2ca,cs.AI,https://arxiv.org/pdf/2501.16375,On Storage Neural Network Augmented Approximate Nearest Neighbor Search,"Taiga Ikeda, Daisuke Miyashita, Jun Deguchi","Machine Learning, Artificial Intelligence, Information Retrieval","Large-scale approximate nearest neighbor search (ANN) has been gaining attention along with the latest machine learning researches employing ANNs. If the data is too large to fit in memory, it is necessary to search for the most similar vectors to a given query vector from the data stored in storage devices, not from that in memory. The storage device such as NAND flash memory has larger capacity than the memory device such as DRAM, but they also have larger latency to read data. Therefore, ANN methods for storage require completely different approaches from conventional in-memory ANN methods. Since the approximation that the time required for search is determined only by the amount of data fetched from storage holds under reasonable assumptions, our goal is to minimize it while maximizing recall. For partitioning-based ANNs, vectors are partitioned into clusters in the index building phase. In the search phase, some of the clusters are chosen, the vectors in the chosen clusters are fetched from storage, and the nearest vector is retrieved from the fetched vectors. Thus, the key point is to accurately select the clusters containing the ground truth nearest neighbor vectors. We accomplish this by proposing a method to predict the correct clusters by means of a neural network that is gradually refined by alternating supervised learning and duplicated cluster assignment. Compared to state-of-the-art SPANN and an exhaustive method usingk-means clustering and linear search, the proposed method achieves90%percent9090\%90 %recall on SIFT1M with80%percent8080\%80 %and58%percent5858\%58 %less data fetched from storage, respectively."
1118,679d459debd8ffd557a2b2cb,cs.AI,https://arxiv.org/pdf/2501.16374,SAFR: Neuron Redistribution for Interpretability,"Ruidi Chang, Chunyuan Deng, Hanjie Chen","Machine Learning, Artificial Intelligence","Superposition refers to encoding representations of multiple features within a single neuron, which is common in transformers. This property allows neurons to combine and represent multiple features, enabling the model to capture intricate information and handle complex tasks. Despite promising performance, the model’s interpretability has been diminished. This paper presents a novel approach to enhance transformer interpretability by regularizing feature superposition. We introduceSAFR111SAFR:Superposition-AwareFeatureRegularization, which simply applies regularizations to the loss function to promote monosemantic representations for important tokens while encouraging polysemanticity for correlated token pairs, where important tokens and correlated token pairs are identified via VMASKChen and Ji (2020)and attention weights. With a transformer model on two classification tasks,SAFRimproves interpretability without compromising prediction performance. Given an input to the model,SAFRprovides an explanation by visualizing the neuron allocation and interaction within the MLP layers."
1119,679d459debd8ffd557a2b2cc,cs.AI,https://arxiv.org/pdf/2501.16373,Unveiling Discrete Clues: Superior Healthcare Predictions for Rare Diseases,"Chuang Zhao, Hui Tang, Jiheng Zhang, Xiaomeng Li","Machine Learning, Artificial Intelligence, Computational Engineering, Finance, and Science","Accurate healthcare prediction is essential for improving patient outcomes.
Existing work primarily leverages advanced frameworks like attention or graph networks to capture the intricate collaborative (CO) signals in electronic health records.
However, prediction for rare diseases remains challenging due to limited co-occurrence and inadequately tailored approaches.
To address this issue, this paper proposes UDC, a novel method thatunveilsdiscreteclues to bridge consistent textual knowledge and CO signals within a unified semantic space, thereby enriching the representation semantics of rare diseases.
Specifically, we focus on addressing two key sub-problems: (1) acquiring distinguishable discrete encodings for precise disease representation and (2) achieving semantic alignment between textual knowledge and the CO signals at the code level.
For the first sub-problem, we refine the standard vector quantized process to include condition awareness. Additionally, we develop an advanced contrastive approach in the decoding stage, leveraging synthetic and mixed-domain targets as hard negatives to enrich the perceptibility of the reconstructed representation for downstream tasks.
For the second sub-problem, we introduce a novel codebook update strategy using co-teacher distillation. This approach facilitates bidirectional supervision between textual knowledge and CO signals, thereby aligning semantically equivalent information in a shared discrete latent space.
Extensive experiments on three datasets demonstrate our superiority."
1120,679d459debd8ffd557a2b2cd,cs.AI,https://arxiv.org/pdf/2501.16371,Which Optimizer Works Best for Physics-Informed Neural Networks and Kolmogorov-Arnold Networks?,"Elham Kiyani, Khemraj Shukla, Jorge F. Urbán, Jérôme Darbon, George Em Karniadakis","Machine Learning, Artificial Intelligence, Optimization and Control","Physics-Informed Neural Networks (PINNs) have revolutionized the computation of PDE solutions by integrating partial differential equations (PDEs) into the neural network’s training process as soft constraints, becoming an important component of the scientific machine learning (SciML) ecosystem. In its current implementation, PINNs are mainly optimized using first-order methods like Adam, as well as quasi-Newton methods such as BFGS and its low-memory variant, L-BFGS.
However, these optimizers often struggle with highly non-linear and non-convex loss landscapes, leading to challenges such as slow convergence, local minima entrapment, and (non)degenerate saddle points.
In this study, we investigate the performance of Self-Scaled Broyden (SSBroyden) methods and other advanced quasi-Newton schemes, including BFGS and L-BFGS with different line search strategies approaches. These methods dynamically rescale updates based on historical gradient information, thus enhancing training efficiency and accuracy. We systematically compare these optimizers on key challenging linear, stiff, multi-scale and non-linear PDEs
benchmarks, including the Burgers, Allen-Cahn, Kuramoto-Sivashinsky, and Ginzburg-Landau equations, and extend our study to Physics-Informed Kolmogorov-Arnold Networks (PIKANs) representation. Our findings provide insights into the effectiveness of second-order optimization strategies in improving the convergence and accurate generalization of PINNs for complex PDEs by orders of magnitude compared to the state-of-the-art."
1121,679d459debd8ffd557a2b2ce,cs.AI,https://arxiv.org/pdf/2501.16370,Advanced Physics-Informed Neural Network with Residuals for Solving Complex Integral Equations,"Mahdi Movahedian Moghaddam, Kourosh Parand, Saeed Reza Kheradpisheh","Machine Learning, Artificial Intelligence, Neural and Evolutionary Computing, Numerical Analysis","In this paper, we present the Residual Integral Solver Network (RISN), a novel neural network architecture designed to solve a wide range of integral and integro-differential equations, including one-dimensional, multi-dimensional, ordinary and partial integro-differential, systems, and fractional types. RISN integrates residual connections with high-accurate numerical methods such as Gaussian quadrature and fractional derivative operational matrices, enabling it to achieve higher accuracy and stability than traditional Physics-Informed Neural Networks (PINN). The residual connections help mitigate vanishing gradient issues, allowing RISN to handle deeper networks and more complex kernels, particularly in multi-dimensional problems. Through extensive experiments, we demonstrate that RISN consistently outperforms PINN, achieving significantly lower Mean Absolute Errors (MAE) across various types of equations. The results highlight RISN’s robustness and efficiency in solving challenging integral and integro-differential problems, making it a valuable tool for real-world applications where traditional methods often struggle."
1122,679d459debd8ffd557a2b2cf,cs.AI,https://arxiv.org/pdf/2501.16369,Blockchain-based Crowdsourced Deep Reinforcement Learning as a Service,"Ahmed Alagha, Hadi Otrok, Shakti Singh, Rabeb Mizouni, Jamal Bentahar","Machine Learning, Artificial Intelligence","Deep Reinforcement Learning (DRL) has emerged as a powerful paradigm for solving complex problems. However, its full potential remains inaccessible to a broader audience due to its complexity, which requires expertise in training and designing DRL solutions, high computational capabilities, and sometimes access to pre-trained models. This necessitates the need for hassle-free services that increase the availability of DRL solutions to a variety of users. To enhance the accessibility to DRL services, this paper proposes a novel blockchain-based crowdsourced DRL as a Service (DRLaaS) framework. The framework provides DRL-related services to users, covering two types of tasks: DRL training and model sharing. Through crowdsourcing, users could benefit from the expertise and computational capabilities of workers to train DRL solutions. Model sharing could help users gain access to pre-trained models, shared by workers in return for incentives, which can help train new DRL solutions using methods in knowledge transfer. The DRLaaS framework is built on top of a Consortium Blockchain to enable traceable and autonomous execution. Smart Contracts are designed to manage worker and model allocation, which are stored using the InterPlanetary File System (IPFS) to ensure tamper-proof data distribution. The framework is tested on several DRL applications, proving its efficacy."
1123,679d459debd8ffd557a2b2d0,cs.AI,https://arxiv.org/pdf/2501.16368,Foundation Models for CPS-IoT: Opportunities and Challenges,"Ozan Baris, Yizhuo Chen, Gaofeng Dong, Liying Han, Tomoyoshi Kimura, Pengrui Quan, Ruijie Wang, Tianchen Wang, Tarek Abdelzaher, Mario Bergés, Paul Pu Liang, Mani Srivastava","Machine Learning, Artificial Intelligence, Systems and Control","Methods from machine learning (ML) have transformed the implementation of Perception-Cognition-Communication-Action loops in Cyber-Physical Systems (CPS) and the Internet of Things (IoT), replacing mechanistic and basic statistical models with those derived from data. However, the first generation of ML approaches, which depend on supervised learning with annotated data to create task-specific models, faces significant limitations in scaling to the diverse sensor modalities, deployment configurations, application tasks, and operating dynamics characterizing real-world CPS-IoT systems. The success of task-agnostic foundation models (FMs), including multimodal large language models (LLMs), in addressing similar challenges across natural language, computer vision, and human speech has generated considerable enthusiasm for and exploration of FMs and LLMs as flexible building blocks in CPS-IoT analytics pipelines, promising to reduce the need for costly task-specific engineering."
1124,679d459debd8ffd557a2b2d1,cs.AI,https://arxiv.org/pdf/2501.16365,CAND: Cross-Domain Ambiguity Inference for Early Detecting Nuanced Illness Deterioration,"Lo Pang-Yun Ting, Zhen Tan, Hong-Pei Chen, Cheng-Te Li, Po-Lin Chen, Kun-Ta Chuang, Huan Liu","Machine Learning, Artificial Intelligence","Early detection of patient deterioration is essential for timely treatment, with vital signs like heart rates being key health indicators. Existing methods tend to solely analyze vital sign waveforms, ignoring transition relationships of waveforms within each vital sign and the correlation strengths among various vital signs. Such studies often overlook nuanced illness deterioration, which is the early sign of worsening health but is difficult to detect. In this paper, we introduceCAND, a novel method that organizes the transition relationships and the correlations within and among vital signs as domain-specific and cross-domain knowledge.CANDjointly models these knowledge in a unified representation space, considerably enhancing the early detection of nuanced illness deterioration. In addition,CANDintegrates a Bayesian inference method that utilizes augmented knowledge from domain-specific and cross-domain knowledge to address the ambiguities in correlation strengths. With this architecture, the correlation strengths can be effectively inferred to guide joint modeling and enhance representations of vital signs. This allows a more holistic and accurate interpretation of patient health. Our experiments on a real-world ICU dataset demonstrate thatCANDsignificantly outperforms existing methods in both effectiveness and earliness in detecting nuanced illness deterioration. Moreover, we conduct a case study for the interpretable detection process to showcase the practicality ofCAND."
1125,679d459debd8ffd557a2b2d2,cs.AI,https://arxiv.org/pdf/2501.16364,Multivariate Time Series Anomaly Detection by Capturing Coarse-Grained Intra- and Inter-Variate Dependencies,"Yongzheng Xie, Hongyu Zhang, Muhammad Ali Babar","Machine Learning, Artificial Intelligence","Multivariate time series anomaly detection is essential for failure management in web application operations, as it directly influences the effectiveness and timeliness of implementing remedial or preventive measures. This task is often framed as a semi-supervised learning problem, where only normal data are available for model training, primarily due to the labor-intensive nature of data labeling and the scarcity of anomalous data. Existing semi-supervised methods often detect anomalies by capturing intra-variate temporal dependencies and/or inter-variate relationships to learn normal patterns, flagging timestamps that deviate from these patterns as anomalies. However, these approaches often fail to capture salient intra-variate temporal and inter-variate dependencies in time series due to their focus on excessively fine granularity, leading to suboptimal performance. In this study, we introduce MtsCID, a novel semi-supervised multivariate time series anomaly detection method. MtsCID employs a dual network architecture: one network operates on the attention maps of multi-scale intra-variate patches for coarse-grained temporal dependency learning, while the other works on variates to capture coarse-grained inter-variate relationships through convolution and interaction with sinusoidal prototypes. This design enhances the ability to capture the patterns from both intra-variate temporal dependencies and inter-variate relationships, resulting in improved performance. Extensive experiments across seven widely used datasets demonstrate that MtsCID achieves performance comparable or superior to state-of-the-art benchmark methods. Our code is available athttps://github.com/ilwoof/MtsCID/."
1126,679d459debd8ffd557a2b2d3,cs.AI,https://arxiv.org/pdf/2501.16361,Large Language Models Meet Graph Neural Networks for Text-Numeric Graph Reasoning,"Haoran Song, Jiarui Feng, Guangfu Li, Michael Province, Philip Payne, Yixin Chen, Fuhai Li","Machine Learning, Artificial Intelligence",
1127,679d459debd8ffd557a2b2d4,cs.AI,https://arxiv.org/pdf/2501.16360,Momentum Contrastive Learning with Enhanced Negative Sampling and Hard Negative Filtering,"Duy Hoang, Huy Ngo, Khoi Pham, Tri Nguyen, Gia Bao, Huy Phan","Machine Learning, Artificial Intelligence",
1128,679d459debd8ffd557a2b2d5,cs.AI,https://arxiv.org/pdf/2501.16357,EVolutionary Independent DEtermiNistiC Explanation,"Vincenzo Dentamaro, Paolo Giglio, Donato Impedovo, Giuseppe Pirlo","Machine Learning, Artificial Intelligence, Signal Processing",
1129,679d459debd8ffd557a2b2d6,cs.AI,https://arxiv.org/pdf/2501.16356,Evaluating Binary Decision Biases in Large Language Models: Implications for Fair Agent-Based Financial Simulations,"Alicia Vidler, Toby Walsh","Machine Learning, Artificial Intelligence","Large Language Models (LLMs) are increasingly being used to simulate human-like decision making in agent-based financial market models (ABMs). As models become more powerful and accessible, researchers can now incorporate individual LLM decisions into ABM environments. However, integration may introduce inherent biases that need careful evaluation. In this paper we test three state-of-the-art GPT models for bias using two model sampling approaches: one-shot and few-shot API queries. We observe significant variations in distributions of outputs between specific models, and model sub versions, with GPT-4o-Mini-2024-07-18 showing notably better performance (32-43% yes responses) compared to GPT-4-0125-preview’s extreme bias (98-99% yes responses). We show that sampling methods and model sub-versions significantly impact results: repeated independent API calls produce different distributions compared to batch sampling within a single call. While no current GPT model can simultaneously achieve a uniform distribution and Markovian properties in one-shot testing, few-shot sampling can approach uniform distributions under certain conditions. We explore the Temperature parameter, providing a definition and comparative results. We further compare our results to true random binary series and test specifically for the common human bias of Negative Recency - finding LLMs have a mixed ability to ’beat’ humans in this one regard. These findings emphasise the critical importance of careful LLM integration into ABMs for financial markets and more broadly."
1130,679d459debd8ffd557a2b2d7,cs.AI,https://arxiv.org/pdf/2501.16355,How Strategic Agents Respond: Comparing Analytical Models with LLM-Generated Responses in Strategic Classification,"Tian Xie, Pavan Rauch, Xueru Zhang","Machine Learning, Artificial Intelligence","When machine learning (ML) algorithms are used to automate human-related decisions, human agents may gain knowledge of the decision policy and behave strategically to obtain desirable outcomes. Strategic Classification (SC) has been proposed to address the interplay between agents and decision-makers in designing trustworthy ML algorithms. Importantly, prior work on SC has relied on pre-specified assumptions that agents are perfectly or approximately rational, responding to decision policies by maximizing their utilities. However, verifying these assumptions is challenging due to the difficulty of collecting real-world agent responses, making it hard to evaluate the theoretical results and algorithms built on these assumptions. Meanwhile, the growing adoption of large language models (LLMs) makes it increasingly likely that human agents in SC settings will seek advice from these tools. We thus propose using strategic advice generated by LLMs to simulate human agent responses in SC. Specifically, we examine five critical SC scenarios—hiring, loan applications, school admissions, personal income, and public assistance programs—and simulate how human agents with diverse profiles seek advice from LLMs and follow their suggestions to achieve positive decision outcomes. We then compare the resulting agent responses with the best responses generated by existing theoretical models. Our findings reveal that: (i) LLMs and theoretical models generally lead to agent score or qualification changes in the same direction across most settings, with both achieving similar levels of fairness; (ii) state-of-the-art commercial LLMs (e.g., GPT-3.5, GPT-4) consistently provide helpful suggestions, though these suggestions typically do not result in maximal score or qualification improvements; and (iii) LLMs tend to produce more diverse agent responses, often favoring more balanced effort allocation strategies. These results suggest that theoretical models align with LLMs to some extent and that leveraging LLMs to simulate more realistic agent responses offers a promising approach to designing trustworthy ML systems."
1131,679d459debd8ffd557a2b2d8,cs.AI,https://arxiv.org/pdf/2501.16354,Adaptive Hoeffding Tree with Transfer Learning for Streaming Synchrophasor Data Sets,"Zakaria El Mrabet, Daisy Flora Selvaraj, Prakash Ranganathan","Machine Learning, Artificial Intelligence",
1132,679d459debd8ffd557a2b2d9,cs.AI,https://arxiv.org/pdf/2501.16353,Synthetic Data Generation by Supervised Neural Gas Network for Physiological Emotion Recognition Data,S. Muhammad Hossein Mousavi,"Neural and Evolutionary Computing, Artificial Intelligence, Machine Learning, Signal Processing",
1133,679d459debd8ffd557a2b2da,cs.AI,https://arxiv.org/pdf/2501.16352,Mixture of Experts (MoE): A Big Data Perspective,"Wensheng Gan, Zhenyao Ning, Zhenlian Qi, Philip S. Yu","Machine Learning, Artificial Intelligence","As the era of big data arrives, traditional artificial intelligence algorithms have difficulty processing the demands of massive and diverse data. Mixture of experts (MoE) has shown excellent performance and broad application prospects. This paper provides an in-depth review and analysis of the latest progress in this field from multiple perspectives, including the basic principles, algorithmic models, key technical challenges, and application practices of MoE. First, we introduce the basic concept of MoE and its core idea and elaborate on its advantages over traditional single models. Then, we discuss the basic architecture of MoE and its main components, including the gating network, expert networks, and learning algorithms. Next, we review the applications of MoE in addressing key technical issues in big data. For each challenge, we provide specific MoE solutions and their innovations. Furthermore, we summarize the typical use cases of MoE in various application domains. This fully demonstrates the powerful capability of MoE in big data processing. We also analyze the advantages of MoE in big data environments. Finally, we explore the future development trends of MoE. We believe that MoE will become an important paradigm of artificial intelligence in the era of big data. In summary, this paper systematically elaborates on the principles, techniques, and applications of MoE in big data processing, providing theoretical and practical references to further promote the application of MoE in real scenarios."
1134,679d459debd8ffd557a2b2db,cs.AI,https://arxiv.org/pdf/2501.16349,Risk-Informed Diffusion Transformer for Long-Tail Trajectory Prediction in the Crash Scenario,"Junlan Chen, Pei Liu, Zihao Zhang, Hongyi Zhao, Yufei Ji, Ziyuan Pu","Machine Learning, Artificial Intelligence",
1135,679d459debd8ffd557a2b2dc,cs.AI,https://arxiv.org/pdf/2501.16348,An Integrated Approach to AI-Generated Content in e-health,"Tasnim Ahmed, Salimur Choudhury","Machine Learning, Artificial Intelligence","Artificial Intelligence-Generated Content, a subset of Generative Artificial Intelligence, holds significant potential for advancing the e-health sector by generating diverse forms of data. In this paper, we propose an end-to-end class-conditioned framework that addresses the challenge of data scarcity in health applications by generating synthetic medical images and text data, evaluating on practical applications such as retinopathy detection, skin infections and mental health assessments. Our framework integrates Diffusion and Large Language Models (LLMs) to generate data that closely match real-world patterns, which is essential for improving downstream task performance and model robustness in e-health applications.
Experimental results demonstrate that the synthetic images produced by the proposed diffusion model outperform traditional GAN architectures. Similarly, in the text modality, data generated by uncensored LLM achieves significantly better alignment with real-world data than censored models in replicating the authentic tone."
1136,679d459debd8ffd557a2b2dd,cs.AI,https://arxiv.org/pdf/2501.16347,Identification of Hardware Trojan Locations in Gate-Level Netlist using Nearest Neighbour Approach integrated with Machine Learning Technique,"Anindita Chattopadhyay, Siddharth Bisariya, Vijay Kumar Sutrakar","Machine Learning, Artificial Intelligence",
1137,679d459debd8ffd557a2b2de,cs.AI,https://arxiv.org/pdf/2501.16346,Self-supervised Graph Transformer with Contrastive Learning for Brain Connectivity Analysis towards Improving Autism Detection,"Yicheng Leng, Syed Muhammad Anwar, Islem Rekik, Sen He, Eung-Joo Lee","Machine Learning, Artificial Intelligence","Functional Magnetic Resonance Imaging (fMRI) provides useful insights into the brain function both during task or rest. Representing fMRI data using correlation matrices is found to be a reliable method of analyzing the inherent connectivity of the brain in the resting and active states. Graph Neural Networks (GNNs) have been widely used for brain network analysis due to their inherent explainability capability. In this work, we introduce a novel framework using contrastive self-supervised learning graph transformers, incorporating a brain network transformer encoder with random graph alterations. The proposed network leverages both contrastive learning and graph alterations to effectively train the graph transformer for autism detection. Our approach, tested on Autism Brain Imaging Data Exchange (ABIDE) data, demonstrates superior autism detection, achieving an AUROC of82.682.682.682.6and an accuracy of 74%, surpassing current state-of-the-art methods."
1138,679d459debd8ffd557a2b2df,cs.AI,https://arxiv.org/pdf/2501.16345,Self-Clustering Graph Transformer Approach to Model Resting-State Functional Brain Activity,"Bishal Thapaliya, Esra Akbas, Ram Sapkota, Bhaskar Ray, Vince Calhoun, Jingyu Liu","Machine Learning, Artificial Intelligence","Resting-state functional magnetic resonance imaging (rs-fMRI) offers valuable insights into the human brain’s functional organization and is a powerful tool for investigating the relationship between brain function and cognitive processes as it allows for the functional organization of the brain to be captured without relying on a specific task or stimuli. In this study, we introduce a novel attention mechanism for graphs with subnetworks, named Self Clustering Graph Transformer (SCGT), designed to handle the issue of uniform node updates in graph transformers. By using static functional connectivity (FC) correlation features as input to the transformer model,SCGTeffectively captures the sub-network structure of the brain by performing cluster-specific updates to the nodes unlike uniform node updates like vanilla graph transformers, further allowing us to learn and interpret the subclusters. We validate our approach on the Adolescent Brain Cognitive Development (ABCD) dataset, comprising 7,957 participants, for the prediction of total cognitive score and gender classification. Our results demonstrate thatSCGToutperforms the vanilla graph transformer method, and other recent models, offering a promising tool for modeling brain functional connectivity and interpreting the underlying subnetwork structures."
1139,679d459debd8ffd557a2b2e0,cs.AI,https://arxiv.org/pdf/2501.16337,Explore Activation Sparsity in Recurrent LLMs for Energy-Efficient Neuromorphic Computing,"Ivan Knunyants, Maryam Tavakol, Manolis Sifalakis, Yingfu Xu, Amirreza Yousefzadeh, Guangzhi Tang","Neural and Evolutionary Computing, Artificial Intelligence, Hardware Architecture, Machine Learning","The recent rise of Large Language Models (LLMs) has revolutionized the deep learning field. However, the desire to deploy LLMs on edge devices introduces energy efficiency and latency challenges. Recurrent LLM (R-LLM) architectures have proven effective in mitigating the quadratic complexity of self-attention, making them a potential paradigm for computing on-edge neuromorphic processors. In this work, we propose a low-cost, training-free algorithm to sparsify R-LLMs’ activations to enhance energy efficiency on neuromorphic hardware. Our approach capitalizes on the inherent structure of these models, rendering them well-suited for energy-constrained environments. Although primarily designed for R-LLMs, this method can be generalized to other LLM architectures, such as transformers, as demonstrated on the OPT model, achieving comparable sparsity and efficiency improvements. Empirical studies illustrate that our method significantly reduces computational demands while maintaining competitive accuracy across multiple zero-shot learning benchmarks. Additionally, hardware simulations with the SENECA neuromorphic processor underscore notable energy savings and latency improvements. These results pave the way for low-power, real-time neuromorphic deployment of LLMs and demonstrate the feasibility of training-free on-chip adaptation using activation sparsity."
1140,679d459debd8ffd557a2b2e1,cs.AI,https://arxiv.org/pdf/2501.16336,Runtime Analysis of Evolutionary Algorithms for Multiparty Multiobjective Optimization,"Yuetong Sun, Peilan Xu, Wenjian Luo","Neural and Evolutionary Computing, Artificial Intelligence","In scenarios where multiple decision-makers operate within a common decision space, each focusing on their own multi-objective optimization problem (e.g., bargaining games), the problem can be modeled as a multi-party multi-objective optimization problem (MPMOP). While numerous evolutionary algorithms have been proposed to solve MPMOPs, most results remain empirical. This paper presents the first theoretical analysis of the expected runtime of evolutionary algorithms on bi-party multi-objective optimization problems (BPMOPs). Our findings demonstrate that employing traditional multi-objective optimization algorithms to solve MPMOPs is both time-consuming and inefficient, as the resulting population contains many solutions that fail to achieve consensus among decision-makers. An alternative approach involves decision-makers individually solving their respective optimization problems and seeking consensus only in the final stage. While feasible for pseudo-Boolean optimization problems, this method may fail to guarantee approximate performance for one party in NP-hard problems. Finally, We propose coevolutionary multi-party multi-objective optimizers (CoEMPMO) for pseudo-Boolean optimization and shortest path problems within a multi-party multi-objective context, which maintains a common solution set among all parties through coevolution. Theoretical and experimental results demonstrate that the proposedCoEMPMOrandomsubscriptCoEMPMOrandom\text{CoEMPMO}_{\text{random}}CoEMPMO start_POSTSUBSCRIPT random end_POSTSUBSCRIPToutperforms previous algorithms in terms of the expected lower bound on runtime for pseudo-Boolean optimization problems. Additionally,CoEMPMOconsSPsuperscriptsubscriptCoEMPMOconsSP\text{CoEMPMO}_{\text{cons}}^{\text{SP}}CoEMPMO start_POSTSUBSCRIPT cons end_POSTSUBSCRIPT start_POSTSUPERSCRIPT SP end_POSTSUPERSCRIPTachieves better efficiency and precision in solving shortest path problems compared to existing algorithms."
1141,679d459debd8ffd557a2b2e2,cs.AI,https://arxiv.org/pdf/2501.16331,Decoding OTC Government Bond Market Liquidity: An ABM Model for Market Dynamics,"Alicia Vidler, Toby Walsh","Trading and Market Microstructure, Artificial Intelligence","The over-the-counter (OTC) government bond markets are characterised by their bilateral trading structures, which pose unique challenges to understanding and ensuring market stability and liquidity. In this paper, we develop a bespoke ABM that simulates market-maker interactions within a stylised government bond market. The model focuses on the dynamics of liquidity and stability in the secondary trading of government bonds, particularly in concentrated markets like those found in Australia and the UK. Through this simulation, we test key hypotheses around improving market stability, focusing on the effects of agent diversity, business costs, and client base size. We demonstrate that greater agent diversity enhances market liquidity and that reducing the costs of market-making can improve overall market stability. The model offers insights into computational finance by simulating trading without price transparency, highlighting how micro-structural elements can affect macro-level market outcomes. This research contributes to the evolving field of computational finance by employing computational intelligence techniques to better understand the fundamental mechanics of government bond markets, providing actionable insights for both academics and practitioners."
1142,679d459debd8ffd557a2b2e3,cs.AI,https://arxiv.org/pdf/2501.16215,Enhancing Visual Inspection Capability of Multi-Modal Large Language Models on Medical Time Series with Supportive Conformalized and Interpretable Small Specialized Models,"Huayu Li, Xiwen Chen, Ci Zhang, Stuart F. Quan, William D.S. Killgore, Shu-Fen Wung, Chen X. Chen, Geng Yuan, Jin Lu, Ao Li","Artificial Intelligence, Machine Learning, Signal Processing","Large language models (LLMs) exhibit remarkable capabilities in visual inspection of medical time-series data, achieving proficiency comparable to human clinicians. However, their broad scope limits domain-specific precision, and proprietary weights hinder fine-tuning for specialized datasets. In contrast, small specialized models (SSMs) excel in targeted tasks but lack the contextual reasoning required for complex clinical decision-making. To address these challenges, we proposeConMIL(ConformalizedMultipleInstanceLearning), a decision-support SSM that integrates seamlessly with LLMs. By using Multiple Instance Learning (MIL) to identify clinically significant signal segments and conformal prediction for calibrated set-valued outputs,ConMILenhances LLMs’ interpretative capabilities for medical time-series analysis. Experimental results demonstrate thatConMILsignificantly improves the performance of state-of-the-art LLMs, such as ChatGPT4.0 and Qwen2-VL-7B. Specifically,ConMIL-supported Qwen2-VL-7B achieves 94.92% and 96.82% precision for confident samples in arrhythmia detection and sleep staging, compared to standalone LLM accuracy of 46.13% and 13.16%. These findings highlight the potential ofConMILto bridge task-specific precision and broader contextual reasoning, enabling more reliable and interpretable AI-driven clinical decision support."
1143,679d459debd8ffd557a2b2e4,cs.AI,https://arxiv.org/pdf/2501.16207,From Informal to Formal -- Incorporating and Evaluating LLMs on Natural Language Requirements to Verifiable Formal Proofs,"Jialun Cao, Yaojie Lu, Meiziniu Li, Haoyang Ma, Haokun Li, Mengda He, Cheng Wen, Le Sun, Hongyu Zhang, Shengchao Qin, Shing-Chi Cheung, Cong Tian",Artificial Intelligence,"The research in AI-based formal mathematical reasoning has shown an unstoppable growth trend. These studies have excelled in mathematical competitions like IMO, showing significant progress. However, these studies intertwined multiple skills simultaneously—problem-solving, reasoning, and writing formal specifications—making it hard to precisely identify the LLMs’ strengths and weaknesses in each task. This paper focuses on formal verification, an immediate application scenario of formal reasoning, and decomposes it into six sub-tasks.
We constructed 18k high-quality instruction-response pairs across five mainstream formal specification languages (Coq, Lean4, Dafny, ACSL, and TLA+) in six formal-verification-related tasks by distilling gpt-4o. They are split into a 14k+ fine-tuning datasetfm-alpacaand a 4k benchmarkfm-bench.
We found that LLMs are good at writing proof segments when given either the code, or the detailed description of proof steps. Also, the fine-tuning brought about a nearly threefold improvement at most.
And interestingly, we observed that fine-tuning with formal data also enhances abilities in mathematics, reasoning, and coding. We hope our findings inspire further research. Fine-tuned models are released to facilitate subsequent studies."
1144,679d459debd8ffd557a2b2e5,cs.AI,https://arxiv.org/pdf/2501.16150,"AI Agents for Computer Use: A Review of Instruction-based Computer Control, GUI Automation, and Operator Assistants","Pascal J. Sager, Benjamin Meyer, Peng Yan, Rebekka von Wartburg-Kottler, Layan Etaiwi, Aref Enayati, Gabriel Nobel, Ahmed Abdulkadir, Benjamin F. Grewe, Thilo Stadelmann","Artificial Intelligence, Human-Computer Interaction, Systems and Control",
1145,679d459debd8ffd557a2b2e6,cs.AI,https://arxiv.org/pdf/2501.15972,Flexible Blood Glucose Control: Offline Reinforcement Learning from Human Feedback,"Harry Emerson, Sam Gordon James, Matthew Guy, Ryan McConville","Artificial Intelligence, Machine Learning","Reinforcement learning (RL) has demonstrated success in automating insulin dosing in simulated type 1 diabetes (T1D) patients but is currently unable to incorporate patient expertise and preference. This work introduces PAINT (Preference Adaptation for INsulin control in T1D), an original RL framework for learning flexible insulin dosing policies from patient records. PAINT employs a sketch-based approach for reward learning, where past data is annotated with a continuous reward signal to reflect patient’s desired outcomes. Labelled data trains a reward model, informing the actions of a novel safety-constrained offline RL algorithm, designed to restrict actions to a safe strategy and enable preference tuning via a sliding scale. In-silico evaluation shows PAINT achieves common glucose goals through simple labelling of desired states, reducing glycaemic risk by 15% over a commercial benchmark. Action labelling can also be used to incorporate patient expertise, demonstrating an ability to pre-empt meals (+10% time-in-range post-meal) and address certain device errors (-1.6% variance post-error) with patient guidance. These results hold under realistic conditions, including limited samples, labelling errors, and intra-patient variability. This work illustrates PAINT’s potential in real-world T1D management and more broadly any tasks requiring rapid and precise preference learning under safety constraints."
1146,679d459debd8ffd557a2b2e7,cs.AI,https://arxiv.org/pdf/2501.15791,Harnessing Diverse Perspectives: A Multi-Agent Framework for Enhanced Error Detection in Knowledge Graphs,"Yu Li, Yi Huang, Guilin Qi, Junlan Feng, Nan Hu, Songlin Zhai, Haohan Xue, Yongrui Chen, Ruoyan Shen, Tongtong Wu","Artificial Intelligence, Multiagent Systems","Knowledge graphs are widely used in industrial applications, making error detection crucial for ensuring the reliability of downstream applications.
Existing error detection methods often fail to effectively leverage fine-grained subgraph information and rely solely on fixed graph structures, while also lacking transparency in their decision-making processes, which results in suboptimal detection performance.
In this paper, we propose a novel Multi-Agent framework for Knowledge Graph Error Detection (MAKGED) that utilizes multiple large language models (LLMs) in a collaborative setting.
By concatenating fine-grained, bidirectional subgraph embeddings with LLM-based query embeddings during training, our framework integrates these representations to produce four specialized agents. These agents utilize subgraph information from different dimensions to engage in multi-round discussions, thereby improving error detection accuracy and ensuring a transparent decision-making process.
Extensive experiments on FB15K and WN18RR demonstrate that MAKGED outperforms state-of-the-art methods, enhancing the accuracy and robustness of KG evaluation.
For specific industrial scenarios, our framework can facilitate the training of specialized agents using domain-specific knowledge graphs for error detection, which highlights the potential industrial application value of our framework.
Our code and datasets are available athttps://github.com/kse-ElEvEn/MAKGED."
1147,679d459debd8ffd557a2b2e8,cs.AI,https://arxiv.org/pdf/2501.15749,LLM-powered Multi-agent Framework for Goal-oriented Learning in Intelligent Tutoring System,"Tianfu Wang, Yi Zhan, Jianxun Lian, Zhengyu Hu, Nicholas Jing Yuan, Qi Zhang, Xing Xie, Hui Xiong","Artificial Intelligence, Multiagent Systems","Intelligent Tutoring Systems (ITSs) have revolutionized education by offering personalized learning experiences. However, as goal-oriented learning, which emphasizes efficiently achieving specific objectives, becomes increasingly important in professional contexts, existing ITSs often struggle to deliver this type of targeted learning experience. In this paper, we proposeGenMentor, an LLM-powered multi-agent framework designed to deliver goal-oriented, personalized learning within ITS. GenMentor begins by accurately mapping learners’ goals to required skills using a fine-tuned LLM trained on a custom goal-to-skill dataset. After identifying the skill gap, it schedules an efficient learning path using an evolving optimization approach, driven by a comprehensive and dynamic profile of learners’ multifaceted status. Additionally, GenMentor tailors learning content with an exploration-drafting-integration mechanism to align with individual learner needs. Extensive automated and human evaluations demonstrate GenMentor’s effectiveness in learning guidance and content quality. Furthermore, we have deployed it in practice and also implemented it as an application. Practical human study with professional learners further highlights its effectiveness in goal alignment and resource targeting, leading to enhanced personalization. Supplementary resources are available athttps://github.com/GeminiLight/gen-mentor."
1148,679d459debd8ffd557a2b2e9,cs.AI,https://arxiv.org/pdf/2501.15740,Propositional Interpretability in Artificial Intelligence,David J. Chalmers,Artificial Intelligence,"Mechanistic interpretability is the program of explaining what AI systems are doing in terms of their internal mechanisms. I analyze some aspects of the program, along with setting out some concrete challenges and assessing progress to date. I argue for the importance of propositional interpretability, which involves interpreting a system’s mechanisms and behavior in terms of propositional attitudes: attitudes (such as belief, desire, or subjective probability) to propositions (e.g. the proposition that it is hot outside). Propositional attitudes are the central way that we interpret and explain human beings and they are likely to be central in AI too. A central challenge is what I call thought logging: creating systems that log all of the relevant propositional attitudes in an AI system over time. I examine currently popular methods of interpretability (such as probing, sparse auto-encoders, and chain of thought methods) as well as philosophical methods of interpretation (including those grounded in psychosemantics) to assess their strengths and weaknesses as methods of propositional interpretability."
1149,679d459debd8ffd557a2b2ea,cs.AI,https://arxiv.org/pdf/2501.15495,Expert-Free Online Transfer Learning in Multi-Agent Reinforcement Learning,Alberto Castagna,"Artificial Intelligence, Machine Learning, Multiagent Systems","Reinforcement Learning(RL) enables an intelligent agent to optimise its performance in a task by continuously taking action from an observed state and receiving a feedback from the environment in form of rewards.RLtypically uses tables or linear approximators to map state-action tuples that maximises the reward.
CombiningRLwith deep neural networks (DRL) significantly increases its scalability and enables it to address more complex problems than before.
However,DRLalso inherits downsides from bothRLand deep learning.
DespiteDRLimproves generalisation across similar state-action pairs when compared to simplerRLpolicy representations like tabular methods, it still requires the agent to adequately explore the state-action space. Additionally, deep methods require more training data, with the volume of data escalating with the complexity and size of the neural network.
As a result, deepRLrequiresalong time to collect enough agent-environment samples and to successfully learn the underlying policy.Furthermore, often even a slight alteration to the taskinvalidates any previous acquired knowledge."
1150,679d459debd8ffd557a2b2eb,cs.AI,https://arxiv.org/pdf/2501.15489,AI in Oncology: Transforming Cancer Detection through Machine Learning and Deep Learning Applications,"Muhammad Aftab, Faisal Mehmood, Chengjuan Zhang, Alishba Nadeem, Zigang Dong, Yanan Jiang, Kangdongs Liu","Artificial Intelligence, Image and Video Processing, Quantitative Methods",
1151,679d459debd8ffd557a2b2ec,cs.AI,https://arxiv.org/pdf/2501.15404,A Neurosymbolic Framework for Geometric Reduction of Binary Forms,"Ilias Kotsireas, Tony Shaska","Artificial Intelligence, Machine Learning","This paper compares Julia reduction and hyperbolic reduction with the aim of finding equivalent binary forms with minimal coefficients. We demonstrate that hyperbolic reduction generally outperforms Julia reduction, particularly in the cases of sextics and decimics, though neither method guarantees achieving the minimal form. We further propose an additional shift and scaling to approximate the minimal form more closely. Finally, we introduce a machine learning framework to identify optimal transformations that minimize the heights of binary forms. This study provides new insights into the geometry and algebra of binary forms and highlights the potential of AI in advancing symbolic computation and reduction techniques. The findings, supported by extensive computational experiments, lay the groundwork for hybrid approaches that integrate traditional reduction methods with data-driven techniques."
1152,679d459debd8ffd557a2b2ed,cs.AI,https://arxiv.org/pdf/2501.15378,How to Mitigate Information Loss in Knowledge Graphs for GraphRAG: Leveraging Triple Context Restoration and Query-Driven Feedback,"Manzong Huang, Chenyang Bu, Yi He, Xindong Wu","Artificial Intelligence, Information Retrieval","Knowledge Graph (KG)-augmented Large Language Models (LLMs)
have recently propelled significant advances in complex reasoning tasks,
thanks to their broad domain knowledge and contextual awareness.
Unfortunately, current methods often assume KGs to be complete,
which is impractical
given the inherent limitations of KG construction and the potential loss of contextual cues when converting unstructured text into entity-relation triples.
In response,
this paper proposes the Triple Context Restoration and Query-driven Feedback (TCR-QF) framework, which reconstructs the textual context underlying each triple to mitigate information loss, while dynamically refining the KG structure
by iteratively incorporating query-relevant missing knowledge.
Experiments on five benchmark question-answering datasets substantiate the effectiveness of TCR-QF in KG and LLM integration, where it
achieves a 29.1% improvement in Exact Match and a 15.5% improvement in F1
over its state-of-the-art GraphRAG competitors."
1153,679d459debd8ffd557a2b2ee,cs.AI,https://arxiv.org/pdf/2501.15280,Who's Driving? Game Theoretic Path Risk of AGI Development,Robin Young,"Artificial Intelligence, Computers and Society, Computer Science and Game Theory","Who controls the development of Artificial General Intelligence (AGI) might matter less than how we handle the fight for control itself. We formalize this ”steering wheel problem” as humanity’s greatest near-term existential risk may stem not from misaligned AGI, but from the dynamics of competing to develop it. Just as a car crash can occur from passengers fighting over the wheel before reaching any destination, catastrophic outcomes could arise from development competition long before AGI exists. While technical alignment research focuses on ensuring safe arrival, we show how coordination failures during development could drive us off the cliff first."
1154,679d459debd8ffd557a2b2ef,cs.AI,https://arxiv.org/pdf/2501.15249,An Automatic Sound and Complete Abstraction Method for Generalized Planning with Baggable Types,"Hao Dong, Zheyuan Shi, Hemeng Zeng, Yongmei Liu",Artificial Intelligence,"Generalized planning is concerned with how to find a single plan to solve multiple similar planning instances. Abstractions are widely used for solving generalized planning, and QNP (qualitative numeric planning) is a popular abstract model. Recently, Cui et al. showed that a plan solves a sound and complete abstraction of a generalized planning problem if and only if the refined plan solves the original problem. However, existing work on automatic abstraction for generalized planning can hardly guarantee soundness let alone completeness. In this paper, we propose an automatic sound and complete abstraction method for generalized planning with baggable types. We use a variant of QNP, called bounded QNP (BQNP), where integer variables are increased or decreased by only one. Since BQNP is undecidable, we propose and implement a sound but incomplete solver for BQNP. We present an automatic method to abstract a BQNP problem from a classical planning instance with baggable types. The basic idea for abstraction is to introduce a counter for each bag of indistinguishable tuples of objects. We define a class of domains called proper baggable domains, and show that for such domains, the BQNP problem got by our automatic method is a sound and complete abstraction for a generalized planning problem whose instances share the same bags with the given instance but the sizes of the bags might be different. Thus, the refined plan of a solution to the BQNP problem is a solution to the generalized planning problem. Finally, we implement our abstraction method and experiments on a number of domains demonstrate the promise of our approach."
1155,679d459debd8ffd557a2b2f0,cs.AI,https://arxiv.org/pdf/2501.15147,A Causality-aware Paradigm for Evaluating Creativity of Multimodal Large Language Models,"Zhongzhan Huang, Shanshan Zhong, Pan Zhou, Shanghua Gao, Marinka Zitnik, Liang Lin","Artificial Intelligence, Human-Computer Interaction","Recently, numerous benchmarks have been developed to evaluate the logical reasoning abilities of large language models (LLMs). However, assessing the equally important creative capabilities of LLMs is challenging due to the subjective, diverse, and data-scarce nature of creativity, especially in multimodal scenarios. In this paper, we consider the comprehensive pipeline for evaluating the creativity of multimodal LLMs, with a focus on suitable evaluation platforms and methodologies. First, we find the Oogiri game—a creativity-driven task requiring humor, associative thinking, and the ability to produce unexpected responses to text, images, or both. This game aligns well with the input-output structure of modern multimodal LLMs and benefits from a rich repository of high-quality, human-annotated creative responses, making it an ideal platform for studying LLM creativity. Next, beyond using the Oogiri game for standard evaluations like ranking and selection, we propose LoTbench, an interactive, causality-aware evaluation framework, to further address some intrinsic risks in standard evaluations, such as information leakage and limited interpretability. The proposed LoTbench not only quantifies LLM creativity more effectively but also visualizes the underlying creative thought processes. Our results show that while most LLMs exhibit constrained creativity, the performance gap between LLMs and humans is not insurmountable. Furthermore, we observe a strong correlation between results from the multimodal cognition benchmark MMMU and LoTbench, but only a weak connection with traditional creativity metrics. This suggests that LoTbench better aligns with human cognitive theories, highlighting cognition as a critical foundation in the early stages of creativity and enabling the bridging of diverse concepts.Project Page."
1156,679d459debd8ffd557a2b2f1,cs.AI,https://arxiv.org/pdf/2501.15105,A New Approach for Knowledge Generation Using Active Inference,"Jamshid Ghasimi, Nazanin Movarraei","Artificial Intelligence, Neurons and Cognition",
1157,679d459debd8ffd557a2b2f2,cs.AI,https://arxiv.org/pdf/2501.15085,Data Center Cooling System Optimization Using Offline Reinforcement Learning,"Xianyuan Zhan, Xiangyu Zhu, Peng Cheng, Xiao Hu, Ziteng He, Hanfei Geng, Jichao Leng, Huiwen Zheng, Chenhui Liu, Tianshun Hong, Yan Liang, Yunxin Liu, Feng Zhao","Artificial Intelligence, Machine Learning, Systems and Control","The recent advances in information technology and artificial intelligence have fueled a rapid expansion of the data center (DC) industry worldwide, accompanied by an immense appetite for electricity to power the DCs. In a typical DC, around 30∼similar-to\sim∼40% of the energy is spent on the cooling system rather than on computer servers, posing a pressing need for developing new energy-saving optimization technologies for DC cooling systems. However, optimizing such real-world industrial systems faces numerous challenges, including but not limited to
a lack of reliable simulation environments, limited historical data, and stringent safety and control robustness requirements.
In this work, we present a novel physics-informed offline reinforcement learning (RL) framework for energy efficiency optimization of DC cooling systems.
The proposed framework models the complex dynamical patterns and physical dependencies inside a server room using a purposely designed graph neural network architecture that is compliant with the fundamental time-reversal symmetry. Because of its well-behaved and generalizable state-action representations, the model enables sample-efficient and robust latent space offline policy learning using limited real-world operational data.
Our framework has been successfullydeployed and verifiedin a large-scale production DC for closed-loop control of its air-cooling units (ACUs). We conducted a total of 2000 hours of short and long-term experiments in the production DC environment. The results show that our method achieves 14∼similar-to\sim∼21% energy savings in the DC cooling system, without any violation of the safety or operational constraints. We have also conducted a comprehensive evaluation of our approach in a real-world DC testbed environment.
Our results have demonstrated the significant potential of offline RL in solving a broad range of data-limited, safety-critical real-world industrial control problems."
1158,679d459debd8ffd557a2b2f3,cs.AI,https://arxiv.org/pdf/2501.15007,Controllable Protein Sequence Generation with LLM Preference Optimization,"Xiangyu Liu, Yi Liu, Silei Chen, Wei Hu","Artificial Intelligence, Computational Engineering, Finance, and Science, Quantitative Methods","Designing proteins with specific attributes offers an important solution to address biomedical challenges.
Pre-trained protein large language models (LLMs) have shown promising results on protein sequence generation.
However, to control sequence generation for specific attributes, existing work still exhibits poor functionality and structural stability.
In this paper, we propose a novel controllable protein design method called CtrlProt.
We finetune a protein LLM with a new multi-listwise preference optimization strategy to improve generation quality and support multi-attribute controllable generation.
Experiments demonstrate that CtrlProt can meet functionality and structural stability requirements effectively, achieving state-of-the-art performance in both single-attribute and multi-attribute protein sequence generation."
1159,679d459debd8ffd557a2b2f4,cs.AI,https://arxiv.org/pdf/2501.14954,MISCON: A Mission-Driven Conversational Consultant for Pre-Venture Entrepreneurs in Food Deserts,"Subhasis Dasgupta, Hans Taparia, Laura Schmidt, Amarnath Gupta","Artificial Intelligence, Computation and Language, Information Retrieval","This work-in-progress report describes MISCON, a conversational consultant being developed for a public mission project called NOURISH. With MISCON, aspiring small business owners in a food-insecure region and their advisors in Community-based organizations would be able to get information, recommendation and analysis regarding setting up food businesses. MISCON conversations are modeled as state machine that uses a heterogeneous knowledge graph as well as several analytical tools and services including a variety of LLMs. In this short report, we present the functional architecture and some design considerations behind MISCON."
1160,679d459debd8ffd557a2b2f5,cs.AI,https://arxiv.org/pdf/2501.14892,Causal Graphs Meet Thoughts: Enhancing Complex Reasoning in Graph-Augmented LLMs,"Hang Luo, Jian Zhang, Chujun Li","Artificial Intelligence, Computation and Language","In knowledge-intensive tasks, especially in high-stakes domains like medicine and law, it is critical not only to retrieve relevant information but also to provide causal reasoning and explainability. Large language models (LLMs) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, integrating knowledge graphs with Graph Retrieval-Augmented Generation (Graph RAG) has emerged as an effective solution. Traditional Graph RAG methods often rely on simple graph traversal or semantic similarity, which do not capture causal relationships or align well with the model’s internal reasoning steps. This paper proposes a novel pipeline that filters large knowledge graphs to emphasize cause-effect edges, aligns the retrieval process with the model’s chain-of-thought (CoT), and enhances reasoning through multi-stage path improvements. Experiments on medical question-answering tasks show consistent gains, with up to a 10% absolute improvement across multiple large language models (LLMs). This approach demonstrates the value of combining causal reasoning with stepwise retrieval, leading to more interpretable and logically grounded solutions for complex queries."
1161,679d459debd8ffd557a2b2f6,cs.AI,https://arxiv.org/pdf/2501.14836,Symbolic Knowledge Extraction and Injection with Sub-symbolic Predictors: A Systematic Literature Review,"Giovanni Ciatto, Federico Sabbatini, Andrea Agiollo, Matteo Magnini, Andrea Omicini","Artificial Intelligence, Machine Learning, Logic in Computer Science",
1162,679d459debd8ffd557a2b2f7,cs.AI,https://arxiv.org/pdf/2501.16329,sDREAMER: Self-distilled Mixture-of-Modality-Experts Transformer for Automatic Sleep Staging,"Jingyuan Chen, Yuan Yao, Mie Anderson, Natalie Hauglund, Celia Kjaerby, Verena Untiet, Maiken Nedergaard, Jiebo Luo","Machine Learning, Artificial Intelligence","Automatic sleep staging based on electroencephalography (EEG) and electromyography (EMG) signals is an important aspect of sleep-related research.
Current sleep staging methods suffer from two major drawbacks.
First, there are limited information interactions between modalities in the existing methods.
Second, current methods do not develop unified models that can handle different sources of input.
To address these issues, we propose a novel sleep stage scoring model sDREAMER, which emphasizes cross-modality interaction and per-channel performance. Specifically, we develop a mixture-of-modality-expert (MoME) model with three pathways for EEG, EMG, and mixed signals with partially shared weights. We further propose a self-distillation training scheme for further information interaction across modalities. Our model is trained with multi-channel inputs and can make classifications on either single-channel or multi-channel inputs. Experiments demonstrate that our model outperforms the existing transformer-based sleep scoring methods for multi-channel inference. For single-channel inference, our model also outperforms the transformer-based models trained with single-channel signals."
1163,679d459debd8ffd557a2b2f8,cs.AI,https://arxiv.org/pdf/2501.16309,Evaluating The Performance of Using Large Language Models to Automate Summarization of CT Simulation Orders in Radiation Oncology,"Meiyun Cao, Shaw Hu, Jason Sharp, Edward Clouser, Jason Holmes, Linda L. Lam, Xiaoning Ding, Diego Santos Toesca, Wendy S. Lindholm, Samir H. Patel, Sujay A. Vora, Peilong Wang, Wei Liu","Medical Physics, Artificial Intelligence","Purpose: In the current clinical workflow of radiation oncology departments, therapists manually summarize CT simulation orders into summaries before CT simulation is performed. This process increases the workload, introduces variability in documentation quality, and is prone to human errors. To address these challenges, this study aims to use a large language model (LLM) to automate the generation of summaries from the CT simulation orders and evaluate its performance."
1164,679d459debd8ffd557a2b2f9,cs.AI,https://arxiv.org/pdf/2501.16288,Upside Down Reinforcement Learning with Policy Generators,"Jacopo Di Ventura, Dylan R. Ashley, Vincent Herrmann, Francesco Faccio, Jürgen Schmidhuber","Machine Learning, Artificial Intelligence",
1165,679d459debd8ffd557a2b2fa,cs.AI,https://arxiv.org/pdf/2501.16274,What is Formal Verification without Specifications? A Survey on mining LTL Specifications,"Daniel Neider, Rajarshi Roy","Formal Languages and Automata Theory, Artificial Intelligence, Logic in Computer Science","Virtually all verification techniques using formal methods rely on the availability of a formal specification, which describes the design requirements precisely.
However, formulating specifications remains a manual task that is notoriously challenging and error-prone.
To address this bottleneck in formal verification, recent research has thus focussed on automatically generating specifications for formal verification from examples of (desired and undesired) system behavior.
In this survey, we list and compare recent advances in mining specifications in Linear Temporal Logic (LTL), the de facto standard specification language for reactive systems.
Several approaches have been designed for learning LTL formulas, which address different aspects and settings of specification design.
Moreover, the approaches rely on a diverse range of techniques such as constraint solving, neural network training, enumerative search, etc.
We survey the current state-of-the-art techniques and compare them for the convenience of the formal methods practitioners."
1166,679d459debd8ffd557a2b2fb,cs.AI,https://arxiv.org/pdf/2501.16271,From Molecules to Mixtures: Learning Representations of Olfactory Mixture Similarity using Inductive Biases,"Gary Tom, Cher Tian Ser, Ella M. Rajaonson, Stanley Lo, Hyun Suk Park, Brian K. Lee, Benjamin Sanchez-Lengeling","Machine Learning, Artificial Intelligence","Olfaction—how molecules are perceived as odors to humans—remains poorly understood. Recently, the principal odor map (POM) was introduced to digitize the olfactory properties of single compounds. However, smells in real life are not pure single molecules, but complex mixtures of molecules, whose representations remain relatively under-explored. In this work, we introducePOMMix, an extension of the POM to represent mixtures. Our representation builds upon the symmetries of the problem space in a hierarchical manner: (1) graph neural networks for building molecular embeddings, (2) attention mechanisms for aggregating molecular representations into mixture representations, and (3) cosine prediction heads to encode olfactory perceptual distance in the mixture embedding space.POMMixachieves state-of-the-art predictive performance across multiple datasets. We also evaluate the generalizability of the representation on multiple splits when applied to unseen molecules and mixture sizes. Our work advances the effort to digitize olfaction, and highlights the synergy of domain expertise and deep learning in crafting expressive representations in low-data regimes."
1167,679d459debd8ffd557a2b2fc,cs.AI,https://arxiv.org/pdf/2501.16243,Accelerating Quantum Reinforcement Learning with a Quantum Natural Policy Gradient Based Approach,"Yang Xu, Vaneet Aggarwal","Quantum Physics, Artificial Intelligence, Machine Learning","We address the problem of quantum reinforcement learning (QRL) under model-free settings with quantum oracle access to the Markov Decision Process (MDP). This paper introduces a Quantum Natural Policy Gradient (QNPG) algorithm, which replaces the random sampling used in classical Natural Policy Gradient (NPG) estimators with a deterministic gradient estimation approach, enabling seamless integration into quantum systems. While this modification introduces a bounded bias in the estimator, the bias decays exponentially with increasing truncation levels. This paper demonstrates that the proposed QNPG algorithm achieves a sample complexity of𝒪~⁢(ϵ−1.5)~𝒪superscriptitalic-ϵ1.5\tilde{\mathcal{O}}(\epsilon^{-1.5})over~ start_ARG caligraphic_O end_ARG ( italic_ϵ start_POSTSUPERSCRIPT - 1.5 end_POSTSUPERSCRIPT )for queries to the quantum oracle, significantly improving the classical lower bound of𝒪~⁢(ϵ−2)~𝒪superscriptitalic-ϵ2\tilde{\mathcal{O}}(\epsilon^{-2})over~ start_ARG caligraphic_O end_ARG ( italic_ϵ start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT )for queries to the MDP."
1168,679d459debd8ffd557a2b2fd,cs.AI,https://arxiv.org/pdf/2501.16224,Language-Based Bayesian Optimization Research Assistant (BORA),"Abdoulatif Cissé, Xenophon Evangelopoulos, Vladimir V. Gusev, Andrew I. Cooper","Machine Learning, Artificial Intelligence","Many important scientific problems involve multivariate optimization coupled with slow and laborious experimental measurements. These complex, high-dimensional searches can be defined by non-convex optimization landscapes that resemble needle-in-a-haystack surfaces, leading to entrapment in local minima. Contextualizing optimizers with human domain knowledge is a powerful approach to guide searches to localized fruitful regions. However, this approach is susceptible to human confirmation bias and it is also challenging for domain experts to keep track of the rapidly expanding scientific literature. Here, we propose the use of Large Language Models (LLMs) for contextualizing Bayesian optimization (BO) via a hybrid optimization framework that intelligently and economically blends stochastic inference with domain knowledge-based insights from the LLM, which is used to suggest new, better-performing areas of the search space for exploration. Our method fosters user engagement by offering real-time commentary on the optimization progress, explaining the reasoning behind the search strategies. We validate the effectiveness of our approach on synthetic benchmarks with up to 15 independent variables and demonstrate the ability of LLMs to reason in four real-world experimental tasks where context-aware suggestions boost optimization performance substantially."
1169,679d459debd8ffd557a2b2fe,cs.AI,https://arxiv.org/pdf/2501.16191,Raiders of the Lost Dependency: Fixing Dependency Conflicts in Python using LLMs,"Antony Bartlett, Cynthia Liem, Annibale Panichella","Software Engineering, Artificial Intelligence","Fixing Python dependency issues is a tedious and error-prone task for developers, who must manually identify and resolve environment dependencies and version constraints of third-party modules and Python interpreters."
1170,679d459debd8ffd557a2b2ff,cs.AI,https://arxiv.org/pdf/2501.16164,MetaDecorator: Generating Immersive Virtual Tours through Multimodality,"Shuang Xie, Yang Liu, Jeannie S.A. Lee, Haiwei Dong","Human-Computer Interaction, Artificial Intelligence, Emerging Technologies, Multimedia","MetaDecorator, is a framework that empowers users to personalize virtual spaces. By leveraging text-driven prompts and image synthesis techniques, MetaDecorator adorns static panoramas captured by 360° imaging devices, transforming them into uniquely styled and visually appealing environments. This significantly enhances the realism and engagement of virtual tours compared to traditional offerings. Beyond the core framework, we also discuss the integration of Large Language Models (LLMs) and haptics in the VR application to provide a more immersive experience."
1171,679d459debd8ffd557a2b300,cs.AI,https://arxiv.org/pdf/2501.16142,Towards General-Purpose Model-Free Reinforcement Learning,"Scott Fujimoto, Pierluca D'Oro, Amy Zhang, Yuandong Tian, Michael Rabbat","Machine Learning, Artificial Intelligence",
1172,679d459debd8ffd557a2b301,cs.AI,https://arxiv.org/pdf/2501.16061,The Unbearable Lightness of Prompting: A Critical Reflection on the Environmental Impact of genAI use in Design Education,"Maria Luce Lupetti, Elena Cavallin, Dave Murray-Rust","Human-Computer Interaction, Artificial Intelligence",
1173,679d459debd8ffd557a2b302,cs.AI,https://arxiv.org/pdf/2501.16050,Skeleton-Guided-Translation: A Benchmarking Framework for Code Repository Translation with Fine-Grained Quality Evaluation,"Xing Zhang, Jiaheng Wen, Fangkai Yang, Pu Zhao, Yu Kang, Junhao Wang, Maoquan Wang, Yufan Huang, Elsie Nallipogu, Qingwei Lin, Yingnong Dang, Saravan Rajmohan, Dongmei Zhang, Qi Zhang","Software Engineering, Artificial Intelligence","The advancement of large language models has intensified the need to modernize enterprise applications and migrate legacy systems to secure, versatile languages. However, existing code translation benchmarks primarily focus on individual functions, overlooking the complexities involved in translating entire repositories, such as maintaining inter-module coherence and managing dependencies. While some recent repository-level translation benchmarks attempt to address these challenges, they still face limitations, including poor maintainability and overly coarse evaluation granularity, which make them less developer-friendly."
1174,679d459debd8ffd557a2b303,cs.AI,https://arxiv.org/pdf/2501.16033,PRISMe: A Novel LLM-Powered Tool for Interactive Privacy Policy Assessment,"Vincent Freiberger, Arthur Fleig, Erik Buchmann","Human-Computer Interaction, Artificial Intelligence","Protecting online privacy requires users to engage with and comprehend website privacy policies, but many policies are difficult and tedious to read.
We present PRISMe (Privacy Risk Information Scanner for Me), a novel Large Language Model (LLM)-driven privacy policy assessment tool, which helps users to understand the essence of a lengthy, complex privacy policy while browsing.
The tool, a browser extension, integrates a dashboard and an LLM chat.
One major contribution is the first rigorous evaluation of such a tool.
In a mixed-methods user study (N=22), we evaluate PRISMe’s efficiency, usability, understandability of the provided information, and impacts on awareness.
While our tool improves privacy awareness by providing a comprehensible quick overview and a quality chat for in-depth discussion, users note issues with consistency and building trust in the tool.
From our insights we derive important design implications to guide future policy analysis tools."
1175,679d459debd8ffd557a2b304,cs.AI,https://arxiv.org/pdf/2501.15987,MultiPDENet: PDE-embedded Learning with Multi-time-stepping for Accelerated Flow Simulation,"Qi Wang, Yuan Mi, Haoyun Wang, Yi Zhang, Ruizhi Chengze, Hongsheng Liu, Ji-Rong Wen, Hao Sun","Numerical Analysis, Artificial Intelligence","Solving partial differential equations (PDEs) by numerical methods meet computational cost challenge for getting the accurate solution since fine grids and small time steps are required. Machine learning can accelerate this process, but struggle with weak generalizability, interpretability, and data dependency, as well as suffer in long-term prediction. To this end, we propose a PDE-embedded network with multiscale time stepping (MultiPDENet), which fuses the scheme of numerical methods and machine learning, for accelerated simulation of flows. In particular, we design a convolutional filter based on the structure of finite difference stencils with a small number of parameters to optimize, which estimates the equivalent form of spatial derivative on a coarse grid to minimize the equation’s residual. A Physics Block with a 4th-order Runge-Kutta integrator at the fine time scale is established that embeds the structure of PDEs to guide the prediction. To alleviate the curse of temporal error accumulation in long-term prediction, we introduce a multiscale time integration approach, where a neural network is used to correct the prediction error at a coarse time scale. Experiments across various PDE systems, including the Navier-Stokes equations, demonstrate that MultiPDENet can accurately predict long-term spatiotemporal dynamics, even given small and incomplete training data, e.g., spatiotemporally down-sampled datasets. MultiPDENet achieves the state-of-the-art performance compared with other neural baseline models, also with clear speedup compared to classical numerical methods."
1176,679d459debd8ffd557a2b305,cs.AI,https://arxiv.org/pdf/2501.15969,An Explainable Disease Surveillance System for Early Prediction of Multiple Chronic Diseases,"Shaheer Ahmad Khan, Muhammad Usamah Shahid, Ahmad Abdullah, Ibrahim Hashmat, Muddassar Farooq","Machine Learning, Artificial Intelligence","This study addresses a critical gap in the healthcare system by developing a clinically meaningful, practical, and explainable disease surveillance system for multiple chronic diseases, utilizing routine EHR data from multiple U.S. practices integrated with CureMD’s EMR/EHR system. Unlike traditional systems – using AI Models that use features from patients labs – our approach focuses on routinely available data – like medical history, vitals, diagnoses, and medications – to preemptively assess the risks of chronic diseases in the next year. We trained three distinct models for each chronic disease: prediction models that forecast the risk of a disease 3, 6, and 12 months before a potential diagnosis. We have developed Random Forest models, which were internally validated using the F1 scores and AUROC as the performance metrics, and were further evaluated by a panel of expert physicians for clinical relevance based on the inference grounded in the medical knowledge. Additionally, we discuss our implementation of integrating these models into a practical EMR system. Beyond using Shapely attributes and surrogate models for explainability, we also introduce a new rule engineering framework to enhance the intrinsic explainability of Random Forests."
1177,679d459debd8ffd557a2b306,cs.AI,https://arxiv.org/pdf/2501.15928,Generative AI for Lyapunov Optimization Theory in UAV-based Low-Altitude Economy Networking,"Zhang Liu, Dusit Niyato, Jiacheng Wang, Geng Sun, Lianfen Huang, Zhibin Gao, Xianbin Wang","Networking and Internet Architecture, Artificial Intelligence, Machine Learning","Lyapunov optimization theory has recently emerged as a powerful mathematical framework for solving complex stochastic optimization problems by transforming long-term objectives into a sequence of real-time short-term decisions while ensuring system stability. This theory is particularly valuable in unmanned aerial vehicle (UAV)-based low-altitude economy (LAE) networking scenarios, where it could effectively address inherent challenges of dynamic network conditions, multiple optimization objectives, and stability requirements. Recently, generative artificial intelligence (GenAI) has garnered significant attention for its unprecedented capability to generate diverse digital content. Extending beyond content generation, in this paper, we propose a framework integrating generative diffusion models with reinforcement learning to address Lyapunov optimization problems in UAV-based LAE networking. We begin by introducing the fundamentals of Lyapunov optimization theory and analyzing the limitations of both conventional methods and traditional AI-enabled approaches. We then examine various GenAI models and comprehensively analyze their potential contributions to Lyapunov optimization. Subsequently, we develop a Lyapunov-guided generative diffusion model-based reinforcement learning framework and validate its effectiveness through a UAV-based LAE networking case study. Finally, we outline several directions for future research."
1178,679d459debd8ffd557a2b307,cs.AI,https://arxiv.org/pdf/2501.15916,Online Housing Market,Julien Lesca,"Computer Science and Game Theory, Artificial Intelligence","This paper studies an online variant of the celebrated housing market problem(?), where each agent has a single house and seeks to exchange it for another based on her preferences. In this online setting, agents may arrive and depart at any time, meaning that not all agents are present on the housing market simultaneously. I extend the well-known serial dictatorship and Gale’s top trading cycle mechanisms to this online scenario, aiming to retain their desirable properties such as Pareto efficiency, individual rationality, and strategy-proofness. These extensions also seek to prevent agents from strategically delaying their arrival or advancing their departure. I demonstrate that achieving all of these properties simultaneously is impossible in the online context, and I present several variants that achieve different subsets of these properties."
1179,679d459debd8ffd557a2b308,cs.AI,https://arxiv.org/pdf/2501.15908,Evidential Physics-Informed Neural Networks,"Hai Siong Tan, Kuancheng Wang, Rafe McBeth","Machine Learning, Artificial Intelligence, Computational Physics","We present a novel class of Physics-Informed Neural Networks that is formulated based on the principles of Evidential Deep Learning, where the model incorporates uncertainty quantification by learning parameters
of a higher-order distribution.
The dependent and trainable variables of the PDE residual loss and data-fitting loss terms are recast as functions of the hyperparameters of an evidential prior distribution. Our model is equipped with an information-theoretic regularizer that contains the Kullback-Leibler divergence
between two inverse-gamma distributions characterizing predictive uncertainty.
Relative to Bayesian-Physics-Informed-Neural-Networks, our framework appeared to exhibit higher sensitivity to data noise, preserve boundary conditions more faithfully and yield empirical coverage probabilities closer to nominal ones. Toward examining its relevance for data mining in scientific discoveries, we demonstrate how to apply our model to inverse problems involving 1D and 2D nonlinear differential equations."
1180,679d459debd8ffd557a2b309,cs.AI,https://arxiv.org/pdf/2501.15889,Adaptive Width Neural Networks,"Federico Errica, Henrik Christiansen, Viktor Zaverkin, Mathias Niepert, Francesco Alesiani","Machine Learning, Artificial Intelligence","For almost 70 years, researchers have mostly relied on hyper-parameter tuning to pick the width of neural networks’ layers out of many possible choices. This paper challenges the status quo by introducing an easy-to-use technique to learn anunboundedwidth of a neural network’s layerduring training. The technique does not rely on alternate optimization nor hand-crafted gradient heuristics; rather, it jointly optimizes the width and the parameters of each layer via simple backpropagation. We apply the technique to a broad range of data domains such as tables, images, texts, and graphs, showing how the width adapts to the task’s difficulty. By imposing a soft ordering of importance among neurons, it is possible totruncatethe trained network at virtually zero cost, achieving a smooth trade-off between performance and compute resources in a structured way. Alternatively, one can dynamically compress the network with no performance degradation.
In light of recent foundation models trained on large datasets, believed to require billions of parameters and where hyper-parameter tuning is unfeasible due to huge training costs, our approach stands as a viable alternative for width learning."
1181,679d459debd8ffd557a2b30a,cs.AI,https://arxiv.org/pdf/2501.15877,Boli: A dataset for understanding stuttering experience and analyzing stuttered speech,"Ashita Batra, Mannas narang, Neeraj Kumar Sharma, Pradip K Das","Human-Computer Interaction, Artificial Intelligence","There is a growing need for diverse, high-quality stuttered speech data, particularly in the context of Indian languages. This paper introduces Project Boli, a multi-lingual stuttered speech dataset designed to advance scientific understanding and technology development for individuals who stutter, particularly in India. The dataset constitutes (a) anonymized metadata (gender, age, country, mother tongue) and responses to a questionnaire about how stuttering affects their daily lives, (b) captures both read speech (using the Rainbow Passage) and spontaneous speech (through image description tasks) for each participant and (c) includes detailed annotations of five stutter types: blocks, prolongations, interjections, sound repetitions and word repetitions.
We present a comprehensive analysis of the dataset, including the data collection procedure, experience summarization of people who stutter, severity assessment of stuttering events and technical validation of the collected data.
The dataset is released as an open access to further speech technology development."
1182,679d459debd8ffd557a2b30b,cs.AI,https://arxiv.org/pdf/2501.15865,Transfer of Knowledge through Reverse Annealing: A Preliminary Analysis of the Benefits and What to Share,"Eneko Osaba, Esther Villar-Rodriguez","Quantum Physics, Artificial Intelligence, Emerging Technologies","Being immersed in the NISQ-era, current quantum annealers present limitations for solving optimization problems efficiently. To mitigate these limitations, D-Wave Systems developed a mechanism called Reverse Annealing, a specific type of quantum annealing designed to perform local refinement of good states found elsewhere. Despite the research activity around Reverse Annealing, none has theorized about the possible benefits related to the transfer of knowledge under this paradigm. This work moves in that direction and is driven by experimentation focused on answering two key research questions:i)is reverse annealing a paradigm that can benefit from knowledge transfer between similar problems?andii)can we infer the characteristics that an input solution should meet to help increase the probability of success?To properly guide the tests in this paper, the well-known Knapsack Problem has been chosen for benchmarking purposes, using a total of 34 instances composed of 14 and 16 items.\helveticabold"
1183,679d459debd8ffd557a2b30c,cs.AI,https://arxiv.org/pdf/2501.15842,Beyond In-Distribution Performance: A Cross-Dataset Study of Trajectory Prediction Robustness,"Yue Yao, Daniel Goehring, Joerg Reichardt","Machine Learning, Artificial Intelligence, Machine Learning","We study the Out-of-Distribution (OoD) generalization ability of three SotA trajectory prediction models with comparable In-Distribution (ID) performance but different model designs. We investigate the influence of inductive bias, size of training data and data augmentation strategy by training the models on Argoverse 2 (A2) and testing on Waymo Open Motion (WO) and vice versa.
We find that the smallest model with highest inductive bias exhibits the best OoD generalization across different augmentation strategies when trained on the smaller A2 dataset and tested on the large WO dataset. In the converse setting, training all models on the larger WO dataset and testing on the smaller A2 dataset, we find that all models generalize poorly, even though the model with the highest inductive bias still exhibits the best generalization ability. We discuss possible reasons for this surprising finding and draw conclusions about the design and test of trajectory prediction models and benchmarks."
1184,679d459debd8ffd557a2b30d,cs.AI,https://arxiv.org/pdf/2501.15838,CrySPAI: A new Crystal Structure Prediction Software Based on Artificial Intelligence,"Zongguo Wang, Ziyi Chen, Yang Yuan, Yangang Wang","Materials Science, Artificial Intelligence",
1185,679d459debd8ffd557a2b30e,cs.AI,https://arxiv.org/pdf/2501.15830,SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model,"Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, Xuelong Li","Robotics, Artificial Intelligence","In this paper, we claim that spatial understanding is the keypoint in robot manipulation, and propose SpatialVLA to explore effective spatial representations for the robot foundation model. Specifically, we introduceEgo3D Position Encodingto inject 3D information into the input observations of the visual-language-action model, and proposeAdaptive Action Gridsto represent spatial robot movement actions with adaptive discretized action grids, facilitating learning generalizable and transferrable spatial action knowledge for cross-robot control.
SpatialVLA is first pre-trained on top of a vision-language model with 1.1 Million real-world robot episodes, to learn a generalist manipulation policy across multiple robot environments and tasks.
After pre-training, SpatialVLA is directly applied to perform numerous tasks in a zero-shot manner. The superior results in both simulation and real-world robots demonstrate its advantage of inferring complex robot motion trajectories and its strong in-domain multi-task generalization ability.
We further show the proposedAdaptive Action Gridsoffer a new and effective way to fine-tune the pre-trained SpatialVLA model for new simulation and real-world setups, where the pre-learned action grids are re-discretized to capture robot-specific spatial action movements of new setups.
The superior results from extensive evaluations demonstrate the exceptional in-distribution generalization and out-of-distribution adaptation capability, highlighting the crucial benefit of the proposed spatial-aware representations for generalist robot policy learning.
All the details and codes will be open-sourced."
1186,679d459debd8ffd557a2b30f,cs.AI,https://arxiv.org/pdf/2501.15820,FuzzyLight: A Robust Two-Stage Fuzzy Approach for Traffic Signal Control Works in Real Cities,"Mingyuan Li, Jiahao Wang, Bo Du, Jun Shen, Qiang Wu","Systems and Control, Artificial Intelligence","Effective traffic signal control (TSC) is crucial in mitigating urban congestion and reducing emissions.
Recently, reinforcement learning (RL) has been the research trend for TSC.
However, existing RL algorithms face several real-world challenges that hinder their practical deployment in TSC:
(1) Sensor accuracy deteriorates with increased sensor detection range, and data transmission is prone to noise, potentially resulting in unsafe TSC decisions.
(2) During the training of online RL, interactions with the environment could be unstable, potentially leading to inappropriate traffic signal phase (TSP) selection and traffic congestion.
(3) Most current TSC algorithms focus only on TSP decisions, overlooking the critical aspect of phase duration, affecting safety and efficiency.
To overcome these challenges, we propose a robust two-stage fuzzy approach called FuzzyLight, which integrates compressed sensing and RL for TSC deployment.
FuzzyLight offers several key contributions:
(1) It employs fuzzy logic and compressed sensing to address sensor noise and enhances the efficiency of TSP decisions.
(2) It maintains stable performance during training and combines fuzzy logic with RL to generate precise phases.
(3) It works in real cities across 22 intersections and demonstrates superior performance in both real-world and simulated environments.
Experimental results indicate that FuzzyLight enhances traffic efficiency by 48% compared to expert-designed timings in the
real world.
Furthermore, it achieves state-of-the-art (SOTA) performance in simulated environments using six real-world datasets with transmission noise.
The code and deployment video are available at the URL111https://github.com/AdvancedAI-ComplexSystem/SmartCity/tree/main/FuzzyLight."
1187,679d459debd8ffd557a2b310,cs.AI,https://arxiv.org/pdf/2501.15817,Long-Term Interest Clock: Fine-Grained Time Perception in Streaming Recommendation System,"Yongchun Zhu, Guanyu Jiang, Jingwu Chen, Feng Zhang, Xiao Yang, Zuotao Liu","Information Retrieval, Artificial Intelligence","User interests manifest a dynamic pattern within the course of a day, e.g., a user usually favors soft music at 8 a.m. but may turn to ambient music at 10 p.m. To model dynamic interests in a day, hour embedding is widely used in traditional daily-trained industrial recommendation systems. However, its discreteness can cause periodical online patterns and instability in recent streaming recommendation systems. Recently, Interest Clock has achieved remarkable performance in streaming recommendation systems. Nevertheless, it models users’ dynamic interests in acoarse-grainedmanner, merely encoding users’discreteinterests of 24 hours fromshort-termbehaviors. In this paper, we propose a fine-grained method for perceiving time information for streaming recommendation systems, namedLong-term Interest Clock(LIC). The key idea of LIC is adaptively calculating current user interests by taking into consideration the relevance of long-term behaviors around current time (e.g., 8 a.m.) given a candidate item.
LIC consists of two modules: (1)Clock-GSUretrieves a sub-sequence by searching through long-term behaviors, using query information from a candidate item and current time, (2)Clock-ESUemploys a time-gap-aware attention mechanism to aggregate sub-sequence with the candidate item. With Clock-GSU and Clock-ESU, LIC is capable of capturing users’ dynamic fine-grained interests from long-term behaviors.
We conduct online A/B tests, obtaining +0.122% improvements on user active days. Besides, the extended offline experiments show improvements as well. Long-term Interest Clock has been integrated into Douyin Music App’s recommendation system."
1188,679d459debd8ffd557a2b311,cs.AI,https://arxiv.org/pdf/2501.15816,AdaF^2M^2: Comprehensive Learning and Responsive Leveraging Features in Recommendation System,"Yongchun Zhu, Jingwu Chen, Ling Chen, Yitan Li, Feng Zhang, Xiao Yang, Zuotao Liu","Information Retrieval, Artificial Intelligence","Feature modeling, which involves feature representation learning and leveraging, plays an essential role in industrial recommendation systems. However, the data distribution in real-world applications usually follows a highly skewed long-tail pattern due to the popularity bias, which easily leads to over-reliance on ID-based features, such as user/item IDs and ID sequences of interactions. Such over-reliance makes it hard for models to learn features comprehensively, especially for those non-ID meta features, e.g., user/item characteristics. Further, it limits the feature leveraging ability in models, getting less generalized and more susceptible to data noise. Previous studies on feature modeling focus on feature extraction and interaction, hardly noticing the problems brought about by the long-tail data distribution. To achieve better feature representation learning and leveraging on real-world data, we propose a model-agnostic framework AdaF2M2, short forAdaptiveFeatureModeling withFeatureMask. The feature-mask mechanism helps comprehensive feature learning via multi-forward training with augmented samples, while the adapter applies adaptive weights on features responsive to different user/item states. By arming base models with AdaF2M2, we conduct online A/B tests on multiple recommendation scenarios, obtaining +1.37% and +1.89% cumulative improvements on user active days and app duration respectively. Besides, the extended offline experiments on different models show improvements as well. AdaF2M2has been widely deployed on both retrieval and ranking tasks in multiple applications of Douyin Group, indicating its superior effectiveness and universality."
1189,679d459debd8ffd557a2b312,cs.AI,https://arxiv.org/pdf/2501.15767,Formal Verification of Markov Processes with Learned Parameters,"Muhammad Maaz, Timothy C. Y. Chan","Machine Learning, Artificial Intelligence, Optimization and Control","We introduce the problem of formally verifying properties of Markov processes where the parameters are the output of machine learning models. Our formulation is general and solves a wide range of problems, including verifying properties of probabilistic programs that use machine learning, and subgroup analysis in healthcare modeling. We show that for a broad class of machine learning models, including linear models, tree-based models, and neural networks, verifying properties of Markov chains like reachability, hitting time, and total reward can be formulated as a bilinear program. We develop a decomposition and bound propagation scheme for solving the bilinear program and show through computational experiments that our method solves the problem to global optimality up to 100x faster than state-of-the-art solvers. We also releasemarkovml, an open-source tool for building Markov processes, integrating pretrained machine learning models, and verifying their properties, available athttps://github.com/mmaaz-git/markovml."
1190,679d459debd8ffd557a2b313,cs.AI,https://arxiv.org/pdf/2501.15731,Renewable Energy Prediction: A Comparative Study of Deep Learning Models for Complex Dataset Analysis,"Haibo Wang, Jun Huang, Lutfu Sua, Bahram Alidaee","Machine Learning, Artificial Intelligence",
1191,679d459debd8ffd557a2b314,cs.AI,https://arxiv.org/pdf/2501.15727,Gensors: Authoring Personalized Visual Sensors with Multimodal Foundation Models and Reasoning,"Michael Xieyang Liu, Savvas Petridis, Vivian Tsai, Alexander J. Fiannaca, Alex Olwal, Michael Terry, Carrie J. Cai","Human-Computer Interaction, Artificial Intelligence","Multimodal large language models (MLLMs), with their expansive world knowledge and reasoning capabilities, present a unique opportunity for end-users to createpersonalized AIsensors capable of reasoning about complex situations. A user could describe a desired sensing task in natural language (e.g., “let me know if my toddler is getting into mischief in the living room”), with the MLLM analyzing the camera feed andresponding within just seconds.
In a formative study, we found that users saw substantial value in defining their own sensors, yet struggled to articulate their unique personal requirements to the model and debug the sensors through prompting alone.
To address these challenges, we developed Gensors, a system that empowers users to definecustomizedsensors supported by the reasoning capabilities of MLLMs. Gensors 1)assists users in eliciting requirementsthrough both automatically-generated and manually created sensor criteria, 2)facilitatesdebugging by allowing users to isolate and test individual criteria in parallel, 3) suggests additional criteria based onuser-provided images, and 4)proposestest cases to help users “stress test” sensors on potentially unforeseen scenarios.
In a 12-participant user study,users reported significantly greater sense of control, understanding, and ease of communicationwhen defining sensors using Gensors.
Beyond addressing model limitations, Gensors supported users in debugging, eliciting requirements, and expressing unique personal requirements to the sensor through criteria-based reasoning;
it also helped uncover users’ own “blind spots” byexposing overlooked criteria and revealing unanticipated failure modes.
Finally, we describe insights into how unique characteristics of MLLMs–such as hallucinations and inconsistent responses–canimpactthe sensor-creation process.
Together, these findings contribute to the design of future MLLM-powered sensing systems that are intuitive and customizable by everyday users."
1192,679d459debd8ffd557a2b315,cs.AI,https://arxiv.org/pdf/2501.15695,Contextual Knowledge Sharing in Multi-Agent Reinforcement Learning with Decentralized Communication and Coordination,"Hung Du, Srikanth Thudumu, Hy Nguyen, Rajesh Vasa, Kon Mouzakis","Multiagent Systems, Artificial Intelligence","Decentralized Multi-Agent Reinforcement Learning (Dec-MARL) has emerged as a pivotal approach for addressing complex tasks in dynamic environments. Existing Multi-Agent Reinforcement Learning (MARL) methodologies typically assume a shared objective among agents and rely on centralized control. However, many real-world scenarios feature agents with individual goals and limited observability of other agents, complicating coordination and hindering adaptability. Existing Dec-MARL strategies prioritize either communication or coordination, lacking an integrated approach that leverages both. This paper presents a novel Dec-MARL framework that integrates peer-to-peer communication and coordination, incorporating goal-awareness and time-awareness into the agents’ knowledge-sharing processes. Our framework equips agents with the ability to (i) share contextually relevant knowledge to assist other agents, and (ii) reason based on information acquired from multiple agents, while considering their own goals and the temporal context of prior knowledge. We evaluate our approach through several complex multi-agent tasks in environments with dynamically appearing obstacles. Our work demonstrates that incorporating goal-aware and time-aware knowledge sharing significantly enhances overall performance."
1193,679d459debd8ffd557a2b316,cs.AI,https://arxiv.org/pdf/2501.15665,StagFormer: Time Staggering Transformer Decoding for RunningLayers In Parallel,"Dylan Cutler, Arun Kandoor, Nishanth Dikkala, Nikunj Saunshi, Xin Wang, Rina Panigrahy","Machine Learning, Artificial Intelligence","Standard decoding in a Transformer based language model is inherently sequential as we wait for a token’s embedding to pass through all the layers in the network before starting the generation of the next token.
In this work, we propose a new architecture StagFormer (Staggered Transformer), which staggered execution along the time axis and thereby enables parallelizing the decoding process along the depth of the model.
We achieve this by breaking the dependency of the token representation at time stepi𝑖iitalic_iin layerl𝑙litalic_lupon the representations of tokens until time stepi𝑖iitalic_ifrom layerl−1𝑙1l-1italic_l - 1. Instead, we stagger the execution and only allow a dependency on token representations until time stepi−1𝑖1i-1italic_i - 1.
The later sections of the Transformer still get access to the “rich"" representations from the prior section but only from those token positions which are one time step behind.
StagFormer allows for different sections of the model to be executed in parallel yielding at potential 33% speedup in decoding while being quality neutral in our simulations.
We also explore many natural variants of this idea. We present how weight-sharing across the different sections being staggered can be more practical in settings with limited memory. We show how one can approximate a recurrent model during inference using such weight-sharing. We explore the efficacy of using a bounded window attention to pass information from one section to another which helps drive further latency gains for some applications. We also explore demonstrate the scalability of the staggering idea over more than 2 sections of the Transformer."
1194,679d459debd8ffd557a2b317,cs.AI,https://arxiv.org/pdf/2501.15661,Constrained Hybrid Metaheuristic Algorithm for Probabilistic Neural Networks Learning,"Piotr A. Kowalski, Szymon Kucharczyk, Jacek Mańdziuk","Neural and Evolutionary Computing, Artificial Intelligence","This study investigates the potential of hybrid metaheuristic algorithms to enhance the training of Probabilistic Neural Networks (PNNs) by leveraging the complementary strengths of multiple optimisation strategies. Traditional learning methods, such as gradient-based approaches, often struggle to optimise high-dimensional and uncertain environments, while single-method metaheuristics may fail to exploit the solution space fully. To address these challenges, we propose the constrained Hybrid Metaheuristic (cHM) algorithm, a novel approach that combines multiple population-based optimisation techniques into a unified framework. The proposed procedure operates in two phases: an initial probing phase evaluates multiple metaheuristics to identify the best-performing one based on the error rate, followed by a fitting phase where the selected metaheuristic refines the PNN to achieve optimal smoothing parameters. This iterative process ensures efficient exploration and convergence, enhancing the network’s generalisation and classification accuracy.
cHM integrates several popular metaheuristics, such as BAT, Simulated Annealing, Flower Pollination Algorithm, Bacterial Foraging Optimization, and Particle Swarm Optimisation as internal optimisers. To evaluate
cHM performance, experiments were conducted on 16 datasets with varying characteristics, including binary and multiclass classification tasks, balanced and imbalanced class distributions, and diverse feature dimensions.
The results demonstrate that
cHM effectively combines the strengths of individual metaheuristics,
leading to faster convergence and more robust learning. By optimising the smoothing parameters of PNNs, the proposed method enhances classification performance across diverse datasets, proving its
application flexibility and efficiency."
1195,679d459debd8ffd557a2b318,cs.AI,https://arxiv.org/pdf/2501.15638,A Comprehensive Survey on Self-Interpretable Neural Networks,"Yang Ji, Ying Sun, Yuting Zhang, Zhigaoyuan Wang, Yuanxin Zhuang, Zheng Gong, Dazhong Shen, Chuan Qin, Hengshu Zhu, Hui Xiong","Machine Learning, Artificial Intelligence","Neural networks have achieved remarkable success across various fields. However, the lack of interpretability limits their practical use, particularly in critical decision-making scenarios. Post-hoc interpretability, which provides explanations for pre-trained models, is often at risk of robustness and fidelity. This has inspired a rising interest in self-interpretable neural networks, which inherently reveal the prediction rationale through the model structures.
Although there exist surveys on post-hoc interpretability, a comprehensive and systematic survey of self-interpretable neural networks is still missing. To address this gap, we first collect and review existing works on self-interpretable neural networks and provide a structured summary of their methodologies from five key perspectives: attribution-based, function-based, concept-based, prototype-based, and rule-based self-interpretation.
We also present concrete, visualized examples of model explanations and discuss their applicability across diverse scenarios, including image, text, graph data, and deep reinforcement learning.
Additionally, we summarize existing evaluation metrics for self-interpretability and identify open challenges in this field, offering insights for future research. To support ongoing developments, we present a publicly accessible resource to track advancements in this domain:https://github.com/yangji721/Awesome-Self-Interpretable-Neural-Network."
1196,679d459debd8ffd557a2b319,cs.AI,https://arxiv.org/pdf/2501.15618,Your Learned Constraint is Secretly a Backward Reachable Tube,"Mohamad Qadri, Gokul Swamy, Jonathan Francis, Michael Kaess, Andrea Bajcsy","Robotics, Artificial Intelligence, Machine Learning","Inverse Constraint Learning (ICL) is the problem of inferring constraints from safe (i.e., constraint-satisfying) demonstrations. The hope is that these inferred constraints can then be used downstream to search for safe policies for new tasks and, potentially, under different dynamics. Our paper explores the question of what mathematical entity ICL recovers. Somewhat surprisingly, we show that both in theory and in practice, ICL recovers the set of states where failure isinevitable, rather than the set of states where failure hasalreadyhappened. In the language of safe control, this means we recover abackwards reachable tube (BRT)rather than afailure set. In contrast to the failure set, the BRT depends on the dynamics of the data collection system. We discuss the implications of the dynamics-conditionedness of the recovered constraint on both the sample-efficiency of policy search and the transferability of learned constraints."
1197,679d459debd8ffd557a2b31a,cs.AI,https://arxiv.org/pdf/2501.15585,Twin Transition or Competing Interests? Validation of the Artificial Intelligence and Sustainability Perceptions Inventory (AISPI),Annika Bush,"Computers and Society, Artificial Intelligence",
1198,679d459debd8ffd557a2b31b,cs.AI,https://arxiv.org/pdf/2501.15564,Diffusion-Based Planning for Autonomous Driving with Flexible Guidance,"Yinan Zheng, Ruiming Liang, Kexin Zheng, Jinliang Zheng, Liyuan Mao, Jianxiong Li, Weihao Gu, Rui Ai, Shengbo Eben Li, Xianyuan Zhan, Jingjing Liu","Robotics, Artificial Intelligence, Machine Learning","Achieving human-like driving behaviors in complex open-world environments is a critical challenge in autonomous driving. Contemporary learning-based planning approaches such as imitation learning methods often struggle to balance competing objectives and lack of safety assurance,
due to limited adaptability and inadequacy in learning complex multi-modal behaviors commonly exhibited in human planning, not to mention their strong reliance on the
fallback strategy with predefined rules. We propose a novel transformer-basedDiffusion Plannerfor closed-loop planning, which can effectively model multi-modal driving behavior and ensure trajectory quality without any rule-based refinement.
Our model supports joint modeling of both prediction and planning tasks under the same architecture, enabling cooperative behaviors between vehicles.
Moreover, by learning the gradient of the trajectory score function and employing a flexible classifier guidance mechanism,Diffusion Plannereffectively achieves safe and adaptable planning behaviors.
Evaluations on the large-scale real-world autonomous planning benchmark nuPlan and our newly collected 200-hour delivery-vehicle driving dataset
demonstrate thatDiffusion Plannerachieves state-of-the-art closed-loop performance with robust transferability in diverse driving styles. Project website:https://zhengyinan-air.github.io/Diffusion-Planner/."
1199,679d459debd8ffd557a2b31c,cs.AI,https://arxiv.org/pdf/2501.15555,Distributionally Robust Graph Out-of-Distribution Recommendation via Diffusion Model,"Chu Zhao, Enneng Yang, Yuliang Liang, Jianzhe Zhao, Guibing Guo, Xingwei Wang","Machine Learning, Artificial Intelligence, Graphics, Machine Learning","The distributionally robust optimization (DRO)-based graph neural network methods improve recommendation systems’ out-of-distribution (OOD) generalization by optimizing the model’s worst-case performance. However, these studies fail to consider the impact of noisy samples in the training data, which results in diminished generalization capabilities and lower accuracy. Through experimental and theoretical analysis, this paper reveals that current DRO-based graph recommendation methods assign greater weight to noise distribution, leading to model parameter learning being dominated by it. When the model overly focuses on fitting noise samples in the training data, it may learn irrelevant or meaningless features that cannot be generalized to OOD data. To address this challenge, we design aDistributionallyRobustGraph model forOOD recommendation (DRGO). Specifically, our method first employs a simple and effective diffusion paradigm to alleviate the noisy effect in the latent space. Additionally, an entropy regularization term is introduced in the DRO objective function to avoid extreme sample weights in the worst-case distribution. Finally, we provide a theoretical proof of the generalization error bound of DRGO as well as a theoretical analysis of how our approach mitigates noisy sample effects, which helps to better understand the proposed framework from a theoretical perspective. We conduct extensive experiments on four datasets to evaluate the effectiveness of our framework against three typical distribution shifts, and the results demonstrate its superiority in both independently and identically distributed distributions (IID) and OOD. Our code is available athttps://github.com/user683/DRGO."
1200,679d459debd8ffd557a2b31d,cs.AI,https://arxiv.org/pdf/2501.15544,Advancing Generative Artificial Intelligence and Large Language Models for Demand Side Management with Electric Vehicles,"Hanwen Zhang, Ruichen Zhang, Wei Zhang, Dusit Niyato, Yonggang Wen","Machine Learning, Artificial Intelligence","Generative artificial intelligence, particularly through large language models (LLMs), is poised to transform energy optimization and demand side management (DSM) within microgrids. This paper explores the integration of LLMs into energy management, emphasizing their roles in automating the optimization of DSM strategies with electric vehicles. We investigate challenges and solutions associated with DSM and explore the new opportunities presented by leveraging LLMs. Then, We propose an innovative solution that enhances LLMs with retrieval-augmented generation for automatic problem formulation, code generation, and customizing optimization. We present a case study to demonstrate the effectiveness of our proposed solution in charging scheduling and optimization for electric vehicles, highlighting our solution’s significant advancements in energy efficiency and user adaptability. This work underscores the potential of LLMs for energy optimization and fosters a new era of intelligent DSM solutions."
1201,679d459debd8ffd557a2b31e,cs.AI,https://arxiv.org/pdf/2501.15442,Overview of the Amphion Toolkit (v0.2),"Jiaqi Li, Xueyao Zhang, Yuancheng Wang, Haorui He, Chaoren Wang, Li Wang, Huan Liao, Junyi Ao, Zeyu Xie, Yiqiao Huang, Junan Zhang, Zhizheng Wu","Sound, Artificial Intelligence, Audio and Speech Processing",
1202,679d459debd8ffd557a2b31f,cs.AI,https://arxiv.org/pdf/2501.15418,Episodic Novelty Through Temporal Distance,"Yuhua Jiang, Qihan Liu, Yiqin Yang, Xiaoteng Ma, Dianyu Zhong, Hao Hu, Jun Yang, Bin Liang, Bo Xu, Chongjie Zhang, Qianchuan Zhao","Machine Learning, Artificial Intelligence","Exploration in sparse reward environments remains a significant challenge in reinforcement learning, particularly in Contextual Markov Decision Processes (CMDPs), where environments differ across episodes. Existing episodic intrinsic motivation methods for CMDPs primarily rely on count-based approaches, which are ineffective in large state spaces, or on similarity-based methods that lack appropriate metrics for state comparison. To address these shortcomings, we proposeEpisodic Novelty ThroughTemporalDistance (ETD), a novel approach that introduces temporal distance as a robust metric for state similarity and intrinsic reward computation. By employing contrastive learning, ETD accurately estimates temporal distances and derives intrinsic rewards based on the novelty of states within the current episode. Extensive experiments on various benchmark tasks demonstrate that ETD significantly outperforms state-of-the-art methods, highlighting its effectiveness in enhancing exploration in sparse reward CMDPs."
1203,679d459debd8ffd557a2b320,cs.AI,https://arxiv.org/pdf/2501.15417,AnyEnhance: A Unified Generative Model with Prompt-Guidance and Self-Critic for Voice Enhancement,"Junan Zhang, Jing Yang, Zihao Fang, Yuancheng Wang, Zehua Zhang, Zhuo Wang, Fan Fan, Zhizheng Wu","Sound, Artificial Intelligence, Audio and Speech Processing","We introduceAnyEnhance, a unified generative model for voice enhancement that processes both speech and singing voices. Based on a masked generative model,AnyEnhanceis capable of handling both speech and singing voices, supporting a wide range of enhancement tasks including denoising, dereverberation, declipping, super-resolution, and target speaker extraction, all simultaneously and without fine-tuning.AnyEnhanceintroduces a prompt-guidance mechanism for in-context learning, which allows the model to natively accept a reference speaker’s timbre. In this way, it could boost enhancement performance when a reference audio is available and enable the target speaker extraction task without altering the underlying architecture. Moreover, we also introduce a self-critic mechanism into the generative process for masked generative models, yielding higher-quality outputs through iterative self-assessment and refinement. Extensive experiments on various enhancement tasks demonstrateAnyEnhanceoutperforms existing methods in terms of both objective metrics and subjective listening tests. Demo audios are publicly available atthis URL."
1204,679d459debd8ffd557a2b321,cs.AI,https://arxiv.org/pdf/2501.15373,Learning-Enhanced Safeguard Control for High-Relative-Degree Systems: Robust Optimization under Disturbances and Faults,"Xinyang Wang, Hongwei Zhang, Shimin Wang, Wei Xiao, Martin Guay","Systems and Control, Artificial Intelligence, Machine Learning, Optimization and Control, Adaptation and Self-Organizing Systems","Merely pursuing performance may adversely affect the safety, while a conservative policy for safe exploration will degrade the performance. How to balance the safety and performance in learning-based control problems is an interesting yet challenging issue.
This paper aims to enhance system performance with safety guarantee in solving the reinforcement learning (RL)-based optimal control problems of nonlinear systems subject to high-relative-degree state constraints and unknown time-varying disturbance/actuator faults.
First, to combine control barrier functions (CBFs) with RL, a new type of CBFs, termed high-order reciprocal control barrier function (HO-RCBF) is proposed to deal with high-relative-degree constraints during the learning process.
Then, the concept of gradient similarity is proposed to quantify the relationship between the gradient of safety and the gradient of performance.
Finally, gradient manipulation and adaptive mechanisms are introduced in the safe RL framework to enhance the performance with a safety guarantee.
Two simulation examples illustrate that the proposed safe RL framework can address high-relative-degree constraint, enhance safety robustness and improve system performance."
1205,679d459debd8ffd557a2b322,cs.AI,https://arxiv.org/pdf/2501.15322,Scaling laws for decoding images from brain activity,"Hubert Banville, Yohann Benchetrit, Stéphane d'Ascoli, Jérémy Rapin, Jean-Rémi King","Image and Video Processing, Artificial Intelligence, Machine Learning, Neurons and Cognition","Generative AI has recently propelled the decoding of images from brain activity.
How do these approaches scale with the amount and type of neural recordings?
Here, we systematically compare image decoding from four types of non-invasive devices: electroencephalography (EEG), magnetoencephalography (MEG), high-field functional Magnetic Resonance Imaging (3T fMRI) and ultra-high field (7T) fMRI.
For this, we evaluate decoding models on the largest benchmark to date, encompassing 8 public datasets, 84 volunteers, 498 hours of brain recording and 2.3 million brain responses to natural images.
Unlike previous work, we focus on single-trial decoding performance to simulate real-time settings.
This systematic comparison reveals three main findings.
First, the most precise neuroimaging devices tend to yield the best decoding performances, when the size of the training sets are similar.
However, thegainenabled by deep learning – in comparison to linear models – is obtained with the noisiest devices.
Second, we do not observe any plateau of decoding performance as the amount of training data increases. Rather, decoding performance scales log-linearly with the amount of brain recording.
Third, this scaling law primarily depends on the amount of data per subject. However, little decoding gain is observed by increasing the number of subjects.
Overall, these findings delineate the path most suitable to scale the decoding of images from non-invasive brain recordings."
1206,679d459debd8ffd557a2b323,cs.AI,https://arxiv.org/pdf/2501.15318,A Post-Processing-Based Fair Federated Learning Framework,"Yi Zhou, Naman Goel","Machine Learning, Artificial Intelligence, Computers and Society","Federated Learning (FL) allows collaborative model training among distributed parties without pooling local datasets at a central server. However, the distributed nature of FL poses challenges in training fair federated learning models. The existing techniques are often limited in offering fairness flexibility to clients and performance. We formally define and empirically analyze a simple and intuitive post-processing-based framework to improve group fairness in FL systems. This framework can be divided into two stages: a standard FL training stage followed by a completely decentralized local debiasing stage. In the first stage, a global model is trained without fairness constraints using a standard federated learning algorithm (e.g. FedAvg). In the second stage, each client applies fairness post-processing on the global model using their respective local dataset. This allows for customized fairness improvements based on clients’ desired and context-guided fairness requirements. We demonstrate two well-established post-processing techniques in this framework: model output post-processing and final layer fine-tuning.
We evaluate the framework against three common baselines on four different datasets, including tabular, signal, and image data, each with varying levels of data heterogeneity across clients.
Our work shows that this framework not only simplifies fairness implementation in FL but also provides significant fairness improvements with minimal accuracy loss or even accuracy gain, across data modalities and machine learning methods, being especially effective in more heterogeneous settings."
1207,679d459debd8ffd557a2b324,cs.AI,https://arxiv.org/pdf/2501.15304,Music Generation using Human-In-The-Loop Reinforcement Learning,Aju Ani Justus,"Sound, Artificial Intelligence, Human-Computer Interaction, Machine Learning, Audio and Speech Processing",
1208,679d459debd8ffd557a2b325,cs.AI,https://arxiv.org/pdf/2501.15276,Exploring the Collaborative Co-Creation Process with AI: A Case Study in Novice Music Production,"Yue Fu, Michele Newman, Lewis Going, Qiuzi Feng, Jin Ha Lee","Human-Computer Interaction, Artificial Intelligence",
1209,679d459debd8ffd557a2b326,cs.AI,https://arxiv.org/pdf/2501.15255,Lightweight and Post-Training Structured Pruning for On-Device Large Lanaguage Models,"Zihuai Xu, Yang Xu, Hongli Xu, Yunming Liao, Zhiwei Yao, Zuan Xie","Machine Learning, Artificial Intelligence","Considering the hardware-friendly characteristics and broad applicability, structured pruning has emerged as an efficient solution to reduce the resource demands of large language models (LLMs) on resource-constrained devices.
Traditional structured pruning methods often need fine-tuning to recover performance loss, which incurs high memory overhead and substantial data requirements, rendering them unsuitable for on-device applications.
Additionally, post-training structured pruning techniques typically necessitate specific activation functions or architectural modifications, thereby limiting their scope of applications.
Herein, we introduce COMP, a lightweight post-training structured pruning method that employs a hybrid-granularity pruning strategy.
COMP initially prunes selected model layers based on their importance at a coarse granularity, followed by fine-grained neuron pruning within the dense layers of each remaining model layer.
To more accurately evaluate neuron importance, COMP introduces a new matrix condition-based metric.
Subsequently, COMP utilizes mask tuning to recover accuracy without the need for fine-tuning, significantly reducing memory consumption.
Experimental results demonstrate that COMP improves performance by 6.13% on the LLaMA-2-7B model with a 20% pruning ratio compared to LLM-Pruner, while simultaneously reducing memory overhead by 80%."
1210,679d459debd8ffd557a2b327,cs.AI,https://arxiv.org/pdf/2501.15240,Hardware-Aware DNN Compression for Homogeneous Edge Devices,"Kunlong Zhang, Guiying Li, Ning Lu, Peng Yang, Ke Tang","Machine Learning, Artificial Intelligence","Deploying deep neural networks (DNNs) across homogeneous edge devices (the devices with the same SKU labeled by the manufacturer) often assumes identical performance among them. However, once a device model is widely deployed, the performance of each device becomes different after a period of running. This is caused by the differences in user configurations, environmental conditions, manufacturing variances, battery degradation, etc. Existing DNN compression methods have not taken this scenario into consideration and can not guarantee good compression results in all homogeneous edge devices. To address this, we propose Homogeneous-Device Aware Pruning (HDAP), a hardware-aware DNN compression framework explicitly designed for homogeneous edge devices, aiming to achieve optimal average performance of the compressed model across all devices. To deal with the difficulty of time-consuming hardware-aware evaluations for thousands or millions of homogeneous edge devices, HDAP partitions all the devices into several device clusters, which can dramatically reduce the number of devices to evaluate and use the surrogate-based evaluation instead of hardware evaluation in real-time. Experiments on ResNet50 and MobileNetV1 with the ImageNet dataset show that HDAP consistently achieves lower average inference latency compared with state-of-the-art methods, with substantial speedup gains (e.g., 2.86×\times×speedup at 1.0G FLOPs for ResNet50) on the homogeneous device clusters. HDAP offers an effective solution for scalable, high-performance DNN deployment methods for homogeneous edge devices."
1211,679d459debd8ffd557a2b328,cs.AI,https://arxiv.org/pdf/2501.15223,Efficient and Interpretable Neural Networks Using Complex Lehmer Transform,"Masoud Ataei, Xiaogang Wang","Machine Learning, Artificial Intelligence","We propose an efficient and interpretable neural network with a novel activation function called the weighted Lehmer transform. This new activation function enables adaptive feature selection and extends to the complex domain, capturing phase-sensitive and hierarchical relationships within data. Notably, it provides greater interpretability and transparency compared to existing machine learning models, facilitating a deeper understanding of its functionality and decision-making processes. We analyze the mathematical properties of both real-valued and complex-valued Lehmer activation units and demonstrate their applications in modeling nonlinear interactions. Empirical evaluations demonstrate that our proposed neural network achieves competitive accuracy on benchmark datasets with significantly improved computational efficiency. A single layer of real-valued or complex-valued Lehmer activation units is shown to deliver state-of-the-art performance, balancing efficiency with interpretability."
1212,679d459debd8ffd557a2b329,cs.AI,https://arxiv.org/pdf/2501.15198,Towards Conscious Service Robots,Sven Behnke,"Robotics, Artificial Intelligence","Deep learning’s success in perception, natural language processing, etc. inspires hopes for advancements in autonomous robotics. However, real-world robotics face challenges like variability, high-dimensional state spaces, non-linear dependencies, and partial observability. A key issue is non-stationarity of robots, environments, and tasks, leading to performance drops with out-of-distribution data. Unlike current machine learning models, humans adapt quickly to changes and new tasks due to a cognitive architecture that enables systematic generalization and meta-cognition. Human brain’s System 1 handles routine tasks unconsciously, while System 2 manages complex tasks consciously, facilitating flexible problem-solving and self-monitoring. For robots to achieve human-like learning and reasoning, they need to integrate causal models, working memory, planning, and metacognitive processing. By incorporating human cognition insights, the next generation of service robots will handle novel situations and monitor themselves to avoid risks and mitigate errors."
1213,679d459debd8ffd557a2b32a,cs.AI,https://arxiv.org/pdf/2501.15149,"Mapping Galaxy Images Across Ultraviolet, Visible and Infrared Bands Using Generative Deep Learning","Youssef Zaazou, Alex Bihlo, Terrence S. Tricco","Instrumentation and Methods for Astrophysics, Astrophysics of Galaxies, Artificial Intelligence",
1214,679d459debd8ffd557a2b32b,cs.AI,https://arxiv.org/pdf/2501.15142,DAGPrompT: Pushing the Limits of Graph Prompting with a Distribution-aware Graph Prompt Tuning Approach,"Qin Chen, Liang Wang, Bo Zheng, Guojie Song","Machine Learning, Artificial Intelligence","The ”pre-train then fine-tune” approach has advanced GNNs by enabling general knowledge capture without task-specific labels. However, an objective gap between pre-training and downstream tasks limits its effectiveness. Recent graph prompting methods aim to close this gap through task reformulations and learnable prompts. Despite this, they struggle with complex graphs like heterophily graphs. Freezing the GNN encoder can reduce the impact of prompting, while simple prompts fail to handle diverse hop-level distributions.
This paper identifies two key challenges in adapting graph prompting methods for complex graphs: (i)adapting the model to new distributions in downstream tasks to mitigate pre-training and fine-tuning discrepancies from heterophilyand (ii)customizing prompts for hop-specific node requirements.To overcome these challenges, we propose Distribution-aware Graph Prompt Tuning (DAGPrompT), which integrates a GLoRA module for optimizing the GNN encoder’s projection matrix and message-passing schema through low-rank adaptation. DAGPrompT also incorporates hop-specific prompts accounting for varying graph structures and distributions among hops. Evaluations on 10 datasets and 14 baselines demonstrate that DAGPrompT improves accuracy by up to 4.79% in node and graph classification tasks, setting a new state-of-the-art while preserving efficiency. Codes are available atGitHub."
1215,679d459debd8ffd557a2b32c,cs.AI,https://arxiv.org/pdf/2501.15109,Clear Preferences Leave Traces: Reference Model-Guided Sampling for Preference Learning,"Nirav Diwan, Tolga Ergen, Dongsub Shim, Honglak Lee","Machine Learning, Artificial Intelligence","Direct Preference Optimization (DPO) has emerged as a de-facto approach for aligning language models with human preferences.
Recent work has shown DPO’s effectiveness relies on training data quality. In particular, clear quality differences between preferred and rejected responses enhance learning performance.
Current methods for identifying and obtaining such high-quality samples demand additional resources or external models.
We discover that reference model probability space naturally detects high-quality training samples. Using this insight, we present a sampling strategy that achieves consistent improvements (+0.10.1+0.1+ 0.1to+0.40.4+0.4+ 0.4) on MT-Bench while using less than half (30303030-50%percent5050\%50 %) of the training data.
We observe substantial improvements (+0.40.4+0.4+ 0.4to+0.980.98+0.98+ 0.98) for technical tasks (coding, math, and reasoning) across multiple models and hyperparameter settings."
1216,679d459debd8ffd557a2b32d,cs.AI,https://arxiv.org/pdf/2501.15103,Each Rank Could be an Expert: Single-Ranked Mixture of Experts LoRA for Multi-Task Learning,"Ziyu Zhao, Yixiao Zhou, Didi Zhu, Tao Shen, Xuwu Wang, Jing Su, Kun Kuang, Zhongyu Wei, Fei Wu, Yu Cheng","Machine Learning, Artificial Intelligence","Low-Rank Adaptation (LoRA) is widely used for adapting large language models (LLMs) to specific domains due to its efficiency and modularity. Meanwhile, vanilla LoRA struggles with task conflicts in multi-task scenarios.
Recent works adopt Mixture of Experts (MoE) by treating each LoRA module as an expert, thereby mitigating task interference through multiple specialized LoRA modules.
While effective, these methods often isolate knowledge within individual tasks, failing to fully exploit the shared knowledge across related tasks.
In this paper, we establish a connection between single LoRA and multi-LoRA MoE, integrating them into a unified framework. We demonstrate that the dynamic routing of multiple LoRAs is functionally equivalent to rank partitioning and block-level activation within a single LoRA.
We further empirically demonstrate that finer-grained LoRA partitioning, within the same total and activated parameter constraints, leads to better performance gains across heterogeneous tasks.
Building on these findings, we propose Single-ranked Mixture of Experts LoRA (SMoRA), which embeds MoE into LoRA bytreating each rank as an independent expert. With adynamic rank-wise activationmechanism, SMoRA promotes finer-grained knowledge sharing while mitigating task conflicts.
Experiments demonstrate that SMoRA activates fewer parameters yet achieves better performance in multi-task scenarios."
1217,679d459debd8ffd557a2b32e,cs.AI,https://arxiv.org/pdf/2501.15098,CFT-RAG: An Entity Tree Based Retrieval Augmented Generation Algorithm With Cuckoo Filter,"Zihang Li, Yangdong Ruan, Wenjun Liu, Zhengyang Wang, Tong Yang","Machine Learning, Artificial Intelligence","Although retrieval-augmented generation(RAG) significantly improves generation quality by retrieving external knowledge bases and integrating generated content, it faces computational efficiency bottlenecks, particularly in knowledge retrieval tasks involving hierarchical structures for Tree-RAG. This paper proposes a Tree-RAG acceleration method based on the improved Cuckoo Filter, which optimizes entity localization during the retrieval process to achieve significant performance improvements. Tree-RAG effectively organizes entities through the introduction of a hierarchical tree structure, while the Cuckoo Filter serves as an efficient data structure that supports rapid membership queries and dynamic updates. The experiment results demonstrate that our method is much faster than naive Tree-RAG while maintaining high levels of generative quality. When the number of trees is large, our method is hundreds of times faster than naive Tree-RAG. Our work is available athttps://github.com/TUPYP7180/CFT-RAG-2025."
1218,679d459debd8ffd557a2b32f,cs.AI,https://arxiv.org/pdf/2501.15081,Can Large Language Models Be Trusted as Black-Box Evolutionary Optimizers for Combinatorial Problems?,"Jie Zhao, Tao Wen, Kang Hao Cheong","Neural and Evolutionary Computing, Artificial Intelligence","Evolutionary computation excels in complex optimization but demands deep domain knowledge, restricting its accessibility. Large Language Models (LLMs) offer a game-changing solution with their extensive knowledge and could democratize the optimization paradigm. Although LLMs possess significant capabilities, they may not be universally effective, particularly since evolutionary optimization encompasses multiple stages. It is therefore imperative to evaluate the suitability of LLMs as evolutionary optimizer (EVO). Thus, we establish a series of rigid standards to thoroughly examine the fidelity of LLM-based EVO output in different stages of evolutionary optimization and then introduce a robust error-correction mechanism to mitigate the output uncertainty. Furthermore, we explore a cost-efficient method that directly operates on entire populations with excellent effectiveness in contrast to individual-level optimization. Through extensive experiments, we rigorously validate the performance of LLMs as operators targeted for combinatorial problems. Our findings provide critical insights and valuable observations, advancing the understanding and application of LLM-based optimization."
1219,679d459debd8ffd557a2b330,cs.AI,https://arxiv.org/pdf/2501.15065,Task Arithmetic in Trust Region: A Training-Free Model Merging Approach to Navigate Knowledge Conflicts,"Wenju Sun, Qingyong Li, Wen Wang, Yangli-ao Geng, Boyang Li","Machine Learning, Artificial Intelligence",
1220,679d459debd8ffd557a2b331,cs.AI,https://arxiv.org/pdf/2501.15055,Group Ligands Docking to Protein Pockets,"Jiaqi Guan, Jiahan Li, Xiangxin Zhou, Xingang Peng, Sheng Wang, Yunan Luo, Jian Peng, Jianzhu Ma","Biomolecules, Artificial Intelligence","Molecular docking is a key task in computational biology that has attracted increasing interest from the machine learning community. While existing methods have achieved success, they generally treat each protein-ligand pair in isolation. Inspired by the biochemical observation that ligands binding to the same target protein tend to adopt similar poses, we proposeGroupBind, a novel molecular docking framework that simultaneously considers multiple ligands docking to a protein. This is achieved by introducing an interaction layer for the group of ligands and a triangle attention module for embedding protein-ligand and group-ligand pairs. By integrating our approach with diffusion-based docking model, we set a new S performance on the PDBBind blind docking benchmark, demonstrating the effectiveness of our proposed molecular docking paradigm."
1221,679d459debd8ffd557a2b332,cs.AI,https://arxiv.org/pdf/2501.15053,Exploring the impact of Optimised Hyperparameters on Bi-LSTM-based Contextual Anomaly Detector,"Aafan Ahmad Toor, Jia-Chun Lin, Ernst Gunnar Gran","Machine Learning, Artificial Intelligence","The exponential growth in the usage of Internet of Things in daily life has caused immense increase in the generation of time series data. Smart homes is one such domain where bulk of data is being generated and anomaly detection is one of the many challenges addressed by researchers in recent years. Contextual anomaly is a kind of anomaly that may show deviation from the normal pattern like point or sequence anomalies, but it also requires prior knowledge about the data domain and the actions that caused the deviation. Recent studies based on Recurrent Neural Networks (RNN) have demonstrated strong performance in anomaly detection. This study explores the impact of automatically tuned hyperparamteres on Unsupervised Online Contextual Anomaly Detection (UoCAD) approach by proposing UoCAD with Optimised Hyperparamnters (UoCAD-OH). UoCAD-OH conducts hyperparameter optimisation on Bi-LSTM model in an offline phase and uses the fine-tuned hyperparameters to detect anomalies during the online phase. The experiments involve evaluating the proposed framework on two smart home air quality datasets containing contextual anomalies. The evaluation metrics used are Precision, Recall, and F1 score."
1222,679d459debd8ffd557a2b333,cs.AI,https://arxiv.org/pdf/2501.15038,Adaptive Client Selection in Federated Learning: A Network Anomaly Detection Use Case,"William Marfo, Deepak K. Tosh, Shirley V. Moore","Machine Learning, Artificial Intelligence","Federated Learning (FL) has become a ubiquitous approach for training machine learning models on decentralized data, addressing the myriad privacy concerns inherent in traditional centralized methods. However, the efficiency of FL depends on effective client selection and robust privacy preservation mechanisms. Inadequate client selection may lead to suboptimal model performance, while insufficient privacy measures risk exposing sensitive data. This paper proposes a client selection framework for FL that integrates differential privacy and fault tolerance. Our adaptive approach dynamically adjusts the number of selected clients based on model performance and system constraints, ensuring privacy through calibrated noise addition. We evaluate our method on a network anomaly detection use case using the UNSW-NB15 and ROAD datasets. Results show up to a 7% increase in accuracy and a 25% reduction in training time compared to FedL2P. Moreover, we highlight the trade-offs between privacy budgets and model performance, with higher privacy budgets reducing noise and improving accuracy. Our fault tolerance mechanism, while causing a slight performance drop, enhances robustness to client failures. Statistical validation using Mann-Whitney U tests confirms the significance of these improvements (p<0.05𝑝0.05p<0.05italic_p < 0.05)."
1223,679d459debd8ffd557a2b334,cs.AI,https://arxiv.org/pdf/2501.15034,Divergence-Augmented Policy Optimization,"Qing Wang, Yingru Li, Jiechao Xiong, Tong Zhang","Machine Learning, Artificial Intelligence, Machine Learning","In deep reinforcement learning, policy optimization methods need to deal with issues such as function approximation and the reuse of off-policy data. Standard policy gradient methods do not handle off-policy data well, leading to premature convergence and instability. This paper introduces a method to stabilize policy optimization when off-policy data are reused. The idea is to include a Bregman divergence between the behavior policy that generates the data and the current policy to
ensure small and safe policy updates with off-policy data.
The Bregman divergence is calculated between the state distributions of two policies, instead of only on the action probabilities, leading to a divergence augmentation formulation.
Empirical experiments on Atari games show that in the data-scarce scenario where the reuse of off-policy data becomes necessary, our method can achieve better performance than other state-of-the-art deep reinforcement learning algorithms."
1224,679d459debd8ffd557a2b335,cs.AI,https://arxiv.org/pdf/2501.15014,On Accelerating Edge AI: Optimizing Resource-Constrained Environments,"Jacob Sander, Achraf Cohen, Venkat R. Dasari, Brent Venable, Brian Jalaian","Machine Learning, Artificial Intelligence, Neural and Evolutionary Computing","Resource-constrained edge deployments demand AI solutions that balance high performance with stringent compute, memory, and energy limitations. In this survey, we present a comprehensive overview of the primary strategies for accelerating deep learning models under such constraints. First, we examinemodel compressiontechniques-pruning, quantization, tensor decomposition, and knowledge distillation-that streamline large models into smaller, faster, and more efficient variants. Next, we exploreNeural Architecture Search (NAS), a class of automated methods that discover architectures inherently optimized for particular tasks and hardware budgets. We then discusscompiler and deployment frameworks, such as TVM, TensorRT, and OpenVINO, which provide hardware-tailored optimizations at inference time. By integrating these three pillars into unified pipelines, practitioners can achieve multi-objective goals, including latency reduction, memory savings, and energy efficiency-all while maintaining competitive accuracy. We also highlight emerging frontiers inhierarchical NAS,neurosymbolic approaches, andadvanced distillationtailored to large language models, underscoring open challenges like pre-training pruning for massive networks. Our survey offers practical insights, identifies current research gaps, and outlines promising directions for building scalable, platform-independent frameworks to accelerate deep learning models at the edge."
1225,679d459debd8ffd557a2b336,cs.AI,https://arxiv.org/pdf/2501.14994,Robust Cross-Etiology and Speaker-Independent Dysarthric Speech Recognition,"Satwinder Singh, Qianli Wang, Zihan Zhong, Clarion Mendes, Mark Hasegawa-Johnson, Waleed Abdulla, Seyed Reza Shahamiri","Sound, Artificial Intelligence, Machine Learning, Audio and Speech Processing","In this paper, we present a speaker-independent dysarthric speech recognition system, with a focus on evaluating the recently released Speech Accessibility Project (SAP-1005) dataset, which includes speech data from individuals with Parkinson’s disease (PD). Despite the growing body of research in dysarthric speech recognition, many existing systems are speaker-dependent and adaptive, limiting their generalizability across different speakers and etiologies. Our primary objective is to develop a robust speaker-independent model capable of accurately recognizing dysarthric speech, irrespective of the speaker. Additionally, as a secondary objective, we aim to test the cross-etiology performance of our model by evaluating it on the TORGO dataset, which contains speech samples from individuals with cerebral palsy (CP) and amyotrophic lateral sclerosis (ALS). By leveraging the Whisper model, our speaker-independent system achieved a CER of 6.99% and a WER of 10.71% on the SAP-1005 dataset. Further, in cross-etiology settings, we achieved a CER of 25.08% and a WER of 39.56% on the TORGO dataset. These results highlight the potential of our approach to generalize across unseen speakers and different etiologies of dysarthria."
1226,679d459debd8ffd557a2b337,cs.AI,https://arxiv.org/pdf/2501.14980,A Deep State Space Model for Rainfall-Runoff Simulations,"Yihan Wang, Lujun Zhang, Annan Yu, N. Benjamin Erichson, Tiantian Yang","Machine Learning, Artificial Intelligence, Atmospheric and Oceanic Physics","The classical way of studying the rainfall-runoff processes in the water cycle relies on conceptual or physically-based hydrologic models. Deep learning (DL) has recently emerged as an alternative and blossomed in hydrology community for rainfall-runoff simulations. However, the decades-old Long Short-Term Memory (LSTM) network remains the benchmark for this task, outperforming newer architectures like Transformers. In this work, we propose a State Space Model (SSM), specifically the Frequency Tuned Diagonal State Space Sequence (S4D-FT) model, for rainfall-runoff simulations. The proposed S4D-FT is benchmarked against the established LSTM and a physically-based Sacramento Soil Moisture Accounting model across 531 watersheds in the contiguous United States (CONUS). Results show that S4D-FT is able to outperform the LSTM model across diverse regions. Our pioneering introduction of the S4D-FT for rainfall-runoff simulations challenges the dominance of LSTM in the hydrology community and expands the arsenal of DL tools available for hydrological modeling."
1227,679d459debd8ffd557a2b338,cs.AI,https://arxiv.org/pdf/2501.14970,"AI-driven Wireless Positioning: Fundamentals, Standards, State-of-the-art, and Challenges","Guangjin Pan, Yuan Gao, Yilin Gao, Zhiyong Zhong, Xiaoyu Yang, Xinyu Guo, Shugong Xu","Signal Processing, Artificial Intelligence, Machine Learning","Wireless positioning technologies hold significant value for applications in autonomous driving, extended reality (XR), unmanned aerial vehicles (UAVs), and more. With the advancement of artificial intelligence (AI), leveraging AI to enhance positioning accuracy and robustness has emerged as a field full of potential. Driven by the requirements and functionalities defined in the 3rd Generation Partnership Project (3GPP) standards, AI/machine learning (ML)-based positioning is becoming a key technology to overcome the limitations of traditional methods. This paper begins with an introduction to the fundamentals of AI and wireless positioning, covering AI models, algorithms, positioning applications, emerging wireless technologies, and the basics of positioning techniques. Subsequently, focusing on standardization progress, we provide a comprehensive review of the evolution of 3GPP positioning standards, with an emphasis on the integration of AI/ML technologies in recent and upcoming releases. Based on the AI/ML-assisted positioning and direct AI/ML positioning schemes outlined in the standards, we conduct an in-depth investigation of related research. we focus on state-of-the-art (SOTA) research in AI-based line-of-sight (LOS)/non-line-of-sight (NLOS) detection, time of arrival (TOA)/time difference of arrival (TDOA) estimation, and angle estimation techniques. For Direct AI/ML Positioning, we explore SOTA advancements in fingerprint-based positioning, knowledge-assisted AI positioning, and channel charting-based positioning. Furthermore, we introduce publicly available datasets for wireless positioning and conclude by summarizing the challenges and opportunities of AI-driven wireless positioning."
1228,679d459debd8ffd557a2b339,cs.AI,https://arxiv.org/pdf/2501.14959,The Curious Case of Arbitrariness in Machine Learning,"Prakhar Ganesh, Afaf Taik, Golnoosh Farnadi","Machine Learning, Artificial Intelligence","Algorithmic modelling relies on limited information in data to extrapolate outcomes for unseen scenarios, often embedding an element of arbitrariness in its decisions. A perspective on this arbitrariness that has recently gained interest is multiplicity—the study of arbitrariness across a set of “good models”, i.e., those likely to be deployed in practice. In this work, we systemize the literature on multiplicity by: (a) formalizing the terminology around model design choices and their contribution to arbitrariness, (b) expanding the definition of multiplicity to incorporate underrepresented forms beyond just predictions and explanations, (c) clarifying the distinction between multiplicity and other traditional lenses of arbitrariness, i.e., uncertainty and variance, and (d) distilling the benefits and potential risks of multiplicity into overarching trends, situating it within the broader landscape of responsible AI. We conclude by identifying open research questions and highlighting emerging trends in this young but rapidly growing area of research."
1229,679d459debd8ffd557a2b33a,cs.AI,https://arxiv.org/pdf/2501.14942,Force-Based Robotic Imitation Learning: A Two-Phase Approach for Construction Assembly Tasks,"Hengxu You, Yang Ye, Tianyu Zhou, Jing Du","Robotics, Artificial Intelligence",
1230,679d459debd8ffd557a2b33b,cs.AI,https://arxiv.org/pdf/2501.14934,Temporal Binding Foundation Model for Material Property Recognition via Tactile Sequence Perception,"Hengxu You, Tianyu Zhou, Jing Du","Robotics, Artificial Intelligence",
1231,679d459debd8ffd557a2b33c,cs.AI,https://arxiv.org/pdf/2501.14932,Explaining Categorical Feature Interactions Using Graph Covariance and LLMs,"Cencheng Shen, Darren Edge, Jonathan Larson, Carey E. Priebe","Machine Learning, Artificial Intelligence, Machine Learning","Modern datasets often consist of numerous samples with abundant features and associated timestamps. Analyzing such datasets to uncover underlying events typically requires complex statistical methods and substantial domain expertise. A notable example, and the primary data focus of this paper, is the global synthetic dataset from the Counter Trafficking Data Collaborative (CTDC) — a global hub of human trafficking data containing over 200,000 anonymized records spanning from 2002 to 2022, with numerous categorical features for each record. In this paper, we propose a fast and scalable method for analyzing and extracting significant categorical feature interactions, and querying large language models (LLMs) to generate data-driven insights that explain these interactions. Our approach begins with a binarization step for categorical features using one-hot encoding, followed by the computation of graph covariance at each time. This graph covariance quantifies temporal changes in dependence structures within categorical data and is established as a consistent dependence measure under the Bernoulli distribution. We use this measure to identify significant feature pairs, such as those with the most frequent trends over time or those exhibiting sudden spikes in dependence at specific moments. These extracted feature pairs, along with their timestamps, are subsequently passed to an LLM tasked with generating potential explanations of the underlying events driving these dependence changes. The effectiveness of our method is demonstrated through extensive simulations, and its application to the CTDC dataset reveals meaningful feature pairs and potential data stories underlying the observed feature interactions."
1232,679d459debd8ffd557a2b33d,cs.AI,https://arxiv.org/pdf/2501.14928,"Decision Making in Changing Environments: Robustness, Query-Based Learning, and Differential Privacy","Fan Chen, Alexander Rakhlin","Machine Learning, Artificial Intelligence, Information Theory, Statistics Theory, Machine Learning",
1233,679d459debd8ffd557a2b33e,cs.AI,https://arxiv.org/pdf/2501.14912,Feasible Learning,"Juan Ramirez, Ignacio Hounie, Juan Elenter, Jose Gallego-Posada, Meraj Hashemizadeh, Alejandro Ribeiro, Simon Lacoste-Julien","Machine Learning, Artificial Intelligence",
1234,679d459debd8ffd557a2b33f,cs.AI,https://arxiv.org/pdf/2501.14856,Noise-conditioned Energy-based Annealed Rewards (NEAR): A Generative Framework for Imitation Learning from Observation,"Anish Abhijit Diwan, Julen Urain, Jens Kober, Jan Peters","Robotics, Artificial Intelligence","This paper introduces a new imitation learning framework based on energy-based generative models capable of learning complex, physics-dependent, robot motion policies through state-only expert motion trajectories. Our algorithm, called Noise-conditioned Energy-based Annealed Rewards (NEAR), constructs several perturbed versions of the expert’s motion data distribution and learns smooth, and well-defined representations of the data distribution’s energy function using denoising score matching. We propose to use these learnt energy functions as reward functions to learn imitation policies via reinforcement learning. We also present a strategy to gradually switch between the learnt energy functions, ensuring that the learnt rewards are always well-defined in the manifold of policy-generated samples. We evaluate our algorithm on complex humanoid tasks such as locomotion and martial arts and compare it with state-only adversarial imitation learning algorithms like Adversarial Motion Priors (AMP). Our framework sidesteps the optimisation challenges of adversarial imitation learning techniques and produces results comparable to AMP in several quantitative metrics across multiple imitation settings. Code and videos available atanishhdiwan.github.io/noise-conditioned-energy-based-annealed-rewards/"
1235,679d459debd8ffd557a2b340,cs.AI,https://arxiv.org/pdf/2501.14826,Multi-Modality Transformer for E-Commerce: Inferring User Purchase Intention to Bridge the Query-Product Gap,"Srivatsa Mallapragada, Ying Xie, Varsha Rani Chawan, Zeyad Hailat, Yuanbo Wang","Information Retrieval, Artificial Intelligence, Machine Learning","E-commerce click-stream data and product catalogs offer critical user behavior insights and product knowledge. This paper propose a multi-modal transformer termed as PINCER, that leverages the above data sources to transform initial user queries into pseudo-product representations. By tapping into these external data sources, our model can infer users’ potential purchase intent from their limited queries and capture query relevant product features. We demonstrate our model’s superior performance over state-of-the-art alternatives on e-commerce online retrieval in both controlled and real-world experiments. Our ablation studies confirm that the proposed transformer architecture and integrated learning strategies enable the mining of key data sources to infer purchase intent, extract product features, and enhance the transformation pipeline from queries to more accurate pseudo-product representations."
1236,679d459debd8ffd557a2b341,cs.AI,https://arxiv.org/pdf/2501.14822,Controlling Ensemble Variance in Diffusion Models: An Application for Reanalyses Downscaling,"Fabio Merizzi, Davide Evangelista, Harilaos Loukos","Applications, Artificial Intelligence, Machine Learning","In recent years, diffusion models have emerged as powerful tools for generating ensemble members in meteorology. In this work, we demonstrate that a Denoising Diffusion Implicit Model (DDIM) can effectively control ensemble variance by varying the number of diffusion steps. Introducing a theoretical framework, we relate diffusion steps to the variance expressed by the reverse diffusion process. Focusing on reanalysis downscaling, we propose an ensemble diffusion model for the full ERA5-to-CERRA domain, generating variance-calibrated ensemble members for wind speed at full spatial and temporal resolution. Our method aligns global mean variance with a reference ensemble dataset and ensures spatial variance is distributed in accordance with observed meteorological variability. Additionally, we address the lack of ensemble information in the CARRA dataset, showcasing the utility of our approach for efficient, high-resolution ensemble generation."
1237,679d459debd8ffd557a2b342,cs.AI,https://arxiv.org/pdf/2501.14809,Towards Foundation Models: Evaluation of Geoscience Artificial Intelligence with Uncertainty,"Samuel Myren, Nidhi Parikh, Rosalyn Rael, Garrison Flynn, Dave Higdon, Emily Casleton","Machine Learning, Artificial Intelligence, Geophysics","Artificial intelligence (AI) has transformed the geoscience community with deep learning models (DLMs) that are trained to complete specific tasks within workflows. This success has led to the development of geoscience foundation models (FMs), which promise to accomplish multiple tasks within a workflow or replace the workflow altogether. However, lack of robust evaluation frameworks, even for traditional DLMs, leaves the geoscience community ill prepared for the inevitable adoption of FMs. We address this gap by designing an evaluation framework that jointly incorporates three crucial aspects to current DLMs and future FMs: performance uncertainty, learning efficiency, and overlapping training-test data splits. To target the three aspects, we meticulously construct the training, validation, and test splits using clustering methods tailored to geoscience data and enact an expansive training design to segregate performance uncertainty arising from stochastic training processes and random data sampling. The framework’s ability to guard against misleading declarations of model superiority is demonstrated through evaluation of PhaseNet, a popular seismic phase picking DLM, under 3 training approaches. Furthermore, we show how the performance gains due to overlapping training-test data can lead to biased FM evaluation. Our framework helps practitioners choose the best model for their problem and set performance expectations by explicitly analyzing model performance at varying budgets of training data."
1238,679d459debd8ffd557a2b343,cs.AI,https://arxiv.org/pdf/2501.14790,Towards Dynamic Neural Communication and Speech Neuroprosthesis Based on Viseme Decoding,"Ji-Ha Park, Seo-Hyun Lee, Soowon Kim, Seong-Whan Lee","Neurons and Cognition, Artificial Intelligence, Sound, Audio and Speech Processing",
1239,679d459debd8ffd557a2b344,cs.AI,https://arxiv.org/pdf/2501.14785,ED-Filter: Dynamic Feature Filtering for Eating Disorder Classification,"Mehdi Naseriparsa, Suku Sukunesan, Zhen Cai, Osama Alfarraj, Amr Tolba, Saba Fathi Rabooki, Feng Xia","Machine Learning, Artificial Intelligence, Machine Learning, Social and Information Networks","Eating disorders (ED) are critical psychiatric problems that have alarmed the mental health community. Mental health professionals are increasingly recognizing the utility of data derived from social media platforms such as Twitter. However, high dimensionality and extensive feature sets of Twitter data present remarkable challenges for ED classification. To overcome these hurdles, we introduce a novel method, an informed branch and bound search technique known as ED-Filter. This strategy significantly improves the drawbacks of conventional feature selection algorithms such as filters and wrappers. ED-Filter iteratively identifies an optimal set of promising features that maximize the eating disorder classification accuracy. In order to adapt to the dynamic nature of Twitter ED data, we enhance the ED-Filter with a hybrid greedy-based deep learning algorithm. This algorithm swiftly identifies sub-optimal features to accommodate the ever-evolving data landscape. Experimental results on Twitter eating disorder data affirm the effectiveness and efficiency of ED-Filter. The method demonstrates significant improvements in classification accuracy and proves its value in eating disorder detection on social media platforms."
1240,679d459debd8ffd557a2b345,cs.AI,https://arxiv.org/pdf/2501.14780,"Perspective Chapter: MOOCs in India: Evolution, Innovation, Impact, and Roadmap",Partha Pratim Das,"Computers and Society, Artificial Intelligence, Digital Libraries",
1241,679d459debd8ffd557a2b346,cs.AI,https://arxiv.org/pdf/2501.14779,The Use of Generative Artificial Intelligence for Upper Secondary Mathematics Education Through the Lens of Technology Acceptance,"Mika Setälä, Ville Heilala, Pieta Sikström, Tommi Kärkkäinen","Computers and Society, Artificial Intelligence, Human-Computer Interaction","This study investigated the students’ perceptions of using Generative Artificial Intelligence (GenAI) in upper-secondary mathematics education. Data was collected from Finnish high school students to represent how key constructs of the Technology Acceptance Model (Perceived Usefulness, Perceived Ease of Use, Perceived Enjoyment, and Intention to Use) influence the adoption of AI tools. First, a structural equation model for a comparative study with a prior study was constructed and analyzed. Then, an extended model with the additional construct of Compatibility, which represents the alignment of AI tools with students’ educational experiences and needs, was proposed and analyzed. The results demonstrated a strong influence of perceived usefulness on the intention to use GenAI, emphasizing the statistically significant role of perceived enjoyment in determining perceived usefulness and ease of use. The inclusion of compatibility improved the model’s explanatory power, particularly in predicting perceived usefulness. This study contributes to a deeper understanding of how AI tools can be integrated into mathematics education and highlights key differences between the Finnish educational context and previous studies based on structural equation modeling."
1242,679d459debd8ffd557a2b347,cs.AI,https://arxiv.org/pdf/2501.14778,Advancing Trustworthy AI for Sustainable Development: Recommendations for Standardising AI Incident Reporting,"Avinash Agarwal, Manisha J Nene","Computers and Society, Artificial Intelligence, Human-Computer Interaction","The increasing use of AI technologies has led to increasing AI incidents, posing risks and causing harm to individuals, organizations, and society. This study recognizes and addresses the lack of standardized protocols for reliably and comprehensively gathering such incident data crucial for preventing future incidents and developing mitigating strategies. Specifically, this study analyses existing open-access AI-incident databases through a systematic methodology and identifies nine gaps in current AI incident reporting practices. Further, it proposes nine actionable recommendations to enhance standardization efforts to address these gaps. Ensuring the trustworthiness of enabling technologies such as AI is necessary for sustainable digital transformation. Our research promotes the development of standards to prevent future AI incidents and promote trustworthy AI, thus facilitating achieving the UN sustainable development goals. Through international cooperation, stakeholders can unlock the transformative potential of AI, enabling a sustainable and inclusive future for all."
1243,679d459debd8ffd557a2b348,cs.AI,https://arxiv.org/pdf/2501.14777,Enhancing Supply Chain Resilience with Metaverse and ChatGPT Technologies,Oumaima Sarhir,"Computers and Society, Artificial Intelligence",
1244,679d459debd8ffd557a2b349,cs.AI,https://arxiv.org/pdf/2501.14776,Green AI: Which Programming Language Consumes the Most?,"Niccolò Marini, Leonardo Pampaloni, Filippo Di Martino, Roberto Verdecchia, Enrico Vicario","Computers and Society, Artificial Intelligence, Programming Languages","AI is demanding an evergrowing portion of environmental resources. Despite their potential impact on AI environmental sustainability, the role that programming languages play in AI (in)efficiency is to date still unknown. With this study, we aim to understand the impact that programming languages can have on AI environmental sustainability. To achieve our goal, we conduct a controlled empirical experiment by considering five programming languages (C++, Java, Python, MATLAB, and R), seven AI algorithms (KNN, SVC, AdaBoost, decision tree, logistic regression, naive bayses, and random forest), three popular datasets, and the training and inference phases. The collected results show that programming languages have a considerable impact on AI environmental sustainability. Compiled and semi-compiled languages (C++, Java) consistently consume less than interpreted languages (Python, MATLAB, R), which require up to 54x more energy. Some languages are cumulatively more efficient in training, while others in inference. Which programming language consumes the most highly depends on the algorithm considered. Ultimately, algorithm implementation might be the most determining factor in Green AI, regardless of the language used. As conclusion, while making AI more environmentally sustainable is paramount, a trade-off between energy efficiency and implementation ease should always be considered. Green AI can be achieved without the need of completely disrupting the development practices and technologies currently in place."
1245,679d459debd8ffd557a2b34a,cs.AI,https://arxiv.org/pdf/2501.14775,Hybrid Firefly-Genetic Algorithm for Single and Multi-dimensional 0-1 Knapsack Problems,"Aswathi Malanthara, Ishaan R Kale","Neural and Evolutionary Computing, Artificial Intelligence",
1246,679d459debd8ffd557a2b34b,cs.AI,https://arxiv.org/pdf/2501.14772,DropMicroFluidAgents (DMFAs): Autonomous Droplet Microfluidic Research Framework Through Large Language Model Agents,"Dinh-Nguyen Nguyen, Raymond Kai-Yu Tong, Ngoc-Duy Dinh","Computers and Society, Artificial Intelligence",
1247,679d459debd8ffd557a2b34c,cs.AI,https://arxiv.org/pdf/2501.14769,A survey on pioneering metaheuristic algorithms between 2019 and 2024,"Tansel Dokeroglu, Deniz Canturk, Tayfun Kucukyilmaz","Neural and Evolutionary Computing, Artificial Intelligence","This review examines over 150 new metaheuristics of the last six years (between 2019-2024), underscoring their profound influence and performance. Over the past three decades, more than 500 new metaheuristic algorithms have been proposed, with no slowdown in sight—an overwhelming abundance that complicates the process of selecting and assessing the most effective solutions for complex optimization challenges. Our evaluation centers on pivotal criteria, including annual citation metrics, the breadth of addressed problem types, source code availability, user-friendly parameter configurations, innovative mechanisms and operators, and approaches designed to mitigate traditional metaheuristic issues such as stagnation and premature convergence. We further explore recent high-impact applications of the past six years’ most influential 23 metaheuristic algorithms, shedding light on their advantages and limitations, while identifying challenges and potential avenues for future research."
1248,679d459debd8ffd557a2b34d,cs.AI,https://arxiv.org/pdf/2501.14768,Equation discovery framework EPDE: Towards a better equation discovery,"Mikhail Maslyaev, Alexander Hvatov","Neural and Evolutionary Computing, Artificial Intelligence, Machine Learning","Equation discovery methods hold promise for extracting knowledge from physics-related data. However, existing approaches often require substantial prior information that significantly reduces the amount of knowledge extracted. In this paper, we enhance the EPDE algorithm – an evolutionary optimization-based discovery framework. In contrast to methods like SINDy, which rely on pre-defined libraries of terms and linearities, our approach generates terms using fundamental building blocks such as elementary functions and individual differentials. Within evolutionary optimization, we may improve the computation of the fitness function as is done in gradient methods and enhance the optimization algorithm itself. By incorporating multi-objective optimization, we effectively explore the search space, yielding more robust equation extraction, even when dealing with complex experimental data. We validate our algorithm’s noise resilience and overall performance by comparing its results with those from the state-of-the-art equation discovery framework SINDy."
1249,679d459debd8ffd557a2b34e,cs.AI,https://arxiv.org/pdf/2501.14766,Artificial Intelligence for Sustainable Urban Biodiversity: A Framework for Monitoring and Conservation,Yasmin Rahmati,"Computers and Society, Artificial Intelligence",
1250,679d459debd8ffd557a2b34f,cs.AI,https://arxiv.org/pdf/2501.14756,Towards An Automated AI Act FRIA Tool That Can Reuse GDPR's DPIA,"Tytti Rintamaki, Harshvardhan J. Pandit","Computers and Society, Artificial Intelligence","The AI Act introduces the obligation to conduct a Fundamental Rights Impact Assessment (FRIA), with the possibility to reuse a Data Protection Impact Assessment (DPIA), and requires the EU Commission to create of an automated tool to support the FRIA process. In this article, we provide our novel exploration of the DPIA and FRIA as information processes to enable the creation of automated tools. We first investigate the information involved in DPIA and FRIA, and then use this to align the two to state where a DPIA can be reused in a FRIA. We then present the FRIA as a 5-step process and discuss the role of an automated tool for each step. Our work provides the necessary foundation for creating and managing information for FRIA and supporting it through an automated tool as required by the AI Act."
1251,679d459debd8ffd557a2b350,cs.AI,https://arxiv.org/pdf/2501.14747,Enhancing Green Economy with Artificial Intelligence: Role of Energy Use and FDI in the United States,"Abdullah Al Abrar Chowdhury, Azizul Hakim Rafi, Adita Sultana, Abdulla All Noman","General Economics, Artificial Intelligence",
1252,679d459debd8ffd557a2b351,cs.AI,https://arxiv.org/pdf/2501.14737,EvalSVA: Multi-Agent Evaluators for Next-Gen Software Vulnerability Assessment,"Xin-Cheng Wen, Jiaxin Ye, Cuiyun Gao, Lianwei Wu, Qing Liao","Software Engineering, Artificial Intelligence","Software Vulnerability (SV) assessment is a crucial process of determining different aspects of SVs (e.g., attack vectors and scope) for developers to effectively prioritize efforts in vulnerability mitigation.
It presents a challenging and laborious process due to the complexity of SVs and the scarcity of labeled data.
To mitigate the above challenges, we introduce EvalSVA, a multi-agent evaluators team to autonomously deliberate and evaluate various aspects of SV assessment.
Specifically, we propose a multi-agent-based framework to simulate vulnerability assessment strategies in real-world scenarios, which employs multiple Large Language Models (LLMs) into an integrated group to enhance the effectiveness of SV assessment in the limited data.
We also design diverse communication strategies to autonomously discuss and assess different aspects of SV.
Furthermore, we construct a multi-lingual SV assessment dataset based on the new standard of CVSS, comprising 699, 888, and 1,310 vulnerability-related commits in C++, Python, and Java, respectively.
Our experimental results demonstrate that EvalSVA averagely outperforms the 44.12% accuracy and 43.29% F1 for SV assessment compared with the previous methods. It shows that EvalSVA offers a human-like process and generates both reason and answer for SV assessment.
EvalSVA can also aid human experts in SV assessment, which provides more explanation and details for SV assessment."
1253,679d459debd8ffd557a2b352,cs.AI,https://arxiv.org/pdf/2501.14735,ARCEAK: An Automated Rule Checking Framework Enhanced with Architectural Knowledge,"Junyong Chen, Ling-I Wu, Minyu Chen, Xiaoying Qian, Haoze Zhu, Qiongfang Zhang, Guoqiang Li","Software Engineering, Artificial Intelligence","Automated Rule Checking (ARC) plays a crucial role in advancing the construction industry by addressing the laborious, inconsistent, and error-prone nature of traditional model review conducted by industry professionals. Manual assessment against intricate sets of rules often leads to significant project delays and expenses. In response to these challenges, ARC offers a promising solution to improve efficiency and compliance in design within the construction sector. However, the main challenge of ARC lies in translating regulatory text into a format suitable for computer processing. Current methods for rule interpretation require extensive manual labor, thereby limiting their practicality. To address this issue, our study introduces a novel approach that decomposes ARC into two distinct tasks: rule information extraction and verification code generation. Leveraging generative pre-trained transformers, our method aims to streamline the interpretation of regulatory texts and simplify the process of generating model compliance checking code. Through empirical evaluation and case studies, we showcase the effectiveness and potential of our approach in automating code compliance checking, enhancing the efficiency and reliability of construction projects."
1254,679d459debd8ffd557a2b353,cs.AI,https://arxiv.org/pdf/2501.12829,A transformer-based deep q learning approach for dynamic load balancing in software-defined networks,"Evans Tetteh Owusu, Kwame Agyemang-Prempeh Agyekum, Marinah Benneh, Pius Ayorna, Justice Owusu Agyemang, George Nii Martey Colley, James Dzisi Gazde","Networking and Internet Architecture, Artificial Intelligence, Emerging Technologies, Machine Learning, Multiagent Systems",
1255,679d459debd8ffd557a2b354,cs.AI,https://arxiv.org/pdf/2501.12121,Optimally-Weighted Maximum Mean Discrepancy Framework for Continual Learning,"KaiHui Huang, RunQing Wu, Fei Ye","Machine Learning, Artificial Intelligence",
1256,679d459debd8ffd557a2b355,cs.AI,https://arxiv.org/pdf/2501.14634,Recommending Actionable Strategies: A Semantic Approach to Integrating Analytical Frameworks with Decision Heuristics,"Renato Ghisellini, Remo Pareschi, Marco Pedroni, Giovanni Battista Raggi",Artificial Intelligence,"We present a novel approach forrecommending actionable strategiesby integrating strategic frameworks with decision heuristics throughsemantic analysis. While strategy frameworks provide systematic models for assessment and planning, and decision heuristics encode experiential knowledge, these traditions have historically remained separate. Our methodology bridges this gap usingadvanced natural language processing (NLP), demonstrated through integrating frameworks like the 6C model with the Thirty-Six Stratagems. The approach employsvector space representationsandsemantic similarity calculationsto map framework parameters to heuristic patterns, supported by a computational architecture that combines deep semantic processing with constrained use of Large Language Models. By processing bothprimary contentandsecondary elements(diagrams, matrices) as complementary linguistic representations, we demonstrate effectiveness through corporate strategy case studies. The methodologygeneralizesto various analytical frameworks and heuristic sets, culminating in aplug-and-play architecturefor generatingrecommender systemsthat enable cohesive integration of strategic frameworks and decision heuristics into actionable guidance."
1257,679d459debd8ffd557a2b356,cs.AI,https://arxiv.org/pdf/2501.14630,Extracting Problem Structure with LLMs for Optimized SAT Local Search,"André Schilder, Stefan Szeider",Artificial Intelligence,"Local search preprocessing makes Conflict-Driven Clause Learning
(CDCL) solvers faster by providing high-quality starting points and
modern SAT solvers have incorporated this technique into their
preprocessing steps. However, these tools rely on basic strategies
that miss the structural patterns in problems. We present a method
that applies Large Language Models (LLMs) to analyze Python-based
encoding code. This reveals hidden structural patterns in how
problems convert into SAT. Our method automatically generates
specialized local search algorithms that find these patterns and use
them to create strong initial assignments. This works for any
problem instance from the same encoding type. Our tests show
encouraging results, achieving faster solving times compared to
baseline preprocessing systems."
1258,679d459debd8ffd557a2b357,cs.AI,https://arxiv.org/pdf/2501.14568,Hybrid Quantum-Classical Multi-Agent Pathfinding,"Thore Gerlach, Loong Kuan Lee, Frédéric Barbaresco, Nico Piatkowski","Artificial Intelligence, Quantum Physics","Multi-Agent Path Finding (Mapf) focuses on determining conflict-free paths for multiple agents navigating through a shared space to reach specified goal locations. This problem becomes computationally challenging, particularly when handling large numbers of agents, as frequently encountered in practical applications like coordinating autonomous vehicles. Quantum computing (QC) is a promising candidate in overcoming such limits. However, current quantum hardware is still in its infancy and thus limited in terms of computing power and error robustness. In this work, we present the first optimal hybrid quantum-classicalMapfalgorithm which is based on branch-and-cut-and-prize. QC is integrated by iteratively solvingQuboproblems, based on conflict graphs. Experiments on actual quantum hardware and results on benchmark data suggest that our approach dominates previousQuboformulations and baselineMapfsolvers."
1259,679d459debd8ffd557a2b358,cs.AI,https://arxiv.org/pdf/2501.14540,VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic Reasoning,"Benjamin Callewaert, Simon Vandevelde, Joost Vennekens",Artificial Intelligence,"A recent approach to neurosymbolic reasoning is to explicitly combine the strengths of large language models (LLMs) and symbolic solvers to tackle complex reasoning tasks. However, current approaches face significant limitations, including poor generalizability due to task-specific prompts, inefficiencies caused by the lack of separation between knowledge and queries, and restricted inferential capabilities. These shortcomings hinder their scalability and applicability across diverse domains.
In this paper, we introduce VERUS-LM, a novel framework designed to address these challenges. VERUS-LM employs a generic prompting mechanism, clearly separates domain knowledge from queries, and supports a wide range of different logical reasoning tasks. This framework enhances adaptability, reduces computational cost, and allows for richer forms of reasoning, such as optimization and constraint satisfaction.
We show that our approach succeeds in diverse reasoning on a novel dataset, markedly outperforming LLMs.
Additionally, our system achieves competitive results on common reasoning benchmarks when compared to other state-of-the-art approaches, and significantly surpasses them on the difficult AR-LSAT dataset.
By pushing the boundaries of hybrid reasoning, VERUS-LM represents a significant step towards more versatile neurosymbolic AI systems.111All code and datasets required to reproduce the results in this text are available online:https://gitlab.com/EAVISE/bca/verus-lm"
1260,679d459debd8ffd557a2b359,cs.AI,https://arxiv.org/pdf/2501.14360,In System Alignments we Trust! Explainable Alignments via Projections,"Dominique Sommers, Natalia Sidorova, Boudewijn van Dongen","Artificial Intelligence, Formal Languages and Automata Theory","Alignments are a well-known process mining technique for reconciling system logs and normative process models. Evidence of certain behaviors in a real system may only be present in one representation – either a log or a model – but not in the other.
Since for processes in which multiple entities, like objects and resources, are involved in the activities, their interactions affect the behavior and are therefore essential to take into account in the alignments."
1261,679d459debd8ffd557a2b35a,cs.AI,https://arxiv.org/pdf/2501.14334,Exploring the sustainable scaling of AI dilemma: A projective study of corporations' AI environmental impacts,"Clément Desroches, Martin Chauvin, Louis Ladan, Caroline Vateau, Simon Gosset, Philippe Cordier","Artificial Intelligence, Computers and Society, Machine Learning",
1262,679d459debd8ffd557a2b35b,cs.AI,https://arxiv.org/pdf/2501.14304,MASTER: A Multi-Agent System with LLM Specialized MCTS,"Bingzheng Gan, Yufan Zhao, Tianyi Zhang, Jing Huang, Yusu Li, Shu Xian Teo, Changwang Zhang, Wei Shi",Artificial Intelligence,"Large Language Models (LLM) are increasingly being explored for problem-solving tasks. However, their strategic planning capability is often viewed with skepticism. Recent studies have incorporated the Monte Carlo Tree Search (MCTS) algorithm to augment the planning capacity of LLM. Despite its potential, MCTS relies on extensive sampling simulations to approximate the true reward distribution, leading to two primary issues. Firstly, MCTS is effective for tasks like the Game of Go, where simulation results can yield objective rewards (e.g., 1 for a win and 0 for a loss). However, for tasks such as question answering, the result of a simulation is the answer to the question, which cannot obtain an objective reward without the ground truth. Secondly, obtaining statistically significant reward estimations typically requires a sample size exceeding 30 simulations, resulting in excessive token usage and time consumption. To address these challenges, we presentMulti-AgentSystem withTacticalExecution andReasoning using LLM Specialized MCTS (MASTER), a novel framework that coordinates agent recruitment and communication using LLM specialized MCTS. This system autonomously adjusts the number of agents based on task complexity and ensures focused communication among them. Comprehensive experiments across various tasks demonstrate the effectiveness of our proposed framework. It achieves 76% accuracy on HotpotQA and 80% on WebShop, setting new state-of-the-art performance on these datasets."
1263,679d459debd8ffd557a2b35c,cs.AI,https://arxiv.org/pdf/2501.14224,Top Ten Challenges Towards Agentic Neural Graph Databases,"Jiaxin Bai, Zihao Wang, Yukun Zhou, Hang Yin, Weizhi Fei, Qi Hu, Zheye Deng, Jiayang Cheng, Tianshi Zheng, Hong Ting Tsang, Yisen Gao, Zhongwei Xie, Yufei Li, Lixin Fan, Binhang Yuan, Wei Wang, Lei Chen, Xiaofang Zhou, Yangqiu Song","Artificial Intelligence, Databases, Machine Learning","Graph databases (GDBs) like Neo4j and TigerGraph excel at handling interconnected data but lack advanced inference capabilities. Neural Graph Databases (NGDBs) address this by integrating Graph Neural Networks (GNNs) for predictive analysis and reasoning over incomplete or noisy data. However, NGDBs rely on predefined queries and lack autonomy and adaptability.
This paper introduces Agentic Neural Graph Databases (Agentic NGDBs), which extend NGDBs with three core functionalities: autonomous query construction, neural query execution, and continuous learning. We identify ten key challenges in realizing Agentic NGDBs: semantic unit representation, abductive reasoning, scalable query execution, and integration with foundation models like large language models (LLMs). By addressing these challenges, Agentic NGDBs can enable intelligent, self-improving systems for modern data-driven applications, paving the way for adaptable and autonomous data management solutions."
1264,679d459debd8ffd557a2b35d,cs.AI,https://arxiv.org/pdf/2501.14189,Distributed Multi-Agent Coordination Using Multi-Modal Foundation Models,"Saaduddin Mahmud, Dorian Benhamou Goldfajn, Shlomo Zilberstein","Artificial Intelligence, Machine Learning, Multiagent Systems","Distributed Constraint Optimization Problems (DCOPs) offer a powerful framework for multi-agent coordination but often rely on labor-intensive, manual problem construction. To address this, we introduce VL-DCOPs, a framework that takes advantage of large multimodal foundation models (LFMs) to automatically generate constraints from both visual and linguistic instructions. We then introduce a spectrum of agent archetypes for solving VL-DCOPs: from a neuro-symbolic agent that delegates some of the algorithmic decisions to an LFM, to a fully neural agent that depends entirely on an LFM for coordination. We evaluate these agent archetypes using state-of-the-art LLMs (large language models) and VLMs (vision language models) on three novel VL-DCOP tasks and compare their respective advantages and drawbacks. Lastly, we discuss how this work extends to broader frontier challenges in the DCOP literature."
1265,679d459debd8ffd557a2b35e,cs.AI,https://arxiv.org/pdf/2501.14035,Human-Alignment Influences the Utility of AI-assisted Decision Making,"Nina L. Corvelo Benz, Manuel Gomez Rodriguez",Artificial Intelligence,"Whenever an AI model is used to predict a relevant (binary) outcome in AI-assisted decision making, it is widely agreed that, together with each prediction, the model should provide an AI confidence value.
However, it has been unclear why decision makers have often difficulties to develop a good sense on when to trust a prediction using AI confidence values.
Very recently, Corvelo Benz and Gomez Rodriguez have argued that, for rational decision makers, the utility of AI-assisted decision making is inherently bounded by the degree of alignment between the AI confidence values and the decision maker’s confidence on their own predictions.
In this work, we empirically investigate to what extent the degree of alignment actually influences the utility of AI-assisted decision making.
To this end, we design and run a large-scale human subject study (n=703𝑛703n=703italic_n = 703) where participants solve a simple decision making task—an online card game—assisted by an AI model with a steerable degree of alignment.
Our results show a positive association between the degree of alignment and the utility of AI-assisted decision making.
In addition, our results also show that post-processing the AI confidence values to achieve multicalibration with respect to the participants’ confidence on their own predictions increases both the degree of alignment and the utility of AI-assisted decision making."
1266,679d459debd8ffd557a2b35f,cs.AI,https://arxiv.org/pdf/2501.13942,Prompt-Based Monte Carlo Tree Search for Mitigating Hallucinations in Large Models,"Zhihua Duan, Jialin Wang",Artificial Intelligence,"With the rapid development of large models in the field of artificial intelligence, how to enhance their application capabilities in handling complex problems in the field of scientific research remains a challenging problem to be solved. This study proposes an improved Monte Carlo Tree Search (MCTS) method based on prompt words. In the simulation search stage, it introduces dynamic adjustment of exploration parameters and adaptive selection strategies, which can better balance exploration and exploitation, thereby reducing the hallucination phenomenon. This paper takes the four subsets of the SciEval dataset as the test objects, and compares the Glm-4-flash+Improved MCTS method with the methods of several existing models. The results show that the Improved MCTS method performs better, providing new ideas and methods for the application of large models in the field of scientific research."
1267,679d459debd8ffd557a2b360,cs.AI,https://arxiv.org/pdf/2501.14694,Towards Automated Self-Supervised Learning for Truly Unsupervised Graph Anomaly Detection,"Zhong Li, Yuhang Wang, Matthijs van Leeuwen","Machine Learning, Artificial Intelligence","Self-supervised learning (SSL) is an emerging paradigm that exploits supervisory signals generated from the data itself, and many recent studies have leveraged SSL to conduct graph anomaly detection. However, we empirically found that three important factors can substantially impact detection performance across datasets: 1) the specific SSL strategy employed; 2) the tuning of the strategy’s hyperparameters; and 3) the allocation of combination weights when using multiple strategies. Most SSL-based graph anomaly detection methods circumvent these issues by arbitrarily or selectively (i.e., guided by label information) choosing SSL strategies, hyperparameter settings, and combination weights. While an arbitrary choice may lead to subpar performance, using label information in an unsupervised setting is label information leakage and leads to severe overestimation of a method’s performance. Leakage has been criticized as “one of the top ten data mining mistakes”, yet many recent studies on SSL-based graph anomaly detection have been using label information to select hyperparameters. To mitigate this issue, we propose to use an internal evaluation strategy (with theoretical analysis) to select hyperparameters in SSL for unsupervised anomaly detection. We perform extensive experiments using 10 recent SSL-based graph anomaly detection algorithms on various benchmark datasets, demonstrating both the prior issues with hyperparameter selection and the effectiveness of our proposed strategy."
1268,679d459debd8ffd557a2b361,cs.AI,https://arxiv.org/pdf/2501.14687,Decoding Generalization from Memorization in Deep Neural Networks,"Simran Ketha, Venkatakrishnan Ramaswamy","Machine Learning, Artificial Intelligence","Overparameterized Deep Neural Networks that generalize well have been key to the dramatic success of Deep Learning in recent years. The reasons for their remarkable ability to generalize are not well understood yet. It has also been known that deep networks possess the ability to memorize training data, as evidenced by perfect or high training accuracies on models trained with corrupted data that have class labels shuffled to varying degrees. Concomitantly, such models are known to generalize poorly, i.e. they suffer from poor test accuracies, due to which it is thought that the act of memorizing substantially degrades the ability to generalize. It has, however, been unclear why the poor generalization that accompanies such memorization, comes about. One possibility is that in the process of training with corrupted data, the layers of the network irretrievably re-organize their representations in a manner that makes generalization difficult. The other possibility is that the network retains significant ability to generalize, but the trained network somehow “chooses” to readout in a manner that is detrimental to generalization. Here, we provide evidence for the latter possibility by demonstrating, empirically, that such models possess information in their representations for substantially improved generalization, even in the face of memorization. Furthermore, such generalization abilities can be easily decoded from the internals of the trained model, and we build a technique to do so from the outputs of specific layers of the network. We demonstrate results on multiple models trained with a number of standard datasets."
1269,679d459debd8ffd557a2b362,cs.AI,https://arxiv.org/pdf/2501.14678,A Predictive Approach for Enhancing Accuracy in Remote Robotic Surgery Using Informer Model,"Muhammad Hanif Lashari, Shakil Ahmed, Wafa Batayneh, Ashfaq Khokhar","Robotics, Artificial Intelligence","Precise and real-time estimation of the robotic arm’s position on the patient’s side is essential for the success of remote robotic surgery in Tactile Internet (TI) environments. This paper presents a prediction model based on the Transformer-based Informer framework for accurate and efficient position estimation. Additionally, combined with a Four-State Hidden Markov Model (4-State HMM) to simulate realistic packet loss scenarios. The proposed approach addresses challenges such as network delays, jitter, and packet loss to ensure reliable and precise operation in remote surgical applications.
The method integrates the optimization problem into the Informer model by embedding constraints such as energy efficiency, smoothness, and robustness into its training process using a differentiable optimization layer. The Informer framework uses features such as ProbSparse attention, attention distilling, and a generative-style decoder to focus on position-critical features while maintaining a low computational complexity ofO⁢(L⁢log⁡L)𝑂𝐿𝐿O(L\log L)italic_O ( italic_L roman_log italic_L ).
The method is evaluated using the JIGSAWS dataset, achieving a prediction accuracy of over 90% under various network scenarios. A comparison with models such as TCN, RNN, and LSTM demonstrates the Informer framework’s superior performance in handling position prediction and meeting real-time requirements, making it suitable for Tactile Internet-enabled robotic surgery."
1270,679d459debd8ffd557a2b363,cs.AI,https://arxiv.org/pdf/2501.14661,Neural-Symbolic Message Passing with Dynamic Pruning,"Chongzhi Zhang, Junhao Zheng, Zhiping Peng, Qianli Ma","Machine Learning, Artificial Intelligence","Complex Query Answering (CQA) over incomplete Knowledge Graphs (KGs) is a challenging task. Recently, a line of message-passing-based research has been proposed to solve CQA. However, they perform unsatisfactorily on negative queries and fail to address the noisy messages between variable nodes in the query graph. Moreover, they offer little interpretability and require complex query data and resource-intensive training. In this paper, we propose a Neural-Symbolic Message Passing (NSMP) framework based on pre-trained neural link predictors. By introducing symbolic reasoning and fuzzy logic, NSMP can generalize to arbitrary existential first order logic queries without requiring training while providing interpretable answers. Furthermore, we introduce a dynamic pruning strategy to filter out noisy messages between variable nodes. Experimental results show that NSMP achieves a strong performance. Additionally, through complexity analysis and empirical verification, we demonstrate the superiority of NSMP in inference time over the current state-of-the-art neural-symbolic method. Compared to this approach, NSMP demonstrates faster inference times across all query types on benchmark datasets, with speedup ranging from 2×\times×to over 150×\times×."
1271,679d459debd8ffd557a2b364,cs.AI,https://arxiv.org/pdf/2501.14654,MedAgentBench: Dataset for Benchmarking LLMs as Agents in Medical Applications,"Yixing Jiang, Kameron C. Black, Gloria Geng, Danny Park, Andrew Y. Ng, Jonathan H. Chen","Machine Learning, Artificial Intelligence, Multiagent Systems","BackgroundRecent large language models (LLMs) have demonstrated significant advancements, particularly in their ability to serve as agents thereby surpassing their traditional role as chatbots. These agents can leverage their planning and tool utilization capabilities to address tasks specified at a high level. This suggests new potential to reduce the burden of administrative tasks and address current healthcare staff shortages. However, a standardized dataset to benchmark the agent capabilities of LLMs in medical applications is currently lacking, making the evaluation of LLMs on complex tasks in interactive healthcare environments challenging."
1272,679d459debd8ffd557a2b365,cs.AI,https://arxiv.org/pdf/2501.14622,ACT-JEPA: Joint-Embedding Predictive Architecture Improves Policy Representation Learning,"Aleksandar Vujinovic, Aleksandar Kovacevic","Machine Learning, Artificial Intelligence",
1273,679d459debd8ffd557a2b366,cs.AI,https://arxiv.org/pdf/2501.14610,Leveraging Spatial Cues from Cochlear Implant Microphones to Efficiently Enhance Speech Separation in Real-World Listening Scenes,"Feyisayo Olalere, Kiki van der Heijden, Christiaan H. Stronks, Jeroen Briaire, Johan HM Frijns, Marcel van Gerven","Sound, Artificial Intelligence, Audio and Speech Processing","Speech separation approaches for single-channel, dry speech mixtures have improved greatly. However, speech separation in real-world, spatial and reverberant acoustic environments remains challenging. This limits the efficiency of existing approaches for real-world speech separation applications in assistive hearing devices such as cochlear implants (CIs). To address this issue, we quantify the impact of real-world acoustic scenes on speech separation and investigate to what extent spatial cues from such real-world scenes can improve separation quality in an efficient manner. Crucially, we characterize speech separation performance as a function of implicit spatial cues (i.e., cues inherently present in the acoustic input that can be learned by the model), as well as of explicit spatial cues (i.e., manually calculated spatial features added as auxiliary input to the model). Our findings show that spatial cues (both implicit and explicit) improve separation performance for mixtures with spatially separated talkers, but also for mixtures with nearby talkers. Further, we demonstrate that spatial cues enhance speech separation in particular when spectral cues for separation are ambiguous, that is, when voices are similar. Finally, we show that the addition of explicit, auxiliary spatial cues is particularly beneficial when implicit spatial cues are weak. For example, microphone recordings from a single CI contain weaker implicit spatial cues than when microphone recordings from two, bilateral CIs are combined. These findings emphasize the importance of training models on real-world data to improve generalisability to everyday listening situations and contribute to the development of more efficient speech separation approaches for CIs or other assistive hearing devices in such real-world listening situations."
1274,679d459debd8ffd557a2b367,cs.AI,https://arxiv.org/pdf/2501.14603,Age and Power Minimization via Meta-Deep Reinforcement Learning in UAV Networks,"Sankani Sarathchandra, Eslam Eldeeb, Mohammad Shehab, Hirley Alves, Konstantin Mikhaylov, Mohamed-Slim Alouini","Machine Learning, Artificial Intelligence","Age-of-information (AoI) and transmission power are crucial performance metrics in low energy wireless networks, where information freshness is of paramount importance. This study examines a power-limited internet of things (IoT) network supported by a flying unmanned aerial vehicle (UAV) that collects data. Our aim is to optimize the UAV’s flight trajectory and scheduling policy to minimize a varying AoI and transmission power combination. To tackle this variation, this paper proposes a meta-deep reinforcement learning (RL) approach that integrates deep Q-networks (DQNs) with model-agnostic meta-learning (MAML). DQNs determine optimal UAV decisions, while MAML enables scalability across varying objective functions. Numerical results indicate that the proposed algorithm converges faster and adapts to new objectives more effectively than traditional deep RL methods, achieving minimal AoI and transmission power overall."
1275,679d459debd8ffd557a2b368,cs.AI,https://arxiv.org/pdf/2501.14577,ZETA: Leveraging Z-order Curves for Efficient Top-k Attention,"Qiuhao Zeng, Jerry Huang, Peng Lu, Gezheng Xu, Boxing Chen, Charles Ling, Boyu Wang","Machine Learning, Artificial Intelligence","The abstract paragraph should be indented 1/2 inch (3 picas) on both left and
right-hand margins. Use 10 point type, with a vertical spacing of 11 points.
The wordAbstractmust be centered, in small caps, and in point size 12. Two
line spaces precede the abstract. The abstract must be limited to one
paragraph."
1276,679d459debd8ffd557a2b369,cs.AI,https://arxiv.org/pdf/2501.14544,Distributed Conformal Prediction via Message Passing,"Haifeng Wen, Hong Xing, Osvaldo Simeone","Machine Learning, Artificial Intelligence, Machine Learning","Post-hoc calibration of pre-trained models is critical for ensuring reliable inference, especially in safety-critical domains such as healthcare. Conformal Prediction (CP) offers a robust post-hoc calibration framework, providing distribution-free statistical coverage guarantees for prediction sets by leveraging held-out datasets. In this work, we address a decentralized setting where each device has limited calibration data and can communicate only with its neighbors over an arbitrary graph topology. We propose two message-passing-based approaches for achieving reliable inference via CP: quantile-based distributed conformal prediction (Q-DCP) and histogram-based distributed conformal prediction (H-DCP). Q-DCP employs distributed quantile regression enhanced with tailored smoothing and regularization terms to accelerate convergence, while H-DCP uses a consensus-based histogram estimation approach. Through extensive experiments, we investigate the trade-offs between hyperparameter tuning requirements, communication overhead, coverage guarantees, and prediction set sizes across different network topologies."
1277,679d459debd8ffd557a2b36a,cs.AI,https://arxiv.org/pdf/2501.14513,ABPT: Amended Backpropagation through Time with Partially Differentiable Rewards,"Fanxing Li, Fangyu Sun, Tianbao Zhang, Danping Zou","Robotics, Artificial Intelligence, Machine Learning","Using the exact gradients of the rewards to directly optimize policy parameters via backpropagation-through-time (BPTT) enables high training performance for quadrotor tasks. However, designing a fully differentiable reward architecture is often challenging. Partially differentiable rewards will result in biased gradient propagation that degrades training performance. To overcome this limitation, we propose Amended Backpropagation-through-Time (ABPT), a novel approach that mitigates gradient bias while preserving the training efficiency of BPTT. ABPT combines 0-step and N-step returns, effectively reducing the bias by leveraging value gradients from the learned Q-value function. Additionally, it adopts entropy regularization and state initialization mechanisms to encourage exploration during training. We evaluate ABPT on four representative quadrotor flight tasks. Experimental results demonstrate that ABPT converges significantly faster and achieves higher ultimate rewards than existing learning algorithms, particularly in tasks involving partially differentiable rewards."
1278,679d459debd8ffd557a2b36b,cs.AI,https://arxiv.org/pdf/2501.14474,The Pseudo-Dimension of Contracts,"Paul Duetting, Michal Feldman, Tomasz Ponitka, Ermis Soumalias","Computer Science and Game Theory, Artificial Intelligence, Machine Learning, Theoretical Economics","Algorithmic contract design
studies scenarios
where a principal incentivizes an agent to exert effort on her behalf.
In this work, we focus on settings where the agent’s type is drawn from an unknown distribution, and formalize an offline learning framework for learning
near-optimal
contracts
from sample agent types.
A central tool in our analysis is the notion ofpseudo-dimensionfrom statistical learning theory.
Beyond its role in establishing upper bounds on the sample complexity, pseudo-dimension measures the intrinsic complexity of a class of contracts, offering a new perspective on the tradeoffs between simplicity and optimality in contract design.
Our main results provide
essentially optimal tradeoffs
between pseudo-dimension and representation error (defined as the loss in principal’s utility) with respect to
linear and bounded contracts.
Using these tradeoffs, we derive sample- and time-efficient learning algorithms,
and demonstrate their near-optimality by
providing almost matching lower bounds on
the sample complexity.
Conversely, for unbounded contracts, we prove an impossibility result showing that no learning algorithm exists."
1279,679d459debd8ffd557a2b36c,cs.AI,https://arxiv.org/pdf/2501.14469,Pesti-Gen: Unleashing a Generative Molecule Approach for Toxicity Aware Pesticide Design,"Taehan Kim, Wonduk Seo","Machine Learning, Artificial Intelligence, Biomolecules, Molecular Networks","Global climate change has reduced crop resilience and pesticide efficacy, making reliance on synthetic pesticides inevitable, even though their widespread use poses significant health and environmental risks. While these pesticides remain a key tool in pest management, previous machine-learning applications in pesticide and agriculture have focused on classification or regression, leaving the fundamental challenge of generating new molecular structures or designing novel candidates unaddressed. In this paper, we proposePesti-Gen, a novel generative model based on variational auto-encoders, designed to create pesticide candidates with optimized properties for the first time. Specifically,Pesti-Genleverages a two-stage learning process: an initial pre-training phase that captures a generalized chemical structure representation, followed by a fine-tuning stage that incorporates toxicity-specific information. The model simultaneously optimizes over multiple toxicity metrics, such as (1) livestock toxicity and (2) aqua toxicity to generate environmentally friendly pesticide candidates. Notably,Pesti-Genachieves approximately 68% structural validity in generating new molecular structures, demonstrating the model’s effectiveness in producing optimized and feasible pesticide candidates, thereby providing a new way for safer and more sustainable pest management solutions."
1280,679d459debd8ffd557a2b36d,cs.AI,https://arxiv.org/pdf/2501.14459,Interpretability Analysis of Domain Adapted Dense Retrievers,"Goksenin Yuksel, Jaap Kamps","Information Retrieval, Artificial Intelligence","Dense retrievers have demonstrated significant potential for neural information retrieval; however, they exhibit a lack of robustness to domain shifts, thereby limiting their efficacy in zero-shot settings across diverse domains.
Previous research has investigated unsupervised domain adaptation techniques to adapt dense retrievers to target domains. However, these studies have not focused on explainability analysis to understand how such adaptations alter the model’s behavior.
In this paper, we propose utilizing the integrated gradients framework to develop an interpretability method that provides both instance-based and ranking-based explanations for dense retrievers.
To generate these explanations, we introduce a novel baseline that reveals both query and document attributions.
This method is used to analyze the effects of domain adaptation on input attributions for query and document tokens across two datasets: the financial question answering dataset (FIQA) and the biomedical information retrieval dataset (TREC-COVID).
Our visualizations reveal that domain-adapted models focus more on in-domain terminology compared to non-adapted models, exemplified by terms such as ”hedge,” ”gold,” ”corona,” and ”disease.”
This research addresses how unsupervised domain adaptation techniques influence the behavior of dense retrievers when adapted to new domains.
Additionally, we demonstrate that integrated gradients are a viable choice for explaining and analyzing the internal mechanisms of these opaque neural models."
1281,679d459debd8ffd557a2b36e,cs.AI,https://arxiv.org/pdf/2501.14443,Learning more with the same effort: how randomization improves the robustness of a robotic deep reinforcement learning agent,"Lucía Güitta-López, Jaime Boal, Álvaro J. López-López","Robotics, Artificial Intelligence","The industrial application of Deep Reinforcement Learning (DRL) is frequently slowed down because of the inability to generate the experience required to train the models. Collecting data often involves considerable time and economic effort that is unaffordable in most cases. Fortunately, devices like robots can be trained with synthetic experience thanks to virtual environments. With this approach, the sample efficiency problems of artificial agents are mitigated, but another issue arises: the need for efficiently transferring the synthetic experience into the real world (sim-to-real)."
1282,679d459debd8ffd557a2b36f,cs.AI,https://arxiv.org/pdf/2501.14400,SKIL: Semantic Keypoint Imitation Learning for Generalizable Data-efficient Manipulation,"Shengjie Wang, Jiacheng You, Yihang Hu, Jiongye Li, Yang Gao","Robotics, Artificial Intelligence","Real-world tasks such as garment manipulation and table rearrangement demand robots to perform generalizable, highly precise, and long-horizon actions. Although imitation learning has proven to be an effective approach for teaching robots new skills, large amounts of expert demonstration data are still indispensible for these complex tasks, resulting in high sample complexity and costly data collection.
To address this, we propose Semantic Keypoint Imitation Learning (SKIL), a framework which automatically obtain semantic keypoints with help of vision foundation models, and forms the descriptor of semantic keypoints that enables effecient imitation learning of complex robotic tasks with significantly lower sample complexity. In real world experiments, SKIL doubles the performance of baseline methods in tasks such as picking a cup or mouse, while demonstrating exceptional robustness to variations in objects, environmental changes, and distractors. For long-horizon tasks like hanging a towel on a rack where previous methods fail completely, SKIL achieves a mean success rate of 70% with as few as 30 demonstrations.
Furthermore, SKIL naturally supports cross-embodiment learning due to its semantic keypoints abstraction, our experiments demonstrate that even human videos bring considerable improvement to the learning performance. All these results demonstrate the great success of SKIL in achieving data-efficint generalizable robotic learning.
Visualizations and code are available at:https://skil-robotics.github.io/SKIL-robotics/."
1283,679d459debd8ffd557a2b370,cs.AI,https://arxiv.org/pdf/2501.14399,Handling Heterophily in Recommender Systems with Wavelet Hypergraph Diffusion,"Darnbi Sakong, Thanh Tam Nguyen","Information Retrieval, Artificial Intelligence, Databases, Machine Learning, Social and Information Networks","Recommender systems are pivotal in delivering personalised user experiences across various domains. However, capturing the heterophily patterns and the multi-dimensional nature of user-item interactions poses significant challenges. To address this, we introduce FWHDNN (Fusion-based Wavelet Hypergraph Diffusion Neural Networks), an innovative framework aimed at advancing representation learning in hypergraph-based recommendation tasks. The model incorporates three key components: (1) a cross-difference relation encoder leveraging heterophily-aware hypergraph diffusion to adapt message-passing for diverse class labels, (2) a multi-level cluster-wise encoder employing wavelet transform-based hypergraph neural network layers to capture multi-scale topological relationships, and (3) an integrated multi-modal fusion mechanism that combines structural and textual information through intermediate and late-fusion strategies. Extensive experiments on real-world datasets demonstrate that FWHDNN surpasses state-of-the-art methods in accuracy, robustness, and scalability in capturing high-order interconnections between users and items."
1284,679d459debd8ffd557a2b371,cs.AI,https://arxiv.org/pdf/2501.14346,HorNets: Learning from Discrete and Continuous Signals with Routing Neural Networks,"Boshko koloski, Nada Lavrač, Blaž Škrlj","Machine Learning, Artificial Intelligence","Construction of neural network architectures suitable for learning from both continuous and discrete tabular data is a challenging research endeavor. Contemporary high-dimensional tabular data sets are often characterized by a relatively small instance count, requiring data-efficient learning. We propose HorNets (Horn Networks), a neural network architecture with state-of-the-art performance on synthetic and real-life data sets from scarce-data tabular domains. HorNets are based on a clipped polynomial-like activation function, extended by a custom discrete-continuous routing mechanism that decides which part of the neural network to optimize based on the input’s cardinality. By explicitly modeling parts of the feature combination space or combining whole space in a linear attention-like manner, HorNets dynamically decide which mode of operation is the most suitable for a given piece of data with no explicit supervision. This architecture is one of the few approaches that reliably retrieves logical clauses (including noisy XNOR) and achieves state-of-the-art classification performance on 14 real-life biomedical high-dimensional data sets.
HorNets are made freely available under a permissive license alongside a synthetic generator of categorical benchmarks."
1285,679d459debd8ffd557a2b372,cs.AI,https://arxiv.org/pdf/2501.14322,Relative Layer-Wise Relevance Propagation: a more Robust Neural Networks eXplaination,"Eric Nyiri, Olivier Gibaru","Machine Learning, Artificial Intelligence",
1286,679d459debd8ffd557a2b373,cs.AI,https://arxiv.org/pdf/2501.14310,Permutation-based multi-objective evolutionary feature selection for high-dimensional data,"Raquel Espinosa, Gracia Sánchez, José Palma, Fernando Jiménez","Machine Learning, Artificial Intelligence","Feature selection is a critical step in the analysis of high-dimensional data, where the number of features often vastly exceeds the number of samples. Effective feature selection not only improves model performance and interpretability but also reduces computational costs and mitigates the risk of overfitting. In this context, we propose a novel feature selection method for high-dimensional data, based on the well-known permutation feature importance approach, but extending it to evaluate subsets of attributes rather than individual features. This extension more effectively captures how interactions among features influence model performance. The proposed method employs a multi-objective evolutionary algorithm to search for candidate feature subsets, with the objectives of maximizing the degradation in model performance when the selected features are shuffled, and minimizing the cardinality of the feature subset. The effectiveness of our method has been validated on a set of 24 publicly available high-dimensional datasets for classification and regression tasks, and compared against 9 well-established feature selection methods designed for high-dimensional problems, including the conventional permutation feature importance method. The results demonstrate the ability of our approach in balancing accuracy and computational efficiency, providing a powerful tool for feature selection in complex, high-dimensional datasets."
1287,679d459debd8ffd557a2b374,cs.AI,https://arxiv.org/pdf/2501.14305,A Zero-Shot LLM Framework for Automatic Assignment Grading in Higher Education,"Calvin Yeung, Jeff Yu, King Chau Cheung, Tat Wing Wong, Chun Man Chan, Kin Chi Wong, Keisuke Fujii","Computers and Society, Artificial Intelligence","Automated grading has become an essential tool in education technology due to its ability to efficiently assess large volumes of student work, provide consistent and unbiased evaluations, and deliver immediate feedback to enhance learning. However, current systems face significant limitations, including the need for large datasets in few-shot learning methods, a lack of personalized and actionable feedback, and an overemphasis on benchmark performance rather than student experience.
To address these challenges, we propose a Zero-Shot Large Language Model (LLM)-Based Automated Assignment Grading (AAG) system. This framework leverages prompt engineering to evaluate both computational and explanatory student responses without requiring additional training or fine-tuning. The AAG system delivers tailored feedback that highlights individual strengths and areas for improvement, thereby enhancing student learning outcomes.
Our study demonstrates the system’s effectiveness through comprehensive evaluations, including survey responses from higher education students that indicate significant improvements in motivation, understanding, and preparedness compared to traditional grading methods. The results validate the AAG system’s potential to transform educational assessment by prioritizing learning experiences and providing scalable, high-quality feedback."
1288,679d459debd8ffd557a2b375,cs.AI,https://arxiv.org/pdf/2501.14278,Active Learning for Continual Learning: Keeping the Past Alive in the Present,"Jaehyun Park, Dongmin Park, Jae-Gil Lee","Machine Learning, Artificial Intelligence","Continual learning (CL)enables deep neural networks to adapt to ever-changing data distributions. In practice, there may be scenarios where annotation is costly, leading toactive continual learning (ACL), which performsactive learning (AL)for the CL scenarios when reducing the labeling cost by selecting the most informative subset is preferable. However, conventional AL strategies are not suitable for ACL, as they focus solely on learning the new knowledge, leading tocatastrophic forgettingof previously learned tasks. Therefore, ACL requires a new AL strategy that can balance the prevention of catastrophic forgetting and the ability to quickly learn new tasks. In this paper, we proposeAccuACL,Accumulated informativeness-basedActiveContinualLearning, by the novel use of the Fisher information matrix as a criterion for sample selection, derived from a theoretical analysis of the Fisher-optimality preservation properties within the framework of ACL, while also addressing the scalability issue of Fisher information-based AL. Extensive experiments demonstrate that AccuACL significantly outperforms AL baselines across various CL algorithms, increasing the average accuracy and forgetting by23.8%percent23.823.8\%23.8 %and17.0%percent17.017.0\%17.0 %, respectively, in average."
1289,679d459debd8ffd557a2b376,cs.AI,https://arxiv.org/pdf/2501.14269,Hierarchical Time-Aware Mixture of Experts for Multi-Modal Sequential Recommendation,"Shengzhe Zhang, Liyi Chen, Dazhong Shen, Chao Wang, Hui Xiong","Information Retrieval, Artificial Intelligence","Multi-modal sequential recommendation (SR) leverages multi-modal data to learn more comprehensive item features and user preferences than traditional SR methods, which has become a critical topic in both academia and industry. Existing methods typically focus on enhancing multi-modal information utility through adaptive modality fusion to capture the evolving of user preference from user-item interaction sequences.
However, most of them overlook the interference caused by redundant interest-irrelevant information contained in rich multi-modal data.
Additionally, they primarily rely on implicit temporal information based solely on chronological ordering, neglecting explicit temporal signals that could more effectively represent dynamic user interest over time.
To address these limitations, we propose aHierarchical time-awareMixture of experts for multi-modalSequentialRecommendation (HM4SR) with a two-level Mixture of Experts (MoE) and a multi-task learning strategy.
Specifically,
the first MoE, named Interactive MoE, extracts essential user interest-related information from the multi-modal data of each item.
Then, the second MoE, termed Temporal MoE, captures user dynamic interests by introducing explicit temporal embeddings from timestamps in modality encoding.
To further address data sparsity, we propose three auxiliary supervision tasks: sequence-level category prediction (CP) for item feature understanding, contrastive learning on ID (IDCL) to align sequence context with user interests, and placeholder contrastive learning (PCL) to integrate temporal information with modalities for dynamic interest modeling.
Extensive experiments on four public datasets verify the effectiveness of HM4SR compared to several state-of-the-art approaches. Our code is available at https://github.com/SStarCCat/HM4SR."
1290,679d459debd8ffd557a2b377,cs.AI,https://arxiv.org/pdf/2501.14268,Pre-train and Fine-tune: Recommenders as Large Models,"Zhenhao Jiang, Chenghao Chen, Hao Feng, Yu Yang, Jin Liu, Jie Zhang, Jia Jia, Ning Hu","Information Retrieval, Artificial Intelligence","In reality, users have different interests in different periods, regions, scenes,etc. Such changes in interest are so drastic that they are difficult to be captured by recommenders. Existing multi-domain learning can alleviate this problem. However, the structure of the industrial recommendation system is complex, the amount of data is huge, and the training cost is extremely high, so it is difficult to modify the structure of the industrial recommender and re-train it. To fill this gap, we consider recommenders as large pre-trained models and fine-tune them. We first propose the theory of the information bottleneck for fine-tuning and present an explanation for the fine-tuning technique in recommenders. To tailor for recommendation, we design an information-aware adaptive kernel (IAK) technique to fine-tune the pre-trained recommender. Specifically, we define fine-tuning as two phases: knowledge compression and knowledge matching and let the training stage of IAK explicitly approximate these two phases. Our proposed approach designed from the essence of fine-tuning is well interpretable. Extensive online and offline experiments show the superiority of our proposed method. Besides, we also share unique and important lessons we learned when deploying the method in a large-scale online platform. We also present the potential issues of fine-tuning techniques in recommendation systems and the corresponding solutions. The recommender with IAK technique has been deployed on the homepage of a billion-scale online food platform for several months and has yielded considerable profits in our business."
1291,679d459debd8ffd557a2b378,cs.AI,https://arxiv.org/pdf/2501.14216,TFG-Flow: Training-free Guidance in Multimodal Generative Flow,"Haowei Lin, Shanda Li, Haotian Ye, Yiming Yang, Stefano Ermon, Yitao Liang, Jianzhu Ma","Machine Learning, Artificial Intelligence, Computational Engineering, Finance, and Science","Given an unconditional generative model and a predictor for a target property (e.g., a classifier), the goal of training-free guidance is to generate samples with desirable target properties without additional training. As a highly efficient technique for steering generative models toward flexible outcomes, training-free guidance has gained increasing attention in diffusion models. However, existing methods only handle data in continuous spaces, while many scientific applications involve both continuous and discrete data (referred to as multimodality). Another emerging trend is the growing use of the simple and general flow matching framework in building generative foundation models, where guided generation remains under-explored.
To address this, we introduceTFG-Flow, a novel training-free guidance method for multimodal generative flow.TFG-Flowaddresses the curse-of-dimensionality while maintaining the property of unbiased sampling in guiding discrete variables. We validateTFG-Flowon four molecular design tasks and show thatTFG-Flowhas great potential in drug design by generating molecules with desired properties.111Code is available athttps://github.com/linhaowei1/TFG-Flow."
1292,679d459debd8ffd557a2b379,cs.AI,https://arxiv.org/pdf/2501.14199,Coordinating Ride-Pooling with Public Transit using Reward-Guided Conservative Q-Learning: An Offline Training and Online Fine-Tuning Reinforcement Learning Framework,"Yulong Hu, Tingting Dong, Sen Li","Machine Learning, Artificial Intelligence, Emerging Technologies",
1293,679d459debd8ffd557a2b37a,cs.AI,https://arxiv.org/pdf/2501.14183,VarDrop: Enhancing Training Efficiency by Reducing Variate Redundancy in Periodic Time Series Forecasting,"Junhyeok Kang, Yooju Shin, Jae-Gil Lee","Machine Learning, Artificial Intelligence","Variate tokenization, which independently embeds each variate as separate tokens, has achieved remarkable improvements in multivariate time series forecasting. However, employing self-attention with variate tokens incurs a quadratic computational cost with respect to the number of variates, thus limiting its training efficiency for large-scale applications. To address this issue, we proposeVarDrop, a simple yet efficient strategy that reduces the token usage by omitting redundant variate tokens during training.VarDropadaptively excludes redundant tokens within a givenbatch, thereby reducing the number of tokens used for dot-product attention while preserving essential information. Specifically, we introducek𝑘kitalic_k-dominant frequency hashing (k𝑘kitalic_k-DFH), which utilizes the ranked dominant frequencies in the frequency domain as a hash value to efficiently group variate tokens exhibiting similar periodic behaviors. Then, only representative tokens in each group are sampled through stratified sampling. By performing sparse attention with these selected tokens, the computational cost of scaled dot-product attention is significantly alleviated. Experiments conducted on public benchmark datasets demonstrate thatVarDropoutperforms existing efficient baselines."
1294,679d459debd8ffd557a2b37b,cs.AI,https://arxiv.org/pdf/2501.14176,RL + Transformer = A General-Purpose Problem Solver,"Micah Rentschler, Jesse Roberts","Machine Learning, Artificial Intelligence","What if artificial intelligence could not only solve problems for which it was trained but alsolearntoteach itselfto solve new problems (i.e., meta-learn)? In this study, we demonstrate that a pre-trained transformer fine-tuned with reinforcement learning over multiple episodes develops the ability to solve problems that it hasnever encountered before—an emergent ability calledIn-Context Reinforcement Learning (ICRL). This powerful meta-learner not only excels in solving unseen in-distribution environments with remarkable sample efficiency, but also shows strong performance in out-of-distribution environments. In addition, we show that it exhibits robustness to the quality of its training data, seamlessly stitches together behaviors from its context, and adapts to non-stationary environments. These behaviors demonstrate that an RL-trained transformer can iteratively improve upon its own solutions, making it an excellentgeneral-purpose problem solver."
1295,679d459debd8ffd557a2b37c,cs.AI,https://arxiv.org/pdf/2501.14165,LoCoML: A Framework for Real-World ML Inference Pipelines,"Kritin Maddireddy, Santhosh Kotekal Methukula, Chandrasekar Sridhar, Karthik Vaidhyanathan","Software Engineering, Artificial Intelligence","The widespread adoption of machine learning (ML) has brought forth diverse models with varying architectures, data requirements, introducing new challenges in integrating these systems into real-world applications. Traditional solutions often struggle to manage the complexities of connecting heterogeneous models, especially when dealing with varied technical specifications. These limitations are amplified in large-scale, collaborative projects where stakeholders contribute models with different technical specifications.
To address these challenges, we developed LoCoML, a low-code framework designed to simplify the integration of diverse ML models within the context of theBhashini Project- a large-scale initiative aimed at integrating AI-driven language technologies such as automatic speech recognition, machine translation, text-to-speech, and optical character recognition to support seamless communication across more than 20 languages. Initial evaluations show that LoCoML adds only a small amount of computational load, making it efficient and effective for large-scale ML integration. Our practical insights show that a low-code approach can be a practical solution for connecting multiple ML models in a collaborative environment."
1296,679d459debd8ffd557a2b37d,cs.AI,https://arxiv.org/pdf/2501.14120,On the Transfer of Knowledge in Quantum Algorithms,"Esther Villar-Rodriguez, Eneko Osaba, Izaskun Oregi, Sebastián V. Romero, Julián Ferreiro-Vélez","Quantum Physics, Artificial Intelligence",
1297,679d459debd8ffd557a2b37e,cs.AI,https://arxiv.org/pdf/2501.14084,The Role of Generative AI in Software Student CollaborAItion,"Natalie Kiesler, Jacqueline Smith, Juho Leinonen, Armando Fox, Stephen MacNeil, Petri Ihantola","Software Engineering, Artificial Intelligence, Computers and Society, Human-Computer Interaction","Collaboration is a crucial part of computing education. The increase in AI capabilities over the last couple of years is bound to profoundly affect all aspects of systems and software engineering, including collaboration. In this position paper, we consider a scenario where AI agents would be able to take on any role in collaborative processes in computing education. We outline these roles, the activities and group dynamics that software development currently include, and discuss if and in what way AI could facilitate these roles and activities. The goal of our work is to envision and critically examine potential futures. We present scenarios suggesting how AI can be integrated into existing collaborations. These are contrasted by design fictions that help demonstrate the new possibilities and challenges for computing education in the AI era."
1298,679d459debd8ffd557a2b37f,cs.AI,https://arxiv.org/pdf/2501.14012,Transfer Learning of Surrogate Models via Domain Affine Transformation Across Synthetic and Real-World Benchmarks,"Shuaiqun Pan, Diederick Vermetten, Manuel López-Ibáñez, Thomas Bäck, Hao Wang","Machine Learning, Artificial Intelligence","Surrogate models are frequently employed as efficient substitutes for the costly execution of real-world processes. However, constructing a high-quality surrogate model often demands extensive data acquisition. A solution to this issue is to transfer pre-trained surrogate models for new tasks, provided that certain invariances exist between tasks. This study focuses on transferring non-differentiable surrogate models (e.g., random forest) from a source function to a target function, where we assume their domains are related by an unknown affine transformation, using only a limited amount of transfer data points evaluated on the target. Previous research attempts to tackle this challenge for differentiable models, e.g., Gaussian process regression, which minimizes the empirical loss on the transfer data by tuning the affine transformations. In this paper, we extend the previous work to the random forest model and assess its effectiveness on a widely-used artificial problem set - Black-Box Optimization Benchmark (BBOB) testbed, and on four real-world transfer learning problems. The results highlight the significant practical advantages of the proposed method, particularly in reducing both the data requirements and computational costs of training surrogate models for complex real-world scenarios."
1299,679d459debd8ffd557a2b380,cs.AI,https://arxiv.org/pdf/2501.14009,Scalable and Explainable Verification of Image-based Neural Network Controllers for Autonomous Vehicles,"Aditya Parameshwaran, Yue Wang","Machine Learning, Artificial Intelligence, Systems and Control","Existing formal verification methods for image-based neural network controllers in autonomous vehicles often struggle with high-dimensional inputs, computational inefficiency, and a lack of explainability. These challenges make it difficult to ensure safety and reliability, as processing high-dimensional image data is computationally intensive and neural networks are typically treated as black boxes. To address these issues, we proposeSEVIN(Scalable and Explainable Verification of Image-Based Neural Network Controllers), a framework that leverages a Variational Autoencoders (VAE) to encode high-dimensional images into a lower-dimensional, explainable latent space. By annotating latent variables with corresponding control actions, we generate convex polytopes that serve as structured input spaces for verification, significantly reducing computational complexity and enhancing scalability. Integrating the VAE’s decoder with the neural network controller allows for formal and robustness verification using these explainable polytopes. Our approach also incorporates robustness verification under real-world perturbations by augmenting the dataset and retraining the VAE to capture environmental variations. Experimental results demonstrate that SEVIN achieves efficient and scalable verification while providing explainable insights into controller behavior, bridging the gap between formal verification techniques and practical applications in safety-critical systems."
1300,679d459debd8ffd557a2b381,cs.AI,https://arxiv.org/pdf/2501.14007,Adaptive Genetic Algorithms for Pulse-Level Quantum Error Mitigation,"William Aguilar-Calvo, Santiago Núñez-Corrales","Quantum Physics, Artificial Intelligence, Hardware Architecture",
1301,679d459debd8ffd557a2b382,cs.AI,https://arxiv.org/pdf/2501.14006,Asymmetrical Latent Representation for Individual Treatment Effect Modeling,"Armand Lacombe, Michèle Sebag","Machine Learning, Artificial Intelligence",
1302,679d459debd8ffd557a2b383,cs.AI,https://arxiv.org/pdf/2501.14003,PaMMA-Net: Plasmas magnetic measurement evolution based on data-driven incremental accumulative prediction,"Yunfei Ling, Zijie Liu, Jun Du, Yao Huang, Yuehang Wang, Bingjia Xiao, Xin Fang","Plasma Physics, Artificial Intelligence","An accurate evolution model is crucial for effective control and in-depth study of fusion plasmas. Evolution methods based on physical models often encounter challenges such as insufficient robustness or excessive computational costs. Given the proven strong fitting capabilities of deep learning methods across various fields, including plasma research, this paper introduces a deep learning-based magnetic measurement evolution method named PaMMA-Net (PlasmaMagneticMeasurements IncrementalAccumulative Prediction Network). This network is capable of evolving magnetic measurements in tokamak discharge experiments over extended periods or, in conjunction with equilibrium reconstruction algorithms, evolving macroscopic parameters such as plasma shape. Leveraging a incremental prediction approach and data augmentation techniques tailored for magnetic measurements, PaMMA-Net achieves superior evolution results compared to existing studies. The tests conducted on real experimental data from EAST validate the high generalization capability of the proposed method."
1303,679d459debd8ffd557a2b384,cs.AI,https://arxiv.org/pdf/2501.14000,Local Control Networks (LCNs): Optimizing Flexibility in Neural Network Data Pattern Capture,"Hy Nguyen, Duy Khoa Pham, Srikanth Thudumu, Hung Du, Rajesh Vasa, Kon Mouzakis","Machine Learning, Artificial Intelligence","The widespread use of Multi-layer perceptrons (MLPs) often relies on a fixed activation function (e.g., ReLU, Sigmoid, Tanh) for all nodes within the hidden layers. While effective in many scenarios, this uniformity may limit the network’s ability to capture complex data patterns. We argue that employing the same activation function at every node is suboptimal and propose leveraging different activation functions at each node to increase flexibility and adaptability. To achieve this, we introduce Local Control Networks (LCNs), which leverage B-spline functions to enable distinct activation curves at each node. Our mathematical analysis demonstrates the properties and benefits of LCNs over conventional MLPs. In addition, we demonstrate that more complex architectures, such as Kolmogorov–Arnold Networks (KANs), are unnecessary in certain scenarios, and LCNs can be a more efficient alternative. Empirical experiments on various benchmarks and datasets validate our theoretical findings. In computer vision tasks, LCNs achieve marginal improvements over MLPs and outperform KANs by approximately 5%, while also being more computationally efficient than KANs. In basic machine learning tasks, LCNs show a 1% improvement over MLPs and a 0.6% improvement over KANs. For symbolic formula representation tasks, LCNs perform on par with KANs, with both architectures outperforming MLPs. Our findings suggest that diverse activations at the node level can lead to improved performance and efficiency."
1304,679d459debd8ffd557a2b385,cs.AI,https://arxiv.org/pdf/2501.13997,Predictive Learning in Energy-based Models with Attractor Structures,"Xingsi Dong, Pengxiang Yuan, Si Wu","Machine Learning, Artificial Intelligence","Predictive models are highly advanced in understanding the mechanisms of brain function. Recent advances in machine learning further underscore the power of prediction for optimal representation in learning. However, there remains a gap in creating a biologically plausible model that explains how the neural system achieves prediction. In this paper, we introduce a framework that employs an energy-based model (EBM) to capture the nuanced processes of predicting observation after action within the neural system, encompassing prediction, learning, and inference. We implement the EBM with a hierarchical structure and integrate a continuous attractor neural network for memory, constructing a biologically plausible model. In experimental evaluations, our model demonstrates efficacy across diverse scenarios. The range of actions includes eye movement, motion in environments, head turning, and static observation while the environment changes. Our model not only makes accurate predictions for environments it was trained on, but also provides reasonable predictions for unseen environments, matching the performances of machine learning methods in multiple tasks. We hope that this study contributes to a deep understanding of how the neural system performs prediction."
1305,679d459debd8ffd557a2b386,cs.AI,https://arxiv.org/pdf/2501.13992,Dual-Branch HNSW Approach with Skip Bridges and LID-Driven Optimization,"Hy Nguyen, Nguyen Hung Nguyen, Nguyen Linh Bao Nguyen, Srikanth Thudumu, Hung Du, Rajesh Vasa, Kon Mouzakis","Machine Learning, Artificial Intelligence","The Hierarchical Navigable Small World (HNSW) algorithm is widely used for approximate nearest neighbor (ANN) search, leveraging the principles of navigable small-world graphs. However, it faces some limitations. The first is the local optima problem, which arises from the algorithm’s greedy search strategy, selecting neighbors based solely on proximity at each step. This often leads to cluster disconnections. The second limitation is that HNSW frequently fails to achieve logarithmic complexity, particularly in high-dimensional datasets, due to the exhaustive traversal through each layer. To address these limitations, we propose a novel algorithm that mitigates local optima and cluster disconnections while enhancing the construction speed, maintaining inference speed. The first component is a dual-branch HNSW structure with LID-based insertion mechanisms, enabling traversal from multiple directions. This improves outlier node capture, enhances cluster connectivity, accelerates construction speed and reduces the risk of local minima. The second component incorporates a bridge-building technique that bypasses redundant intermediate layers, maintaining inference and making up the additional computational overhead introduced by the dual-branch structure. Experiments on various benchmarks and datasets showed that our algorithm outperforms the original HNSW in both accuracy and speed. We evaluated six datasets across Computer Vision (CV), and Natural Language Processing (NLP), showing recall improvements of 18% in NLP, and up to 30% in CV tasks while reducing the construction time by up to 20% and maintaining the inference speed. We did not observe any trade-offs in our algorithm. Ablation studies revealed that LID-based insertion had the greatest impact on performance, followed by the dual-branch structure and bridge-building components."
1306,679d459debd8ffd557a2b387,cs.AI,https://arxiv.org/pdf/2501.13989,FreEformer: Frequency Enhanced Transformer for Multivariate Time Series Forecasting,"Wenzhen Yue, Yong Liu, Xianghua Ying, Bowei Xing, Ruohao Guo, Ji Shi","Machine Learning, Artificial Intelligence","This paper presentsFreEformer, a simple yet effective model that leverages aFrequencyEnhanced Transformerfor multivariate time series forecasting. Our work is based on the assumption that the frequency spectrum provides a global perspective on the composition of series across various frequencies and is highly suitable for robust representation learning. Specifically, we first convert time series into the complex frequency domain using the Discrete Fourier Transform (DFT). The Transformer architecture is then applied to the frequency spectra to capture cross-variate dependencies, with the real and imaginary parts processed independently. However, we observe that the vanilla attention matrix exhibits a low-rank characteristic, thus limiting representation diversity. This could be attributed to the inherent sparsity of the frequency domain and the strong-value-focused nature of Softmax in vanilla attention. To address this, we enhance the vanilla attention mechanism by introducing an additional learnable matrix to the original attention matrix, followed by row-wise L1 normalization. Theoretical analysis demonstrates that this enhanced attention mechanism improves both feature diversity and gradient flow. Extensive experiments demonstrate that FreEformer consistently outperforms state-of-the-art models on eighteen real-world benchmarks covering electricity, traffic, weather, healthcare and finance. Notably, the enhanced attention mechanism also consistently improves the performance of state-of-the-art Transformer-based forecasters."
1307,679d459debd8ffd557a2b388,cs.AI,https://arxiv.org/pdf/2501.13987,OstQuant: Refining Large Language Model Quantization with Orthogonal and Scaling Transformations for Better Distribution Fitting,"Xing Hu, Yuan Cheng, Dawei Yang, Zukang Xu, Zhihang Yuan, Jiangyong Yu, Chen Xu, Zhe Jiang, Sifan Zhou","Machine Learning, Artificial Intelligence",
1308,679d459debd8ffd557a2b389,cs.AI,https://arxiv.org/pdf/2501.13986,An Efficient Sparse Kernel Generator for O(3)-Equivariant Deep Networks,"Vivek Bharadwaj, Austin Glover, Aydin Buluc, James Demmel","Machine Learning, Artificial Intelligence","Rotation equivariant graph neural networks, i.e.
networks designed to guarantee certain geometric relations between their inputs and outputs,
yield
state of the art performance on spatial deep learning tasks. They
exhibit high data efficiency during training and
significantly reduced inference time for
interatomic potential calculations compared to
classical approaches. Key to these models is the
Clebsch-Gordon (CG) tensor product, a kernel that
contracts two dense feature vectors with a
highly-structured sparse tensor to produce a
dense output vector. The operation, which may
be repeated millions of times for typical equivariant
models, is a costly and inefficient bottleneck. We introduce a
GPU sparse kernel generator for the CG
tensor product that provides significant
speedups over the best existing open
and closed-source implementations. Our
implementation achieves high
performance by carefully
managing the limited GPU shared memory
through static analysis at model compile-time,
minimizing reads and writes to global memory.
We break the tensor product into a series of
smaller kernels with operands that fit entirely
into registers, enabling us to emit
long arithmetic instruction streams that maximize
instruction-level parallelism. By fusing the
CG tensor product with a subsequent
graph convolution , we reduce both intermediate storage
and global memory
traffic over naïve approaches that duplicate
input data. We also provide optimized kernels
for the gradient of the CG tensor product and
a novel identity for the higher partial derivatives
required to predict interatomic forces. Our
fused kernels offer up to 4.8x speedup for the
forward pass and 3x for the backward pass over
NVIDIA’s closed-source cuEquivariance package, as
well as>10absent10>10> 10x speedup over the widely-used e3nn
package. We offer up to 5.4x
inference-time speedup for the MACE chemistry
foundation model over the original unoptimized version."
1309,679d459debd8ffd557a2b38a,cs.DB,https://arxiv.org/pdf/2501.18377,Using Read Promotion and Mixed Isolation Levels for Performant Yet Serializable Execution of Transaction Programs,"Brecht Vandevoort, Alan Fekete, Bas Ketsman, Frank Neven, Stijn Vansummeren",Databases,"We propose a theory that can determine the lowest isolation level that can be allocated to each transaction program in an application in a mixed-isolation-level setting, to guarantee that all executions will be serializable and thus preserve all integrity constraints, even those that are not explicitly declared. This extends prior work applied to completely known transactions, to deal with the realistic situation where transactions are generated by running programs with parameters that are not known in advance. Using our theory, we propose an optimization method that allows for high throughput while ensuring that all executions are serializable. Our method is based on searching for application code modifications that are semantics-preserving while improving the isolation level allocation. We illustrate our approach to the SmallBank benchmark."
1310,679d459debd8ffd557a2b38b,cs.DB,https://arxiv.org/pdf/2501.17074,DataLens: ML-Oriented Interactive Tabular Data Quality Dashboard,"Mohamed Abdelaal, Samuel Lokadjaja, Arne Kreuz, Harald Schöning",Databases,"Maintaining high data quality is crucial for reliable data analysis and machine learning (ML). However, existing data quality management tools often lack automation, interactivity, and integration with ML workflows. This demonstration paper introduces DataLens111A recording of the demonstration can be found athttps://youtu.be/tW5qqFqDFYI., a novel interactive dashboard designed to streamline and automate the data quality management process for tabular data. DataLens integrates a suite of data profiling, error detection, and repair tools, including statistical, rule-based, and ML-based methods. It features a user-in-the-loop module for interactive rule validation, data labeling, and custom rule definition, enabling domain experts to guide the cleaning process. Furthermore, DataLens implements an iterative cleaning module that automatically selects optimal cleaning tools based on downstream ML model performance. To ensure reproducibility, DataLens generates DataSheets capturing essential metadata and integrates with MLflow and Delta Lake for experiment tracking and data version control. This demonstration showcases DataLens’s capabilities in effectively identifying and correcting data errors, improving data quality for downstream tasks, and promoting reproducibility in data cleaning pipelines."
1311,679d459debd8ffd557a2b38c,cs.DB,https://arxiv.org/pdf/2501.16759,Are Joins over LSM-trees Ready: Take RocksDB as an Example,"Weiping Yu, Fan Wang, Xuwei Zhang, Siqiang Luo",Databases,"LSM-tree-based data stores are widely adopted in industries for their excellent performance. As data scales increase, disk-based join operations become indispensable yet costly for the database, making the selection of suitable join methods crucial for system optimization. Current LSM-based stores generally adhere to conventional relational database practices and support only a limited number of join methods. However, the LSM-tree delivers distinct read and write efficiency compared to the relational databases, which could accordingly impact the performance of various join methods. Therefore, it is necessary to reconsider the selection of join methods in this context to fully explore the potential of various join algorithms and index designs.
In this work, we present a systematic study and an exhaustive benchmark for joins over LSM-trees. We define a configuration space for join methods, encompassing various join algorithms, secondary index types, and consistency strategies. We also summarize a theoretical analysis to evaluate the overhead of each join method for an in-depth understanding. Furthermore, we implement all join methods in the configuration space on a unified platform and compare their performance through extensive experiments. Our theoretical and experimental results yield several insights and takeaways tailored to joins in LSM-based stores that aid developers in choosing proper join methods based on their working conditions."
1312,679d459debd8ffd557a2b38d,cs.DB,https://arxiv.org/pdf/2501.16574,Using Database Dependencies to Constrain Approval-Based Committee Voting in the Presence of Context,"Roi Yona, Benny Kimelfeld",Databases,"In Approval-Based Committee (ABC) voting, each voter lists the candidates they approve and then a voting rule aggregates the individual approvals into a committee that represents the collective choice of the voters. An extensively studied class of such rules is the class of ABC scoring rules, where each voter contributes to each possible committee a score based on the voter’s approvals. We initiate a study of ABC voting in the presence of constraints about the general context surrounding the candidates. Specifically, we consider a framework in which there is a relational database with information about the candidates together with integrity constraints on the relational database extended with a virtual relation representing the committee. For an ABC scoring rule, the goal is to find a committee of maximum score such that all integrity constraints hold in the extended database."
1313,679d459debd8ffd557a2b38e,cs.DB,https://arxiv.org/pdf/2501.16544,PLANSIEVE: Real-time Suboptimal Query Plan Detection Through Incremental Refinements,"Asoke Datta, Yesdaulet Izenov, Brian Tsan, Abylay Amanbayev, Florin Rusu",Databases,"Cardinality estimation remains a fundamental challenge in query optimization, often resulting in sub-optimal execution plans and degraded performance. While errors in cardinality estimation are inevitable, existing methods for identifying sub-optimal plans — such as metrics like Q-error, P-error, or L1-error — are limited to post-execution analysis, requiring complete knowledge of true cardinalities and failing to prevent the execution of sub-optimal plans in real-time. This paper introduces PLANSIEVE, a novel framework that identifies sub-optimal plans during query optimization. PLANSIEVE operates by analyzing the relative order of sub-plans generated by the optimizer based on estimated and true cardinalities. It begins with surrogate cardinalities from any third-party estimator and incrementally refines these surrogates as the system processes more queries. Experimental results on the augmented JOB-LIGHT-SCALE and STATS-CEB-SCALE workloads demonstrate that PLANSIEVE achieves an accuracy of up to 88.7% in predicting sub-optimal plans."
1314,679d459debd8ffd557a2b38f,cs.DB,https://arxiv.org/pdf/2501.16256,Improving DBMS Scheduling Decisions with Fine-grained Performance Prediction on Concurrent Queries -- Extended,"Ziniu Wu, Markos Markakis, Chunwei Liu, Peter Baile Chen, Balakrishnan Narayanaswamy, Tim Kraska, Samuel Madden","Databases, Machine Learning","Query scheduling is a critical task that directly impacts query performance in database management systems (DBMS). Deeply integrated schedulers, which require changes to DBMS internals,
are usually customized for a specific engine and can take months to implement. In contrast, non-intrusive schedulers make coarse-grained decisions, such as controlling query admission and re-ordering query execution, without requiring modifications to DBMS internals. They require much less engineering effort and can be applied across a wide range of DBMS engines, offering immediate benefits to end users. However, most existing non-intrusive scheduling systems rely on simplified cost models and heuristics that cannot accurately model query interactions under concurrency and different system states, possibly leading to suboptimal scheduling decisions."
1315,679d459debd8ffd557a2b390,cs.DB,https://arxiv.org/pdf/2501.15738,Towards Interoperable Data Spaces: Comparative Analysis of Data Space Implementations between Japan and Europe,"Shun Ishihara, Taka Matsutsuka",Databases,"The rapid evolution of data spaces is transforming the landscape of secure and interoperable data sharing across industries and geographies. In Europe, the concept of data spaces, supported by initiatives such as the European Data Strategy, emphasises the importance of trust, sovereignty, and interoperability. Meanwhile, Japan has been developing its approach to data sharing, in line with global trends but also to address unique domestic challenges. Despite these parallel advances, achieving interoperability between European and Japanese data spaces remains a critical challenge due to differences in governance, technology standards, and authentication frameworks. This paper undertakes a comparative analysis of DATA-EX and Catena-X to explore the challenges and opportunities for achieving interoperability between Japanese and European data spaces. By examining common data exchange processes, key objects such as participants, datasets, and data catalogs, and specific evaluation criteria, the study identifies gaps and proposes actionable solutions. Through this analysis, the paper aims to contribute to the ongoing discourse on global data interoperability. It proposes an interoperable architecture that bridges regional differences while addressing common challenges. It also identifies challenges that should be addressed to achieve interoperability."
1316,679d459debd8ffd557a2b391,cs.DB,https://arxiv.org/pdf/2501.15120,Technology Mapping with Large Language Models,"Minh Hieu Nguyen, Hien Thu Pham, Hiep Minh Ha, Ngoc Quang Hung Le, Jun Jo","Information Retrieval, Databases, Emerging Technologies, Machine Learning","In today’s fast-evolving business landscape, having insight into the technology stacks that organizations use is crucial for forging partnerships, uncovering market openings, and informing strategic choices. However, conventional technology mapping, which typically hinges on keyword searches, struggles with the sheer scale and variety of data available, often failing to capture nascent technologies. To overcome these hurdles, we present STARS (Semantic Technology and Retrieval System), a novel framework that harnesses Large Language Models (LLMs) and Sentence-BERT to pinpoint relevant technologies within unstructured content, build comprehensive company profiles, and rank each firm’s technologies according to their operational importance. By integrating entity extraction with Chain-of-Thought prompting and employing semantic ranking, STARS provides a precise method for mapping corporate technology portfolios. Experimental results show that STARS markedly boosts retrieval accuracy, offering a versatile and high-performance solution for cross-industry technology mapping."
1317,679d459debd8ffd557a2b392,cs.DB,https://arxiv.org/pdf/2501.14473,XFSC: A Catalogue of Trustable Semantic Metadata for Data Services and Providers,"Benedikt T. Arnold, Khalil Baydoun, Diego Collarana, Sebastian Duda, Christina Gillmann, Ahmad Hemid, Philipp Hertweck, Paul Moosmann, Denis Sukhoroslov, Christoph Lange",Databases,"In dataspaces, federation services facilitate key functions such as enabling participating organizations to establish mutual trust and assisting them in discovering data and services available for consumption.
Discovery is enabled by a catalogue, where participants publish metadata describing themselves and their data and service offerings as Verifiable Presentations (VPs), such that other participants may query them.
This paper presents the Eclipse Cross Federation Services Components (XFSC) Catalogue, which originated as a catalogue reference implementation for the Gaia-X federated cloud service architecture but is also generally applicable to metadata required to be trustable.
This implementation provides basic lifecycle management for DCAT-style metadata records and schemas.
It validates submitted VPs for their cryptographic integrity and trustability, and for their conformance to an extensible collection of semantic schemas.
The claims in the latest versions of valid VP submissions are extracted into a searchable graph database. The implementation scales to large numbers of records and is secure by design."
1318,679d459debd8ffd557a2b393,cs.DB,https://arxiv.org/pdf/2501.14432,CAMEO: Autocorrelation-Preserving Line Simplification for Lossy Time Series Compression,"Carlos Enrique Muñiz-Cuza, Matthias Boehm, Torben Bach Pedersen","Databases, Information Retrieval, Information Theory","Time series data from a variety of sensors and IoT devices need effective compression to reduce storage and I/O bandwidth requirements. While most time series databases and systems rely on lossless compression, lossy techniques offer even greater space-saving with a small loss in precision. However, the unknown impact on downstream analytics applications requires a semi-manual trial-and-error exploration. We initiate work on lossy compression that provides guarantees on complex statistical features (which are strongly correlated with the accuracy of the downstream analytics). Specifically, we propose a new lossy compression method that provides guarantees on the autocorrelation and partial-autocorrelation functions (ACF/PACF) of a time series. Our method leverages line simplification techniques as well as incremental maintenance of aggregates, blocking, and parallelization strategies for effective and efficient compression. The results show that our method improves compression ratios by 2x on average and up to 54x on selected datasets, compared to previous lossy and lossless compression methods. Moreover, we maintain—and sometimes even improve—the forecasting accuracy by preserving the autocorrelation properties of the time series. Our framework is extensible to multivariate time series and other statistical features of the time series."
1319,679d459debd8ffd557a2b394,cs.DB,https://arxiv.org/pdf/2501.14345,A Ground Truth Approach for Assessing Process Mining Techniques,"Dominique Sommers, Natalia Sidorova, Boudewijn van Dongen",Databases,"The assessment of process mining techniques using real-life data is often compromised by the lack of ground truth knowledge, the presence of non-essential outliers in system behavior and recording errors in event logs. Using synthetically generated data could leverage ground truth for better evaluation. Existing log generation tools inject noise directly into the logs, which does not capture many typical behavioral deviations. Furthermore, the link between the model and the log, which is needed for later assessment, becomes lost."
1320,679d459debd8ffd557a2b395,cs.LG,https://arxiv.org/pdf/2501.18582,Accuracy and Robustness of Weight-Balancing Methods for Training PINNs,"Matthieu Barreau, Haoming Shen",Machine Learning,"Physics-Informed Neural Networks (PINNs) have emerged as powerful tools for integrating physics-based models with data by minimizing both data and physics losses. However, this multi-objective optimization problem is notoriously challenging, with some benchmark problems leading to unfeasible solutions. To address these issues, various strategies have been proposed, including adaptive weight adjustments in the loss function. In this work, we introduce clear definitions of accuracy and robustness in the context of PINNs and propose a novel training algorithm based on the Primal-Dual (PD) optimization framework. Our approach enhances the robustness of PINNs while maintaining comparable performance to existing weight-balancing methods. Numerical experiments demonstrate that the PD method consistently achieves reliable solutions across all investigated cases and can be easily implemented, facilitating its practical adoption. The code is available onGitHub."
1321,679d459debd8ffd557a2b396,cs.LG,https://arxiv.org/pdf/2501.18581,Bias-variance decompositions: the exclusive privilege of Bregman divergences,Tom Heskes,Machine Learning,
1322,679d459debd8ffd557a2b397,cs.LG,https://arxiv.org/pdf/2501.18580,Node Classification and Search on the Rubik's Cube Graph with GNNs,Alessandro Barro,Machine Learning,"This study focuses on the application of deep geometric models to solve the 3x3x3 Rubik’s Cube. We begin by discussing the cube’s graph representation and defining distance as the model’s optimization objective. The distance approximation task is reformulated as a node classification problem, effectively addressed using Graph Neural Networks (GNNs). After training the model on a random subgraph, the predicted classes are used to construct a heuristic forA∗superscript𝐴A^{*}italic_A start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPTsearch. We conclude with experiments comparing our heuristic to that of the DeepCubeA model."
1323,679d459debd8ffd557a2b398,cs.LG,https://arxiv.org/pdf/2501.18576,"Token-Hungry, Yet Precise: DeepSeek R1 Highlights the Need for Multi-Step Reasoning Over Speed in MATH",Evgenii Evstafev,Machine Learning,
1324,679d459debd8ffd557a2b399,cs.LG,https://arxiv.org/pdf/2501.18563,No Equations Needed: Learning System Dynamics Without Relying on Closed-Form ODEs,"Krzysztof Kacprzyk, Mihaela van der Schaar",Machine Learning,"Data-driven modeling of dynamical systems is a crucial area of machine learning. In many scenarios, a thorough understanding of the model’s behavior becomes essential for practical applications. For instance, understanding the behavior of a pharmacokinetic model, constructed as part of drug development, may allow us to both verify its biological plausibility (e.g., the drug concentration curve is non-negative and decays to zero in the long term) and to design dosing guidelines (e.g., by looking at the peak concentration and its timing). Discovery of closed-form ordinary differential equations (ODEs) can be employed to obtain such insights by finding a compact mathematical equation and then analyzing it (a two-step approach). However, its widespread use (in pharmacology and other domains) is currently hindered because the analysis process may be time-consuming, requiring substantial mathematical expertise, or even impossible if the equation is too complex. Moreover, if the found equation’s behavior does not satisfy the requirements, editing it or influencing the discovery algorithms to rectify it is challenging as the link between the symbolic form of an ODE and its behavior can be elusive. This paper proposes a conceptual shift to modeling low-dimensional dynamical systems by departing from the traditional two-step modeling process. Instead of first discovering a closed-form equation and then analyzing it, our approach, direct semantic modeling, predicts the semantic representation of the dynamical system (i.e., description of its behavior) directly from data, bypassing the need for complex post-hoc analysis. This direct approach also allows the incorporation of intuitive inductive biases into the optimization algorithm and editing the model’s behavior directly, ensuring that the model meets the desired specifications. Our approach not only simplifies the modeling pipeline but also enhances the transparency and flexibility of the resulting models compared to traditional closed-form ODEs. We validate the effectiveness of this method through extensive experiments, demonstrating its advantages in terms of both performance and practical usability."
1325,679d459debd8ffd557a2b39a,cs.LG,https://arxiv.org/pdf/2501.18560,Bandits with Anytime Knapsacks,"Eray Can Elumar, Cem Tekin, Osman Yagan",Machine Learning,"We consider bandits with anytime knapsacks (BwAK), a novel version of the BwK problem where there is ananytimecost constraint instead of a total cost budget. This problem setting introduces additional complexities as it mandates adherence to the constraint throughout the decision-making process. We propose SUAK, an algorithm that utilizes upper confidence bounds to identify the optimal mixture of arms while maintaining a balance between exploration and exploitation. SUAK is an adaptive algorithm that strategically utilizes the available budget in each round in the decision-making process and skips a round when it is possible to violate the anytime cost constraint. In particular, SUAK slightly under-utilizes the available cost budget to reduce the need for skipping rounds. We show that SUAK attains the same problem-dependent regret upper bound ofO⁢(K⁢log⁡T)𝑂𝐾𝑇O(K\log T)italic_O ( italic_K roman_log italic_T )established in prior work under the simpler BwK framework. Finally, we provide simulations to verify the utility of SUAK in practical settings."
1326,679d459debd8ffd557a2b39b,cs.LG,https://arxiv.org/pdf/2501.18537,Loss Functions and Operators Generated by f-Divergences,"Vincent Roulet, Tianlin Liu, Nino Vieillard, Michael E. Sander, Mathieu Blondel","Machine Learning, Machine Learning","The logistic loss (a.k.a. cross-entropy loss) is one of the most popular loss functions used for multiclass classification. It is also the loss function of choice for next-token prediction in language modeling.
It is associated with the Kullback–Leibler (KL) divergence and the softargmax operator.
In this work, we propose to construct new convex loss functions based onf𝑓fitalic_f-divergences. Our loss functions generalize the logistic loss in two directions: i) by replacing the KL divergence withf𝑓fitalic_f-divergences and ii) by allowing non-uniform reference measures.
We instantiate our framework for numerousf𝑓fitalic_f-divergences, recovering existing losses and creating new ones.
By analogy with the logistic loss, the loss function generated by anf𝑓fitalic_f-divergence is associated with an operator, that we dubf𝑓fitalic_f-softargmax. We derive a novel parallelizable bisection algorithm for computing thef𝑓fitalic_f-softargmax associated with anyf𝑓fitalic_f-divergence.
On the empirical side, one of the goals of this paper is to determine the effectiveness of loss functions beyond the classical cross-entropy in a language model setting, including on pre-training, post-training (SFT) and distillation. We show that the loss function generated by theα𝛼\alphaitalic_α-divergence (which is equivalent to Tsallisα𝛼\alphaitalic_α-negentropy in the case of unit reference measures) withα=1.5𝛼1.5\alpha=1.5italic_α = 1.5performs well across several tasks."
1327,679d459debd8ffd557a2b39c,cs.LG,https://arxiv.org/pdf/2501.18528,Joint Learning of Energy-based Models and their Partition Function,"Michael E. Sander, Vincent Roulet, Tianlin Liu, Mathieu Blondel","Machine Learning, Machine Learning","Energy-based models (EBMs) offer a flexible framework for parameterizing probability distributions using neural networks.
However, learning EBMs by exact maximum likelihood estimation (MLE) is generally intractable, due to the need to compute the partition function (normalization constant).
In this paper, we propose a novel formulation for approximately learning probabilistic EBMs in combinatorially-large discrete spaces, such as sets or permutations.
Our key idea is to jointly learn both an energy model and its log-partition, both parameterized as a neural network.
Our approach not only provides a novel tractable objective criterion to learn EBMs by stochastic gradient descent (without relying on MCMC), but also a novel means to estimate the log-partition function on unseen data points.
On the theoretical side, we show that our approach recovers the optimal MLE solution when optimizing in the space of continuous functions.
Furthermore, we show that our approach naturally extends to the broader family of Fenchel-Young losses, allowing us to obtain
the first tractable method for optimizing the sparsemax loss in combinatorially-large spaces.
We demonstrate our approach on multilabel classification and label ranking."
1328,679d459debd8ffd557a2b39d,cs.LG,https://arxiv.org/pdf/2501.18527,Neural Discovery in Mathematics: Do Machines Dream of Colored Planes?,"Konrad Mundinger, Max Zimmer, Aldo Kiem, Christoph Spiegel, Sebastian Pokutta","Machine Learning, Combinatorics",
1329,679d459debd8ffd557a2b39e,cs.LG,https://arxiv.org/pdf/2501.18439,MolGraph-xLSTM: A graph-based dual-level xLSTM framework with multi-head mixture-of-experts for enhanced molecular representation and interpretability,"Yan Sun, Yutong Lu, Yan Yi Li, Zihao Jing, Carson K. Leung, Pingzhao Hu","Machine Learning, Biomolecules","Predicting molecular properties is essential for drug discovery, and computational methods can greatly enhance this process. Molecular graphs have become a focus for representation learning, with Graph Neural Networks (GNNs) widely used. However, GNNs often struggle with capturing long-range dependencies. To address this, we propose MolGraph-xLSTM, a novel graph-based xLSTM model that enhances feature extraction and effectively models molecule long-range interactions."
1330,679d459debd8ffd557a2b39f,cs.LG,https://arxiv.org/pdf/2501.18417,Causal Inference Real-Time Anomaly Detection with Synthetic Anomaly Monitoring (SAM),"Emanuele Luzio, Moacir Antonelli Ponti",Machine Learning,
1331,679d459debd8ffd557a2b3a0,cs.LG,https://arxiv.org/pdf/2501.18416,Exploring Potential Prompt Injection Attacks in Federated Military LLMs and Their Mitigation,"Youngjoon Lee, Taehyun Park, Yunho Lee, Jinu Gong, Joonhyuk Kang",Machine Learning,"Federated Learning (FL) is increasingly being adopted in military collaborations to develop Large Language Models (LLMs) while preserving data sovereignty.
However, prompt injection attacks—malicious manipulations of input prompts—pose new threats that may undermine operational security, disrupt decision-making, and erode trust among allies.
This perspective paper highlights four potential vulnerabilities in federated military LLMs: secret data leakage, free-rider exploitation, system disruption, and misinformation spread.
To address these potential risks, we propose a human–AI collaborative framework that introduces both technical and policy countermeasures.
On the technical side, our framework uses red/blue team wargaming and quality assurance to detect and mitigate adversarial behaviors of shared LLM weights.
On the policy side, it promotes joint AI–human policy development and verification of security protocols.
Our findings will guide future research and emphasize proactive strategies for emerging military contexts."
1332,679d459debd8ffd557a2b3a1,cs.LG,https://arxiv.org/pdf/2501.18405,Segmentation of cracks in 3d images of fiber reinforced concrete using deep learning,"Anna Nowacka, Katja Schladitz, Szymon Grzesiak, Matthias Pahn",Machine Learning,
1333,679d459debd8ffd557a2b3a2,cs.LG,https://arxiv.org/pdf/2501.18388,Improved Replicable Boosting with Majority-of-Majorities,"Kasper Green Larsen, Markus Engelund Mathiasen, Clement Svendsen",Machine Learning,"We introduce a new replicable boosting algorithm which significantly improves the sample complexity compared to previous algorithms. The algorithm works by doing two layers of majority voting, using an improved version of the replicable boosting algorithm introduced byImpagliazzo et al. (2022)in the bottom layer."
1334,679d459debd8ffd557a2b3a3,cs.LG,https://arxiv.org/pdf/2501.18373,Function Encoders: A Principled Approach to Transfer Learning in Hilbert Spaces,"Tyler Ingebrand, Adam J. Thorpe, Ufuk Topcu",Machine Learning,"A central challenge in transfer learning is designing algorithms that can quickly adapt and generalize to new tasks without retraining. Yet, the conditions of when and how algorithms can effectively transfer to new tasks is poorly characterized. We introduce a geometric characterization of transfer in Hilbert spaces and define three types of inductive transfer: interpolation within the convex hull, extrapolation to the linear span, and extrapolation outside the span. We propose a method grounded in the theory of function encoders to achieve all three types of transfer. Specifically, we introduce a novel training scheme for function encoders using least-squares optimization, prove a universal approximation theorem for function encoders, and provide a comprehensive comparison with existing approaches such as transformers and meta-learning on four diverse benchmarks. Our experiments demonstrate that the function encoder outperforms state-of-the-art methods on four benchmark tasks and on all three types of transfer."
1335,679d459debd8ffd557a2b3a4,cs.LG,https://arxiv.org/pdf/2501.18369,A Cartesian Encoding Graph Neural Network for Crystal Structures Property Prediction: Application to Thermal Ellipsoid Estimation,"Àlex Solé, Albert Mosella-Montoro, Joan Cardona, Silvia Gómez-Coca, Daniel Aravena, Eliseo Ruiz, Javier Ruiz-Hidalgo",Machine Learning,"In the diffraction resolution of crystal structures, the thermal ellipsoids are a critical parameter that is usually more difficult to determine than atomic positions.
These ellipsoids are quantified through the Anisotropic Displacement Parameters (ADPs), which provide critical insights into atomic vibrations within crystalline structures.
ADPs reflect the thermal behaviour and structural properties of crystal structures.
However, traditional methods to compute ADPs are computationally intensive. This paper presents CartNet, a novel graph neural network (GNN) architecture designed to predict properties of crystal structures efficiently by encoding the atomic structural geometry to the cartesian axes and the temperature of the crystal structure.
Additionally, CartNet employs a neighbour equalization technique for message passing to help emphasise the covalent and contact interactions, and a novel Cholesky-based head to ensure valid ADP predictions.
Furthermore, a rotational SO(3) data augmentation technique has been proposed during the training phase to generalize unseen rotations.
To corroborate such procedure, an ADP dataset with over 200,000 experimental crystal structures from the Cambridge Structural Database (CSD) has been curated.
The model significantly reduces computational costs and outperforms existing previously resported methods in ADP prediction by 10.87%, while demonstrating a 34.77% improvement over the tested theoretical computation methods.
Moreover, we have employed CartNet for other already known datasets that included different material properties, such as formation energy, band gap, total energy, energy above the convex hull, bulk moduli, and shear moduli.
The proposed architecture outperformed previously reported methods by 7.71% in the Jarvis Dataset and 13.16% in the Materials Project Dataset, proving CarNet’s capability to achieve state-of-the-art results in several tasks.
Project website with online demo available at:https://www.ee.ub.edu/cartnet"
1336,679d459debd8ffd557a2b3a5,cs.LG,https://arxiv.org/pdf/2501.18363,Robust Online Conformal Prediction under Uniform Label Noise,"Huajun Xi, Kangdao Liu, Hao Zeng, Wenguang Sun, Hongxin Wei",Machine Learning,
1337,679d459debd8ffd557a2b3a6,cs.LG,https://arxiv.org/pdf/2501.18357,Contrastive Learning Meets Pseudo-label-assisted Mixup Augmentation: A Comprehensive Graph Representation Framework from Local to Global,"Jinlu Wang, Yanfeng Sun, Jiapu Wang, Junbin Gao, Shaofan Wang, Jipeng Guo",Machine Learning,"Graph Neural Networks (GNNs) have demonstrated remarkable effectiveness in various graph representation learning tasks. However, most existing GNNs focus primarily on capturing local information through explicit graph convolution, often neglecting global message-passing. This limitation hinders the establishment of a collaborative interaction between global and local information, which is crucial for comprehensively understanding graph data. To address these challenges, we propose a novel framework calledComprehensiveGraphRepresentationLearning (ComGRL). ComGRL integrates local information into global information to derive powerful representations. It achieves this by implicitly smoothing local information through flexible graph contrastive learning, ensuring reliable representations for subsequent global exploration. Then ComGRL transfers the locally derived representations to a multi-head self-attention module, enhancing their discriminative ability by uncovering diverse and rich global correlations.
To further optimize local information dynamically under the self-supervision of pseudo-labels, ComGRL employs a triple sampling strategy to construct mixed node pairs and applies reliable Mixup augmentation across attributes and structure for local contrastive learning. This approach broadens the receptive field and facilitates coordination between local and global representation learning, enabling them to reinforce each other. Experimental results across six widely used graph datasets demonstrate that ComGRL achieves excellent performance in node classification tasks. The code could be available athttps://github.com/JinluWang1002/ComGRL"
1338,679d459debd8ffd557a2b3a7,cs.LG,https://arxiv.org/pdf/2501.18331,Stream-Based Monitoring of Algorithmic Fairness,"Jan Baumeister, Bernd Finkbeiner, Frederik Scheerer, Julian Siber, Tobias Wagenpfeil","Machine Learning, Logic in Computer Science, Software Engineering","Automatic decision and prediction systems are increasingly deployed in applications where they significantly impact the livelihood of people, such as for predicting the creditworthiness of loan applicants or the recidivism risk of defendants. These applications have given rise to a new class ofalgorithmic-fairnessspecifications that require the systems to decide and predict without bias against social groups. Verifying these specifications statically is often out of reach for realistic systems, since the systems may, e.g., employ complex learning components, and reason over a large input space. In this paper, we therefore propose stream-based monitoring as a solution for verifying the algorithmic fairness of decision and prediction systems at runtime. Concretely, we present a principled way to formalize algorithmic fairness over temporal data streams in the specification language RTLola and demonstrate the efficacy of this approach on a number of benchmarks. Besides synthetic scenarios that particularly highlight its efficiency on streams with a scaling amount of data, we notably evaluate the monitor on real-world data from the recidivism prediction tool COMPAS."
1339,679d459debd8ffd557a2b3a8,cs.LG,https://arxiv.org/pdf/2501.18322,A Unified Perspective on the Dynamics of Deep Transformers,"Valérie Castin, Pierre Ablin, José Antonio Carrillo, Gabriel Peyré","Machine Learning, Analysis of PDEs","Transformers, which are state-of-the-art in most machine learning tasks, represent the data as sequences of vectors called tokens.
This representation is then exploited by the attention function, which learns dependencies between tokens and is key to the success of Transformers.
However, the iterative application of attention across layers induces complex dynamics that remain to be fully understood.
To analyze these dynamics, we identify each input sequence with a probability measure and model its evolution as a Vlasov equation called Transformer PDE, whose velocity field is non-linear in the probability measure. Our first set of contributions focuses on compactly supported initial data. We show the Transformer PDE is well-posed and is the mean-field limit of an interacting particle system, thus generalizing and extending previous analysis to several variants of self-attention: multi-head attention, L2 attention, Sinkhorn attention, Sigmoid attention, and masked attention—leveraging a conditional Wasserstein framework.
In a second set of contributions, we are the first to study non-compactly supported initial conditions, by focusing on Gaussian initial data. Again for different types of attention, we show that the Transformer PDE preserves the space of Gaussian measures, which allows us to analyze the Gaussian case theoretically and numerically to identify typical behaviors.
This Gaussian analysis captures the evolution of data anisotropy through a deep Transformer.
In particular, we highlight a clustering phenomenon that parallels previous results in the non-normalized discrete case."
1340,679d459debd8ffd557a2b3a9,cs.LG,https://arxiv.org/pdf/2501.18282,Leveraging Sparsity for Sample-Efficient Preference Learning: A Theoretical Perspective,"Yunzhen Yao, Lie He, Michael Gastpar",Machine Learning,
1341,679d459debd8ffd557a2b3aa,cs.LG,https://arxiv.org/pdf/2501.18278,ReactEmbed: A Cross-Domain Framework for Protein-Molecule Representation Learning via Biochemical Reaction Networks,"Amitay Sicherman, Kira Radinsky",Machine Learning,"The challenge in computational biology and drug discovery lies in creating comprehensive representations of proteins and molecules that capture their intrinsic properties and interactions. Traditional methods often focus on unimodal data, such as protein sequences or molecular structures, limiting their ability to capture complex biochemical relationships.
This work enhances these representations by integrating biochemical reactions encompassing interactions between molecules and proteins. By leveraging reaction data alongside pre-trained embeddings from state-of-the-art protein and molecule models, we develop ReactEmbed, a novel method that creates a unified embedding space through contrastive learning.
We evaluate ReactEmbed across diverse tasks, including drug-target interaction, protein-protein interaction, protein property prediction, and molecular property prediction, consistently surpassing all current state-of-the-art models.
Notably, we showcase ReactEmbed’s practical utility through successful implementation in lipid nanoparticle-based drug delivery, enabling zero-shot prediction of blood-brain barrier permeability for protein-nanoparticle complexes. The code and comprehensive database of reaction pairs are available for open use atGitHub."
1342,679d459debd8ffd557a2b3ab,cs.LG,https://arxiv.org/pdf/2501.18277,Sebra: Debiasing Through Self-Guided Bias Ranking,"Adarsh Kappiyath, Abhra Chaudhuri, Ajay Jaiswal, Ziquan Liu, Yunpeng Li, Xiatian Zhu, Lu Yin",Machine Learning,"Ranking samples by fine-grained estimates of spuriosity (the degree to which spurious cues are present) has recently been shown to significantly benefit bias mitigation, over the traditional binary biased-vs-unbiased partitioning of train sets. However, this spuriousity ranking comes with the requirement of human supervision. In this paper, we propose a debiasing framework based on our novelSelf-GuidedBiasRanking (Sebra), that mitigates biases (spurious correlations) via an automatic ranking of data points by spuriosity within their respective classes.
Sebra leverages a key local symmetry in Empirical Risk Minimization (ERM) training – the ease of learning a sample via ERM inversely correlates with its spuriousity; the fewer spurious correlations a sample exhibits, the harder it is to learn, and vice versa. However, globally across iterations, ERM tends to deviate from this symmetry. Sebra dynamically steers ERM to correct this deviation, facilitating the sequential learning of attributes in increasing order of difficulty,i.e., decreasing order of spuriosity. As a result, the sequence in which Sebra learns samples naturally provides spuriousity rankings.
We use the resulting fine-grained bias characterization in a contrastive learning framework to mitigate biases from multiple sources. Extensive experiments show that Sebra consistently outperforms previous state-of-the-art unsupervised debiasing techniques across multiple standard benchmarks, including UrbanCars, BAR, CelebA, and ImageNet-1K. Code, pretrained models, and training logs are available athttps://kadarsh22.github.io/sebra_iclr25/."
1343,679d459debd8ffd557a2b3ac,cs.LG,https://arxiv.org/pdf/2501.18268,Reducing Aleatoric and Epistemic Uncertainty through Multi-modal Data Acquisition,"Arthur Hoarau, Benjamin Quost, Sébastien Destercke, Willem Waegeman",Machine Learning,"To generate accurate and reliable predictions, modern AI systems need to combine data from multiple modalities, such as text, images, audio, spreadsheets, and time series. Multi-modal data introduces new opportunities and challenges for disentangling uncertainty: it is commonly assumed in the machine learning community that epistemic uncertainty can be reduced by collecting more data, while aleatoric uncertainty is irreducible. However, this assumption is challenged in modern AI systems when information is obtained from different modalities. This paper introduces an innovative data acquisition framework where uncertainty disentanglement leads to actionable decisions, allowing sampling in two directions: sample size and data modality. The main hypothesis is that aleatoric uncertainty decreases as the number of modalities increases, while epistemic uncertainty decreases by collecting more observations. We provide proof-of-concept implementations on two multi-modal datasets to showcase our data acquisition framework, which combines ideas from active learning, active feature acquisition and uncertainty quantification."
1344,679d459debd8ffd557a2b3ad,cs.LG,https://arxiv.org/pdf/2501.18197,Fundamental Challenges in Evaluating Text2SQL Solutions and Detecting Their Limitations,"Cedric Renggli, Ihab F. Ilyas, Theodoros Rekatsinas",Machine Learning,"In this work, we dive into the fundamental challenges of evaluating Text2SQL solutions and highlight potential failure causes and the potential risks of relying on aggregate metrics in existing benchmarks. We identify two largely unaddressed limitations in current open benchmarks: (1) data quality issues in the evaluation data, mainly attributed to the lack of capturing the probabilistic nature of translating a natural language description into a structured query (e.g., NL ambiguity), and (2) the bias introduced by using different match functions as approximations for SQL equivalence."
1345,679d459debd8ffd557a2b3ae,cs.LG,https://arxiv.org/pdf/2501.18196,GDformer: Going Beyond Subsequence Isolation for Multivariate Time Series Anomaly Detection,"Qingxiang Liu, Chenghao Liu, Sheng Sun, Di Yao, Yuxuan Liang",Machine Learning,"Unsupervised anomaly detection of multivariate time series is a challenging task, given the requirements of deriving a compact detection criterion without accessing the anomaly points.
The existing methods are mainly based on reconstruction error or association divergence, which are both confined toisolated subsequenceswith limited horizons, hardly promising unified series-level criterion.
In this paper, we propose theGlobalDictionary-enhanced Transformer(𝙶𝙳𝚏𝚘𝚛𝚖𝚎𝚛𝙶𝙳𝚏𝚘𝚛𝚖𝚎𝚛\mathtt{GDformer}typewriter_GDformer) with a renovated dictionary-based cross attention mechanism to cultivate the global representations shared by all normal points in the entire series.
Accordingly, the cross-attention maps reflect the correlation weights between the point and global representations, which naturally leads to the representation-wisesimilarity-based detection criterion.
To foster more compact detection boundary, prototypes are introduced to capture the distribution of normal point-global correlation weights.𝙶𝙳𝚏𝚘𝚛𝚖𝚎𝚛𝙶𝙳𝚏𝚘𝚛𝚖𝚎𝚛\mathtt{GDformer}typewriter_GDformerconsistently achieves state-of-the-art unsupervised anomaly detection performance on five real-world benchmark datasets. Further experiments validate the global dictionary has great transferability among various datasets.
The code is available at𝙶𝙳𝚏𝚘𝚛𝚖𝚎𝚛𝙶𝙳𝚏𝚘𝚛𝚖𝚎𝚛\mathtt{GDformer}typewriter_GDformer."
1346,679d459debd8ffd557a2b3af,cs.LG,https://arxiv.org/pdf/2501.18189,Neural Network Modeling of Microstructure Complexity Using Digital Libraries,"Yingjie Zhao, Zhiping Xu","Machine Learning, Materials Science, Computational Engineering, Finance, and Science, Pattern Formation and Solitons, Computational Physics","Microstructure evolution in matter is often modeled numerically using field or level-set solvers, mirroring the dual representation of spatiotemporal complexity in terms of pixel or voxel data, and geometrical forms in vector graphics.
Motivated by this analog, as well as the structural and event-driven nature of artificial and spiking neural networks, respectively, we evaluate their performance in learning and predicting fatigue crack growth and Turing pattern development.
Predictions are made based on digital libraries constructed from computer simulations, which can be replaced by experimental data to lift the mathematical overconstraints of physics.
Our assessment suggests that the leaky integrate-and-fire neuron model offers superior predictive accuracy with fewer parameters and less memory usage, alleviating the accuracy-cost tradeoff in contrast to the common practices in computer vision tasks.
Examination of network architectures shows that these benefits arise from its reduced weight range and sparser connections.
The study highlights the capability of event-driven models in tackling problems with evolutionary bulk-phase and interface behaviors using the digital library approach."
1347,679d459debd8ffd557a2b3b0,cs.LG,https://arxiv.org/pdf/2501.18184,Genetic Algorithm with Border Trades (GAB),Qingchuan Lyu,"Machine Learning, Neural and Evolutionary Computing, Computation","This paper introduces a novel approach to improving Genetic Algorithms (GA) in large or complex problem spaces by incorporating new chromosome patterns in the breeding process through border trade activities. These strategies increase chromosome diversity, preventing premature convergence and enhancing the GA’s ability to explore the solution space more effectively. Empirical evidence demonstrates significant improvements in convergence behavior. This approach offers a promising pathway to addressing challenges in optimizing large or complex problem domains."
1348,679d459debd8ffd557a2b3b1,cs.LG,https://arxiv.org/pdf/2501.18174,Advancing Personalized Federated Learning: Integrative Approaches with AI for Enhanced Privacy and Customization,"Kevin Cooper, Michael Geller","Machine Learning, Signal Processing","In the age of data-driven decision making, preserving privacy while providing personalized experiences has become paramount. Personalized Federated Learning (PFL) offers a promising framework by decentralizing the learning process, thus ensuring data privacy and reducing reliance on centralized data repositories. However, the integration of advanced Artificial Intelligence (AI) techniques within PFL remains underexplored. This paper proposes a novel approach that enhances PFL with cutting-edge AI methodologies including adaptive optimization, transfer learning, and differential privacy. We present a model that not only boosts the performance of individual client models but also ensures robust privacy-preserving mechanisms and efficient resource utilization across heterogeneous networks. Empirical results demonstrate significant improvements in model accuracy and personalization, along with stringent privacy adherence, as compared to conventional federated learning models. This work paves the way for a new era of truly personalized and privacy-conscious AI systems, offering significant implications for industries requiring compliance with stringent data protection regulations."
1349,679d459debd8ffd557a2b3b2,cs.LG,https://arxiv.org/pdf/2501.18170,Continually Evolved Multimodal Foundation Models for Cancer Prognosis,"Jie Peng, Shuang Zhou, Longwei Yang, Yiran Song, Mohan Zhang, Kaixiong Zhou, Feng Xie, Mingquan Lin, Rui Zhang, Tianlong Chen",Machine Learning,"Cancer prognosis is a critical task that involves predicting patient outcomes and survival rates. To enhance prediction accuracy, previous studies have integrated diverse data modalities, such as clinical notes, medical images, and genomic data, leveraging their complementary information. However, existing approaches face two major limitations. First, they struggle to incorporate newly arrived data with varying distributions into training, such as patient records from different hospitals, thus rendering sub-optimal generalizability and limited utility in real-world applications. Second, most multimodal integration methods rely on simplistic concatenation or task-specific pipelines, which fail to capture the complex interdependencies across modalities.
To address these, we propose a continually evolving multi-modal foundation model. Extensive experiments on the TCGA dataset demonstrate the effectiveness of our approach, highlighting its potential to advance cancer prognosis by enabling robust and adaptive multimodal integration."
1350,679d459debd8ffd557a2b3b3,cs.LG,https://arxiv.org/pdf/2501.18164,Faster Convergence of Riemannian Stochastic Gradient Descent with Increasing Batch Size,"Kanata Oowada, Hideaki Iiduka","Machine Learning, Optimization and Control","Many models used in machine learning have become so large that even computer computation of the full gradient of the loss function is impractical. This has made it necessary to efficiently train models using limited available information, such as batch size and learning rate. We have theoretically analyzed the use of Riemannian stochastic gradient descent (RSGD) and found that using an increasing batch size leads to faster RSGD convergence than using a constant batch size not only with a constant learning rate but also with a decaying learning rate, such as cosine annealing decay and polynomial decay. In particular, RSGD has a better convergence rateO⁢(1T)𝑂1𝑇O(\frac{1}{\sqrt{T}})italic_O ( divide start_ARG 1 end_ARG start_ARG square-root start_ARG italic_T end_ARG end_ARG )than the existing rateO⁢(log⁡TT4)𝑂𝑇4𝑇O(\frac{\sqrt{\log T}}{\sqrt[4]{T}})italic_O ( divide start_ARG square-root start_ARG roman_log italic_T end_ARG end_ARG start_ARG nth-root start_ARG 4 end_ARG start_ARG italic_T end_ARG end_ARG )with a diminishing learning rate, whereT𝑇Titalic_Tis the number of iterations. The results of experiments on principal component analysis and low-rank matrix completion problems confirmed that, except for the MovieLens dataset and a constant learning rate, using a polynomial growth batch size or an exponential growth batch size results in better performance than using a constant batch size."
1351,679d459debd8ffd557a2b3b4,cs.LG,https://arxiv.org/pdf/2501.18143,Dual-Bounded Nonlinear Optimal Transport for Size Constrained Min Cut Clustering,"Fangyuan Xie, Jinghui Yuan, Feiping Nie, Xuelong Li",Machine Learning,"Min cut is an important graph partitioning method. However, current solutions to the min cut problem suffer from slow speeds, difficulty in solving, and often converge to simple solutions. To address these issues, we relax the min cut problem into a dual-bounded constraint and, for the first time, treat the min cut problem as a dual-bounded nonlinear optimal transport problem. Additionally, we develop a method for solving dual-bounded nonlinear optimal transport based on the Frank-Wolfe method (abbreviated as DNF). Notably, DNF not only solves the size constrained min cut problem but is also applicable to all dual-bounded nonlinear optimal transport problems. We prove that for convex problems satisfying Lipschitz smoothness, the DNF method can achieve a convergence rate of𝒪⁢(1t)𝒪1𝑡\mathcal{O}(\frac{1}{t})caligraphic_O ( divide start_ARG 1 end_ARG start_ARG italic_t end_ARG ). We apply the DNF method to the min cut problem and find that it achieves state-of-the-art performance in terms of both the loss function and clustering accuracy at the fastest speed, with a convergence rate of𝒪⁢(1t)𝒪1𝑡\mathcal{O}(\frac{1}{\sqrt{t}})caligraphic_O ( divide start_ARG 1 end_ARG start_ARG square-root start_ARG italic_t end_ARG end_ARG ). Moreover, the DNF method for the size constrained min cut problem requires no parameters and exhibits better stability."
1352,679d459debd8ffd557a2b3b5,cs.LG,https://arxiv.org/pdf/2501.18138,B3C: A Minimalist Approach to Offline Multi-Agent Reinforcement Learning,"Woojun Kim, Katia Sycara",Machine Learning,"Overestimation arising from selecting unseen actions during policy evaluation is a major challenge in offline reinforcement learning (RL). A minimalist approach in the single-agent setting—adding behavior cloning (BC) regularization to existing online RL algorithms—has been shown to be effective; however, this approach is understudied in multi-agent settings. In particular, overestimation becomes worse in multi-agent settings due to the presence of multiple actions, resulting in the BC regularization-based approach easily suffering from either over-regularization or critic divergence. To address this, we propose a simple yet effective method, Behavior Cloning regularization with Critic Clipping (B3C), which clips the target critic value in policy evaluation based on the maximum return in the dataset and pushes the limit of the weight on the RL objective over BC regularization, thereby improving performance. Additionally, we leverage existing value factorization techniques, particularly non-linear factorization, which is understudied in offline settings. Integrated with non-linear value factorization, B3C outperforms state-of-the-art algorithms on various offline multi-agent benchmarks."
1353,679d459debd8ffd557a2b3b6,cs.LG,https://arxiv.org/pdf/2501.18123,Battery State of Health Estimation Using LLM Framework,"Aybars Yunusoglu, Dexter Le, Karn Tiwari, Murat Isik, I. Can Dikmen","Machine Learning, Signal Processing","Battery health monitoring is critical for the efficient and reliable operation of electric vehicles (EVs). This study introduces a transformer-based framework for estimating the State of Health (SoH) and predicting the Remaining Useful Life (RUL) of lithium titanate (LTO) battery cells by utilizing both cycle-based and instantaneous discharge data. Testing on eight LTO cells under various cycling conditions over 500 cycles, we demonstrate the impact of charge durations on energy storage trends and apply Differential Voltage Analysis (DVA) to monitor capacity changes (dQ/dV) across voltage ranges. Our LLM model achieves superior performance, with a Mean Absolute Error (MAE) as low as 0.87% and varied latency metrics that support efficient processing, demonstrating its strong potential for real-time integration into EVs. The framework effectively identifies early signs of degradation through anomaly detection in high-resolution data, facilitating predictive maintenance to prevent sudden battery failures and enhance energy efficiency."
1354,679d459debd8ffd557a2b3b7,cs.LG,https://arxiv.org/pdf/2501.18112,ACTGNN: Assessment of Clustering Tendency with Synthetically-Trained Graph Neural Networks,"Yiran Luo, Evangelos E. Papalexakis",Machine Learning,"Determining clustering tendency in datasets is a fundamental but challenging task, especially in noisy or high-dimensional settings where traditional methods, such as the Hopkins Statistic and Visual Assessment of Tendency (VAT), often struggle to produce reliable results. In this paper, we propose ACTGNN, a graph-based framework designed to assess clustering tendency by leveraging graph representations of data. Node features are constructed using Locality-Sensitive Hashing (LSH), which captures local neighborhood information, while edge features incorporate multiple similarity metrics, such as the Radial Basis Function (RBF) kernel, to model pairwise relationships. A Graph Neural Network (GNN) is trained exclusively on synthetic datasets, enabling robust learning of clustering structures under controlled conditions. Extensive experiments demonstrate that ACTGNN significantly outperforms baseline methods on both synthetic and real-world datasets, exhibiting superior performance in detecting faint clustering structures, even in high-dimensional or noisy data. Our results highlight the generalizability and effectiveness of the proposed approach, making it a promising tool for robust clustering tendency assessment."
1355,679d459debd8ffd557a2b3b8,cs.LG,https://arxiv.org/pdf/2501.18094,AlphaAdam:Asynchronous Masked Optimization with Dynamic Alpha for Selective Updates,"Da Chang, Yu Li, Ganzhao Yuan",Machine Learning,"In the training of large language models (LLMs), updating parameters more efficiently and stably has always been an important challenge. To achieve efficient parameter updates, existing methods usually achieve performance comparable to full parameter updates through methods such as low-dimensional decomposition or layer-wise selective updates.
In this work, we propose AlphaAdam, an optimization framework for LLM from the perspective of intra-layer parameter updates. By decoupling parameter updates and dynamically adjusting their strength, AlphaAdam accelerates convergence and improves training stability.
We construct parameter masks based on the consistency of historical momentum and gradient direction and combine them with an adaptive mask strength strategy to ensure efficient optimization and theoretical convergence guarantees, which is also applicable to most momentum-based optimizers.
Extensive experiments show that AlphaAdam outperforms state-of-the-art methods such as AdamW in terms of convergence speed and computational efficiency across tasks, including GPT-2 pre-trained and fine-tuned RoBERTa and Llama-7B. Our AlphaAdam implements an optimizer enhancement framework for LLMs through intra-layer asynchronous masked adaptive updates. Our code is available in thislink."
1356,679d459debd8ffd557a2b3b9,cs.LG,https://arxiv.org/pdf/2501.18093,Reward Prediction Error Prioritisation in Experience Replay: The RPE-PER Method,"Hoda Yamani, Yuning Xing, Lee Violet C. Ong, Bruce A. MacDonald, Henry Williams","Machine Learning, Robotics","Reinforcement Learning algorithms aim to learn optimal control strategies through iterative interactions with an environment. A critical element in this process is the experience replay buffer, which stores past experiences, allowing the algorithm to learn from a diverse range of interactions rather than just the most recent ones. This buffer is especially essential in dynamic environments with limited experiences. However, efficiently selecting high-value experiences to accelerate training remains a challenge. Drawing inspiration from the role of reward prediction errors (RPEs) in biological systems, where they are essential for adaptive behaviour and learning, we introduce Reward Predictive Error Prioritised Experience Replay (RPE-PER). This novel approach prioritises experiences in the buffer based on RPEs. Our method employs a critic network, EMCN, that predicts rewards in addition to the Q-values produced by standard critic networks. The discrepancy between these predicted and actual rewards is computed as RPE and utilised as a signal for experience prioritisation. Experimental evaluations across various continuous control tasks demonstrate RPE-PER’s effectiveness in enhancing the learning speed and performance of off-policy actor-critic algorithms compared to baseline approaches."
1357,679d459debd8ffd557a2b3ba,cs.LG,https://arxiv.org/pdf/2501.18092,Learning Provablely Improves the Convergence of Gradient Descent,"Qingyu Song, Wei Lin, Hong Xu","Machine Learning, Optimization and Control",
1358,679d459debd8ffd557a2b3bb,cs.LG,https://arxiv.org/pdf/2501.18049,Joint Pricing and Resource Allocation: An Optimal Online-Learning Approach,"Jianyu Xu, Xuan Wang, Yu-Xiang Wang, Jiashuo Jiang","Machine Learning, Optimization and Control, Machine Learning","We study an online learning problem on dynamic pricing and resource allocation, where we make joint pricing and inventory decisions to maximize the overall net profit.
We consider the stochastic dependence of demands on the price, which complicates the resource allocation process and introduces significant non-convexity and non-smoothness to the problem. To solve this problem, we develop an efficient algorithm that utilizes a “Lower-Confidence Bound (LCB)” meta-strategy over multiple OCO agents.
Our algorithm achievesO~⁢(T⁢m⁢n)~𝑂𝑇𝑚𝑛\tilde{O}(\sqrt{Tmn})over~ start_ARG italic_O end_ARG ( square-root start_ARG italic_T italic_m italic_n end_ARG )regret (form𝑚mitalic_msuppliers andn𝑛nitalic_nconsumers), which isoptimalwith respect to the time horizonT𝑇Titalic_T.
Our results illustrate an effective integration of statistical learning methodologies with complex operations research problems."
1359,679d459debd8ffd557a2b3bc,cs.LG,https://arxiv.org/pdf/2501.18028,KNN and K-means in Gini Prametric Spaces,"Cassandra Mussard, Arthur Charpentier, Stéphane Mussard",Machine Learning,"This paper introduces innovative enhancements to the K-means and K-nearest neighbors (KNN) algorithms based on the concept of Gini prametric spaces. Unlike traditional distance metrics, Gini prametrics incorporate both value-based and rank-based measures, offering robustness to noise and outliers. The paper’s main contributions include (1) proposing a Gini prametric that captures rank information alongside value distances, (2) presenting a Gini K-means algorithm that is proven to converge and demonstrates resilience to noisy data, and (3) introducing a Gini KNN method that rivals state-of-the-art approaches like Hassanat’s distance in noisy environments. Experimental evaluations on 14 datasets from the UCI repository reveal the superior performance and efficiency of Gini-based algorithms in clustering and classification tasks. This work opens new avenues for leveraging rank-based prametrics in machine learning and statistical analysis."
1360,679d459debd8ffd557a2b3bd,cs.LG,https://arxiv.org/pdf/2501.18015,A Proximal Operator for Inducing 2:4-Sparsity,"Jonas M Kübler, Yu-Xiang Wang, Shoham Sabach, Navid Ansari, Matthäus Kleindessner, Kailash Budhathoki, Volkan Cevher, George Karypis",Machine Learning,"Recent hardware advancements in AI Accelerators and GPUs allow to efficiently compute sparse matrix multiplications, especially when2222out of4444consecutive weights are set to zero.
However, this so-called2222:4444sparsity usually comes at a decreased accuracy of the model.
We derive a regularizer that exploits the local correlation of features to find better sparsity masks in trained models.
We minimize the regularizer jointly with a local squared loss by deriving the proximal operator for which we show that it has an efficient solution in the2222:4444-sparse case.
After optimizing the mask, we introduce masked-gradient updates to further minimize the local squared loss.
We illustrate our method on toy problems and apply it to pruning entire large language models up to 70B parameters.
On models up to 13B we improve over previous state of the art algorithms, whilst on 70B models we match their performance."
1361,679d459debd8ffd557a2b3be,cs.LG,https://arxiv.org/pdf/2501.18012,When less is more: evolving large neural networks from small ones,"Anil Radhakrishnan, John F. Lindner, Scott T. Miller, Sudeshna Sinha, William L. Ditto","Machine Learning, Disordered Systems and Neural Networks","In contrast to conventional artificial neural networks, which are large and structurally static, we study feed-forward neural networks that are small and dynamic, whose nodes can be added (or subtracted) during training. A single neuronal weight in the network controls the network’s size, while the weight itself is optimized by the same gradient-descent algorithm that optimizes the network’s other weights and biases, but with a size-dependent objective or loss function. We train and evaluate such Nimble Neural Networks on nonlinear regression and classification tasks where they outperform the corresponding static networks. Growing networks to minimal, appropriate, or optimal sizes while training elucidates network dynamics and contrasts with pruning large networks after training but before deployment."
1362,679d459debd8ffd557a2b3bf,cs.LG,https://arxiv.org/pdf/2501.17976,KoopAGRU: A Koopman-based Anomaly Detection in Time-Series using Gated Recurrent Units,"Issam Ait Yahia, Ismail Berrada",Machine Learning,"Anomaly detection in real-world time-series data is a challenging task due to the complex and nonlinear temporal dynamics involved. This paper introduces KoopAGRU, a new deep learning model designed to tackle this problem by combining Fast Fourier Transform (FFT), Deep Dynamic Mode Decomposition (DeepDMD), and Koopman theory. FFT allows KoopAGRU to decompose temporal data into time-variant and time-invariant components providing precise modeling of complex patterns. To better control these two components, KoopAGRU utilizes Gate Recurrent Unit (GRU) encoders to learn Koopman observables, enhancing the detection capability across multiple temporal scales. KoopAGRU is trained in a single process and offers fast inference times. Extensive tests on various benchmark datasets show that KoopAGRU outperforms other leading methods, achieving a new average F1-score of 90.88% on the well-known anomalies detection task of times series datasets, and proves to be efficient and reliable in detecting anomalies in real-world scenarios."
1363,679d459debd8ffd557a2b3c0,cs.LG,https://arxiv.org/pdf/2501.17965,Variational Combinatorial Sequential Monte Carlo for Bayesian Phylogenetics in Hyperbolic Space,"Alex Chen, Philipe Chlenski, Kenneth Munyuza, Antonio Khalil Moretti, Christian A. Naesseth, Itsik Pe'er","Machine Learning, Machine Learning","Hyperbolic space naturally encodes hierarchical structures such as phylogenies (binary trees), where inward-bending geodesics reflect paths through least common ancestors, and the exponential growth of neighborhoods mirrors the super-exponential scaling of topologies. This scaling challenge limits the efficiency of Euclidean-based approximate inference methods. Motivated by the geometric connections between trees and hyperbolic space, we develop novel hyperbolic extensions of two sequential search algorithms: Combinatorial and Nested Combinatorial Sequential Monte Carlo (CsmcandNcsmc). Our approach introduces consistent and unbiased estimators, along with variational inference methods (H-VcsmcandH-Vncsmc), which outperform their Euclidean counterparts. Empirical results demonstrate improved speed, scalability and performance in high-dimensional phylogenetic inference tasks."
1364,679d459debd8ffd557a2b3c1,cs.LG,https://arxiv.org/pdf/2501.17900,Shared DIFF Transformer,"Yueyang Cang, Yuhang Liu, Xiaoteng Zhang, Xiangju Wang",Machine Learning,"DIFF Transformer improves attention allocation by enhancing focus on relevant context while suppressing noise. It introduces a differential attention mechanism that calculates the difference between two independently generated attention distributions, effectively reducing noise and promoting sparse attention patterns. However, the independent signal generation in DIFF Transformer results in parameter redundancy and suboptimal utilization of information. In this work, we propose Shared DIFF Transformer, which draws on the idea of a differential amplifier by introducing a shared base matrix to model global patterns and incorporating low-rank updates to enhance task-specific flexibility. This design significantly reduces parameter redundancy, improves efficiency, and retains strong noise suppression capabilities. Experimental results show that, compared to DIFF Transformer, our method achieves better performance in tasks such as long-sequence modeling, key information retrieval, and in-context learning. Our work provides a novel and efficient approach to optimizing differential attention mechanisms and advancing robust Transformer architectures."
1365,679d459debd8ffd557a2b3c2,cs.LG,https://arxiv.org/pdf/2501.17896,Explainable Machine Learning: An Illustration of Kolmogorov-Arnold Network Model for Airfoil Lift Prediction,Sudhanva Kulkarni,Machine Learning,
1366,679d459debd8ffd557a2b3c3,cs.LG,https://arxiv.org/pdf/2501.18531,Graph Learning for Bidirectional Disease Contact Tracing on Real Human Mobility Data,"Sofia Hurtado, Radu Marculescu","Social and Information Networks, Machine Learning","For rapidly spreading diseases where many cases show no symptoms, swift and effective contact tracing is essential. While exposure notification applications provide alerts on potential exposures, a fully automated system is needed to track the infectious transmission routes. To this end, our research leverages large-scale contact networks from real human mobility data to identify the path of transmission. More precisely, we introduce a new Infectious Path Centrality network metric that informs a graph learning edge classifier to identify important transmission events, achieving an F1-score of 94%. Additionally, we explore bidirectional contact tracing, which quarantines individuals both retroactively and proactively, and compare its effectiveness against traditional forward tracing, which only isolates individuals after testing positive. Our results indicate that when only 30% of symptomatic individuals are tested, bidirectional tracing can reduce infectious effective reproduction rate by 71%, thus significantly controlling the outbreak."
1367,679d459debd8ffd557a2b3c4,cs.LG,https://arxiv.org/pdf/2501.18530,Optimal generalisation and learning transition in extensive-width shallow neural networks near interpolation,"Jean Barbier, Francesco Camilli, Minh-Toan Nguyen, Mauro Pastore, Rudy Skerk","Machine Learning, Disordered Systems and Neural Networks, Statistical Mechanics, Information Theory, Machine Learning","We consider a teacher-student model of supervised learning with a fully-trained 2-layer neural network whose widthk𝑘kitalic_kand input dimensiond𝑑ditalic_dare large and proportional. We compute the Bayes-optimal generalisation error of the network for any activation function in the regime where the number of training datan𝑛nitalic_nscales quadratically with the input dimension, i.e., around the interpolation threshold where the number of trainable parametersk⁢d+k𝑘𝑑𝑘kd+kitalic_k italic_d + italic_kand of data pointsn𝑛nitalic_nare comparable. Our analysis tackles generic weight distributions. Focusing on binary weights, we uncover a discontinuous phase transition separating a “universal” phase from a “specialisation” phase. In the first, the generalisation error is independent of the weight distribution and decays slowly with the sampling raten/d2𝑛superscript𝑑2n/d^{2}italic_n / italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, with the student learning only some non-linear combinations of the teacher weights. In the latter, the error is weight distribution-dependent and decays faster due to the alignment of the student towards the teacher network. We thus unveil the existence of a highly predictive solution near interpolation, which is however potentially hard to find."
1368,679d459debd8ffd557a2b3c5,cs.LG,https://arxiv.org/pdf/2501.18470,Resampling Filter Design for Multirate Neural Audio Effect Processing,"Alistair Carson, Vesa Välimäki, Alec Wright, Stefan Bilbao","Audio and Speech Processing, Machine Learning, Sound, Signal Processing","Neural networks have become ubiquitous in audio effects modelling, especially for guitar amplifiers and distortion pedals. One limitation of such models is that the sample rate of the training data is implicitly encoded in the model weights and therefore not readily adjustable at inference. Recent work explored modifications to recurrent neural network architecture to approximate a sample rate independent system, enabling audio processing at a rate that differs from the original training rate. This method works well for integer oversampling and can reduce aliasing caused by nonlinear activation functions. For small fractional changes in sample rate, fractional delay filters can be used to approximate sample rate independence, but in some cases this method fails entirely. Here, we explore the use of signal resampling at the input and output of the neural network as an alternative solution. We investigate several resampling filter designs and show that a two-stage design consisting of a half-band IIR filter cascaded with a Kaiser window FIR filter can give similar or better results to the previously proposed model adjustment method with many fewer operations per sample and less than one millisecond of latency at typical audio rates. Furthermore, we investigate interpolation and decimation filters for the task of integer oversampling and show that cascaded half-band IIR and FIR designs can be used in conjunction with the model adjustment method to reduce aliasing in a range of distortion effect models."
1369,679d459debd8ffd557a2b3c6,cs.LG,https://arxiv.org/pdf/2501.18456,adabmDCA 2.0 -- a flexible but easy-to-use package for Direct Coupling Analysis,"Lorenzo Rosset, Roberto Netti, Anna Paola Muntoni, Martin Weigt, Francesco Zamponi","Quantitative Methods, Machine Learning, Biological Physics","In this methods article, we provide a flexible but easy-to-use implementation of Direct Coupling Analysis (DCA) based on Boltzmann machine learning, together with a tutorial on how to use it. The packageadabmDCA 2.0is available in different programming languages (C++, Julia, Python) usable on different architectures (single-core and multi-core CPU, GPU) using a common front-end interface. In addition to several learning protocols for dense and sparse generative DCA models, it allows to directly address common downstream tasks like residue-residue contact prediction, mutational-effect prediction, scoring of sequence libraries and generation of artificial sequences for sequence design. It is readily applicable to protein and RNA sequence data."
1370,679d459debd8ffd557a2b3c7,cs.LG,https://arxiv.org/pdf/2501.18423,DeepExtractor: Time-domain reconstruction of signals and glitches in gravitational wave data with deep learning,"Tom Dooney, Harsh Narola, Stefano Bromuri, R. Lyana Curier, Chris Van Den Broeck, Sarah Caudill, Daniel Stanley Tan","General Relativity and Quantum Cosmology, Instrumentation and Methods for Astrophysics, Machine Learning, Data Analysis, Statistics and Probability, Instrumentation and Detectors","Gravitational wave (GW) interferometers, such as LIGO, Virgo, and KAGRA, detect faint signals from distant astrophysical events, such as binary black hole mergers.
However, their high sensitivity also makes them susceptible to background noise, which can obscure these signals.
This noise often includes transient artifacts called “glitches”, that can mimic genuine astrophysical signals or mask their true characteristics.
Fast and accurate reconstruction of both signals and glitches is crucial for reliable scientific inference.
Accurate GW signal reconstruction facilitates parameter estimation of the GW source, while glitch reconstruction—challenging due to the unmodeled nature of glitches—supports glitch mitigation and enables mock data challenges involving realistic noise artifacts.
In this study, we presentDeepExtractor, a deep learning framework that is designed to reconstruct signals and glitches with power exceeding interferometer noise, regardless of their source.
We designDeepExtractorto model the inherent noise distribution of GW interferometers, following conventional assumptions that the noise is Gaussian and stationary over short time scales.
It operates by predicting and subsequently subtracting the noise component of the input mixture of noise and signal or glitch, retaining only the clean reconstruction.
Our innovative approach achieves superior generalization capabilities for arbitrary signals and glitches compared to methods that directly map mixture inputs to the clean training waveforms, yielding precise reconstruction of time-domain signals and glitches.
We focus on applications related to glitches and validateDeepExtractor’seffectiveness through three experiments: (1) reconstructing simulated glitches injected into detector noise, (2) comparing its performance with the state-of-the-artBayesWavealgorithm, and (3) analyzing real data from the Gravity Spy dataset to demonstrate effective glitch subtraction from LIGO strain data.
Our proposed model achieves a median mismatch of only0.9%percent0.90.9\%0.9 %for simulated glitches, outperforming several deep learning baselines. Additionally,DeepExtractorsurpassesBayesWavein glitch recovery, offering a dramatic computational speedup by reconstructing one glitch sample in approximately0.10.10.10.1seconds on a CPU, compared toBayesWave’sprocessing time of approximately one hour per glitch."
1371,679d459debd8ffd557a2b3c8,cs.LG,https://arxiv.org/pdf/2501.18415,Consensus statement on the credibility assessment of ML predictors,"Alessandra Aldieri, Thiranja Prasad Babarenda Gamage, Antonino Amedeo La Mattina, Yi Li, Axel Loewe, Francesco Pappalardo, Marco Viceconti Italy","Quantitative Methods, Machine Learning",
1372,679d459debd8ffd557a2b3c9,cs.LG,https://arxiv.org/pdf/2501.18381,Implicit Riemannian Optimism with Applications to Min-Max Problems,"Christophe Roux, David Martínez-Rubio, Sebastian Pokutta","Optimization and Control, Machine Learning",
1373,679d459debd8ffd557a2b3ca,cs.LG,https://arxiv.org/pdf/2501.18359,Contextual Online Decision Making with Infinite-Dimensional Functional Regression,"Haichen Hu, Rui Ai, Stephen Bates, David Simchi-Levi","Machine Learning, Machine Learning","Contextual sequential decision-making problems play a crucial role in machine learning, encompassing a wide range of downstream applications such as bandits, sequential hypothesis testing and online risk control. These applications often require different statistical measures, including expectation, variance and quantiles.
In this paper, we provide a universal admissible algorithm framework for dealing with all kinds of contextual online decision-making problems that directly learns the whole underlying unknown distribution instead of focusing on individual statistics. This is much more difficult because the dimension of the regression is uncountably infinite, and any existing linear contextual bandits algorithm will result in infinite regret. To overcome this issue, we propose an efficient infinite-dimensional functional regression oracle for contextual cumulative distribution functions (CDFs), where each data point is modeled as a combination of context-dependent CDF basis functions.
Our analysis reveals that the decay rate of the eigenvalue sequence of the design integral operator governs the regression error rate and, consequently, the utility regret rate. Specifically, when the eigenvalue sequence exhibits a polynomial decay of order1γ≥11𝛾1\frac{1}{\gamma}\geq 1divide start_ARG 1 end_ARG start_ARG italic_γ end_ARG ≥ 1, the utility regret is bounded by𝒪~⁢(T3⁢γ+22⁢(γ+2))~𝒪superscript𝑇3𝛾22𝛾2\widetilde{\mathcal{O}}\Big{(}T^{\frac{3\gamma+2}{2(\gamma+2)}}\Big{)}over~ start_ARG caligraphic_O end_ARG ( italic_T start_POSTSUPERSCRIPT divide start_ARG 3 italic_γ + 2 end_ARG start_ARG 2 ( italic_γ + 2 ) end_ARG end_POSTSUPERSCRIPT ). By settingγ=0𝛾0\gamma=0italic_γ = 0, this recovers the existing optimal regret rate for contextual bandits with finite-dimensional regression and is optimal under a stronger exponential decay assumption. Additionally, we provide a numerical method to compute the eigenvalue sequence of the integral operator, enabling the practical implementation of our framework."
1374,679d459debd8ffd557a2b3cb,cs.LG,https://arxiv.org/pdf/2501.18283,Random Feature Representation Boosting,"Nikita Zozoulenko, Thomas Cass, Lukas Gonon","Machine Learning, Machine Learning","We introduce Random Feature Representation Boosting (RFRBoost), a novel method for constructing deep residual random feature neural networks (RFNNs) using boosting theory. RFRBoost uses random features at each layer to learn the functional gradient of the network representation, enhancing performance while preserving the convex optimization benefits of RFNNs. In the case of MSE loss, we obtain closed-form solutions to greedy layer-wise boosting with random features. For general loss functions, we show that fitting random feature residual blocks reduces to solving a quadratically constrained least squares problem. We demonstrate, through numerical experiments on 91 tabular datasets for regression and classification, that RFRBoost significantly outperforms traditional RFNNs and end-to-end trained MLP ResNets, while offering substantial computational advantages and theoretical guarantees stemming from boosting theory."
1375,679d459debd8ffd557a2b3cc,cs.LG,https://arxiv.org/pdf/2501.18183,Decentralized Projection-free Online Upper-Linearizable Optimization with Applications to DR-Submodular Optimization,"Yiyang Lu, Mohammad Pedramfar, Vaneet Aggarwal","Optimization and Control, Computational Complexity, Machine Learning, Machine Learning","We introduce a novel framework for decentralized projection-free optimization, extending projection-free methods to a broader class of upper-linearizable functions. Our approach leverages decentralized optimization techniques with the flexibility of upper-linearizable function frameworks, effectively generalizing traditional DR-submodular function optimization. We obtain the regret ofO⁢(T1−θ/2)𝑂superscript𝑇1𝜃2O(T^{1-\theta/2})italic_O ( italic_T start_POSTSUPERSCRIPT 1 - italic_θ / 2 end_POSTSUPERSCRIPT )with communication complexity ofO⁢(Tθ)𝑂superscript𝑇𝜃O(T^{\theta})italic_O ( italic_T start_POSTSUPERSCRIPT italic_θ end_POSTSUPERSCRIPT )and number of linear optimization oracle calls ofO⁢(T2⁢θ)𝑂superscript𝑇2𝜃O(T^{2\theta})italic_O ( italic_T start_POSTSUPERSCRIPT 2 italic_θ end_POSTSUPERSCRIPT )for decentralized upper-linearizable function optimization, for any0≤θ≤10𝜃10\leq\theta\leq 10 ≤ italic_θ ≤ 1. This approach allows for the first results for monotone up-concave optimization with general convex constraints and non-monotone up-concave optimization with general convex constraints. Further, the above results for first order feedback are extended to zeroth order, semi-bandit, and bandit feedback."
1376,679d459debd8ffd557a2b3cd,cs.LG,https://arxiv.org/pdf/2501.18178,Estimating Multi-chirp Parameters using Curvature-guided Langevin Monte Carlo,"Sattwik Basu, Debottam Dutta, Yu-Lin Wei, Romit Roy Choudhury","Signal Processing, Machine Learning, Machine Learning","This paper considers the problem of estimating chirp parameters from a noisy mixture of chirps.
While a rich body of work exists in this area, challenges remain when extending these techniques to chirps of higher order polynomials.
We formulate this as a non-convex optimization problem and propose a modified Langevin Monte Carlo (LMC) sampler that exploits the averagecurvatureof the objective function to reliably find the minimizer.
Results show that ourCurvature-guidedLMC (CG-LMC) algorithm is robust and succeeds even in low SNR regimes, making it viable for practical applications."
1377,679d459debd8ffd557a2b3ce,cs.LG,https://arxiv.org/pdf/2501.18126,HyperZero: A Customized End-to-End Auto-Tuning System for Recommendation with Hourly Feedback,"Xufeng Cai, Ziwei Guan, Lei Yuan, Ali Selman Aydin, Tengyu Xu, Boying Liu, Wenbo Ren, Renkai Xiang, Songyi He, Haichuan Yang, Serena Li, Mingze Gao, Yue Weng, Ji Liu","Information Retrieval, Machine Learning","Modern recommendation systems can be broadly divided into two key stages: the ranking stage, where the system predicts various user engagements (e.g., click-through rate, like rate, follow rate, watch time), and the value model stage, which aggregates these predictive scores through a function (e.g., a linear combination defined by a weight vector) to measure the value of each content by a single numerical score. Both stages play roughly equally important roles in real industrial systems; however, how to optimize the model weights for the second stage still lacks systematic study. This paper focuses on optimizing the second stage through auto-tuning technology.
Although general auto-tuning systems and solutions - both from established production practices and open-source solutions - can address this problem, they typically require weeks or even months to identify a feasible solution. Such prolonged tuning processes are unacceptable in production environments for recommendation systems, as suboptimal value models can severely degrade user experience.
An effective auto-tuning solution is required to identify a viable model within 2-3 days, rather than the extended timelines typically associated with existing approaches.
In this paper, we introduce a practical auto-tuning system namedHyperZerothat addresses these time constraints while effectively solving the unique challenges inherent in modern recommendation systems. Moreover, this framework has the potential to be expanded to broader tuning tasks within recommendation systems."
1378,679d459debd8ffd557a2b3cf,cs.LG,https://arxiv.org/pdf/2501.18115,A spectral clustering-type algorithm for the consistent estimation of the Hurst distribution in moderately high dimensions,"Patrice Abry, Gustavo Didier, Oliver Orejola, Herwig Wendt","Methodology, Machine Learning, Machine Learning",
1379,679d459debd8ffd557a2b3d0,cs.LG,https://arxiv.org/pdf/2501.18114,DCatalyst: A Unified Accelerated Framework for Decentralized Optimization,"Tianyu Cao, Xiaokai Chen, Gesualdo Scutari","Optimization and Control, Machine Learning","We study decentralized optimization over a network of agents, modeled as graphs, with no central server. The goal is to minimizef+r𝑓𝑟f+ritalic_f + italic_r, wheref𝑓fitalic_frepresents a (strongly) convex function averaging the local agents’ losses, andr𝑟ritalic_ris a convex, extended-value function."
1380,679d459debd8ffd557a2b3d1,cs.LG,https://arxiv.org/pdf/2501.18089,ISAM-MTL: Cross-subject multi-task learning model with identifiable spikes and associative memory networks,"Junyan Li, Bin Hu, Zhi-Hong Guan","Neural and Evolutionary Computing, Machine Learning, Neurons and Cognition","Cross-subject variability in EEG degrades performance of current deep learning models, limiting the development of brain-computer interface (BCI). This paper proposes ISAM-MTL, which is a multi-task learning (MTL) EEG classification model based on identifiable spiking (IS) representations and associative memory (AM) networks. The proposed model treats EEG classification of each subject as an independent task and leverages cross-subject data training to facilitate feature sharing across subjects. ISAM-MTL consists of a spiking feature extractor that captures shared features across subjects and a subject-specific bidirectional associative memory network that is trained by Hebbian learning for efficient and fast within-subject EEG classification. ISAM-MTL integrates learned spiking neural representations with bidirectional associative memory for cross-subject EEG classification. The model employs label-guided variational inference to construct identifiable spike representations, enhancing classification accuracy. Experimental results on two BCI Competition datasets demonstrate that ISAM-MTL improves the average accuracy of cross-subject EEG classification while reducing performance variability among subjects. The model further exhibits the characteristics of few-shot learning and identifiable neural activity beneath EEG, enabling rapid and interpretable calibration for BCI systems."
1381,679d459debd8ffd557a2b3d2,cs.LG,https://arxiv.org/pdf/2501.18084,U-aggregation: Unsupervised Aggregation of Multiple Learning Algorithms,Rui Duan,"Machine Learning, Machine Learning","Across various domains, the growing advocacy for open science and open-source machine learning has made an increasing number of models publicly available. These models allow practitioners to integrate them into their own contexts, reducing the need for extensive data labeling, training, and calibration. However, selecting the best model for a specific target population remains challenging due to issues like limited transferability, data heterogeneity, and the difficulty of obtaining true labels or outcomes in real-world settings. In this paper, we propose an unsupervised model aggregation method, U-aggregation, designed to integrate multiple pre-trained models for enhanced and robust performance in new populations. Unlike existing supervised model aggregation or super learner approaches, U-aggregation assumes no observed labels or outcomes in the target population. Our method addresses limitations in existing unsupervised model aggregation techniques by accommodating more realistic settings, including heteroskedasticity at both the model and individual levels, and the presence of adversarial models. Drawing on insights from random matrix theory, U-aggregation incorporates a variance stabilization step and an iterative sparse signal recovery process. These steps improve the estimation of individuals’ true underlying risks in the target population and evaluate the relative performance of candidate models. We provide a theoretical investigation and systematic numerical experiments to elucidate the properties of U-aggregation. We demonstrate its potential real-world application by using U-aggregation to enhance genetic risk prediction of complex traits, leveraging publicly available models from the PGS Catalog."
1382,679d459debd8ffd557a2b3d3,cs.LG,https://arxiv.org/pdf/2501.18060,Noise-Adaptive Conformal Classification with Marginal Coverage,"Teresa Bortolotti, Y. X. Rachel Wang, Xin Tong, Alessandra Menafoglio, Simone Vantini, Matteo Sesia","Methodology, Machine Learning, Machine Learning","Conformal inference provides a rigorous statistical framework for uncertainty quantification in machine learning, enabling well-calibrated prediction sets with precise coverage guarantees for any classification model. However, its reliance on the idealized assumption of perfect data exchangeability limits its effectiveness in the presence of real-world complications, such as low-quality labels—a widespread issue in modern large-scale data sets.
This work tackles this open problem by introducing an adaptive conformal inference method capable of efficiently handling deviations from exchangeability caused by random label noise, leading to informative prediction sets with tight marginal coverage guarantees even in those challenging scenarios.
We validate our method through extensive numerical experiments demonstrating its effectiveness on synthetic and real data sets, including CIFAR-10H and BigEarthNet."
1383,679d459debd8ffd557a2b3d4,cs.LG,https://arxiv.org/pdf/2501.18018,Perforated Backpropagation: A Neuroscience Inspired Extension to Artificial Neural Networks,"Rorry Brenner, Laurent Itti","Neural and Evolutionary Computing, Machine Learning, Neurons and Cognition",
1384,679d459debd8ffd557a2b3d5,cs.LG,https://arxiv.org/pdf/2501.18005,Fault Localization via Fine-tuning Large Language Models with Mutation Generated Stack Traces,"Neetha Jambigi, Bartosz Bogacz, Moritz Mueller, Thomas Bach, Michael Felderer","Software Engineering, Machine Learning","Abrupt and unexpected terminations of software are termed as software crashes.
They can be challenging to analyze. Finding the root cause requires extensive manual effort and expertise to connect information sources like stack traces, source code, and logs.
Typical approaches to fault localization require either test failures or source code. Crashes occurring in production environments, such as that of SAP HANA, provide solely crash logs and stack traces.
We present a novel approach to localize faults based only on the stack trace information and no additional runtime information, by fine-tuning large language models (LLMs). We address complex cases where the root cause of a crash differs from the technical cause, and is not located in the innermost frame of the stack trace. As the number of historic crashes is insufficient to fine-tune LLMs, we augment our dataset by leveraging code mutators to inject synthetic crashes into the code base.
By fine-tuning on 64,369 crashes resulting from 4.1 million mutations of the HANA code base, we can correctly predict the root cause location of a crash with an accuracy of 66.9% while baselines only achieve 12.6% and 10.6%.
We substantiate the generalizability of our approach by evaluating on two additional open-source databases, SQLite and DuckDB, achieving accuracies of 63% and 74%, respectively. Across all our experiments, fine-tuning consistently outperformed prompting non-finetuned LLMs for localizing faults in our datasets."
1385,679d459debd8ffd557a2b3d6,cs.LG,https://arxiv.org/pdf/2501.17992,Reinforcement-Learning Portfolio Allocation with Dynamic Embedding of Market Information,"Jinghai He, Cheng Hua, Chunyang Zhou, Zeyu Zheng","Portfolio Management, Machine Learning",
1386,679d459debd8ffd557a2b3d7,cs.LG,https://arxiv.org/pdf/2501.17904,A Robust Support Vector Machine Approach for Raman COVID-19 Data Classification,"Marco Piazza, Andrea Spinelli, Francesca Maggioni, Marzia Bedoni, Enza Messina","Quantitative Methods, Machine Learning","Recent advances in healthcare technologies have led to the availability of large amounts of biological samples across several techniques and applications. In particular, in the last few years, Raman spectroscopy analysis of biological samples has been successfully applied for early-stage diagnosis. However, spectra’ inherent complexity and variability make the manual analysis challenging, even for domain experts. For the same reason, the use of traditional Statistical andMachine Learning(ML) techniques could not guarantee for accurate and reliable results. ML models, combined with robust optimization techniques, offer the possibility to improve the classification accuracy and enhance the resilience of predictive models. In this paper, we investigate the performance of a novel robust formulation for Support Vector Machine (SVM) in classifying COVID-19 samples obtained from Raman Spectroscopy. Given the noisy and perturbed nature of biological samples, we protect the classification process against uncertainty through the application of robust optimization techniques. Specifically, we derive robust counterpart models of deterministic formulations using bounded-by-norm uncertainty sets around each observation. We explore the cases of both linear and kernel-induced classifiers to address binary and multiclass classification tasks. The effectiveness of our approach is validated on real-world COVID-19 datasets provided by Italian hospitals by comparing the results of our simulations with a state-of-the-art classifier."
1387,679d459debd8ffd557a2b3d8,cs.LG,https://arxiv.org/pdf/2501.17901,Molecular Fingerprints Are Strong Models for Peptide Function Prediction,"Jakub Adamczyk, Piotr Ludynia, Wojciech Czech","Biomolecules, Machine Learning","We study the effectiveness of molecular fingerprints for peptide property prediction and demonstrate that domain-specific feature extraction from molecular graphs can outperform complex and computationally expensive models such as GNNs, pretrained sequence-based transformers and multimodal ensembles, even without hyperparameter tuning. To this end, we perform a thorough evaluation on 126 datasets, achieving state-of-the-art results on LRGB and 5 other peptide function prediction benchmarks. We show that models based on count variants of ECFP, Topological Torsion, and RDKit molecular fingerprints and LightGBM as classification head are remarkably robust. The strong performance of molecular fingerprints, which are intrinsically very short-range feature encoders, challenges the presumed importance of long-range interactions in peptides. Our conclusion is that the use of molecular fingerprints for larger molecules, such as peptides, can be a computationally feasible, low-parameter, and versatile alternative to sophisticated deep learning models."
1388,679d459debd8ffd557a2b3d9,cs.LG,https://arxiv.org/pdf/2501.17898,Distilling Knowledge for Designing Computational Imaging Systems,"Leon Suarez-Rodriguez, Roman Jacome, Henry Arguello","Image and Video Processing, Machine Learning","Designing the physical encoder is crucial for accurate image reconstruction in computational imaging (CI) systems. Currently, these systems are designed via end-to-end (E2E) optimization, where the encoder is modeled as a neural network layer and is jointly optimized with the decoder. However, the performance of E2E optimization is significantly reduced by the physical constraints imposed on the encoder. Additionally, since the E2E learns the parameters of the encoder by backpropagating the reconstruction error, it does not promote optimal intermediate outputs and suffers from gradient vanishing. To address these limitations, we reinterpret the concept of knowledge distillation (KD) for designing a physically constrained CI system by transferring the knowledge of a pretrained, less-constrained CI system. Our approach involves three steps: (1) Given the original CI system (student), a teacher system is created by relaxing the constraints on the student’s encoder. (2) The teacher is optimized to solve a less-constrained version of the student’s problem. (3) The teacher guides the training of the student through two proposed knowledge transfer functions, targeting both the encoder and the decoder feature space. The proposed method can be employed to any imaging modality since the relaxation scheme and the loss functions can be adapted according to the physical acquisition and the employed decoder. This approach was validated on three representative CI modalities: magnetic resonance, single-pixel, and compressive spectral imaging. Simulations show that a teacher system with an encoder that has a structure similar to that of the student encoder provides effective guidance. Our approach achieves significantly improved reconstruction performance and encoder design, outperforming both E2E optimization and traditional non-data-driven encoder designs."
1389,679d459debd8ffd557a2b3da,cs.LG,https://arxiv.org/pdf/2501.17893,Language Modelling for Speaker Diarization in Telephonic Interviews,"Miquel India, Javier Hernando, José A.R. Fonollosa","Audio and Speech Processing, Machine Learning, Sound","The aim of this paper is to investigate the benefit of combining both language and acoustic modelling for speaker diarization. Although conventional systems only use acoustic features, in some scenarios linguistic data contain high discriminative speaker information, even more reliable than the acoustic ones. In this study we analyze how an appropriate fusion of both kind of features is able to obtain good results in these cases. The proposed system is based on an iterative algorithm where a LSTM network is used as a speaker classifier. The network is fed with character-level word embeddings and a GMM based acoustic score created with the output labels from previous iterations. The presented algorithm has been evaluated in a Call-Center database, which is composed of telephone interview audios. The combination of acoustic features and linguistic content shows a 84.29% improvement in terms of a word-level DER as compared to a HMM/VB baseline system. The results of this study confirms that linguistic content can be efficiently used for some speaker recognition tasks."
1390,679d459debd8ffd557a2b3db,cs.LG,https://arxiv.org/pdf/2501.17882,Heterogeneous Multi-Player Multi-Armed Bandits Robust To Adversarial Attacks,"Akshayaa Magesh, Venugopal V. Veeravalli","Machine Learning, Machine Learning","We consider a multi-player multi-armed bandit setting in the presence of adversaries that attempt to negatively affect the rewards received by the players in the system. The reward distributions for any given arm are heterogeneous across the players. In the event of a collision (more than one player choosing the same arm), all the colliding users receive zero rewards. The adversaries use collisions to affect the rewards received by the players, i.e., if an adversary attacks an arm, any player choosing that arm will receive zero reward. At any time step, the adversaries may attack more than one arm. It is assumed that the players in the system do not deviate from
a pre-determined policy used by all the players, and that the probability that none of the arms face adversarial attacks is strictly positive at every time step. In order to combat the adversarial attacks, the players are allowed to communicate using a single bit forO⁢(log⁡T)𝑂𝑇O(\log T)italic_O ( roman_log italic_T )time units, whereT𝑇Titalic_Tis the time horizon, and each player can only observe their own actions and rewards at all time steps. We propose a policy that is used by all the players, which achieves near order optimal regret of orderO⁢(log1+δ⁡T+W)𝑂superscript1𝛿𝑇𝑊O(\log^{1+\delta}T+W)italic_O ( roman_log start_POSTSUPERSCRIPT 1 + italic_δ end_POSTSUPERSCRIPT italic_T + italic_W ), whereW𝑊Witalic_Wis total number of time units for which there was an adversarial attack on at least one arm."
1391,679d459debd8ffd557a2b3dc,cs.LG,https://arxiv.org/pdf/2501.17878,Performance Analysis of NR Sidelink and Wi-Fi Coexistence Networks in Unlicensed Spectrum,"Zhuangzhuang Yan, Xinyu Gu, Zhenyu Liu","Signal Processing, Machine Learning","With the rapid development of various internet of things (IoT) applications, including industrial IoT (IIoT) and visual IoT (VIoT), the demand for direct device-to-device communication to support high data rates continues to grow. To address this demand, 5G-Advanced has introduced sidelink communication over the unlicensed spectrum (SL-U) as a method to increase data rates. However, the primary challenge of SL-U in the unlicensed spectrum is ensuring fair coexistence with other incumbent systems, such as Wi-Fi. In this paper, we address the challenge by designing channel access mechanisms and power control strategies to mitigate interference and ensure fair coexistence. First, we propose a novel collaborative channel access (CCHA) mechanism that integrates channel access with resource allocation through collaborative interactions between base stations (BS) and SL-U users. This mechanism ensures fair coexistence with incumbent systems while improving resource utilization. Second, we mathematically model the joint channel access and power control problems, analyzing the trade-off between fairness and transmission rate to minimize interference and optimize performance in the coexistence system. Finally, we develop a collaborative subgoal-based hierarchical deep reinforcement learning (C-GHDRL) framework. This framework enables SL-U users to make globally optimal decisions by leveraging collaborative operations between the BS and SL-U users, effectively overcoming the limitations of traditional optimization methods in solving joint optimization problems with nonlinear constraints. Simulation results demonstrate that the proposed scheme significantly enhances the coexistence system’s performance while ensuring fair coexistence between SL-U and Wi-Fi users."
1392,679d459debd8ffd557a2b3dd,cs.LG,https://arxiv.org/pdf/2501.17871,On the challenges of detecting MCI using EEG in the wild,"Aayush Mishra, David Joffe, Sankara Surendra Telidevara, David S Oakley, Anqi Liu","Signal Processing, Machine Learning","Recent studies have shown promising results in the detection of Mild Cognitive Impairment (MCI) using easily accessible Electroencephalogram (EEG) data which would help administer early and effective treatment for dementia patients. However, the reliability and practicality of such systems remains unclear.
In this work, we investigate the potential limitations and challenges in developing a robust MCI detection method using
two contrasting datasets: 1) CAUEEG, collected and annotated by expert neurologists in controlled settings and 2) GENEEG, a new dataset collected and annotated in general practice clinics, a setting where routine MCI diagnoses are typically made. We find that training on small datasets, as is done by most previous works, tends to produce high variance models that make overconfident predictions, and are unreliable in practice. Additionally, distribution shifts between datasets make cross-domain generalization challenging.
Finally, we show that MCI detection using EEG may suffer from fundamental limitations because of the overlapping nature of feature distributions with control groups.
We call for more effort in high-quality data collection in actionable settings (like general practice clinics) to make progress towards this salient goal of non-invasive MCI detection."
1393,679d459debd8ffd557a2b3de,cs.LG,https://arxiv.org/pdf/2501.17859,rEGGression: an Interactive and Agnostic Tool for the Exploration of Symbolic Regression Models,"Fabricio Olivetti de Franca, Gabriel Kronberger",Machine Learning,"Regression analysis is used for prediction and to understand the effect of independent variables on dependent variables.
Symbolic regression (SR) automates the search for non-linear regression models, delivering a set of hypotheses that balances accuracy with the possibility to understand the phenomena. Many SR implementations return a Pareto front allowing the choice of the best trade-off. However, this hides alternatives that are close to non-domination, limiting these choices. Equality graphs (e-graphs) allow to represent large sets of expressions compactly by efficiently handling duplicated parts occurring in multiple expressions. E-graphs allow to store and query all SR solution candidates visited in one or multiple GP runs efficiently and open the possibility to analyse much larger sets of SR solution candidates.
We introduce rEGGression, a tool using e-graphs to enable the exploration of a large set of symbolic expressions which provides querying, filtering, and pattern matching features creating an interactive experience to gain insights about SR models. The main highlight is its focus in the exploration of the building blocks found during the search that can help the experts to find insights about the studied phenomena.This is possible by exploiting the pattern matching capability of the e-graph data structure."
1394,679d459debd8ffd557a2b3df,cs.LG,https://arxiv.org/pdf/2501.17848,Improving Genetic Programming for Symbolic Regression with Equality Graphs,"Fabricio Olivetti de Franca, Gabriel Kronberger",Machine Learning,"The search for symbolic regression models with genetic programming (GP) has a tendency of revisiting expressions in their original or equivalent forms. Repeatedly evaluating equivalent expressions is inefficient, as it does not immediately lead to better solutions.
However, evolutionary algorithms require diversity and should allow the accumulation of inactive building blocks that can play an important role at a later point.
The equality graph is a data structure capable of compactly storing expressions and their equivalent forms allowing an efficient verification of whether an expression has been visited in any of their stored equivalent forms.
We exploit the e-graph to adapt the subtree operators to reduce the chances of revisiting expressions. Our adaptation, called eggp, stores every visited expression in the e-graph, allowing us to filter out from the available selection of subtrees all the combinations that would create already visited expressions.
Results show that, for small expressions, this approach improves the performance of a simple GP algorithm to compete with PySR and Operon without increasing computational cost. As a highlight, eggp was capable of reliably delivering short and at the same time accurate models for a selected set of benchmarks from SRBench and a set of real-world datasets."
1395,679d459debd8ffd557a2b3e0,cs.LG,https://arxiv.org/pdf/2501.17834,Hierarchical Fallback Architecture for High Risk Online Machine Learning Inference,"Gustavo Polleti, Marlesson Santana, Felipe Sassi Del Sant, Eduardo Fontes","Machine Learning, Computational Engineering, Finance, and Science, Software Engineering","Open Banking powered machine learning applications require novel robustness approaches to deal with challenging stress and failure scenarios. In this paper we propose an hierarchical fallback architecture for improving robustness in high risk machine learning applications with a focus in the financial domain. We define generic failure scenarios often found in online inference that depend on external data providers and we describe in detail how to apply the hierarchical fallback architecture to address them. Finally, we offer a real world example of its applicability in the industry for near-real time transactional fraud risk evaluation using Open Banking data and under extreme stress scenarios."
1396,679d459debd8ffd557a2b3e1,cs.LG,https://arxiv.org/pdf/2501.17827,Langevin Soft Actor-Critic: Efficient Exploration through Uncertainty-Driven Critic Learning,"Haque Ishfaq, Guangyuan Wang, Sami Nur Islam, Doina Precup",Machine Learning,
1397,679d459debd8ffd557a2b3e2,cs.LG,https://arxiv.org/pdf/2501.17802,LEKA:LLM-Enhanced Knowledge Augmentation,"Xinhao Zhang, Jinghan Zhang, Fengran Mo, Dongjie Wang, Yanjie Fu, Kunpeng Liu",Machine Learning,"Humans excel in analogical learning and knowledge transfer and, more importantly, possess a unique understanding of identifying appropriate sources of knowledge. From a model’s perspective, this presents an interesting challenge. If models could autonomously retrieve knowledge useful for transfer or decision-making to solve problems, they would transition from passively acquiring to actively accessing and learning from knowledge. However, filling models with knowledge is relatively straightforward—it simply requires more training and accessible knowledge bases. The more complex task is teaching models about which knowledge can be analogized and transferred. Therefore, we design a knowledge augmentation method LEKA for knowledge transfer that actively searches for suitable knowledge sources that can enrich the target domain’s knowledge. This LEKA method extracts key information from textual information from the target domain, retrieves pertinent data from external data libraries, and harmonizes retrieved data with the target domain data in feature space and marginal probability measures. We validate the effectiveness of our approach through extensive experiments across various domains and demonstrate significant improvements over traditional methods in reducing computational costs, automating data alignment, and optimizing transfer learning outcomes."
1398,679d459debd8ffd557a2b3e3,cs.LG,https://arxiv.org/pdf/2501.17787,Detecting Anomalies Using Rotated Isolation Forest,"Vahideh Monemizadeh, Kourosh Kiani",Machine Learning,"The Isolation Forest (iForest), proposed by Liu, Ting, and Zhou at TKDE 2012, has become a prominent tool for unsupervised anomaly detection.
However, recent research by Hariri, Kind, and Brunner, published in TKDE 2021, has revealed issues with iForest. They identified the presence of axis-aligned ghost clusters that can be misidentified as normal clusters, leading to biased anomaly scores and inaccurate predictions.
In response, they developed the Extended Isolation Forest (EIF), which effectively solves these issues by eliminating the ghost clusters introduced by iForest. This enhancement results in improved consistency of anomaly scores and superior performance. We reveal a previously overlooked problem in the Extended Isolation Forest (EIF), showing that it is vulnerable to
ghost inter-clusters between normal clusters of data points.
In this paper, we introduce the Rotated Isolation Forest (RIF) algorithm which effectively addresses both the axis-aligned ghost clusters observed in iForest and the ghost inter-clusters seen in EIF. RIF accomplishes this by randomly rotating the dataset (usingrandom rotation matricesandQR decomposition) before feeding it into the iForest construction, thereby increasing dataset variation and eliminating ghost clusters.
Our experiments conclusively demonstrate that the RIF algorithm outperforms iForest and EIF, as evidenced by the results obtained from both synthetic datasets and real-world datasets."
1399,679d459debd8ffd557a2b3e4,cs.LG,https://arxiv.org/pdf/2501.17784,AdditiveLLM: Large Language Models Predict Defects in Additive Manufacturing,"Peter Pak, Amir Barati Farimani",Machine Learning,"In this work we investigate the ability of large language models to predict
additive manufacturing defect regimes given a set of process parameter inputs.
For this task we utilize a process parameter defect dataset to fine-tune a
collection of models, titledAdditiveLLM, for the purpose of predicting
potential defect regimes includingKeyholing,Lack of Fusion,
andBalling. We compare different methods of input formatting in order
to gauge the model’s performance to correctly predict defect regimes on our
sparseBaselinedataset and our natural languagePromptdataset. The model displays robust predictive capability, achieving an accuracy
of 93% when asked to provide the defect regimes associated with a set of
process parameters. The incorporation of natural language input further
simplifies the task of process parameters selection, enabling users to identify
optimal settings specific to their build."
1400,679d459debd8ffd557a2b3e5,cs.LG,https://arxiv.org/pdf/2501.17782,Picard-KKT-hPINN: Enforcing Nonlinear Enthalpy Balances for Physically Consistent Neural Networks,"Giacomo Lastrucci, Tanuj Karia, Zoë Gromotka, Artur M. Schweidtmann",Machine Learning,"Neural networks (NNs) are widely used as surrogate models but they do not guarantee physically consistent predictions thereby preventing adoption in various applications. We propose a method that can enforce NNs to satisfy physical laws that are nonlinear in nature such as enthalpy balances. Our approach, inspired by Picard’s successive approximations method, aims to enforce multiplicatively separable constraints by sequentiallyfreezingand projecting a set of the participating variables. We demonstrate our Picard-KKT-hPINN for surrogate modeling of a catalytic packed bed reactor for methanol synthesis. Our results show that the method efficiently enforces nonlinear enthalpy and linear atomic balances at machine-level precision. Additionally, we show that enforcing conservation laws can improve accuracy in data-scarce conditions compared tovanillamultilayer perceptron."
1401,679d459debd8ffd557a2b3e6,cs.LG,https://arxiv.org/pdf/2501.17770,Generative Unordered Flow for Set-Structured Data Generation,"Yangming Li, Carola-Bibiane Schönlieb",Machine Learning,"Flow-based generative models have demonstrated promising performance across a broad spectrum of data modalities (e.g., image and text). However, there are few works exploring their extension to unordered data (e.g., spatial point set), which is not trivial because previous models are mostly designed for vector data that are naturally ordered. In this paper, we presentunordered flow, a type of flow-based generative model for set-structured data generation. Specifically, we convert unordered data into an appropriate function representation, and learn the probability measure of such representations through function-valued flow matching. For the inverse map from a function representation to unordered data, we propose a method similar to particle filtering, with Langevin dynamics to first warm-up the initial particles and gradient-based search to update them until convergence. We have conducted extensive experiments on multiple real-world datasets, showing that ourunordered flowmodel is very effective in generating set-structured data and significantly outperforms previous baselines."
1402,679d459debd8ffd557a2b3e7,cs.LG,https://arxiv.org/pdf/2501.17745,Dynamics of Transient Structure in In-Context Linear Regression Transformers,"Liam Carroll, Jesse Hoogland, Matthew Farrugia-Roberts, Daniel Murfet",Machine Learning,"Modern deep neural networks display striking examples of rich internal computational structure. Uncovering principles governing the development of such structure is a priority for the science of deep learning. In this paper, we explore the transient ridge phenomenon: when transformers are trained on in-context linear regression tasks with intermediate task diversity, they initially behave like ridge regression before specializing to the tasks in their training distribution. This transition from a general solution to a specialized solution is revealed by joint trajectory principal component analysis. Further, we draw on the theory of Bayesian internal model selection to suggest a general explanation for the phenomena of transient structure in transformers, based on an evolving tradeoff between loss and complexity. This explanation is grounded in empirical measurements of model complexity using the local learning coefficient."
1403,679d459debd8ffd557a2b3e8,cs.LG,https://arxiv.org/pdf/2501.17737,"Sparser, Better, Faster, Stronger: Efficient Automatic Differentiation for Sparse Jacobians and Hessians","Adrian Hill, Guillaume Dalle","Machine Learning, Mathematical Software","From implicit differentiation to probabilistic modeling, Jacobians and Hessians have many potential use cases in Machine Learning (ML), but conventional wisdom views them as computationally prohibitive.
Fortunately, these matrices often exhibit sparsity, which can be leveraged to significantly speed up the process of Automatic Differentiation (AD).
This paper presents advances in Automatic Sparse Differentiation (ASD),
starting with a new perspective on sparsity detection.
Our refreshed exposition is based on operator overloading, able to detect both local and global sparsity patterns, and naturally avoids dead ends in the control flow graph.
We also describe a novel ASD pipeline in Julia,
consisting of independent software packages for sparsity detection, matrix coloring, and differentiation,
which together enable ASD based on arbitrary AD backends.
Our pipeline is fully automatic and requires no modification of existing code,
making it compatible with existing ML codebases.
We demonstrate that this pipeline unlocks Jacobian and Hessian matrices at scales where they were considered too expensive to compute.
On real-world problems from scientific ML and optimization,
we show significant speed-ups of up to three orders of magnitude.
Notably, our ASD pipeline often outperforms standard AD for one-off computations, once thought impractical due to slower sparsity detection methods."
1404,679d459debd8ffd557a2b3e9,cs.LG,https://arxiv.org/pdf/2501.17727,Sparse Autoencoders Can Interpret Randomly Initialized Transformers,"Thomas Heap, Tim Lawson, Lucy Farnik, Laurence Aitchison",Machine Learning,"Sparse autoencoders (SAEs) are an increasingly popular technique for interpreting the internal representations of transformers.
In this paper, we apply SAEs to ‘interpret’ random transformers, i.e., transformers where the parameters are sampled IID from a Gaussian rather than trained on text data.
We find that random and trained transformers produce similarly interpretable SAE latents, and we confirm this finding quantitatively using an open-source auto-interpretability pipeline.
Further, we find that SAE quality metrics are broadly similar for random and trained transformers.
We find that these results hold across model sizes and layers.
We discuss a number of number interesting questions that this work raises for the use of SAEs and auto-interpretability in the context of mechanistic interpretability."
1405,679d459debd8ffd557a2b3ea,cs.LG,https://arxiv.org/pdf/2501.17711,STGCN-LSTM for Olympic Medal Prediction: Dynamic Power Modeling and Causal Policy Optimization,"Yiquan Wang, Jiaying Wang, Jingyi Yang, Zihao Xu",Machine Learning,"This paper proposes a novel hybrid model, STGCN-LSTM, to forecast Olympic medal distributions by integrating the spatio-temporal relationships among countries and the long-term dependencies of national performance. The Spatial-Temporal Graph Convolution Network (STGCN) captures geographic and interactive factors-such as coaching exchange and socio-economic links-while the Long Short-Term Memory (LSTM) module models historical trends in medal counts, economic data, and demographics. To address zero-inflated outputs (i.e., the disparity between countries that consistently yield wins and those never having won medals), a Zero-Inflated Compound Poisson (ZICP) framework is incorporated to separate random zeros from structural zeros, providing a clearer view of potential breakthrough performances. Validation includes historical backtracking, policy shock simulations, and causal inference checks, confirming the robustness of the proposed method. Results shed light on the influence of coaching mobility, event specialization, and strategic investment on medal forecasts, offering a data-driven foundation for optimizing sports policies and resource allocation in diverse Olympic contexts."
1406,679d459debd8ffd557a2b3eb,cs.LG,https://arxiv.org/pdf/2501.17683,Temperature-Free Loss Function for Contrastive Learning,"Bum Jun Kim, Sang Woo Kim",Machine Learning,"As one of the most promising methods in self-supervised learning, contrastive learning has achieved a series of breakthroughs across numerous fields. A predominant approach to implementing contrastive learning is applying InfoNCE loss: By capturing the similarities between pairs, InfoNCE loss enables learning the representation of data. Albeit its success, adopting InfoNCE loss requires tuning a temperature, which is a core hyperparameter for calibrating similarity scores. Despite its significance and sensitivity to performance being emphasized by several studies, searching for a valid temperature requires extensive trial-and-error-based experiments, which increases the difficulty of adopting InfoNCE loss. To address this difficulty, we propose a novel method to deploy InfoNCE loss without temperature. Specifically, we replace temperature scaling with the inverse hyperbolic tangent function, resulting in a modified InfoNCE loss. In addition to hyperparameter-free deployment, we observed that the proposed method even yielded a performance gain in contrastive learning. Our detailed theoretical analysis discovers that the current practice of temperature scaling in InfoNCE loss causes serious problems in gradient descent, whereas our method provides desirable gradient properties. The proposed method was validated on five benchmarks on contrastive learning, yielding satisfactory results without temperature tuning."
1407,679d459debd8ffd557a2b3ec,cs.LG,https://arxiv.org/pdf/2501.17676,Explainable Artificial Intelligence for identifying profitability predictors in Financial Statements,"Marco Piazza, Mauro Passacantando, Francesca Magli, Federica Doni, Andrea Amaduzzi, Enza Messina",Machine Learning,"The interconnected nature of the economic variables influencing a firm’s performance makes the prediction of a company’s earning trend a challenging task.
Existing methodologies often rely on simplistic models and financial ratios failing to capture the complexity of interacting influences.
In this paper, we apply Machine Learning techniques to raw financial statements data taken from AIDA, a Database comprising Italian listed companies’ data from 2013 to 2022."
1408,679d459debd8ffd557a2b3ed,cs.LG,https://arxiv.org/pdf/2501.17663,Landscape Features in Single-Objective Continuous Optimization: Have We Hit a Wall in Algorithm Selection Generalization?,"Gjorgjina Cenikj, Gašper Petelin, Moritz Seiler, Nikola Cenikj, Tome Eftimov",Machine Learning,"The process of identifying the most suitable optimization algorithm for a specific problem, referred to as algorithm selection (AS), entails training models that leverage problem landscape features to forecast algorithm performance. A significant challenge in this domain is ensuring that AS models can generalize effectively to novel, unseen problems. This study evaluates the generalizability of AS models based on different problem representations in the context of single-objective continuous optimization. In particular, it considers the most widely used Exploratory Landscape Analysis features, as well as recently proposed Topological Landscape Analysis features, and features based on deep learning, such as DeepELA, TransOptAS and Doe2Vec. Our results indicate that when presented with out-of-distribution evaluation data, none of the feature-based AS models outperform a simple baseline model, i.e., a Single Best Solver."
1409,679d459debd8ffd557a2b3ee,cs.LG,https://arxiv.org/pdf/2501.17653,Drivetrain simulation using variational autoencoders,"Pallavi Sharma, Jorge-Humberto Urrea-Quintero, Bogdan Bogdan, Adrian-Dumitru Ciotec, Laura Vasilie, Henning Wessels, Matteo Skull","Machine Learning, Computational Engineering, Finance, and Science, Signal Processing","This work proposes variational autoencoders (VAEs) to predict a vehicle’s jerk from a given torque demand, addressing the limitations of sparse real-world datasets. Specifically, we implement unconditional and conditional VAEs to generate jerk signals that integrate features from different drivetrain scenarios. The VAEs are trained on experimental data collected from two variants of a fully electric SUV, which differ in maximum torque delivery and drivetrain configuration.
New meaningful jerk signals are generated within an engineering context through the interpretation of the VAE’s latent space. A performance comparison with baseline physics-based and hybrid models confirms the effectiveness of the VAEs. We show that VAEs bypass the need for exhaustive manual system parametrization while maintaining physical plausibility by conditioning data generation on specific inputs."
1410,679d459debd8ffd557a2b3ef,cs.LG,https://arxiv.org/pdf/2501.17604,nabqr: Python package for improving probabilistic forecasts,"Bastian Schmidt Jørgensena, Jan Kloppenborg Møller, Peter Nystrup, Henrik Madsen","Machine Learning, Applications, Computation","We introduce the open-source Python package NABQR: Neural Adaptive Basis for (time-adaptive) Quantile Regression that provides reliable probabilistic forecasts.
NABQR corrects ensembles (scenarios) with LSTM networks and then applies time-adaptive quantile regression to the corrected ensembles to obtain improved and more reliable forecasts.
With the suggested package, accuracy improvements of up to 40% in mean absolute terms can be achieved in day-ahead forecasting of onshore and offshore wind power production in Denmark."
1411,679d459debd8ffd557a2b3f0,cs.LG,https://arxiv.org/pdf/2501.17599,RegionGCN: Spatial-Heterogeneity-Aware Graph Convolutional Networks,"Hao Guo, Han Wang, Di Zhu, Lun Wu, A. Stewart Fotheringham, Yu Liu",Machine Learning,
1412,679d459debd8ffd557a2b3f1,cs.LG,https://arxiv.org/pdf/2501.17568,Histogram approaches for imbalanced data streams regression,"Ehsan Aminian, Joao Gama, Rita P. Ribeiro",Machine Learning,"Handling imbalanced data streams in regression tasks presents a significant challenge, as rare instances can appear anywhere in the target distribution rather than being confined to its extreme values. In this paper, we introduce novel data-level sampling strategies, HistUS and HistOS, that utilize histogram-based approaches to dynamically balance data streams. Unlike previous methods based on Chebyshev’s inequality, our proposed techniques identify and handle rare cases across the entire distribution effectively. We demonstrate that HistUS and HistOS outperform traditional methods through extensive experiments on synthetic and real-world datasets, leading to more accurate and robust regression models in streaming environments."
1413,679d459debd8ffd557a2b3f2,cs.LG,https://arxiv.org/pdf/2501.17557,Heuristic-Informed Mixture of Experts for Link Prediction in Multilayer Networks,"Lucio La Cava, Domenico Mandaglio, Lorenzo Zangari, Andrea Tagarelli","Machine Learning, Social and Information Networks, Physics and Society",
1414,679d459debd8ffd557a2b3f3,cs.LG,https://arxiv.org/pdf/2501.17553,Closing the Gap Between Synthetic and Ground Truth Time Series Distributions via Neural Mapping,"Daesoo Lee, Sara Malacarne, Erlend Aune","Machine Learning, Machine Learning","In this paper, we introduce Neural Mapper for Vector Quantized Time Series Generator (NM-VQTSG), a novel method aimed at addressing fidelity challenges in vector quantized (VQ) time series generation. VQ-based methods, such as TimeVQVAE, have demonstrated success in generating time series but are hindered by two critical bottlenecks: information loss during compression into discrete latent spaces and deviations in the learned prior distribution from the ground truth distribution. These challenges result in synthetic time series with compromised fidelity and distributional accuracy.
To overcome these limitations, NM-VQTSG leverages a U-Net-based neural mapping model to bridge the distributional gap between synthetic and ground truth time series. To be more specific, the model refines synthetic data by addressing artifacts introduced during generation, effectively aligning the distributions of synthetic and real data. Importantly, NM-VQTSG can be used for synthetic time series generated by any VQ-based generative method.
We evaluate NM-VQTSG across diverse datasets from the UCR Time Series Classification archive, demonstrating its capability to consistently enhance fidelity in both unconditional and conditional generation tasks. The improvements are evidenced by significant improvements in FID, IS, and conditional FID, additionally backed up by visual inspection in a data space and a latent space.
Our findings establish NM-VQTSG as a new method to improve the quality of synthetic time series.
Our implementation is available onhttps://github.com/ML4ITS/TimeVQVAE."
1415,679d459debd8ffd557a2b3f4,cs.LG,https://arxiv.org/pdf/2501.17450,NF-MKV Net: A Constraint-Preserving Neural Network Approach to Solving Mean-Field Games Equilibrium,"Jinwei Liu, Lu Ren, Wang Yao, Xiao Zhang",Machine Learning,"Neural network-based methods for solving Mean-Field Games (MFGs) equilibria have garnered significant attention for their effectiveness in high-dimensional problems. However, many algorithms struggle with ensuring that the evolution of the density distribution adheres to the required mathematical constraints. This paper investigates a neural network approach to solving MFGs equilibria through a stochastic process perspective. It integrates process-regularized Normalizing Flow (NF) frameworks with state-policy-connected time-series neural networks to address McKean-Vlasov-type Forward-Backward Stochastic Differential Equation (MKV FBSDE) fixed-point problems, equivalent to MFGs equilibria. First, we reformulate MFGs equilibria as MKV FBSDEs, embedding the density distribution into the equation coefficients within a probabilistic framework. Neural networks are then designed to approximate value functions and gradients derived from these equations. Second, we employ NF architectures, a class of generative neural network models, and impose loss constraints on each density transfer function to ensure volumetric invariance and time continuity. Additionally, this paper presents theoretical proofs of the algorithm’s validity and demonstrates its applicability across diverse scenarios, highlighting its superior effectiveness compared to existing methods."
1416,679d459debd8ffd557a2b3f5,cs.LG,https://arxiv.org/pdf/2501.17443,Gradual Domain Adaptation for Graph Learning,"Pui Ieng Lei, Ximing Chen, Yijun Sheng, Yanyan Liu, Jingzhi Guo, Zhiguo Gong",Machine Learning,"Existing literature lacks a graph domain adaptation technique for handling large distribution shifts, primarily due to the difficulty in simulating an evolving path from source to target graph. To make a breakthrough, we present agraph gradual domain adaptation(GGDA) framework with the construction of a compact domain sequence that minimizes information loss in adaptations.
Our approach starts with an efficient generation of knowledge-preserving intermediate graphs over the Fused Gromov-Wasserstein (FGW) metric. With the bridging data pool, GGDA domains are then constructed via a novel vertex-based domain progression, which comprises ”close” vertex selections and adaptive domain advancement to enhance inter-domain information transferability. Theoretically, our framework concretizes the intractable inter-domain distanceWp⁢(μt,μt+1)subscript𝑊𝑝subscript𝜇𝑡subscript𝜇𝑡1W_{p}(\mu_{t},\mu_{t+1})italic_W start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_μ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_μ start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT )via implementable upper and lower bounds, enabling flexible adjustments of this metric for optimizing domain formation. Extensive experiments under various transfer scenarios validate the superior performance of our GGDA framework."
1417,679d459debd8ffd557a2b3f6,cs.LG,https://arxiv.org/pdf/2501.17431,Human-Aligned Skill Discovery: Balancing Behaviour Exploration and Alignment,"Maxence Hussonnois, Thommen George Karimpanal, Santu Rana","Machine Learning, Robotics","This document outlines the formatting instructions for submissions to
AAMAS-2025. You can use its source file as a template when writing
your own paper. It is based on the file ‘sample-sigconf.tex’
distributed with the ACM article template forLaTeX."
1418,679d459debd8ffd557a2b3f7,cs.LG,https://arxiv.org/pdf/2501.17428,WCDT: Systematic WCET Optimization for Decision Tree Implementations,"Nils Hölscher, Christian Hakert, Georg von der Brüggen, Jian-Jia Chen, Kuan-Hsun Chen, Jan Reineke","Machine Learning, Performance","Machine-learning models are increasingly deployed on resource-constrained embedded systems with strict timing constraints.
In such scenarios, the worst-case execution time (WCET) of the models
is required to ensure safe operation. Specifically, decision trees are a prominent class of machine-learning models and the main building blocks of tree-based ensemble models (e.g., random forests), which are commonly employed in resource-constrained embedded systems."
1419,679d459debd8ffd557a2b3f8,cs.LG,https://arxiv.org/pdf/2501.17415,si4onnx: A Python package for Selective Inference in Deep Learning Models,"Teruyuki Katsuoka, Tomohiro Shiraishi, Daiki Miwa, Shuichi Nishino, Ichiro Takeuchi","Machine Learning, Machine Learning",
1420,679d459debd8ffd557a2b3f9,cs.LG,https://arxiv.org/pdf/2501.17372,Data-Informed Model Complexity Metric for Optimizing Symbolic Regression Models,"Nathan Haut, Zenas Huang, Adam Alessio","Machine Learning, Neural and Evolutionary Computing","Choosing models from a well-fitted evolved population that generalizes beyond training data is difficult. We introduce a pragmatic method to estimate model complexity using Hessian rank for post-processing selection. Complexity is approximated by averaging the model output Hessian rank across a few points (N=3), offering efficient and accurate rank estimates. This method aligns model selection with input data complexity, calculated using intrinsic dimensionality (ID) estimators. Using the StackGP system, we develop symbolic regression models for the Penn Machine Learning Benchmark and employ twelve scikit-dimension library methods to estimate ID, aligning model expressiveness with dataset ID. Our data-informed complexity metric finds the ideal complexity window, balancing model expressiveness and accuracy, enhancing generalizability without bias common in methods reliant on user-defined parameters, such as parsimony pressure in weight selection."
1421,679d459debd8ffd557a2b3fa,cs.LG,https://arxiv.org/pdf/2501.17370,Breaking the $\log(1/\Delta_2)$ Barrier: Better Batched Best Arm Identification with Adaptive Grids,"Tianyuan Jin, Qin Zhang, Dongruo Zhou",Machine Learning,
1422,679d459debd8ffd557a2b3fb,cs.LG,https://arxiv.org/pdf/2501.17324,CardiCat: a Variational Autoencoder for High-Cardinality Tabular Data,"Lee Carlin, Yuval Benjamini","Machine Learning, Machine Learning","High-cardinality categorical features are a common characteristic of mixed-type tabular datasets. Existing generative model architectures struggle to learn the complexities of such data at scale, primarily due to the difficulty of parameterizing the categorical features. In this paper, we present a general variational autoencoder model, CardiCat, that can accurately fit imbalanced high-cardinality and heterogeneous tabular data. Our method substitutes one-hot encoding with regularized dual encoder-decoder embedding layers, which are jointly learned. This approach enables us to use embeddings that depend also on the other covariates, leading to a compact and homogenized parameterization of categorical features. Our model employs a considerably smaller trainable parameter space than competing methods, enabling learning at a large scale. CardiCat generates high-quality synthetic data that better represent high-cardinality and imbalanced features compared to competing VAE models for multiple real and simulated datasets."
1423,679d459debd8ffd557a2b3fc,cs.LG,https://arxiv.org/pdf/2501.17323,Exploring Non-Convex Discrete Energy Landscapes: A Langevin-Like Sampler with Replica Exchange,"Haoyang Zheng, Ruqi Zhang, Guang Lin","Machine Learning, Machine Learning","Gradient-based Discrete Samplers (GDSs) are effective for sampling discrete energy landscapes. However, they often stagnate in complex, non-convex settings. To improve exploration, we introduce the Discrete Replica EXchangE Langevin (DREXEL) sampler and its variant with Adjusted Metropolis (DREAM). These samplers use two GDSs(Zhang et al.,2022b)at different temperatures and step sizes: one focuses on local exploitation, while the other explores broader energy landscapes.
When energy differences are significant, sample swaps occur, which are determined by a mechanism tailored for discrete sampling to ensure detailed balance.
Theoretically, we prove both DREXEL and DREAM converge asymptotically to the target energy and exhibit faster mixing than a single GDS. Experiments further confirm their efficiency in exploring non-convex discrete energy landscapes."
1424,679d459debd8ffd557a2b3fd,cs.LG,https://arxiv.org/pdf/2501.17319,MDDM: A Molecular Dynamics Diffusion Model to Predict Particle Self-Assembly,"Kevin Ferguson, Yu-hsuan Chen, Levent Burak Kara","Machine Learning, Computational Physics","The discovery and study of new material systems relies on molecular simulations that often come with significant computational expense. We propose MDDM, a Molecular Dynamics Diffusion Model, which is capable of predicting a valid output conformation for a given input pair potential function. After training MDDM on a large dataset of molecular dynamics self-assembly results, the proposed model can convert uniform noise into a meaningful output particle structure corresponding to an arbitrary input potential. The model’s architecture has domain-specific properties built-in, such as satisfying periodic boundaries and being invariant to translation. The model significantly outperforms the baseline point-cloud diffusion model for both unconditional and conditional generation tasks."
1425,679d459debd8ffd557a2b3fe,cs.LG,https://arxiv.org/pdf/2501.17284,Nonlinear dynamics of localization in neural receptive fields,"Leon Lufkin, Andrew M. Saxe, Erin Grant",Machine Learning,
1426,679d459debd8ffd557a2b3ff,cs.LG,https://arxiv.org/pdf/2501.17281,Stiff Transfer Learning for Physics-Informed Neural Networks,"Emilien Seiler, Wanzhou Lei, Pavlos Protopapas","Machine Learning, Analysis of PDEs","Stiff differential equations are prevalent in various scientific domains, posing significant challenges due to the disparate time scales of their components. As computational power grows, physics-informed neural networks (PINNs) have led to significant improvements in modeling physical processes described by differential equations. Despite their promising outcomes, vanilla PINNs face limitations when dealing with stiff systems, known as “failure modes”. In response, we propose a novel approach, stiff transfer learning for physics-informed neural networks (STL-PINNs), to effectively tackle stiff ordinary differential equations (ODEs) and partial differential equations (PDEs). Our methodology involves training a Multi-Head-PINN in a low-stiff regime, and obtaining the final solution in a high stiff regime by transfer learning. This addresses the failure modes related to stiffness in PINNs while maintaining computational efficiency by computing “one-shot” solutions. The proposed approach demonstrates superior accuracy and speed compared to PINNs-based methods, as well as comparable computational efficiency with implicit numerical methods in solving stiff-parameterized linear and polynomial nonlinear ODEs and PDEs under stiff conditions. Furthermore, we demonstrate the scalability of such an approach and the superior speed it offers for simulations involving initial conditions and forcing function reparametrization."
1427,679d459debd8ffd557a2b400,cs.LG,https://arxiv.org/pdf/2501.17269,A 1-D CNN inference engine for constrained platforms,"Ishwar Mudraje, Kai Vogelgesang, Thorsten Herfet",Machine Learning,"1D-CNNs are used for time series classification in various domains with a high degree of accuracy.
Most implementations collect the incoming data samples in a buffer before performing inference on it.
On edge devices, which are typically constrained and single-threaded, such an implementation may interfere with time-critical tasks.
One such task is that of sample acquisition.
In this work, we propose an inference scheme that interleaves the convolution operations between sample intervals, which allows us to reduce the inference latency.
Furthermore, our scheme is well-suited for storing data in ring buffers, yielding a small memory footprint.
We demonstrate these improvements by comparing our approach to TFLite’s inference method, giving a 10% reduction in the inference delay while almost halving the memory usage.
Our approach is feasible on common consumer devices, which we show using an AVR-based Arduino board and an ARM-based Arduino board."
1428,679d459debd8ffd557a2b401,cs.LG,https://arxiv.org/pdf/2501.17256,Increasing Information for Model Predictive Control with Semi-Markov Decision Processes,"Rémy Hosseinkhan Boucher, Onofrio Semeraro, Lionel Mathelin",Machine Learning,"Recent works in Learning-Based Model Predictive Control of
dynamical systems show impressive sample complexity performances
using criteria from Information Theory to accelerate
the learning procedure.
However, the sequential exploration opportunities are limited by
the system local state, restraining the amount of information
of the observations from the current exploration trajectory.
This article resolves this limitation by introducing temporal abstraction through the framework
of Semi-Markov Decision Processes.
The framework increases the
total information of the gathered data for a fixed sampling budget, thus reducing the sample complexity."
1429,679d459debd8ffd557a2b402,cs.LG,https://arxiv.org/pdf/2501.17216,Amplifier: Bringing Attention to Neglected Low-Energy Components in Time Series Forecasting,"Jingru Fei, Kun Yi, Wei Fan, Qi Zhang, Zhendong Niu",Machine Learning,"We propose anenergy amplification techniqueto address the issue that existing models easily overlook low-energy components in time series forecasting. This technique comprises anenergy amplification blockand anenergy restoration block. The energy amplification block enhances the energy of low-energy components to improve the model’s learning efficiency for these components, while the energy restoration block returns the energy to its original level. Moreover, considering that the energy-amplified data typically displays two distinct energy peaks in the frequency spectrum, we integrate the energy amplification technique with a seasonal-trend forecaster to model the temporal relationships of these two peaks independently, serving as the backbone for our proposed model,Amplifier. Additionally, we propose asemi-channel interaction temporal relationship enhancement blockfor Amplifier, which enhances the model’s ability to capture temporal relationships from the perspective of thecommonalityandspecificityof each channel in the data. Extensive experiments on eight time series forecasting benchmarks consistently demonstrate our model’s superiority in both effectiveness and efficiency compared to state-of-the-art methods."
1430,679d459debd8ffd557a2b403,cs.LG,https://arxiv.org/pdf/2501.17841,acoupi: An Open-Source Python Framework for Deploying Bioacoustic AI Models on Edge Devices,"Aude Vuilliomenet, Santiago Martínez Balvanera, Oisin Mac Aodha, Kate E. Jones, Duncan Wilson","Sound, Machine Learning, Audio and Speech Processing",Abstract
1431,679d459debd8ffd557a2b404,cs.LG,https://arxiv.org/pdf/2501.17781,Long-term prediction of El Ni\~no-Southern Oscillation using reservoir computing with data-driven realtime filter,"Takuya Jinno, Takahito Mitsui, Kengo Nakai, Yoshitaka Saiki, Tsuyoshi Yoneda","Computational Physics, Machine Learning, Atmospheric and Oceanic Physics","In recent years, the application of machine learning approaches to time-series forecasting of climate dynamical phenomena has become increasingly active. It is known that applying a band-pass filter to a time-series data is a key to obtaining a high-quality data-driven model. Here, to obtain longer-term predictability of machine learning models, we introduce a new type of band-pass filter. It can be applied to realtime operational prediction workflows since it relies solely on past time series. We combine the filter with reservoir computing, which is a machine-learning technique that employs a data-driven dynamical system. As an application, we predict the multi-year dynamics of the El Niño-Southern Oscillation with the prediction horizon of 24 months using only past time series."
1432,679d459debd8ffd557a2b405,cs.LG,https://arxiv.org/pdf/2501.17772,Self-Supervised Frameworks for Speaker Verification via Bootstrapped Positive Sampling,"Theo Lepage, Reda Dehak","Audio and Speech Processing, Machine Learning, Sound","Recent developments in Self-Supervised Learning (SSL) have demonstrated significant potential for Speaker Verification (SV), but closing the performance gap with supervised systems remains an ongoing challenge. Standard SSL frameworks rely on anchor-positive pairs extracted from the same audio utterances. Hence, positives have channel characteristics similar to those of their corresponding anchors, even with extensive data-augmentation. Therefore, this positive sampling strategy is a fundamental limitation as it encodes too much information regarding the recording source in the learned representations. This article introduces Self-Supervised Positive Sampling (SSPS), a bootstrapped technique for sampling appropriate and diverse positives in SSL frameworks for SV. SSPS samples positives close to their anchor in the representation space, as we assume that these pseudo-positives belong to the same speaker identity but correspond to different recording conditions. This method demonstrates consistent improvements in SV performance on VoxCeleb benchmarks when implemented in major SSL frameworks, such as SimCLR, SwAV, VICReg, and DINO. Using SSPS, SimCLR, and DINO achieve 2.57% and 2.53% EER on VoxCeleb1-O. SimCLR yields a 58% relative reduction in EER, getting comparable performance to DINO with a simpler training framework. Furthermore, SSPS lowers intra-class variance and reduces channel information in speaker representations while exhibiting greater robustness without data-augmentation."
1433,679d459debd8ffd557a2b406,cs.LG,https://arxiv.org/pdf/2501.17689,Machine-Learning-Enhanced Optimization of Noise-Resilient Variational Quantum Eigensolvers,"Kim A. Nicoli, Luca J. Wagner, Lena Funcke","Quantum Physics, Machine Learning, High Energy Physics - Lattice","Variational Quantum Eigensolvers (VQEs) are a powerful class of hybrid quantum-classical algorithms designed to approximate the ground state of a quantum system described by its Hamiltonian. VQEs hold promise for various applications, including lattice field theory.
However, the inherent noise of Noisy Intermediate-Scale Quantum (NISQ) devices poses a significant challenge for running VQEs as these algorithms are particularly susceptible to noise, e.g., measurement shot noise and hardware noise."
1434,679d459debd8ffd557a2b407,cs.LG,https://arxiv.org/pdf/2501.17589,Extracting Inter-Protein Interactions Via Multitasking Graph Structure Learning,"Jiang Li, Yuan-Ting Li","Quantitative Methods, Emerging Technologies, Machine Learning","Identifying protein-protein interactions (PPI) is crucial for gaining in-depth insights into numerous biological processes within cells and holds significant guiding value in areas such as drug development and disease treatment. Currently, most PPI prediction methods focus primarily on the study of protein sequences, neglecting the critical role of the internal structure of proteins. This paper proposes a novel PPI prediction method named MgslaPPI, which utilizes graph attention to mine protein structural information and enhances the expressive power of the protein encoder through multitask learning strategy. Specifically, we decompose the end-to-end PPI prediction process into two stages: amino acid residue reconstruction (A2RR) and protein interaction prediction (PIP). In the A2RR stage, we employ a graph attention-based residue reconstruction method to explore the internal relationships and features of proteins. In the PIP stage, in addition to the basic interaction prediction task, we introduce two auxiliary tasks, i.e., protein feature reconstruction (PFR) and masked interaction prediction (MIP). The PFR task aims to reconstruct the representation of proteins in the PIP stage, while the MIP task uses partially masked protein features for PPI prediction, with both working in concert to prompt MgslaPPI to capture more useful information. Experimental results demonstrate that MgslaPPI significantly outperforms existing state-of-the-art methods under various data partitioning schemes."
1435,679d459debd8ffd557a2b408,cs.LG,https://arxiv.org/pdf/2501.17513,Sequential Learning of the Pareto Front for Multi-objective Bandits,"Elise Crépon, Aurélien Garivier, Wouter M Koolen","Machine Learning, Machine Learning",
1436,679d459debd8ffd557a2b409,cs.LG,https://arxiv.org/pdf/2501.17512,A Survey on Cluster-based Federated Learning,"Omar El-Rifai, Michael Ben Ali, Imen Megdiche, André Peninou, Olivier Teste","Machine Learning, Machine Learning",
1437,679d459debd8ffd557a2b40a,cs.LG,https://arxiv.org/pdf/2501.17424,Certificated Actor-Critic: Hierarchical Reinforcement Learning with Control Barrier Functions for Safe Navigation,"Junjun Xie, Shuhao Zhao, Liang Hu, Huijun Gao","Robotics, Machine Learning","Control Barrier Functions (CBFs) have emerged as a prominent approach to designing safe navigation systems of robots. Despite their popularity, current CBF-based methods exhibit some limitations: optimization-based safe control techniques tend to be either myopic or computationally intensive, and they rely on simplified system models; conversely, the learning-based methods suffer from the lack of quantitative indication in terms of navigation performance and safety. In this paper, we present a new model-free reinforcement learning algorithm called Certificated Actor-Critic (CAC), which introduces a hierarchical reinforcement learning framework and well-defined reward functions derived from CBFs. We carry out theoretical analysis and proof of our algorithm, and propose several improvements in algorithm implementation. Our analysis is validated by two simulation experiments, showing the effectiveness of our proposed CAC algorithm."
1438,679d459debd8ffd557a2b40b,cs.LG,https://arxiv.org/pdf/2501.17354,Fundamental Computational Limits in Pursuing Invariant Causal Prediction and Invariance-Guided Regularization,"Yihong Gu, Cong Fang, Yang Xu, Zijian Guo, Jianqing Fan","Statistics Theory, Machine Learning, Methodology, Machine Learning",
1439,679d459debd8ffd557a2b40c,cs.LG,https://arxiv.org/pdf/2501.17345,Testing Conditional Mean Independence Using Generative Neural Networks,"Yi Zhang, Linjun Huang, Yun Yang, Xiaofeng Shao","Machine Learning, Machine Learning","Conditional mean independence (CMI) testing is crucial for statistical tasks including model determination and variable importance evaluation. In this work, we introduce a novel population CMI measure and a bootstrap-based testing procedure that utilizes deep generative neural networks to estimate the conditional mean functions involved in the population measure. The test statistic is thoughtfully constructed to ensure that even slowly decaying nonparametric estimation errors do not affect the asymptotic accuracy of the test. Our approach demonstrates strong empirical performance in scenarios with high-dimensional covariates and response variable, can handle multivariate responses, and maintains nontrivial power against local alternatives outside ann−1/2superscript𝑛12n^{-1/2}italic_n start_POSTSUPERSCRIPT - 1 / 2 end_POSTSUPERSCRIPTneighborhood of the null hypothesis. We also use numerical simulations and real-world imaging data applications to highlight the efficacy and versatility of our testing procedure."
1440,679d459debd8ffd557a2b40d,cs.LG,https://arxiv.org/pdf/2501.17333,A Guaranteed-Stable Neural Network Approach for Optimal Control of Nonlinear Systems,"Anran Li, John P. Swensen, Mehdi Hosseinzadeh","Optimization and Control, Machine Learning","A promising approach to optimal control of nonlinear systems involves iteratively linearizing the system and solving an optimization problem at each time instant to determine the optimal control input. Since this approach relies on online optimization, it can be computationally expensive, and thus unrealistic for systems with limited computing resources. One potential solution to this issue is to incorporate a Neural Network (NN) into the control loop to emulate the behavior of the optimal control scheme. Ensuring stability and reference tracking in the resulting NN-based closed-loop system requires modifications to the primary optimization problem. These modifications often introduce non-convexity and nonlinearity with respect to the decision variables, which may surpass the capabilities of existing solvers and complicate the generation of the training dataset. To address this issue, this paper develops a Neural Optimization Machine (NOM) to solve the resulting optimization problems. The central concept of a NOM is to transform the optimization challenges into the problem of training a NN. Rigorous proofs demonstrate that when a NN trained on data generated by the NOM is used in the control loop, all signals remain bounded and the system states asymptotically converge to a neighborhood around the desired equilibrium point, with a tunable proximity threshold. Simulation and experimental studies are provided to illustrate the effectiveness of the proposed methodology."
1441,679d459debd8ffd557a2b40e,cs.LG,https://arxiv.org/pdf/2501.17332,Compact Neural TTS Voices for Accessibility,"Kunal Jain, Eoin Murphy, Deepanshu Gupta, Jonathan Dyke, Saumya Shah, Vasilieios Tsiaras, Petko Petkov, Alistair Conkie","Sound, Machine Learning, Audio and Speech Processing","Contemporary text-to-speech solutions for accessibility applications can typically be classified into two categories: (i) device-based statistical parametric speech synthesis (SPSS) or unit selection (USEL) and (ii) cloud-based neural TTS.
SPSS and USEL offer low latency and low disk footprint at the expense of naturalness and audio quality. Cloud-based neural TTS systems provide significantly better audio quality and naturalness but regress in terms of latency and responsiveness, rendering these impractical for real-world applications. More recently, neural TTS models were made deployable to run on handheld devices. Nevertheless, latency remains higher than SPSS and USEL, while disk footprint prohibits pre-installation for multiple voices at once.
In this work, we describe a high-quality compact neural TTS system achieving latency on the order of 15 ms with low disk footprint. The proposed solution is capable of running on low-power devices."
1442,679d459debd8ffd557a2b40f,cs.LG,https://arxiv.org/pdf/2501.17311,RLPP: A Residual Method for Zero-Shot Real-World Autonomous Racing on Scaled Platforms,"Edoardo Ghignone, Nicolas Baumann, Cheng Hu, Jonathan Wang, Lei Xie, Andrea Carron, Michele Magno","Robotics, Machine Learning","Autonomous racing presents a complex environment requiring robust controllers capable of making rapid decisions under dynamic conditions. While traditional controllers based on tire models are reliable, they often demand extensive tuning or system identification.Reinforcement Learning (RL)methods offer significant potential due to their ability to learn directly from interaction, yet they typically suffer from the Sim-to-Real gap, where policies trained in simulation fail to perform effectively in the real world. In this paper, we propose RLPP, a residualRLframework that enhances aPure Pursuit (PP)controller with anRL-based residual. This hybrid approach leverages the reliability and interpretability ofPPwhile usingRLto fine-tune the controller’s performance in real-world scenarios. Extensive testing on the F1TENTH platform demonstrates that RLPP improves lap times by up to\qty6.37, closing the gap to theState-of-the-Art (SotA)methods by more than\qty52 and providing reliable performance in zero-shot real-world deployment, overcoming key challenges associated with the Sim-to-Real transfer and reducing the performance gap from simulation to reality by more than 8-fold when compared to the baselineRLcontroller. The RLPP framework is made available as an open-source tool, encouraging further exploration and advancement in autonomous racing research.
The code is available at:www.github.com/forzaeth/rlpp."
1443,679d459debd8ffd557a2b410,cs.LG,https://arxiv.org/pdf/2501.17304,Summary of the NOTSOFAR-1 Challenge: Highlights and Learnings,"Igor Abramovski, Alon Vinnikov, Shalev Shaer, Naoyuki Kanda, Xiaofei Wang, Amir Ivry, Eyal Krupka","Sound, Machine Learning, Audio and Speech Processing","The first Natural Office Talkers in Settings of Far-field
Audio Recordings (NOTSOFAR-1) Challenge is a pivotal initiative that sets new benchmarks by offering datasets more representative of the needs of real-world business applications than those previously available. The challenge provides a unique combination of 280 recorded meetings across 30 diverse environments, capturing real-world acoustic conditions and conversational dynamics, and a 1000-hour simulated training dataset, synthesized with enhanced authenticity for real-world generalization, incorporating 15,000 real acoustic transfer functions. In this paper, we provide an overview of the systems submitted to the challenge and analyze the top-performing approaches, hypothesizing the factors behind their success. Additionally, we highlight promising directions left unexplored by participants. By presenting key findings and actionable insights, this work aims to drive further innovation and progress in DASR research and applications."
1444,679d459debd8ffd557a2b411,cs.LG,https://arxiv.org/pdf/2501.17211,MR imaging in the low-field: Leveraging the power of machine learning,"Andreas Kofler, Dongyue Si, David Schote, Rene M Botnar, Christoph Kolbitsch, Claudia Prieto","Image and Video Processing, Machine Learning",
1445,679d459debd8ffd557a2b412,cs.LG,https://arxiv.org/pdf/2501.17184,Deep Learning in Wireless Communication Receiver: A Survey,"Shadman Rahman Doha, Ahmed Abdelhadi","Information Theory, Machine Learning, Networking and Internet Architecture","The design of wireless communication receivers to enhance signal processing in complex and dynamic environments is going through a transformation by leveraging deep neural networks (DNNs). Traditional wireless receivers depend on mathematical models and algorithms, which do not have the ability to adapt or learn from data. In contrast, deep learning-based receivers are more suitable for modern wireless communication systems because they can learn from data and adapt accordingly. This survey explores various deep learning architectures such as multilayer perceptrons (MLPs), convolutional neural networks (CNNs), recurrent neural networks (RNNs), generative adversarial networks (GANs), and autoencoders, focusing on their application in the design of wireless receivers. Key modules of a receiver such as synchronization, channel estimation, equalization, space-time decoding, demodulation, decoding, interference cancellation, and modulation classification are discussed in the context of advanced wireless technologies like orthogonal frequency division multiplexing (OFDM), multiple input multiple output (MIMO), semantic communication, task-oriented communication, and next-generation (Next-G) networks. The survey not only emphasizes the potential of deep learning-based receivers in future wireless communication but also investigates different challenges of deep learning-based receivers, such as data availability, security and privacy concerns, model interpretability, computational complexity, and integration with legacy systems."
1446,679d459debd8ffd557a2b413,cs.LG,https://arxiv.org/pdf/2501.17151,Scanning Trojaned Models Using Out-of-Distribution Samples,"Hossein Mirzaei, Ali Ansari, Bahar Dibaei Nia, Mojtaba Nafez, Moein Madadi, Sepehr Rezaee, Zeinab Sadat Taghavi, Arad Maleki, Kian Shamsaie, Mahdi Hajialilue, Jafar Habibi, Mohammad Sabokrou, Mohammad Hossein Rohban",Machine Learning,
1447,679d459debd8ffd557a2b414,cs.LG,https://arxiv.org/pdf/2501.17125,CoRe-Net: Co-Operational Regressor Network with Progressive Transfer Learning for Blind Radar Signal Restoration,"Muhammad Uzair Zahid, Serkan Kiranyaz, Alper Yildirim, Moncef Gabbouj",Machine Learning,"Real-world radar signals are frequently corrupted by various artifacts, including sensor noise, echoes, interference, and intentional jamming, differing in type, severity, and duration. Traditional radar signal restoration methods often address a single restoration problem, such as a specific noise type, and typically assume a fixed signal-to-noise ratio (SNR) level. This severely limits their usability to practical real-world applications. Several Deep Learning models have been proposed for restoration and recently Operational Generative Adversarial Networks (OpGANs) have achieved state-of-the-art performance levels. However, adversarial-based methods may still face instability and challenges with limited restoration quality refinement. This pilot study introduces a novel model, called Co-Operational Regressor Network (CoRe-Net) for blind radar signal restoration, designed to address such limitations and drawbacks. CoRe-Net replaces adversarial training with a novel cooperative learning strategy, leveraging the complementary roles of its Apprentice Regressor (AR) and Master Regressor (MR). The AR restores radar signals corrupted by various artifacts, while the MR evaluates the quality of the restoration and provides immediate and task-specific feedback, ensuring stable and efficient learning. The AR, therefore, has the advantage of both self-learning and assistive learning by the MR. The proposed model has been extensively evaluated over the benchmark Blind Radar Signal Restoration (BRSR) dataset, which simulates diverse real-world artifact scenarios. Under the fair experimental setup, this study shows that the CoRe-Net surpasses the Op-GANs over a 1 dB mean SNR improvement. To further boost the performance gain, this study proposes multi-pass restoration by cascaded CoRe-Nets trained with a novel paradigm called Progressive Transfer Learning (PTL), which enables iterative refinement, thus achieving an additional 2 dB mean SNR enhancement. Multi-pass CoRe-Net training by PTL consistently yields incremental performance improvements through successive restoration passes whilst highlighting CoRe-Net’s ability to handle such a complex and varying blend of artifacts. As a result, CoRe-Net sets a new benchmark for blind radar signal restoration by achieving state-of-the-art performance with a significant gap over a wide range of SNR levels of the input signals corrupted by various artifact types."
1448,679d459debd8ffd557a2b415,cs.LG,https://arxiv.org/pdf/2501.17115,Evidence on the Regularisation Properties of Maximum-Entropy Reinforcement Learning,"Rémy Hosseinkhan Boucher, Onofrio Semeraro, Lionel Mathelin",Machine Learning,"The generalisation and robustness properties of policies learnt through
Maximum-Entropy Reinforcement Learning are investigated
on chaotic dynamical systems with Gaussian noise on the observable.
First, the robustness under noise contamination of the agent’s observation
of entropy regularised policies is observed.
Second, notions of statistical learning theory,
such as complexity measures on the learnt model, are borrowed
to explain and predict the phenomenon.
Results show the existence of a relationship between entropy-regularised policy optimisation and robustness to noise, which can be described by the chosen complexity measures."
1449,679d459debd8ffd557a2b416,cs.LG,https://arxiv.org/pdf/2501.17112,Unlocking Transparent Alignment Through Enhanced Inverse Constitutional AI for Principle Extraction,"Carl-Leander Henneking, Claas Beger",Machine Learning,"Traditional methods for aligning Large Language Models (LLMs), such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO), rely on implicit principles, limiting interpretability. Constitutional AI (CAI) offers an explicit, rule-based framework for guiding model outputs. Building on this, we refine the Inverse Constitutional AI (ICAI) algorithm, which extracts constitutions from preference datasets. By improving principle generation, clustering, and embedding processes, our approach enhances the accuracy and generalizability of extracted principles across synthetic and real-world datasets. While in-context alignment yields modest improvements, our results highlight the potential of these principles to foster more transparent and adaptable alignment methods, offering a promising direction for future advancements beyond traditional fine-tuning."
1450,679d459debd8ffd557a2b417,cs.LG,https://arxiv.org/pdf/2501.17086,Accelerated Training through Iterative Gradient Propagation Along the Residual Path,"Erwan Fagnou, Paul Caillon, Blaise Delattre, Alexandre Allauzen",Machine Learning,"Despite being the cornerstone of deep learning, backpropagation is criticized for its inherent sequentiality, which can limit the scalability of very deep models.
Such models faced convergence issues due to vanishing gradient, later resolved using residual connections. Variants of these are now widely used in modern architecture.
However, the computational cost of backpropagation remains a major burden, accounting for most of the training time.
Taking advantage of residual-like architectural designs, we introduce Highway backpropagation, a parallelizable iterative algorithm that approximates backpropagation, by alternatively i) accumulating the gradient estimates along the residual path, and ii) backpropagating them through every layer in parallel. This algorithm is naturally derived from a decomposition of the gradient as the sum of gradients flowing through all paths and is adaptable to a diverse set of common architectures, ranging from ResNets and Transformers to recurrent neural networks.
Through an extensive empirical study on a large selection of tasks and models, we evaluate Highway-BP and show that major speedups can be achieved with minimal performance degradation."
1451,679d459debd8ffd557a2b418,cs.LG,https://arxiv.org/pdf/2501.17084,Token-by-Token Regeneration and Domain Biases: A Benchmark of LLMs on Advanced Mathematical Problem-Solving,Evgenii Evstafev,Machine Learning,
1452,679d459debd8ffd557a2b419,cs.LG,https://arxiv.org/pdf/2501.16932,Online-BLS: An Accurate and Efficient Online Broad Learning System for Data Stream Classification,"Chunyu Lei, Guang-Ze Chen, C. L. Philip Chen, Tong Zhang",Machine Learning,"The state-of-the-art online learning models generally conduct a single online gradient descent when a new sample arrives and thus suffer from suboptimal model weights. To this end, we introduce an online broad learning system framework with closed-form solutions for each online update. Different from employing existing incremental broad learning algorithms for online learning tasks, which tend to incur degraded accuracy and expensive online update overhead, we design an effective weight estimation algorithm and an efficient online updating strategy to remedy the above two deficiencies, respectively. Specifically, an effective weight estimation algorithm is first developed by replacing notorious matrix inverse operations with Cholesky decomposition and forward-backward substitution to improve model accuracy. Second, we devise an efficient online updating strategy that dramatically reduces online update time. Theoretical analysis exhibits the splendid error bound and low time complexity of our model. The most popular test-then-training evaluation experiments on various real-world datasets prove its superiority and efficiency. Furthermore, our framework is naturally extended to data stream scenarios with concept drift and exceeds state-of-the-art baselines."
1453,679d459debd8ffd557a2b41a,cs.LG,https://arxiv.org/pdf/2501.16931,Quantifying Uncertainty and Variability in Machine Learning: Confidence Intervals for Quantiles in Performance Metric Distributions,"Christoph Lehmann, Yahor Paromau","Machine Learning, Applications","Machine learning (ML) models are widely used in applications where reliability and robustness are critical, including healthcare, finance, and infrastructure management.
Model evaluation often relies on single-point estimates of performance metrics such as accuracy, F1 score, or mean squared error, that fail to capture the inherent variability in model performance.
This variability arises from multiple sources, including train-test split, data augmentation strategies, weights initialization, and hyperparameter tuning.
Investigating the characteristics of performance metric distributions, rather than focusing on a single point only, is essential for informed decision-making during model selection and optimization, especially in high-stakes settings."
1454,679d459debd8ffd557a2b41b,cs.LG,https://arxiv.org/pdf/2501.16919,Projection-free Algorithms for Online Convex Optimization with Adversarial Constraints,"Dhruv Sarkar, Aprameyo Chakrabartty, Subhamon Supantha, Palash Dey, Abhishek Sinha",Machine Learning,
1455,679d459debd8ffd557a2b41c,cs.LG,https://arxiv.org/pdf/2501.16918,On Rollouts in Model-Based Reinforcement Learning,"Bernd Frauenknecht, Devdutt Subhasish, Friedrich Solowjow, Sebastian Trimpe",Machine Learning,"Model-based reinforcement learning (MBRL) seeks to enhance data efficiency by learning a model of the environment and generating synthetic rollouts from it.
However, accumulated model errors during these rollouts can distort the data distribution, negatively impacting policy learning and hindering long-term planning.
Thus, the accumulation of model errors is a key bottleneck in current MBRL methods.
We proposeInfoprop, a model-based rollout mechanism that separates aleatoric from epistemic model uncertainty and reduces the influence of the latter on the data distribution. Further, Infoprop keeps track of accumulated model errors along a model rollout and provides termination criteria to limit data corruption.
We demonstrate the capabilities of Infoprop in theInfoprop-Dynaalgorithm, reporting state-of-the-art performance in Dyna-style MBRL on common MuJoCo benchmark tasks while substantially increasing rollout length and data quality."
1456,679d459debd8ffd557a2b41d,cs.LG,https://arxiv.org/pdf/2501.16912,A Unified Evaluation Framework for Epistemic Predictions,"Shireen Kudukkil Manchingal, Muhammad Mubashar, Kaizheng Wang, Fabio Cuzzolin",Machine Learning,
1457,679d459debd8ffd557a2b41e,cs.LG,https://arxiv.org/pdf/2501.16900,RAINER: A Robust Ensemble Learning Grid Search-Tuned Framework for Rainfall Patterns Prediction,"Zhenqi Li, Junhao Zhong, Hewei Wang, Jinfeng Xu, Yijie Li, Jinjiang You, Jiayi Zhang, Runzhi Wu, Soumyabrata Dev",Machine Learning,"Rainfall prediction remains a persistent challenge due to the highly nonlinear and complex nature of meteorological data. Existing approaches lack systematic utilization of grid search for optimal hyperparameter tuning, relying instead on heuristic or manual selection, frequently resulting in sub-optimal results. Additionally, these methods rarely incorporate newly constructed meteorological features such as differences between temperature and humidity, to capture critical weather dynamics. Furthermore, there is a lack of systematic evaluation of ensemble learning techniques and limited exploration of diverse advanced models introduced in the past one or two years. To address these limitations, we propose aRobust ensemble LeArning grId search-tuNed framEwoRk (RAINER) for rainfall prediction. RAINER incorporates a comprehensive feature engineering pipeline, including outlier removal, imputation of missing values, feature reconstruction, and dimensionality reduction via Principal Component Analysis (PCA). The framework integrates novel meteorological features to capture dynamic weather patterns and systematically evaluates non-learning mathematical-based methods and a variety of machine learning models, from weak classifiers (e.g., KNN, LASSO, and Random Forest) to advanced neural networks such as Kolmogorov-Arnold Networks (KAN). By leveraging grid search for hyperparameter tuning and ensemble voting techniques, RAINER achieves state-of-the-art results within real-world dataset. Extensive experiments demonstrate the framework’s superior performance across diverse metrics, such as Accuracy, Precision, Recall, F1-score, and AUC, highlighting the pivotal role of feature engineering, grid search, and ensemble learning in rainfall prediction tasks."
1458,679d459debd8ffd557a2b41f,cs.LG,https://arxiv.org/pdf/2501.16894,DBSCAN in domains with periodic boundary conditions,"Xander M. de Wit, Alessandro Gabbana","Machine Learning, Computational Physics, Fluid Dynamics","Many scientific problems involve data that is embedded in a space with periodic boundary conditions. This can for instance be related to an inherent cyclic or rotational symmetry in the data or a spatially extended periodicity. When analyzing such data, well-tailored methods are needed to obtain efficient approaches that obey the periodic boundary conditions of the problem. In this work, we present a method for applying a clustering algorithm to data embedded in a periodic domain based on the DBSCAN algorithm, a widely used unsupervised machine learning method that identifies clusters in data. The proposed method internally leverages the conventional DBSCAN algorithm for domains with open boundaries, such that it remains compatible with all optimized implementations for neighborhood searches in open domains. In this way, it retains the same optimized runtime complexity of𝒪⁢(N⁢log⁡N)𝒪𝑁𝑁\mathcal{O}(N\log N)caligraphic_O ( italic_N roman_log italic_N ). We demonstrate the workings of the proposed method using synthetic data in one, two and three dimensions and also apply it to a real-world example involving the clustering of bubbles in a turbulent flow. The proposed approach is implemented in a ready-to-use Python package that we make publicly available."
1459,679d459debd8ffd557a2b420,cs.LG,https://arxiv.org/pdf/2501.16863,HD-CB: The First Exploration of Hyperdimensional Computing for Contextual Bandits Problems,"Marco Angioli, Antonello Rosato, Marcello Barbirotta, Rocco Martino, Francesco Menichelli, Mauro Olivieri",Machine Learning,"Hyperdimensional Computing (HDC), also known as Vector Symbolic Architectures, is a computing paradigm that combines the strengths of symbolic reasoning with the efficiency and scalability of distributed connectionist models in artificial intelligence.
HDC has recently emerged as a promising alternative for performing learning tasks in resource-constrained environments thanks to its energy and computational efficiency, inherent parallelism, and resilience to noise and hardware faults."
1460,679d459debd8ffd557a2b421,cs.LG,https://arxiv.org/pdf/2501.16848,Hybrid Phenology Modeling for Predicting Temperature Effects on Tree Dormancy,"Ron van Bree, Diego Marcos, Ioannis Athanasiadis",Machine Learning,"Biophysical models offer valuable insights into climate-phenology relationships in both natural and agricultural settings. However, there are substantial structural discrepancies across models which require site-specific recalibration, often yielding inconsistent predictions under similar climate scenarios. Machine learning methods offer data-driven solutions, but often lack interpretability and alignment with existing knowledge. We present a phenology model describing dormancy in fruit trees, integrating conventional biophysical models with a neural network to address their structural disparities. We evaluate our hybrid model in an extensive case study predicting cherry tree phenology in Japan, South Korea and Switzerland. Our approach consistently outperforms both traditional biophysical and machine learning models in predicting blooming dates across years. Additionally, the neural network’s adaptability facilitates parameter learning for specific tree varieties, enabling robust generalization to new sites without site-specific recalibration. This hybrid model leverages both biophysical constraints and data-driven flexibility, offering a promising avenue for accurate and interpretable phenology modeling."
1461,679d459debd8ffd557a2b422,cs.LG,https://arxiv.org/pdf/2501.16839,"Flow Matching: Markov Kernels, Stochastic Processes and Transport Plans","Christian Wald, Gabriele Steidl","Machine Learning, Probability","Among generative neural models, flow matching techniques stand
out for their simple applicability and good scaling properties.
Here, velocity fields of curves connecting a
simple latent and a target distribution are learned.
Then the corresponding ordinary differential equation can be used to sample from a target distribution, starting in samples from the latent one.
This paper reviews from a mathematical point of view
different techniques to learn the velocity fields of
absolutely continuous curves in the Wasserstein geometry.
We show how the velocity fields can be characterized and learned via i) transport plans (couplings) between latent and target distributions,
ii) Markov kernels and iii) stochastic processes, where the latter two include the coupling approach, but are in general broader."
1462,679d459debd8ffd557a2b423,cs.LG,https://arxiv.org/pdf/2501.16831,Data-Driven vs Traditional Approaches to Power Transformer's Top-Oil Temperature Estimation,"Francis Tembo, Federica Bragone, Tor Laneryd, Matthieu Barreau, Kateryna Morozovska",Machine Learning,"Power transformers are subjected to electrical currents and temperature fluctuations that, if not properly controlled, can lead to major deterioration of their insulation system. Therefore, monitoring the temperature of a power transformer is fundamental to ensure a long-term operational life. Models presented in the IEC 60076-7 and IEEE standards, for example, monitor the temperature by calculating the top-oil and the hot-spot temperatures. However, these models are not very accurate and rely on the power transformers’ properties. This paper focuses on finding an alternative method to predict the top-oil temperatures given previous measurements. Given the large quantities of data available, machine learning methods for time series forecasting are analyzed and compared to the real measurements and the corresponding prediction of the IEC standard. The methods tested are Artificial Neural Networks (ANNs), Time-series Dense Encoder (TiDE), and Temporal Convolutional Networks (TCN) using different combinations of historical measurements. Each of these methods outperformed the IEC 60076-7 model and they are extended to estimate the temperature rise over ambient. To enhance prediction reliability, we explore the application of quantile regression to construct prediction intervals for the expected top-oil temperature ranges. The best-performing model successfully estimates conditional quantiles that provide sufficient coverage."
1463,679d459debd8ffd557a2b424,cs.LG,https://arxiv.org/pdf/2501.16828,Late Breaking Results: Energy-Efficient Printed Machine Learning Classifiers with Sequential SVMs,"Spyridon Besias, Ilias Sertaridis, Florentia Afentaki, Konstantinos Balaskas, Georgios Zervakis","Machine Learning, Image and Video Processing, Systems and Control","Printed Electronics (PE)provide a mechanically flexible and cost-effective solution for machine learning (ML) circuits, compared to silicon-based technologies.
However, due to large feature sizes, printed classifiers are limited by high power, area, and energy overheads, which restricts the realization of battery-powered systems.
In this work, we design sequential printed bespokeSupport Vector Machine (SVM)circuits that adhere to the power constraints of existing printed batteries while minimizing energy consumption, thereby boosting battery life.
Our results show 6.5x energy savings while maintaining higher accuracy compared to the state of the art."
1464,679d459debd8ffd557a2b425,cs.LG,https://arxiv.org/pdf/2501.16825,Can Transformers Learn Full Bayesian Inference in Context?,"Arik Reuter, Tim G. J. Rudner, Vincent Fortuin, David Rügamer",Machine Learning,"Transformers have emerged as the dominant architecture in the field of deep learning, with a broad range of applications and remarkable in-context learning (ICL) capabilities. While not yet fully understood, ICL has already proved to be an intriguing phenomenon, allowing transformers to learn in context—without requiring further training. In this paper, we further advance the understanding of ICL by demonstrating that transformers can perform full Bayesian inference for commonly used statistical models in context. More specifically, we introduce a general framework that builds on ideas from prior fitted networks and continuous normalizing flows which enables us to infer complex posterior distributions for methods such as generalized linear models and latent factor models. Extensive experiments on real-world datasets demonstrate that our ICL approach yields posterior samples that are similar in quality to state-of-the-art MCMC or variational inference methods not operating in context."
1465,679d459debd8ffd557a2b426,cs.LG,https://arxiv.org/pdf/2501.16756,Random Forest Calibration,"Mohammad Hossein Shaker, Eyke Hüllermeier",Machine Learning,"The Random Forest (RF) classifier is often claimed to be relatively well calibrated when compared with other machine learning methods. Moreover, the existing literature suggests that traditional calibration methods, such as isotonic regression, do not substantially enhance the calibration of RF probability estimates unless supplied with extensive calibration data sets, which can represent a significant obstacle in cases of limited data availability. Nevertheless, there seems to be no comprehensive study validating such claims and systematically comparing state-of-the-art calibration methods specifically for RF. To close this gap, we investigate a broad spectrum of calibration methods tailored to or at least applicable to RF, ranging from scaling techniques to more advanced algorithms. Our results based on synthetic as well as real-world data unravel the intricacies of RF probability estimates, scrutinize the impacts of hyper-parameters, compare calibration methods in a systematic way. We show that a well-optimized RF performs as well as or better than leading calibration approaches."
1466,679d459debd8ffd557a2b427,cs.LG,https://arxiv.org/pdf/2501.16730,Growing the Efficient Frontier on Panel Trees,"Lin William Cong, Guanhao Feng, Jingyu He, Xin He","Machine Learning, Pricing of Securities, Machine Learning","We introduce a new class of tree-based models, P-Trees, for analyzing (unbalanced) panel of individual asset returns, generalizing high-dimensional sorting with economic guidance and interpretability. Under the mean-variance efficient framework, P-Trees construct test assets that significantly advance the efficient frontier compared to commonly used test assets, with alphas unexplained by benchmark pricing models. P-Tree tangency portfolios also constitute traded factors, recovering the pricing kernel and outperforming popular observable and latent factor models for investments and cross-sectional pricing. Finally, P-Trees capture the complexity of asset returns with sparsity, achieving out-of-sample Sharpe ratios close to those attained only by over-parameterized large models."
1467,679d459debd8ffd557a2b428,cs.LG,https://arxiv.org/pdf/2501.16718,Outlier Synthesis via Hamiltonian Monte Carlo for Out-of-Distribution Detection,"Hengzhuang Li, Teng Zhang",Machine Learning,"Out-of-distribution (OOD) detection is crucial for developing trustworthy and reliable machine learning systems. Recent advances in training with auxiliary OOD data demonstrate efficacy in enhancing detection capabilities. Nonetheless, these methods heavily rely on acquiring a large pool of high-quality natural outliers. Some prior methods try to alleviate this problem by synthesizing virtual outliers but suffer from either poor quality or high cost due to the monotonous sampling strategy and the heavy-parameterized generative models. In this paper, we overcome all these problems by proposing theHamiltonian Monte CarloOutlierSynthesis(HamOS) framework, which views the synthesis process as sampling from Markov chains. Based solely on the in-distribution data, the Markov chains can extensively traverse the feature space and generate diverse and representative outliers, hence exposing the model to miscellaneous potential OOD scenarios. The Hamiltonian Monte Carlo with sampling acceptance rate almost close to 1 also makes our framework enjoy great efficiency. By empirically competing with SOTA baselines on both standard and large-scale benchmarks, we verify the efficacy and efficiency of our proposed HamOS. Our code is available at:https://github.com/Fir-lat/HamOS_OOD."
1468,679d459debd8ffd557a2b429,cs.LG,https://arxiv.org/pdf/2501.16656,Data Mining in Transportation Networks with Graph Neural Networks: A Review and Outlook,"Jiawei Xue, Ruichen Tan, Jianzhu Ma, Satish V. Ukkusuri",Machine Learning,"Data mining in transportation networks (DMTNs) refers to using diverse types of spatio-temporal data for various transportation tasks, including pattern analysis, traffic prediction, and traffic controls. Graph neural networks (GNNs) are essential in many DMTN problems due to their capability to represent spatial correlations between entities. Between 2016 and 2024, the notable applications of GNNs in DMTNs have extended to multiple fields such as traffic prediction and operation. However, existing reviews have primarily focused on traffic prediction tasks. To fill this gap, this study provides a timely and insightful summary of GNNs in DMTNs, highlighting new progress in prediction and operation from academic and industry perspectives since 2023. First, we present and analyze various DMTN problems, followed by classical and recent GNN models. Second, we delve into key works in three areas: (1) traffic prediction, (2) traffic operation, and (3) industry involvement, such as Google Maps, Amap, and Baidu Maps. Along these directions, we discuss new research opportunities based on the significance of transportation problems and data availability. Finally, we compile resources such as data, code, and other learning materials to foster interdisciplinary communication. This review, driven by recent trends in GNNs in DMTN studies since 2023, could democratize abundant datasets and efficient GNN methods for various transportation problems including prediction and operation."
1469,679d459debd8ffd557a2b42a,cs.LG,https://arxiv.org/pdf/2501.16615,Sparse Autoencoders Trained on the Same Data Learn Different Features,"Gonçalo Paulo, Nora Belrose",Machine Learning,"Sparse autoencoders (SAEs) are a useful tool for uncovering human-interpretable features in the activations of large language models (LLMs). While some expect SAEs to find the true underlying features used by a model, our research shows that SAEs trained on the same model and data, differing only in the random seed used to initialize their weights, identify different sets of features. For example, in an SAE with 131K latents trained on a feedforward network in Llama 3 8B, only 30% of the features were shared across different seeds. We observed this phenomenon across multiple layers of three different LLMs, two datasets, and several SAE architectures. While ReLU SAEs trained with the L1 sparsity loss showed greater stability across seeds, SAEs using the state-of-the-art TopK activation function were more seed-dependent, even when controlling for the level of sparsity. Our results suggest that the set of features uncovered by an SAE should be viewed as a pragmatically useful decomposition of activation space, rather than an exhaustive and universal list of features “truly used” by the model."
1470,679d459debd8ffd557a2b42b,cs.LG,https://arxiv.org/pdf/2501.16614,FUNU: Boosting Machine Unlearning Efficiency by Filtering Unnecessary Unlearning,"Zitong Li, Qingqing Ye, Haibo Hu",Machine Learning,"Machine unlearning is an emerging field that selectively removes specific data samples from a trained model. This capability is crucial for addressing privacy concerns, complying with data protection regulations, and correcting errors or biases introduced by certain data. Unlike traditional machine learning, where models are typically static once trained, machine unlearning facilitates dynamic updates that enable the model to “forget” information without requiring complete retraining from scratch. There are various machine unlearning methods, some of which are more time-efficient when data removal requests are fewer."
1471,679d459debd8ffd557a2b42c,cs.LG,https://arxiv.org/pdf/2501.16599,Toward Safe Integration of UAM in Terminal Airspace: UAM Route Feasibility Assessment using Probabilistic Aircraft Trajectory Prediction,"Jungwoo Cho, Seongjin Choi",Machine Learning,"Integrating Urban Air Mobility (UAM) into airspace managed by Air Traffic Control (ATC) poses significant challenges, particularly in congested terminal environments. This study proposes a framework to assess the feasibility of UAM route integration using probabilistic aircraft trajectory prediction. By leveraging conditional Normalizing Flows, the framework predicts short-term trajectory distributions of conventional aircraft, enabling UAM vehicles to dynamically adjust speeds and maintain safe separations. The methodology was applied to airspace over Seoul metropolitan area, encompassing interactions between UAM and conventional traffic at multiple altitudes and lanes. The results reveal that different physical locations of lanes and routes experience varying interaction patterns and encounter dynamics. For instance, Lane 1 at lower altitudes (1,500 ft and 2,000 ft) exhibited minimal interactions with conventional aircraft, resulting in the largest separations and the most stable delay proportions. In contrast, Lane 4 near the airport experienced more frequent and complex interactions due to its proximity to departing traffic. The limited trajectory data for departing aircraft in this region occasionally led to tighter separations and increased operational challenges. This study underscores the potential of predictive modeling in facilitating UAM integration while highlighting critical trade-offs between safety and efficiency. The findings contribute to refining airspace management strategies and offer insights for scaling UAM operations in complex urban environments."
1472,679d459debd8ffd557a2b42d,cs.LG,https://arxiv.org/pdf/2501.16588,Fine-Tuned Language Models as Space Systems Controllers,"Enrico M. Zucchelli, Di Wu, Julia Briden, Christian Hofmann, Victor Rodriguez-Fernandez, Richard Linares","Machine Learning, Systems and Control","Large language models (LLMs), or foundation models (FMs), are pretrained transformers that coherently complete sentences auto-regressively. In this paper, we show that LLMs can control simplified space systems after some additional training, called fine-tuning. We look at relatively small language models, ranging between 7 and 13 billion parameters. We focus on four problems: a three-dimensional spring toy problem, low-thrust orbit transfer, low-thrust cislunar control, and powered descent guidance. The fine-tuned LLMs are capable of controlling systems by generating sufficiently accurate outputs that are multi-dimensional vectors with up to 10 significant digits. We show that for several problems the amount of data required to perform fine-tuning is smaller than what is generally required of traditional deep neural networks (DNNs), and that fine-tuned LLMs are good at generalizing outside of the training dataset. Further, the same LLM can be fine-tuned with data from different problems, with only minor performance degradation with respect to LLMs trained for a single application. This work is intended as a first step towards the development of a general space systems controller."
1473,679d459debd8ffd557a2b42e,cs.LG,https://arxiv.org/pdf/2501.16587,HopCast: Calibration of Autoregressive Dynamics Models,"Muhammad Bilal Shahid, Cody Fleming",Machine Learning,
1474,679d459debd8ffd557a2b42f,cs.LG,https://arxiv.org/pdf/2501.16573,Optimization Landscapes Learned: Proxy Networks Boost Convergence in Physics-based Inverse Problems,"Girnar Goyal, Philipp Holl, Sweta Agrawal, Nils Thuerey","Machine Learning, Optimization and Control","Solving inverse problems in physics is central to understanding complex systems and advancing technologies in various fields. Iterative optimization algorithms, commonly used to solve these problems, often encounter local minima, chaos, or regions with zero gradients. This is due to their overreliance on local information and highly chaotic inverse loss landscapes governed by underlying partial differential equations (PDEs). In this work, we show that deep neural networks successfully replicate such complex loss landscapes through spatio-temporal trajectory inputs. They also offer the potential to control the underlying complexity of these chaotic loss landscapes during training through various regularization methods. We show that optimizing on network-smoothened loss landscapes leads to improved convergence in predicting optimum inverse parameters over conventional momentum-based optimizers such asBFGSon multiple challenging problems."
1475,679d459debd8ffd557a2b430,cs.LG,https://arxiv.org/pdf/2501.16562,C-HDNet: A Fast Hyperdimensional Computing Based Method for Causal Effect Estimation from Networked Observational Data,"Abhishek Dalvi, Neil Ashtekar, Vasant Honavar","Machine Learning, Methodology","We consider the problem of estimating causal effects from observational data in the presence of network confounding. In this context, an individual’s treatment assignment and outcomes may be affected by their neighbors within the network. We propose a novel matching technique which leverages hyperdimensional computing to model network information and improve predictive performance. We present results of extensive experiments which show that the proposed method outperforms or is competitive with the state-of-the-art methods for causal effect estimation from network data, including advanced computationally demanding deep learning methods. Further, our technique benefits from simplicity and speed, with roughly an order of magnitude lower runtime compared to state-of-the-art methods, while offering similar causal effect estimation error rates."
1476,679d459debd8ffd557a2b431,cs.LG,https://arxiv.org/pdf/2501.16496,Open Problems in Mechanistic Interpretability,"Lee Sharkey, Bilal Chughtai, Joshua Batson, Jack Lindsey, Jeff Wu, Lucius Bushnaq, Nicholas Goldowsky-Dill, Stefan Heimersheim, Alejandro Ortega, Joseph Bloom, Stella Biderman, Adria Garriga-Alonso, Arthur Conmy, Neel Nanda, Jessica Rumbelow, Martin Wattenberg, Nandi Schoots, Joseph Miller, Eric J. Michaud, Stephen Casper, Max Tegmark, William Saunders, David Bau, Eric Todd, Atticus Geiger, Mor Geva, Jesse Hoogland, Daniel Murfet, Tom McGrath",Machine Learning,"Mechanistic interpretability aims to understand the computational mechanisms underlying neural networks’ capabilities in order to accomplish concrete scientific and engineering goals. Progress in this field thus promises to provide greater assurance over AI system behavior and shed light on exciting scientific questions about the nature of intelligence. Despite recent progress toward these goals, there are many open problems in the field that require solutions before many scientific and practical benefits can be realized: Our methods require both conceptual and practical improvements to reveal deeper insights; we must figure out how best to apply our methods in pursuit of specific goals; and the field must grapple with socio-technical challenges that influence and are influenced by our work. This forward-facing review discusses the current frontier of mechanistic interpretability and the open problems that the field may benefit from prioritizing."
1477,679d459debd8ffd557a2b432,cs.LG,https://arxiv.org/pdf/2501.16476,Closed-Form Feedback-Free Learning with Forward Projection,"Robert O'Shea, Bipin Rajendran","Machine Learning, Machine Learning","State-of-the-art methods for backpropagation-free learning employ local error feedback to direct iterative optimisation via gradient descent. In this study, we examine the more restrictive setting where retrograde communication from neuronal outputs is unavailable for pre-synaptic weight optimisation. To address this challenge, we proposeForward Projection(FP). This novel randomised closed-form training method requires only a single forward pass over the entire dataset for model fitting, without retrograde communication. Our key innovation is generating target values for pre-activation membrane potentials at each layer via nonlinear projections of pre-synaptic inputs and the labels. Our approach generates target potentials from randomised encodings combining information from inputs and labels. Local loss functions are optimised over pre-synaptic inputs using closed-form regression, without feedback from neuronal outputs or downstream layers. Interpretability is a key advantage of FP training; membrane potentials of hidden neurons in FP-trained networks encode information which is interpretable layer-wise as label predictions. We demonstrate the effectiveness of FP across four biomedical datasets, comparing it with backpropagation and local learning techniques such as forward-forward training and local supervision in multi-layer perceptron and convolutional architectures. In few-shot learning tasks, FP yielded more generalisable models than those optimised via backpropagation. In large-sample tasks, FP-based models achieve generalisation comparable to gradient descent-based local learning methods while requiring only a single forward propagation step, achieving significant speed up for training. Interpretation functions defined on local neuronal activity in FP-based models successfully identified clinically salient features for diagnosis in two distinct datasets – identification of key signatures of myocardial infarction in time-series data from electrocardiogram sequences and detection of regions of choroid neovascularisation in optical coherence tomography images."
1478,679d459debd8ffd557a2b433,cs.LG,https://arxiv.org/pdf/2501.16456,CoCoNUT: Structural Code Understanding does not fall out of a tree,"Claas Beger, Saikat Dutta","Machine Learning, Software Engineering","Large Language Models (LLMs) have shown impressive performance across a wide array of tasks involving both structured and unstructured textual data. More recently, LLMs have demonstrated remarkable abilities to generate code across different programming languages. Recent results on various benchmarks for code generation, repair, or completion suggest that certain models have programming abilities comparable to or even surpass humans. In this work, we demonstrate that the high performance on such benchmarks does not correlate to humans’ innate ability to understand the structural control flow of code."
1479,679d459debd8ffd557a2b434,cs.LG,https://arxiv.org/pdf/2501.16443,Objects matter: object-centric world models improve reinforcement learning in visually complex environments,"Weipu Zhang, Adam Jelley, Trevor McInroe, Amos Storkey","Machine Learning, Computer Vision and Pattern Recognition","Deep reinforcement learning has achieved remarkable success in learning control policies from pixels across a wide range of tasks, yet its application remains hindered by low sample efficiency, requiring significantly more environment interactions than humans to reach comparable performance.
Model-based reinforcement learning (MBRL) offers a solution by leveraging learnt world models to generate simulated experience, thereby improving sample efficiency.
However, in visually complex environments, small or dynamic elements can be critical for decision-making.
Yet, traditional MBRL methods in pixel-based environments typically rely on auto-encoding with anL2subscript𝐿2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTloss, which is dominated by large areas and often fails to capture decision-relevant details.
To address these limitations, we propose anobject-centric MBRL pipeline, which integrates recent advances in computer vision to allow agents to focus on key decision-related elements.
Our approach consists of four main steps: (1) annotating key objects related to rewards and goals with segmentation masks, (2) extracting object features using a pre-trained, frozen foundation vision model, (3) incorporating these object features with the raw observations to predict environmental dynamics, and (4) training the policy using imagined trajectories generated by this object-centric world model.
Building on the efficient MBRL algorithm STORM, we call this pipelineOC-STORM.
We demonstrate OC-STORM’s practical value in overcoming the limitations of conventional MBRL approaches on both Atari games and the visually complex game Hollow Knight."
1480,679d459debd8ffd557a2b435,cs.LG,https://arxiv.org/pdf/2501.16399,Detecting clinician implicit biases in diagnoses using proximal causal inference,"Kara Liu, Russ Altman, Vasilis Syrgkanis","Machine Learning, Applications","Clinical decisions to treat and diagnose patients are affected by implicit biases formed by racism, ableism, sexism, and other stereotypes. These biases reflect broader systemic discrimination in healthcare and risk marginalizing already disadvantaged groups. Existing methods for measuring implicit biases require controlled randomized testing and only capture individual attitudes rather than outcomes. However, the ”big-data” revolution has led to the availability of large observational medical datasets, like EHRs and biobanks, that provide the opportunity to investigate discrepancies in patient health outcomes. In this work, we propose a causal inference approach to detect the effect of clinician implicit biases on patient outcomes in large-scale medical data. Specifically, our method uses proximal mediation to disentangle pathway-specific effects of a patient’s sociodemographic attribute on a clinician’s diagnosis decision. We test our method on real-world data from the UK Biobank. Our work can serve as a tool that initiates conversation and brings awareness to unequal health outcomes caused by implicit biases.111Our method is available athttps://github.com/syrgkanislab/hidden_mediators"
1481,679d459debd8ffd557a2b436,cs.LG,https://arxiv.org/pdf/2501.16398,Visualizing the Local Atomic Environment Features of Machine Learning Interatomic Potential,"Xuqiang Shao, Yuqi Zhang, Di Zhang, Tianxiang Gao, Xinyuan Liu, Zhiran Gan, Fanshun Meng, Hao Li, Weijie Yang","Machine Learning, Atomic Physics",
1482,679d459debd8ffd557a2b437,cs.LG,https://arxiv.org/pdf/2501.16397,THOR: A Generic Energy Estimation Approach for On-Device Training,"Jiaru Zhang, Zesong Wang, Hao Wang, Tao Song, Huai-an Su, Rui Chen, Yang Hua, Xiangwei Zhou, Ruhui Ma, Miao Pan, Haibing Guan",Machine Learning,"Battery-powered mobile devices (e.g., smartphones, AR/VR glasses, and various IoT devices) are increasingly being used for AI training due to their growing computational power and easy access to valuable, diverse, and real-time data.
On-device training is highly energy-intensive, making accurate energy consumption estimation crucial for effective job scheduling and sustainable AI.
However, the heterogeneity of devices and the complexity of models challenge the accuracy and generalizability of existing methods.
This paper proposesTHOR, a generic approach for energy consumption estimation indeep neural network(DNN) training.
First, we examine the layer-wise energy additivity property ofDNNsand strategically partition the entire model into layers for fine-grained energy consumption profiling.
Then, we fitGaussian Process(GP) models to learn from layer-wise energy consumption measurements and estimate aDNN’s overall energy consumption based on its layer-wise energy additivity property.
We conduct extensive experiments with various types of models across different real-world platforms.
The results demonstrate thatTHORhas effectively reduced theMean Absolute Percentage Error(MAPE) by up to 30%.
Moreover,THORis applied in guiding energy-aware pruning, successfully reducing energy consumption by 50%, thereby further demonstrating its generality and potential."
1483,679d459debd8ffd557a2b438,cs.LG,https://arxiv.org/pdf/2501.16396,TopoNets: High Performing Vision and Language Models with Brain-Like Topography,"Mayukh Deb, Mainak Deb, N. Apurva Ratan Murty","Machine Learning, Neural and Evolutionary Computing, Neurons and Cognition","Neurons in the brain are organized such that nearby cells tend to share similar functions. AI models lack this organization, and past efforts to introduce topography have often led to trade-offs between topography and task performance. In this work, we presentTopoLoss, a new loss function that promotes spatially organized topographic representations in AI models without significantly sacrificing task performance. TopoLoss is highly adaptable and can be seamlessly integrated into the training of leading model architectures. We validate our method on both vision (ResNet-18, ResNet-50, ViT) and language models (GPT-Neo-125M, NanoGPT), collectivelyTopoNets. TopoNets are the highest performing supervised topographic models to date, exhibiting brain-like properties such as localized feature processing, lower dimensionality, and increased efficiency. TopoNets also predict responses in the brain and replicate the key topographic signatures observed in the brain’s visual and language cortices. Together this work establishes a robust and generalizable framework for integrating topography into leading model architectures, advancing the development of high performing models that more closely emulate the computational strategies of the human brain."
1484,679d459debd8ffd557a2b439,cs.LG,https://arxiv.org/pdf/2501.16394,Transformer^-1: Input-Adaptive Computation for Resource-Constrained Deployment,"Lumen AI, Tengzhou No. 1 Middle School, Shihao Ji, Zihui Song, Fucheng Zhong, Jisen Jia, Zhaobo Wu, Zheyi Cao, Xu Tianhao",Machine Learning,
1485,679d459debd8ffd557a2b43a,cs.LG,https://arxiv.org/pdf/2501.16393,"Improving Network Threat Detection by Knowledge Graph, Large Language Model, and Imbalanced Learning","Lili Zhang, Quanyan Zhu, Herman Ray, Ying Xie","Machine Learning, Cryptography and Security, Machine Learning",
1486,679d459debd8ffd557a2b43b,cs.LG,https://arxiv.org/pdf/2501.16392,HMCGeo: IP Region Prediction Based on Hierarchical Multi-label Classification,"Tianzi Zhao, Xinran Liu, Zhaoxin Zhang, Dong Zhao, Ning Li, Zhichao Zhang, Xinye Wang",Machine Learning,"Fine-grained IP geolocation plays a critical role in applications such as location-based services and cybersecurity. Most existing fine-grained IP geolocation methods are regression-based; however, due to noise in the input data, these methods typically encounter kilometer-level prediction errors and provide incorrect region information for users. To address this issue, this paper proposes a novel hierarchical multi-label classification framework for IP region prediction, named HMCGeo. This framework treats IP geolocation as a hierarchical multi-label classification problem and employs residual connection-based feature extraction and attention prediction units to predict the target host region across multiple geographical granularities. Furthermore, we introduce probabilistic classification loss during training, combining it with hierarchical cross-entropy loss to form a composite loss function. This approach optimizes predictions by utilizing hierarchical constraints between regions at different granularities. IP region prediction experiments on the New York, Los Angeles, and Shanghai datasets demonstrate that HMCGeo achieves superior performance across all geographical granularities, significantly outperforming existing IP geolocation methods."
1487,679d459debd8ffd557a2b43c,cs.LG,https://arxiv.org/pdf/2501.16388,Development and Validation of a Dynamic Kidney Failure Prediction Model based on Deep Learning: A Real-World Study with External Validation,"Jingying Ma, Jinwei Wang, Lanlan Lu, Yexiang Sun, Mengling Feng, Peng Shen, Zhiqin Jiang, Shenda Hong, Luxia Zhang","Machine Learning, Applications","Chronic kidney disease (CKD), a progressive disease with high morbidity and mortality, has become a significant global public health problem. At present, most of the models used for predicting the progression of CKD are static models. We aim to develop a dynamic kidney failure prediction model based on deep learning (KFDeep) for CKD patients, utilizing all available data on common clinical indicators from real-world Electronic Health Records (EHRs) to provide real-time predictions."
1488,679d459debd8ffd557a2b43d,cs.LG,https://arxiv.org/pdf/2501.16385,FBQuant: FeedBack Quantization for Large Language Models,"Yijiang Liu, Hengyu Fang, Liulu He, Rongyu Zhang, Yichuan Bai, Yuan Du, Li Du","Machine Learning, Computation and Language","Deploying Large Language Models (LLMs) on edge devices is increasingly important, as it eliminates reliance on network connections, reduces expensive API calls, and enhances user privacy. However, on-device deployment is challenging due to the limited computational resources of edge devices. In particular, the key bottleneck stems from memory bandwidth constraints related to weight loading.
Weight-only quantization effectively reduces memory access, yet often induces significant accuracy degradation.
Recent efforts to incorporate sub-branches have shown promise for mitigating quantization errors, but these methods either lack robust optimization strategies or rely on suboptimal objectives. To address these gaps, we propose FeedBack Quantization (FBQuant), a novel approach inspired by negative feedback mechanisms in automatic control.
FBQuant inherently ensures that the reconstructed weights remain bounded by the quantization process, thereby reducing the risk of overfitting.
To further offset the additional latency introduced by sub-branches, we develop an efficient CUDA kernel that decreases 60% of extra inference time.
Comprehensive experiments demonstrate the efficiency and effectiveness of FBQuant across various LLMs. Notably, for 3-bit Llama2-7B, FBQuant improves zero-shot accuracy by 1.2%."
1489,679d459debd8ffd557a2b43e,cs.LG,https://arxiv.org/pdf/2501.16381,Reduced-order modeling and classification of hydrodynamic pattern formation in gravure printing,"Pauline Rothmann-Brumm, Steven L. Brunton, Isabel Scherl","Machine Learning, Fluid Dynamics",
1490,679d459debd8ffd557a2b43f,cs.LG,https://arxiv.org/pdf/2501.16362,A novel Trunk Branch-net PINN for flow and heat transfer prediction in porous medium,"Haoyun Xing, Kaiyan Jin, Guice Yao, Jin Zhao, Dichu Xu, Dongsheng Wen","Machine Learning, Fluid Dynamics",
1491,679d459debd8ffd557a2b440,cs.LG,https://arxiv.org/pdf/2501.16358,The OpenLAM Challenges,"Anyang Peng, Xinzijian Liu, Ming-Yu Guo, Linfeng Zhang, Han Wang","Machine Learning, Materials Science, Computational Physics",
1492,679d459debd8ffd557a2b441,cs.LG,https://arxiv.org/pdf/2501.17122,Convergence of two-timescale gradient descent ascent dynamics: finite-dimensional and mean-field perspectives,"Jing An, Jianfeng Lu","Optimization and Control, Machine Learning, Numerical Analysis","The two-timescale gradient descent-ascent (GDA) is a canonical gradient algorithm designed to find Nash equilibria in min-max games. We analyze the two-timescale GDA by investigating the effects of learning rate ratios on convergence behavior in both finite-dimensional and mean-field settings. In particular, for finite-dimensional quadratic min-max games, we obtain long-time convergence in near quasi-static regimes through the hypocoercivity method. For mean-field GDA dynamics, we investigate convergence under a finite-scale ratio using a mixed synchronous-reflection coupling technique."
1493,679d459debd8ffd557a2b442,cs.LG,https://arxiv.org/pdf/2501.17110,Solving Roughly Forced Nonlinear PDEs via Misspecified Kernel Methods and Neural Networks,"Ricardo Baptista, Edoardo Calvello, Matthieu Darcy, Houman Owhadi, Andrew M. Stuart, Xianjin Yang","Numerical Analysis, Machine Learning","We consider the use of Gaussian Processes (GPs) or Neural Networks (NNs) to numerically approximate the solutions to nonlinear partial differential equations (PDEs) with rough forcing or source terms, which commonly arise as pathwise solutions to stochastic PDEs.
Kernel methods have recently been generalized to solve nonlinear PDEs by approximating their solutions as the maximum a posteriori estimator of GPs that are conditioned to satisfy the PDE at a finite set of collocation points. The convergence and error guarantees of these methods, however, rely on the PDE being defined in a classical sense and its solution possessing sufficient regularity to belong to the associated reproducing kernel Hilbert space.
We propose a generalization of these methods to handle roughly forced nonlinear PDEs while preserving convergence guarantees with an oversmoothing GP kernel that is misspecified relative to the true solution’s regularity.
This is achieved by conditioning a regular GP to satisfy the PDE with a modified source term in a weak sense (when integrated against a finite number of test functions). This is equivalent to replacing the empiricalL2superscript𝐿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT-loss on the PDE constraint by an empirical negative-Sobolev norm.
We further show that this loss function can be used to extend physics-informed neural networks (PINNs) to stochastic equations, thereby resulting in a new NN-based variant termed Negative Sobolev Norm-PINN (NeS-PINN)."
1494,679d459debd8ffd557a2b443,cs.LG,https://arxiv.org/pdf/2501.17054,Generative diffusion models from a PDE perspective,"Fei Cao, Kimball Johnston, Thomas Laurent, Justin Le, Sébastien Motsch","Probability, Machine Learning","Diffusion models have become the de facto framework for generating new datasets. The core of these models lies in the ability to reverse a diffusion process in time. The goal of this manuscript is to explain, from a PDE perspective, how this method works and how to derive the PDE governing the reverse dynamics as well as to study its solution analytically. By linking forward and reverse dynamics, we show that the reverse process’s distribution has its support contained within the original distribution. Consequently, diffusion methods, in their analytical formulation, do not inherently regularize the original distribution, and thus, there is no generalization principle. This raises a question: where does generalization arise, given that in practice it does occur?
Moreover, we derive an explicit solution to the reverse process’s SDE under the assumption that the starting point of the forward process is fixed. This provides a new derivation that links two popular approaches to generative diffusion models: stable diffusion (discrete dynamics) and the score-based approach (continuous dynamics). Finally, we explore the case where the original distribution consists of a finite set of data points. In this scenario, the reverse dynamics are explicit (i.e., the loss function has a clear minimizer), and solving the dynamics fails to generate new samples: the dynamics converge to the original samples. In a sense, solving the minimization problem exactly is“too good for its own good”(i.e., an overfitting regime)."
1495,679d459debd8ffd557a2b444,cs.LG,https://arxiv.org/pdf/2501.17049,Hellinger-Kantorovich Gradient Flows: Global Exponential Decay of Entropy Functionals,"Alexander Mielke, Jia-Jie Zhu","Analysis of PDEs, Machine Learning, Optimization and Control, Machine Learning","We investigate a family of gradient flows of positive and probability measures, focusing on the Hellinger–Kantorovich (HK) geometry, which unifies transport mechanism of Otto-Wasserstein, and the birth–death mechanism of Hellinger (or Fisher-Rao).
A central contribution is a complete characterization of global exponential decay behaviors of entropy functionals (e.g. KL,χ2superscript𝜒2\chi^{2}italic_χ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT) under Otto-Wasserstein and Hellinger-type gradient flows.
In particular, for the more challenging analysis of HK gradient flows on positive measures—where the typical log-Sobolev arguments fail—we develop a specialized shape-mass decomposition that enables new analysis results.
Our approach also leverages the (Polyak-)Łojasiewicz-type functional inequalities and a careful extension of classical dissipation estimates.
These findings provide a unified and complete theoretical framework for gradient flows and underpin applications in computational algorithms for statistical inference, optimization, and machine learning."
1496,679d459debd8ffd557a2b445,cs.LG,https://arxiv.org/pdf/2501.17011,MIDI-GPT: A Controllable Generative Model for Computer-Assisted Multitrack Music Composition,"Philippe Pasquier, Jeff Ens, Nathan Fradet, Paul Triana, Davide Rizzotti, Jean-Baptiste Rolland, Maryam Safi","Sound, Machine Learning, Multimedia, Audio and Speech Processing","We present and release MIDI-GPT, a generative system based on the Transformer architecture that is designed for computer-assisted music composition workflows. MIDI-GPT supports the infilling of musical material at the track and bar level, and can condition generation on attributes including: instrument type, musical style, note density, polyphony level, and note duration. In order to integrate these features, we employ an alternative representation for musical material, creating a time-ordered sequence of musical events for each track and concatenating several tracks into a single sequence, rather than using a single time-ordered sequence where the musical events corresponding to different tracks are interleaved. We also propose a variation of our representation allowing for expressiveness. We present experimental results that demonstrate that MIDI-GPT is able to consistently avoid duplicating the musical material it was trained on, generate music that is stylistically similar to the training dataset, and that attribute controls allow enforcing various constraints on the generated material. We also outline several real-world applications of MIDI-GPT, including collaborations with industry partners that explore the integration and evaluation of MIDI-GPT into commercial products, as well as several artistic works produced using it."
1497,679d459debd8ffd557a2b446,cs.LG,https://arxiv.org/pdf/2501.16988,Marginal and Conditional Importance Measures from Machine Learning Models and Their Relationship with Conditional Average Treatment Effect,"Mohammad Kaviul Anam Khan, Olli Saarela, Rafal Kustra","Machine Learning, Machine Learning","Interpreting black-box machine learning models is challenging due to their strong dependence on data and inherently non-parametric nature. This paper reintroduces the concept of importance through “Marginal Variable Importance Metric” (MVIM), a model-agnostic measure of predictor importance based on the true conditional expectation function. MVIM evaluates predictors’ influence on continuous or discrete outcomes. A permutation-based estimation approach, inspired byBreiman, (2001)andFisher et al., 2019a, is proposed to estimate MVIM. MVIM estimator is biased when predictors are highly correlated, as black-box models struggle to extrapolate in low-probability regions. To address this, we investigated the bias-variance decomposition of MVIM to understand the source and pattern of the bias under high correlation. A Conditional Variable Importance Metric (CVIM), adapted fromStrobl et al., (2008), is introduced to reduce this bias. Both MVIM and CVIM exhibit a quadratic relationship with the conditional average treatment effect (CATE)."
1498,679d459debd8ffd557a2b447,cs.LG,https://arxiv.org/pdf/2501.16974,Excited-state nonadiabatic dynamics in explicit solvent using machine learned interatomic potentials,"Maximilian X. Tiefenbacher, Brigitta Bachmair, Cheng Giuseppe Chen, Julia Westermayr, Philipp Marquetand, Johannes C. B. Dietschreit, Leticia González","Chemical Physics, Machine Learning",
1499,679d459debd8ffd557a2b448,cs.LG,https://arxiv.org/pdf/2501.16875,Enhancing Web Service Anomaly Detection via Fine-grained Multi-modal Association and Frequency Domain Analysis,"Xixuan Yang, Xin Huang, Chiming Duan, Tong Jia, Shandong Dong, Ying Li, Gang Huang","Software Engineering, Machine Learning","Anomaly detection is crucial for ensuring the stability and reliability of web service systems. Logs and metrics contain multiple information that can reflect the system’s operational state and potential anomalies. Thus, existing anomaly detection methods use logs and metrics to detect web service systems’ anomalies through data fusion approaches. They associate logs and metrics using coarse-grained time window alignment and capture the normal patterns of system operation through reconstruction. However, these methods have two issues that limit their performance in anomaly detection. First, due to asynchrony between logs and metrics, coarse-grained time window alignment cannot achieve a precise association between the two modalities. Second, reconstruction-based methods suffer from severe overgeneralization problems, resulting in anomalies being accurately reconstructed. In this paper, we propose a novel anomaly detection method named FFAD to address these two issues. On the one hand, FFAD employs graph-based alignment to mine and extract associations between the modalities from the constructed log-metric relation graph, achieving precise associations between logs and metrics. On the other hand, we improve the model’s fit to normal data distributions through Fourier Frequency Focus, thereby enhancing the effectiveness of anomaly detection. We validated the effectiveness of our model on two real-world industrial datasets and one open-source dataset. The results show that our method achieves an average anomaly detection F1-score of 93.6%, representing an 8.8% improvement over previous state-of-the-art methods."
1500,679d459debd8ffd557a2b449,cs.LG,https://arxiv.org/pdf/2501.16867,Empirical modeling and hybrid machine learning framework for nucleate pool boiling on microchannel structured surfaces,"Vijay Kuberan, Sateesh Gedupudi","Applied Physics, Machine Learning","Micro-structured surfaces influence nucleation characteristics and bubble dynamics besides increasing the heat transfer surface area, thus enabling efficient nucleate boiling heat transfer. Modeling the pool boiling heat transfer characteristics of these surfaces under varied conditions is essential in diverse applications. A new empirical correlation for nucleate boiling on microchannel structured surfaces has been proposed with the data collected from various experiments in previous studies since the existing correlations are limited by their accuracy and narrow operating ranges. This study also examines various Machine Learning (ML) algorithms and Deep Neural Networks (DNN) on the microchannel structured surfaces dataset to predict the nucleate pool boiling Heat Transfer Coefficient (HTC). With the aim to integrate both the ML and domain knowledge, a Physics-Informed Machine Learning Aided Framework (PIMLAF) is proposed. The proposed correlation in this study is employed as the prior physics-based model for PIMLAF, and a DNN is employed to model the residuals of the prior model. This hybrid framework achieved the best performance in comparison to the other ML models and DNNs. This framework is able to generalize well for different datasets because the proposed correlation provides the baseline knowledge of the boiling behavior. Also, SHAP interpretation analysis identifies the critical parameters impacting the model predictions and their effect on HTC prediction. This analysis further makes the model more robust and reliable."
1501,679d459debd8ffd557a2b44a,cs.LG,https://arxiv.org/pdf/2501.16847,Optimization and Learning in Open Multi-Agent Systems,"Diego Deplano, Nicola Bastianello, Mauro Franceschelli, Karl H. Johansson","Optimization and Control, Machine Learning, Multiagent Systems, Systems and Control",
1502,679d459debd8ffd557a2b44b,cs.LG,https://arxiv.org/pdf/2501.16830,Statistical Analysis of Risk Assessment Factors and Metrics to Evaluate Radicalisation in Twitter,"Raul Lara-Cabrera, Antonio Gonzalez-Pardo, David Camacho","Social and Information Networks, Computers and Society, Machine Learning","Nowadays, Social Networks have become an essential communication tools producing a large amount of information about their users and their interactions, which can be analysed with Data Mining methods. In the last years, Social Networks are being used to radicalise people. In this paper, we study the performance of a set of indicators and their respective metrics, devoted to assess the risk of radicalisation of a precise individual on three different datasets. Keyword-based metrics, even though depending on the written language, performs well when measuring frustration, perception of discrimination as well as declaration of negative and positive ideas about Western society and Jihadism, respectively. However, metrics based on frequent habits such as writing ellipses are not well enough to characterise a user in risk of radicalisation. The paper presents a detailed description of both, the set of indicators used to asses the radicalisation in Social Networks and the set of datasets used to evaluate them. Finally, an experimental study over these datasets are carried out to evaluate the performance of the metrics considered."
1503,679d459debd8ffd557a2b44c,cs.LG,https://arxiv.org/pdf/2501.16817,Enhancing Non-Intrusive Load Monitoring with Features Extracted by Independent Component Analysis,"Sahar Moghimian Hoosh, Ilia Kamyshev, Henni Ouerdane","Systems and Control, Machine Learning","In this paper, a novel neural network architecture is proposed to address the challenges in energy disaggregation algorithms. These challenges include the limited availability of data and the complexity of disaggregating a large number of appliances operating simultaneously. The proposed model utilizes independent component analysis as the backbone of the neural network and is evaluated using the F1-score for varying numbers of appliances working concurrently. Our results demonstrate that the model is less prone to overfitting, exhibits low complexity, and effectively decomposes signals with many individual components. Furthermore, we show that the proposed model outperforms existing algorithms when applied to real-world data."
1504,679d459debd8ffd557a2b44d,cs.LG,https://arxiv.org/pdf/2501.16790,Exponential Family Attention,"Kevin Christian Wibisono, Yixin Wang","Machine Learning, Machine Learning","The self-attention mechanism is the backbone of the transformer neural network underlying most large language models. It can capture complex word patterns and long-range dependencies in natural language. This paper introducesexponential family attention (EFA), a probabilistic generative model that extends self-attention to handle high-dimensional sequence, spatial, or spatial-temporal data of mixed data types, including both discrete and continuous observations. The key idea of EFA is to model each observation conditional on all other existing observations, called the context, whose relevance is learned in a data-driven way via an attention-based latent factor model. In particular, unlike static latent embeddings, EFA uses the self-attention mechanism to capture dynamic interactions in the context, where the relevance of each context observations depends on other observations. We establish an identifiability result and provide a generalization guarantee on excess loss for EFA. Across real-world and synthetic data sets—including U.S. city temperatures, Instacart shopping baskets, and MovieLens ratings—we find that EFA consistently outperforms existing models in capturing complex latent structures and reconstructing held-out data.111Software that replicates the empirical studies can be found athttps://github.com/yixinw-lab/EFA."
1505,679d459debd8ffd557a2b44e,cs.LG,https://arxiv.org/pdf/2501.16768,Towards the Generalization of Multi-view Learning: An Information-theoretical Analysis,"Wen Wen, Tieliang Gong, Yuxin Dong, Shujian Yu, Weizhan Zhang","Machine Learning, Machine Learning","Multiview learning has drawn widespread attention for its efficacy in leveraging cross-view consensus and complementarity information to achieve a comprehensive representation of data. While multi-view learning has undergone vigorous development and achieved remarkable success, the theoretical understanding of its generalization behavior remains elusive. This paper aims to bridge this gap by developing information-theoretic generalization bounds for multi-view learning, with a particular focus on multi-view reconstruction and classification tasks. Our bounds underscore the importance of capturing both consensus and complementary information from multiple different views to achieve maximally disentangled representations. These results also indicate that applying the multi-view information bottleneck regularizer is beneficial for satisfactory generalization performance. Additionally, we derive novel data-dependent bounds under both leave-one-out and supersample settings, yielding computational tractable and tighter bounds. In the interpolating regime, we further establish the fast-rate bound for multi-view learning, exhibiting a faster convergence rate compared to conventional square-root bounds. Numerical results indicate a strong correlation between the true generalization gap and the derived bounds across various learning scenarios."
1506,679d459debd8ffd557a2b44f,cs.LG,https://arxiv.org/pdf/2501.16675,"Variational Schr\""odinger Momentum Diffusion","Kevin Rojas, Yixin Tan, Molei Tao, Yuriy Nevmyvaka, Wei Deng","Machine Learning, Machine Learning","The Momentum Schrödinger Bridge (mSB)(Chen et al., 2023c,)has emerged as a leading method for accelerating generative diffusion processes and reducing transport costs. However, the lack of simulation-free properties inevitably results in high training costs and affects scalability. To obtain a trade-off between transport properties and scalability, we introduce variational Schrödinger momentum diffusion (VSMD), which employs linearized forward score functions (variational scores) to eliminate the dependence on simulated forward trajectories. Our approach leverages a multivariate diffusion process with adaptively transport-optimized variational scores. Additionally, we apply a critical-damping transform to stabilize training by removing the need for score estimations for both velocity and samples. Theoretically, we prove the convergence of samples generated with optimal variational scores and momentum diffusion. Empirical results demonstrate that VSMD efficiently generates anisotropic shapes while maintaining transport efficacy, outperforming overdamped alternatives, and avoiding complex denoising processes. Our approach also scales effectively to real-world data, achieving competitive results in time series and image generation."
1507,679d459debd8ffd557a2b450,cs.LG,https://arxiv.org/pdf/2501.16642,FlowDAS: A Flow-Based Framework for Data Assimilation,"Siyi Chen, Yixuan Jia, Qing Qu, He Sun, Jeffrey A Fessler","Signal Processing, Machine Learning, Image and Video Processing","Data assimilation (DA) is crucial for improving the accuracy of state estimation
in complex dynamical systems
by integrating observational data with physical models.
Traditional solutions rely on either pure model-driven approaches,
such as Bayesian filters that struggle with nonlinearity,
or data-driven methods using deep learning priors,
which often lack generalizability and physical interpretability.
Recently, score-based DA methods have been introduced,
focusing on learning prior distributions but neglecting explicit state transition dynamics,
leading to limited accuracy improvements.
To tackle the challenge, we introduceFlowDAS,
a novel generative model-based framework
using the stochastic interpolants
to unify the learning of state transition dynamics and generative priors.
FlowDAS achieves stable and observation-consistent inference
by initializing from proximal previous states,
mitigating the instability seen in score-based methods.
Our extensive experiments demonstrate FlowDAS’s superior performance on various benchmarks,
from the Lorenz system to high-dimensional fluid super-resolution tasks.
FlowDAS also demonstrates improved tracking accuracy
on practical Particle Image Velocimetry (PIV) task,
showcasing its effectiveness in complex flow field reconstruction."
1508,679d459debd8ffd557a2b451,cs.LG,https://arxiv.org/pdf/2501.16626,Subject Representation Learning from EEG using Graph Convolutional Variational Autoencoders,"Aditya Mishra, Ahnaf Mozib Samin, Ali Etemad, Javad Hashemi","Signal Processing, Machine Learning","We propose GC-VASE, a graph convolutional-based variational autoencoder that leverages contrastive learning for subject representation learning from EEG data. Our method successfully learns robust subject-specific latent representations using the split-latent space architecture tailored for subject identification. To enhance the model’s adaptability to unseen subjects without extensive retraining, we introduce an attention-based adapter network for fine-tuning, which reduces the computational cost of adapting the model to new subjects. Our method significantly outperforms other deep learning approaches, achieving state-of-the-art results with a subject balanced accuracy of 89.81% on the ERP-Core dataset and 70.85% on the SleepEDFx-20 dataset. After subject adaptive fine-tuning using adapters and attention layers, GC-VASE further improves the subject balanced accuracy to 90.31% on ERP-Core. Additionally, we perform a detailed ablation study to highlight the impact of the key components of our method."
1509,679d459debd8ffd557a2b452,cs.LG,https://arxiv.org/pdf/2501.16625,A General Bayesian Framework for Informative Input Design in System Identification,"Alexandros E. Tzikas, Mykel J. Kochenderfer","Systems and Control, Machine Learning","We tackle the problem of informative input design for system identification, where we select inputs, observe the corresponding outputs from the true system, and optimize the parameters of our model to best fit the data. We propose a methodology that is compatible with any system and parametric family of models. Our approach only requires input-output data from the system and first-order information from the model with respect to the parameters.
Our algorithm consists of two modules. First, we formulate the problem of system identification from a Bayesian perspective and propose an approximate iterative method to optimize the model’s parameters. Based on this Bayesian formulation, we are able to define a Gaussian-based uncertainty measure for the model parameters, which we can then minimize with respect to the next selected input. Our method outperforms model-free baselineswithvarious linear and nonlinear dynamics."
1510,679d459debd8ffd557a2b453,cs.LG,https://arxiv.org/pdf/2501.16600,The Power of Perturbation under Sampling in Solving Extensive-Form Games,"Wataru Masaka, Mitsuki Sakamoto, Kenshi Abe, Kaito Ariu, Tuomas Sandholm, Atsushi Iwasaki","Computer Science and Game Theory, Machine Learning, Multiagent Systems","This paper investigates how perturbation does and does not improve the Follow-the-Regularized-Leader (FTRL) algorithm in imperfect-information extensive-form games.
Perturbing the expected payoffs guarantees that the FTRL dynamics reach
an approximate equilibrium, and proper adjustments of the magnitude of the perturbation lead to a Nash equilibrium (last-iterate convergence).
This
approach is robust even when payoffs
are estimated using sampling—as is the case for large games—while the optimistic approach
often becomes unstable.
Building upon those insights, we first develop a general framework for perturbed FTRL algorithms undersampling.
We then empirically show that
in the last-iterate sense, the perturbed FTRL consistently
outperforms the non-perturbed FTRL.
We further identify a divergence function that reduces the variance of the estimates for perturbed payoffs, with which
it significantly outperforms the prior algorithms on Leduc poker
(whose structure is more asymmetric in a sense than that of the other benchmark games)
and consistently performs smooth convergence behavior on all the benchmark games."
1511,679d459debd8ffd557a2b454,cs.LG,https://arxiv.org/pdf/2501.16549,Reconciling Predictive Multiplicity in Practice,"Tina Behzad, Sílvia Casacuberta, Emily Ruth Diana, Alexander Williams Tolbert","Computers and Society, Machine Learning","Many machine learning applications predict individual probabilities, such as the likelihood that a person develops a particular illness. Since these probabilities are unknown, a key question is how to address situations in which different models trained on the same dataset produce varying predictions for certain individuals. This issue is exemplified by the model multiplicity (MM) phenomenon, where a set of comparable models yield inconsistent predictions. Roth, Tolbert, and Weinstein recently introduced a reconciliation procedure, theReconcile algorithm, to address this problem. Given two disagreeing models, the algorithm leverages their disagreement to falsify and improve at least one of the models.
In this paper, we empirically analyze the Reconcile algorithm using five widely-used fairness datasets: COMPAS, Communities and Crime, Adult, Statlog (German Credit Data), and the ACS Dataset. We examine how Reconcile fits within the model multiplicity literature and compare it to existing MM solutions, demonstrating its effectiveness. We also discuss potential improvements to the Reconcile algorithm theoretically and practically. Finally, we extend the Reconcile algorithm to the setting of causal inference, given that different competing estimators can again disagree on specific causal average treatment effect (CATE) values. We present the first extension of the Reconcile algorithm in causal inference, analyze its theoretical properties, and conduct empirical tests. Our results confirm the practical effectiveness of Reconcile and its applicability across various domains."
1512,679d459debd8ffd557a2b455,cs.LG,https://arxiv.org/pdf/2501.16542,UniPET-SPK: A Unified Framework for Parameter-Efficient Tuning of Pre-trained Speech Models for Robust Speaker Verification,"Mufan Sang, John H. L. Hansen","Audio and Speech Processing, Machine Learning, Sound","With excellent generalization ability, self-supervised speech models have shown impressive performance on various downstream speech tasks in the pre-training and fine-tuning paradigm. However, as the size of pre-trained models grows, fine-tuning becomes practically unfeasible due to expanding computation and storage requirements, as well as the risk of overfitting. In this study, we concentrate on exploring parameter-efficient tuning (PET) methods for adapting large-scale pre-trained self-supervised speech models to the speaker verification task. Correspondingly, we propose three parameter-efficient tuning methods: (i) an adapter-tuning method, (ii) a prompt-tuning method, and (iii) a unified framework that effectively incorporates adapter-tuning and prompt-tuning with a dynamically learnable gating mechanism. First, we propose the Inner+Inter Adapter framework, which inserts two types of adapters into pre-trained models, allowing for adaptation of latent features within the intermediate Transformer layers and output embeddings from all Transformer layers, through a parallel adapter design. Second, we propose the Deep Speaker Prompting method that concatenates trainable prompt tokens into the input space of pre-trained models to guide adaptation. Lastly, we propose the UniPET-SPK, a unified framework that effectively incorporates these two alternate PET methods into a single framework with a dynamic trainable gating mechanism. The proposed UniPET-SPK learns to find the optimal mixture of PET methods to match different datasets and scenarios. We conduct a comprehensive set of experiments on several datasets to validate the effectiveness of the proposed parameter-efficient tuning methods. Experimental results on the VoxCeleb, CN-Celeb, and the 1st48-UTD forensic datasets demonstrate that the proposed UniPET-SPK can consistently outperform the two PET methods, fine-tuning, and other parameter-efficient tuning methods, achieving superior performance while updating only 5.4% of the parameters. We further conduct experiments on the CN-Celeb and 1st48-UTD datasets to demonstrate the robustness and generalization ability of the proposed methods for speaker verification in different languages and more challenging scenarios."
1513,679d459debd8ffd557a2b456,cs.LG,https://arxiv.org/pdf/2501.16520,Safe Gradient Flow for Bilevel Optimization,"Sina Sharifi, Nazanin Abolfazli, Erfan Yazdandoost Hamedani, Mahyar Fazlyab","Optimization and Control, Machine Learning, Systems and Control","Bilevel optimization is a key framework in hierarchical decision-making, where one problem is embedded within the constraints of another. In this work, we propose a control-theoretic approach to solving bilevel optimization problems. Our method consists of two components: a gradient flow mechanism to minimize the upper-level objective and a safety filter to enforce the constraints imposed by the lower-level problem. Together, these components form a safe gradient flow that solves the bilevel problem in a single loop. To improve scalability with respect to the lower-level problem’s dimensions, we introduce a relaxed formulation and design a compact variant of the safe gradient flow. This variant minimizes the upper-level objective while ensuring the lower-level solution remains within a user-defined distance. Using Lyapunov analysis, we establish convergence guarantees for the dynamics, proving that they converge to a neighborhood of the optimal solution. Numerical experiments further validate the effectiveness of the proposed approaches. Our contributions provide both theoretical insights and practical tools for efficiently solving bilevel optimization problems."
1514,679d459debd8ffd557a2b457,cs.LG,https://arxiv.org/pdf/2501.16489,Nonparametric Sparse Online Learning of the Koopman Operator,"Boya Hou, Sina Sanjari, Nathan Dahlin, Alec Koppel, Subhonmesh Bose","Machine Learning, Machine Learning, Systems and Control",
1515,679d459debd8ffd557a2b458,cs.LG,https://arxiv.org/pdf/2501.16405,DepoRanker: A Web Tool to predict Klebsiella Depolymerases using Machine Learning,"George Wright, Slawomir Michniewski, Eleanor Jameson, Fayyaz ul Amir Afsar Minhas","Genomics, Machine Learning","Background: Phage therapy shows promise for treating antibiotic-resistantKlebsiellainfections. Identifying phage depolymerases that targetKlebsiellacapsular polysaccharides is crucial, as these capsules contribute to biofilm formation and virulence. However, homology-based searches have limitations in novel depolymerase discovery."
1516,679d459debd8ffd557a2b459,cs.LG,https://arxiv.org/pdf/2501.16386,ILETIA: An AI-enhanced method for individualized trigger-oocyte pickup interval estimation of progestin-primed ovarian stimulation protocol,"Binjian Wu, Qian Li, Zhe Kuang, Hongyuan Gao, Xinyi Liu, Haiyan Guo, Qiuju Chen, Xinyi Liu, Yangruizhe Jiang, Yuqi Zhang, Jinyin Zha, Mingyu Li, Qiuhan Ren, Sishuo Feng, Haicang Zhang, Xuefeng Lu, Jian Zhang","Quantitative Methods, Machine Learning",
1517,679d459debd8ffd557a2b45a,cs.LG,https://arxiv.org/pdf/2501.16384,MambaTron: Efficient Cross-Modal Point Cloud Enhancement using Aggregate Selective State Space Modeling,"Sai Tarun Inaganti, Gennady Petrenko","Signal Processing, Machine Learning","Point cloud enhancement is the process of generating a high-quality point cloud from an incomplete input. This is done by filling in the missing details from a reference like the ground truth via regression, for example. In addition to unimodal image and point cloud reconstruction, we focus on the task of view-guided point cloud completion, where we gather the missing information from an image, which represents a view of the point cloud and use it to generate the output point cloud. With the recent research efforts surrounding state-space models, originally in natural language processing and now in 2D and 3D vision, Mamba has shown promising results as an efficient alternative to the self-attention mechanism. However, there is limited research towards employing Mamba for cross-attention between the image and the input point cloud, which is crucial in multi-modal problems. In this paper, we introduce MambaTron, a Mamba-Transformer cell that serves as a building block for our network which is capable of unimodal and cross-modal reconstruction which includes view-guided point cloud completion.We explore the benefits of Mamba’s long-sequence efficiency coupled with the Transformer’s excellent analytical capabilities through MambaTron. This approach is one of the first attempts to implement a Mamba-based analogue of cross-attention, especially in computer vision. Our model demonstrates a degree of performance comparable to the current state-of-the-art techniques while using a fraction of the computation resources."
1518,679d459debd8ffd557a2b45b,cs.LG,https://arxiv.org/pdf/2501.16334,RNN-Based Models for Predicting Seizure Onset in Epileptic Patients,"Mathan Kumar Mounagurusamy, Thiyagarajan V S, Abdur Rahman, Shravan Chandak, D. Balaji, Venkateswara Rao Jallepalli","Signal Processing, Machine Learning",
1519,679d459debd8ffd557a2b45c,cs.LG,https://arxiv.org/pdf/2501.16325,Tailored Forecasting from Short Time Series via Meta-learning,"Declan A. Norton, Edward Ott, Andrew Pomerance, Brian Hunt, Michelle Girvan","Machine Learning, Chaotic Dynamics, Computational Physics","Machine learning (ML) models can be effective for forecasting the dynamics of unknown systems from time-series data, but they often require large amounts of data and struggle to generalize across systems with varying dynamics. Combined, these issues make forecasting from short time series particularly challenging. To address this problem, we introduce Meta-learning for Tailored Forecasting from Related Time Series (METAFORS), which uses related systems with longer time-series data to supplement limited data from the system of interest. By leveraging a library of models trained on related systems, METAFORS builds tailored models to forecast system evolution with limited data. Using a reservoir computing implementation and testing on simulated chaotic systems, we demonstrate METAFORS’ ability to predict both short-term dynamics and long-term statistics, even when test and related systems exhibit significantly different behaviors and the available data are scarce, highlighting its robustness and versatility in data-limited scenarios."
1520,679d459debd8ffd557a2b45d,cs.LG,https://arxiv.org/pdf/2501.16322,Implicit Bias in Matrix Factorization and its Explicit Realization in a New Architecture,"Yikun Hou, Suvrit Sra, Alp Yurtsever","Machine Learning, Optimization and Control, Machine Learning","Gradient descent for matrix factorization is known to exhibit an implicit bias toward approximately low-rank solutions. While existing theories often assume the boundedness of iterates, empirically the bias persists even with unbounded sequences. We thus hypothesize that implicit bias is driven by divergent dynamics markedly different from the convergent dynamics for data fitting. Using this perspective, we introduce a new factorization model:X≈U⁢D⁢V⊤𝑋𝑈𝐷superscript𝑉topX\approx UDV^{\top}italic_X ≈ italic_U italic_D italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT, whereU𝑈Uitalic_UandV𝑉Vitalic_Vare constrained within norm balls, whileD𝐷Ditalic_Dis a diagonal factor allowing the model to span the entire search space. Our experiments reveal that this model exhibits a strong implicit bias regardless of initialization and step size, yielding truly (rather than approximately) low-rank solutions. Furthermore, drawing parallels between matrix factorization and neural networks, we propose a novel neural network model featuring constrained layers and diagonal components. This model achieves strong performance across various regression and classification tasks while finding low-rank solutions, resulting in efficient and lightweight networks."
1521,679d459debd8ffd557a2b45e,cs.LG,https://arxiv.org/pdf/2501.16265,Training Dynamics of In-Context Learning in Linear Attention,"Yedi Zhang, Aaditya K. Singh, Peter E. Latham, Andrew Saxe",Machine Learning,"While attention-based models have demonstrated the remarkable ability of in-context learning, the theoretical understanding of how these models acquired this ability through gradient descent training is still preliminary. Towards answering this question, we study the gradient descent dynamics of multi-head linear self-attention trained for in-context linear regression. We examine two parametrizations of linear self-attention: one with the key and query weights merged as a single matrix (common in theoretical studies), and one with separate key and query matrices (closer to practical settings). For the merged parametrization, we show the training dynamics has two fixed points and the loss trajectory exhibits a single, abrupt drop. We derive an analytical time-course solution for a certain class of datasets and initialization. For the separate parametrization, we show the training dynamics has exponentially many fixed points and the loss exhibits saddle-to-saddle dynamics, which we reduce to scalar ordinary differential equations. During training, the model implements principal component regression in context with the number of principal components increasing over training time. Overall, we characterize how in-context learning abilities evolve during gradient descent training of linear attention, revealing dynamics of abrupt acquisition versus progressive improvements in models with different parametrizations."
1522,679d459debd8ffd557a2b45f,cs.LG,https://arxiv.org/pdf/2501.16254,Multi-Agent Geospatial Copilots for Remote Sensing Workflows,"Chaehong Lee, Varatheepan Paramanayakam, Andreas Karatzas, Yanan Jian, Michael Fore, Heming Liao, Fuxun Yu, Ruopu Li, Iraklis Anagnostopoulos, Dimitrios Stamoulis",Machine Learning,"We present GeoLLM-Squad, a geospatial Copilot that introduces the novelmulti-agentparadigm to remote sensing (RS) workflows. Unlike existing single-agent approaches that rely on monolithic large language models (LLM), GeoLLM-Squadseparates agentic orchestration from geospatial task-solving, by delegating RS tasks to specialized sub-agents. Built on the open-source AutoGen and GeoLLM-Engine frameworks, our work enables the modular integration of diverse applications, spanning urban monitoring, forestry protection, climate analysis, and agriculture studies. Our results demonstrate that while single-agent systems struggle to scale with increasing RS task complexity, GeoLLM-Squad maintains robust performance, achieving a 17% improvement in agentic correctness over state-of-the-art baselines. Our findings highlight the potential of multi-agent AI in advancing RS workflows."
1523,679d459debd8ffd557a2b460,cs.LG,https://arxiv.org/pdf/2501.16237,Application of Structured State Space Models to High energy physics with locality-sensitive hashing,"Cheng Jiang, Sitian Qian","Machine Learning, Instrumentation and Detectors","Modern high-energy physics (HEP) experiments are increasingly challenged by the vast size and complexity of their datasets, particularly regarding large-scale point cloud processing and long sequences. In this study, to address these challenges, we explore the application of structured state space models (SSMs), proposing one of the first trials to integrate local-sensitive hashing into either a hybrid or pure Mamba Model. Our results demonstrate that pure SSMs could serve as powerful backbones for HEP problems involving tasks for long sequence data with local inductive bias. By integrating locality-sensitive hashing into Mamba blocks, we achieve significant improvements over traditional backbones in key HEP tasks, surpassing them in inference speed and physics metrics while reducing computational overhead. In key tests, our approach demonstrated promising results, presenting a viable alternative to traditional transformer backbones by significantly reducing FLOPS while maintaining robust performance."
1524,679d459debd8ffd557a2b461,cs.LG,https://arxiv.org/pdf/2501.16186,Learn to Optimize Resource Allocation under QoS Constraint of AR,"Shiyong Chen, Yuwei Dai, Shengqian Han",Machine Learning,"This paper studies the uplink and downlink power allocation for interactive augmented reality (AR) services, where live video captured by an AR device is uploaded to the network edge and then the augmented video is subsequently downloaded. By modeling the AR transmission process as a tandem queuing system, we derive an upper bound for the probabilistic quality of service (QoS) requirement concerning end-to-end latency and reliability. The resource allocation with the QoS constraints results in a functional optimization problem. To address it, we design a deep neural network to learn the power allocation policy, leveraging the structure of
optimal power allocation to enhance learning performance. Simulation results demonstrate that the proposed method effectively reduces transmit powers while meeting the QoS requirement."
1525,679d459debd8ffd557a2b462,cs.LG,https://arxiv.org/pdf/2501.16178,SWIFT: Mapping Sub-series with Wavelet Decomposition Improves Time Series Forecasting,"Wenxuan Xie, Fanpu Cao","Machine Learning, Machine Learning","In recent work on time-series prediction, Transformers and even large language models have garnered significant attention due to their strong capabilities in sequence modeling. However, in practical deployments, time-series prediction often requires operation in resource-constrained environments, such as edge devices, which are unable to handle the computational overhead of large models. To address such scenarios, some lightweight models have been proposed, but they exhibit poor performance on non-stationary sequences. In this paper, we proposeSWIFT, a lightweight model that is not only powerful, but also efficient in deployment and inference for Long-term Time Series Forecasting (LTSF). Our model is based on three key points: (i) Utilizing wavelet transform to perform lossless downsampling of time series. (ii) Achieving cross-band information fusion with a learnable filter. (iii) Using only one shared linear layer or one shallow MLP for sub-series’ mapping. We conduct comprehensive experiments, and the results show thatSWIFTachieves state-of-the-art (SOTA) performance on multiple datasets, offering a promising method for edge computing and deployment in this task. Moreover, it is noteworthy that the number of parameters inSWIFT-Linearis only 25% of what it would be with a single-layer linear model for time-domain prediction. Our code is available athttps://github.com/LancelotXWX/SWIFT."
1526,679d459debd8ffd557a2b463,cs.LG,https://arxiv.org/pdf/2501.16153,MILP initialization for solving parabolic PDEs with PINNs,"Sirui Li, Federica Bragone, Matthieu Barreau, Kateryna Morozovska",Machine Learning,"Physics-Informed Neural Networks (PINNs) are a powerful deep learning method capable of providing solutions and parameter estimations of physical systems. Given the complexity of their neural network structure, the convergence speed is still limited compared to numerical methods, mainly when used in applications that model realistic systems. The network initialization follows a random distribution of the initial weights, as in the case of traditional neural networks, which could lead to severe model convergence bottlenecks.
To overcome this problem, we follow current studies that deal with optimal initial weights in traditional neural networks. In this paper, we use a convex optimization model to improve the initialization of the weights in PINNs and accelerate convergence. We investigate two optimization models as a first training step, defined as pre-training, one involving only the boundaries and one including physics. The optimization is focused on the first layer of the neural network part of the PINN model, while the other weights are randomly initialized. We test the methods using a practical application of the heat diffusion equation to model the temperature distribution of power transformers. The PINN model with boundary pre-training is the fastest converging method at the current stage."
1527,679d459debd8ffd557a2b464,cs.LG,https://arxiv.org/pdf/2501.16130,ReFill: Reinforcement Learning for Fill-In Minimization,"Elfarouk Harb, Ho Shan Lam",Machine Learning,"Efficiently solving sparse linear systems𝐀𝐱=𝐛𝐀𝐱𝐛\mathbf{Ax}=\mathbf{b}bold_Ax = bold_b, where𝐀𝐀\mathbf{A}bold_Ais a large, sparse, symmetric positive semi-definite matrix, is a core challenge in scientific computing, machine learning, and optimization. A major bottleneck in Gaussian elimination for these systems isfill-in—the creation of non-zero entries that increase memory and computational cost. Minimizing fill-in is NP-hard, and existing heuristics like Minimum Degree and Nested Dissection offer limited adaptability across diverse problem instances."
1528,679d459debd8ffd557a2b465,cs.LG,https://arxiv.org/pdf/2501.16117,A Unified Analysis of Stochastic Gradient Descent with Arbitrary Data Permutations and Beyond,"Yipeng Li, Xinchen Lyu, Zhenyu Liu",Machine Learning,
1529,679d459debd8ffd557a2b466,cs.LG,https://arxiv.org/pdf/2501.16113,Fixed-sized clusters $k$-Means,"Mikko I. Malinen, Pasi Fränti",Machine Learning,"We present ak𝑘kitalic_k-means-based clustering algorithm, which optimizes the mean square error, for given cluster sizes. A straightforward application is balanced clustering, where the sizes of each cluster are equal. In thek𝑘kitalic_k-means assignment phase, the algorithm solves an assignment problem using the Hungarian algorithm. This makes the assignment phase time complexityO⁢(n3)𝑂superscript𝑛3O(n^{3})italic_O ( italic_n start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ). This enables clustering of datasets of size more than 5000 points."
1530,679d459debd8ffd557a2b467,cs.LG,https://arxiv.org/pdf/2501.16080,Generating Spatial Synthetic Populations Using Wasserstein Generative Adversarial Network: A Case Study with EU-SILC Data for Helsinki and Thessaloniki,Vanja Falck,"Machine Learning, Multiagent Systems","Using agent-based social simulations can enhance our understanding of urban planning, public health, and economic forecasting. Realistic synthetic populations with numerous attributes strengthen these simulations. The Wasserstein Generative Adversarial Network, trained on census data like EU-SILC, can create robust synthetic populations. These methods, aided by external statistics or EU-SILC weights, generate spatial synthetic populations for agent-based models. The increased access to high-quality micro-data has sparked interest in synthetic populations, which preserve demographic profiles and analytical strength while ensuring privacy and preventing discrimination. This study uses national data from Finland and Greece for Helsinki and Thessaloniki to explore balanced spatial synthetic population generation. Results show challenges related to balancing data with or without aggregated statistics for the target population and the general under-representation of fringe profiles by deep generative methods. The latter can lead to discrimination in agent-based simulations."
1531,679d459debd8ffd557a2b468,cs.LG,https://arxiv.org/pdf/2501.16046,Revisiting Projection-Free Online Learning with Time-Varying Constraints,"Yibo Wang, Yuanyu Wan, Lijun Zhang","Machine Learning, Machine Learning","We investigate constrained online convex optimization, in which decisions must belong to a fixed and typically complicated domain, and are required to approximately satisfy additional time-varying constraints over the long term. In this setting, the commonly used projection operations are often computationally expensive or even intractable. To avoid the time-consuming operation, several projection-free methods have been proposed with an𝒪⁢(T3/4⁢log⁡T)𝒪superscript𝑇34𝑇\mathcal{O}(T^{3/4}\sqrt{\log T})caligraphic_O ( italic_T start_POSTSUPERSCRIPT 3 / 4 end_POSTSUPERSCRIPT square-root start_ARG roman_log italic_T end_ARG )regret bound and an𝒪⁢(T7/8)𝒪superscript𝑇78\mathcal{O}(T^{7/8})caligraphic_O ( italic_T start_POSTSUPERSCRIPT 7 / 8 end_POSTSUPERSCRIPT )cumulative constraint violation (CCV) bound for general convex losses. In this paper, we improve this result and further establishnovelregret and CCV bounds when loss functions are strongly convex. The primary idea is to first construct a composite surrogate loss, involving the original loss and constraint functions, by utilizing the Lyapunov-based technique. Then, we propose a parameter-free variant of the classical projection-free method, namely online Frank-Wolfe (OFW), and run this new extension over the online-generated surrogate loss. Theoretically, for general convex losses, we achieve an𝒪⁢(T3/4)𝒪superscript𝑇34\mathcal{O}(T^{3/4})caligraphic_O ( italic_T start_POSTSUPERSCRIPT 3 / 4 end_POSTSUPERSCRIPT )regret bound and an𝒪⁢(T3/4⁢log⁡T)𝒪superscript𝑇34𝑇\mathcal{O}(T^{3/4}\log T)caligraphic_O ( italic_T start_POSTSUPERSCRIPT 3 / 4 end_POSTSUPERSCRIPT roman_log italic_T )CCV bound, both of which are order-wise tighter than existing results. For strongly convex losses, we establish new guarantees of an𝒪⁢(T2/3)𝒪superscript𝑇23\mathcal{O}(T^{2/3})caligraphic_O ( italic_T start_POSTSUPERSCRIPT 2 / 3 end_POSTSUPERSCRIPT )regret bound and an𝒪⁢(T5/6)𝒪superscript𝑇56\mathcal{O}(T^{5/6})caligraphic_O ( italic_T start_POSTSUPERSCRIPT 5 / 6 end_POSTSUPERSCRIPT )CCV bound. Moreover, we also extend our methods to a more challenging setting with bandit feedback, obtaining similar theoretical findings. Empirically, experiments on real-world datasets have demonstrated the effectiveness of our methods."
1532,679d459debd8ffd557a2b469,cs.LG,https://arxiv.org/pdf/2501.16018,Strategic Multi-Armed Bandit Problems Under Debt-Free Reporting,"Ahmed Ben Yahmed, Clément Calauzènes, Vianney Perchet","Machine Learning, Computer Science and Game Theory",
1533,679d459debd8ffd557a2b46a,cs.LG,https://arxiv.org/pdf/2501.16002,ScaDyG:A New Paradigm for Large-scale Dynamic Graph Learning,"Xiang Wu, Xunkai Li, Rong-Hua Li, Kangfei Zhao, Guoren Wang",Machine Learning,"Dynamic graphs (DGs), which capture time-evolving relationships between graph entities, have widespread real-world applications.
To efficiently encode DGs for downstream tasks, most dynamic graph neural networks follow the traditional message-passing mechanism and extend it with time-based techniques.
Despite their effectiveness, the growth of historical interactions introduces significant scalability issues, particularly in industry scenarios.
To address this limitation, we propose ScaDyG, with the core idea of designing a time-aware scalable learning paradigm as follows:
1) Time-aware Topology Reformulation:
ScaDyG first segments historical interactions into time steps (intra and inter) based on dynamic modeling, enabling weight-free and time-aware graph propagation within pre-processing.
2) Dynamic Temporal Encoding:
To further achieve fine-grained graph propagation within time steps, ScaDyG integrates temporal encoding through a combination of exponential functions in a scalable manner.
3) Hypernetwork-driven Message Aggregation:
After obtaining the propagated features (i.e., messages), ScaDyG utilizes hypernetwork to analyze historical dependencies, implementing node-wise representation by an adaptive temporal fusion.
Extensive experiments on 12 datasets demonstrate that ScaDyG performs comparably well or even outperforms other SOTA methods in both node and link-level downstream tasks, with fewer learnable parameters and higher efficiency."
1534,679d459debd8ffd557a2b46b,cs.LG,https://arxiv.org/pdf/2501.15977,Classification Error Bound for Low Bayes Error Conditions in Machine Learning,"Zijian Yang, Vahe Eminyan, Ralf Schlüter, Hermann Ney","Machine Learning, Machine Learning","In statistical classification and machine learning, classification error is an important performance measure, which is minimized by the Bayes decision rule. In practice, the unknown true distribution is usually replaced with a model distribution estimated from the training data in the Bayes decision rule.
This substitution introduces a mismatch between the Bayes error and the model-based classification error. In this work, we apply classification error bounds to study the relationship between the error mismatch and the Kullback-Leibler divergence in machine learning. Motivated by recent observations of low model-based classification errors in many machine learning tasks, bounding the Bayes error to be lower, we propose a linear approximation of the classification error bound for low Bayes error conditions. Then, the bound for class priors are discussed. Moreover, we extend the classification error bound for sequences. Using automatic speech recognition as a representative example of machine learning applications, this work analytically discusses the correlations among different performance measures with extended bounds, including cross-entropy loss, language model perplexity, and word error rate."
1535,679d459debd8ffd557a2b46c,cs.LG,https://arxiv.org/pdf/2501.15973,Integrating Probabilistic Trees and Causal Networks for Clinical and Epidemiological Data,"Sheresh Zahoor, Pietro Liò, Gaël Dias, Mohammed Hasanuzzaman","Machine Learning, Quantitative Methods",
1536,679d459debd8ffd557a2b46d,cs.LG,https://arxiv.org/pdf/2501.15971,REINFORCE-ING Chemical Language Models in Drug Design,"Morgan Thomas, Albert Bou, Gianni De Fabritiis",Machine Learning,"Chemical language models, combined with reinforcement learning, have shown significant promise to efficiently traverse large chemical spaces in drug design. However, the performance of various RL algorithms and their best practices for practical drug design are still unclear. Here, starting from the principles of the REINFORCE algorithm, we investigate the effect of different components from RL theory including experience replay, hill-climbing, baselines to reduce variance, and alternative reward shaping. Additionally we demonstrate how RL hyperparameters can be fine-tuned for effectiveness, efficiency, or chemical regularization as demonstrated using the MolOpt benchmark."
1537,679d459debd8ffd557a2b46e,cs.LG,https://arxiv.org/pdf/2501.15957,Inverse Reinforcement Learning via Convex Optimization,"Hao Zhu, Yuan Zhang, Joschka Boedecker","Machine Learning, Computational Engineering, Finance, and Science, Optimization and Control, Neurons and Cognition","We consider the inverse reinforcement learning (IRL) problem, where an unknown reward function of some Markov decision process is estimated based on observed expert demonstrations.
In most existing approaches, IRL is formulated and solved as a nonconvex optimization problem, posing challenges in scenarios where robustness and reproducibility are critical.
We discuss a convex formulation of the IRL problem (CIRL) initially proposed by Ng and Russel, and reformulate the problem such that the domain-specific languageCVXPYcan be applied directly to specify and solve the convex problem.
We also extend the CIRL problem to scenarios where the expert policy is not given analytically but by trajectory as state-action pairs, which can be strongly inconsistent with optimality, by augmenting some of the constraints.
Theoretical analysis and practical implementation for hyperparameter auto-selection are introduced.
This note helps the users to easily apply CIRL for their problems, without background knowledge on convex optimization."
1538,679d459debd8ffd557a2b46f,cs.LG,https://arxiv.org/pdf/2501.15949,Enhancing the Convergence of Federated Learning Aggregation Strategies with Limited Data,"Judith Sáinz-Pardo Díaz, Álvaro López García",Machine Learning,"The development of deep learning techniques is a leading field applied to cases in which medical data is used, particularly in cases of image diagnosis. This type of data has privacy and legal restrictions that in many cases prevent it from being processed from central servers. However, in this area collaboration between different research centers, in order to create models as robust as possible, trained with the largest quantity and diversity of data available, is a critical point to be taken into account. In this sense, the application of privacy aware distributed architectures, such as federated learning arises. When applying this type of architecture, the server aggregates the different local models trained with the data of each data owner to build a global model. This point is critical and therefore it is fundamental to analyze different ways of aggregation according to the use case, taking into account the distribution of the clients, the characteristics of the model, etc. In this paper we propose a novel aggregation strategy and we apply it to a use case of cerebral magnetic resonance image classification. In this use case the aggregation function proposed manages to improve the convergence obtained over the rounds of the federated learning process in relation to different aggregation strategies classically implemented and applied."
1539,679d459debd8ffd557a2b470,cs.LG,https://arxiv.org/pdf/2501.15942,TimeHF: Billion-Scale Time Series Models Guided by Human Feedback,"Yongzhi Qi, Hao Hu, Dazhou Lei, Jianshen Zhang, Zhengxin Shi, Yulin Huang, Zhengyu Chen, Xiaoming Lin, Zuo-Jun Max Shen",Machine Learning,"Time series neural networks perform exceptionally well in real-world applications but encounter challenges such as limited scalability, poor generalization, and suboptimal zero-shot performance. Inspired by large language models, there’s interest in developing large time series models (LTM) to address these issues. However, current methods struggle with training complexity, adapting human feedback, and achieving high predictive accuracy. We introduce TimeHF, a novel pipeline for creating LTMs with 6 billion parameters, incorporating human feedback. We use patch convolutional embedding to capture long time series information and design a human feedback mechanism called time-series policy optimization. Deployed in JD.com’s supply chain, TimeHF handles automated replenishment for over 20,000 products, improving prediction accuracy by 33.21% over existing methods. This work advances LTM technology and shows significant industrial benefits."
1540,679d459debd8ffd557a2b471,cs.LG,https://arxiv.org/pdf/2501.15925,Efficient Distillation of Deep Spiking Neural Networks for Full-Range Timestep Deployment,"Chengting Yu, Xiaochen Zhao, Lei Liu, Shu Yang, Gaoang Wang, Erping Li, Aili Wang","Machine Learning, Neurons and Cognition","Spiking Neural Networks (SNNs) are emerging as a brain-inspired alternative to traditional Artificial Neural Networks (ANNs), prized for their potential energy efficiency on neuromorphic hardware. Despite this, SNNs often suffer from accuracy degradation compared to ANNs and face deployment challenges due to fixed inference timesteps, which require retraining for adjustments, limiting operational flexibility. To address these issues, our work considers the spatio-temporal property inherent in SNNs, and proposes a novel distillation framework for deep SNNs that optimizes performance across full-range timesteps without specific retraining, enhancing both efficacy and deployment adaptability. We provide both theoretical analysis and empirical validations to illustrate that training guarantees the convergence of all implicit models across full-range timesteps. Experimental results on CIFAR-10, CIFAR-100, CIFAR10-DVS, and ImageNet demonstrate state-of-the-art performance among distillation-based SNNs training methods."
1541,679d459debd8ffd557a2b472,cs.LG,https://arxiv.org/pdf/2501.15910,The Sample Complexity of Online Reinforcement Learning: A Multi-model Perspective,"Michael Muehlebach, Zhiyu He, Michael I. Jordan","Machine Learning, Systems and Control, Optimization and Control, Machine Learning","We study the sample complexity of online reinforcement learning for nonlinear dynamical systems with continuous state and action spaces. Our analysis accommodates a large class of dynamical systems ranging from a finite set of nonlinear candidate models to models with bounded and Lipschitz continuous dynamics, to systems that are parametrized by a compact and real-valued set of parameters. In the most general setting, our algorithm achieves a policy regret of𝒪⁢(N⁢ϵ2+ln⁢(m⁢(ϵ))/ϵ2)𝒪𝑁superscriptitalic-ϵ2ln𝑚italic-ϵsuperscriptitalic-ϵ2\mathcal{O}(N\epsilon^{2}+\mathrm{ln}(m(\epsilon))/\epsilon^{2})caligraphic_O ( italic_N italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + roman_ln ( italic_m ( italic_ϵ ) ) / italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ), whereN𝑁Nitalic_Nis the time horizon,ϵitalic-ϵ\epsilonitalic_ϵis a user-specified discretization width, andm⁢(ϵ)𝑚italic-ϵm(\epsilon)italic_m ( italic_ϵ )measures the complexity of the function class under consideration via its packing number. In the special case where the dynamics are parametrized by a compact and real-valued set of parameters (such as neural networks, transformers, etc.), we prove a policy regret of𝒪⁢(N⁢p)𝒪𝑁𝑝\mathcal{O}(\sqrt{Np})caligraphic_O ( square-root start_ARG italic_N italic_p end_ARG ), wherep𝑝pitalic_pdenotes the number of parameters, recovering earlier sample-complexity results that were derived forlineartime-invariantdynamical systems. While this article focuses on characterizing sample complexity, the proposed algorithms are likely to be useful in practice, due to their simplicity, the ability to incorporate prior knowledge, and their benign transient behavior."
1542,679d459debd8ffd557a2b473,cs.LG,https://arxiv.org/pdf/2501.15900,Investigating the Sensitivity of Pre-trained Audio Embeddings to Common Effects,"Victor Deng, Changhong Wang, Gael Richard, Brian McFee",Machine Learning,
1543,679d459debd8ffd557a2b474,cs.LG,https://arxiv.org/pdf/2501.15881,Multivariate Feature Selection and Autoencoder Embeddings of Ovarian Cancer Clinical and Genetic Data,"Luis Bote-Curiel, Sergio Ruiz-Llorente, Sergio Muñoz-Romero, Mónica Yagüe-Fernández, Arantzazu Barquín, Jesús García-Donas, José Luis Rojo-Álvarez",Machine Learning,
1544,679d459debd8ffd557a2b475,cs.LG,https://arxiv.org/pdf/2501.15850,LLM-attacker: Enhancing Closed-loop Adversarial Scenario Generation for Autonomous Driving with Large Language Models,"Yuewen Mei, Tong Nie, Jian Sun, Ye Tian","Machine Learning, Computer Vision and Pattern Recognition, Robotics","Ensuring and improving the safety of autonomous driving systems (ADS) is crucial for the deployment of highly automated vehicles, especially in safety-critical events.
To address the rarity issue, adversarial scenario generation methods are developed, in which behaviors of traffic participants are manipulated to induce safety-critical events. However, existing methods still face two limitations.
First, identification of the adversarial participant directly impacts the effectiveness of the generation.
However, the complexity of real-world scenarios, with numerous participants and diverse behaviors, makes identification challenging.
Second, the potential of generated safety-critical scenarios to continuously improve ADS performance remains underexplored.
To address these issues, we propose LLM-attacker: a closed-loop adversarial scenario generation framework leveraging large language models (LLMs). Specifically, multiple LLM agents are designed and coordinated to identify optimal attackers. Then, the trajectories of the attackers are optimized to generate adversarial scenarios. These scenarios are iteratively refined based on the performance of ADS, forming a feedback loop to improve ADS.
Experimental results show that LLM-attacker can create more dangerous scenarios than other methods, and the ADS trained with it achieves a collision rate half that of training with normal scenarios.
This indicates the ability of LLM-attacker to test and enhance the safety and robustness of ADS. Video demonstrations are provided at:https://drive.google.com/file/d/1Zv4V3iG7825oyiKbUwS2Y-rR0DQIE1ZA/view."
1545,679d459debd8ffd557a2b476,cs.LG,https://arxiv.org/pdf/2501.15790,Enhancing Synthetic Oversampling for Imbalanced Datasets Using Proxima-Orion Neighbors and q-Gaussian Weighting Technique,"Pankaj Yadav, Vivek Vijay, Gulshan Sihag","Machine Learning, Machine Learning","In this article, we propose a novel oversampling algorithm to increase the number of instances of minority class in an imbalanced dataset. We select two instances, Proxima and Orion, from the set of all minority class instances, based on a combination of relative distance weights and density estimation of majority class instances. Furthermore, the q-Gaussian distribution is used as a weighting mechanism to produce new synthetic instances to improve the representation and diversity. We conduct a comprehensive experiment on 42 datasets extracted from KEEL software and eight datasets from the UCI ML repository to evaluate the usefulness of the proposed (PO-QG) algorithm. Wilcoxon signed-rank test is used to compare the proposed algorithm with five other existing algorithms. The test results show
that the proposed technique improves the overall classification performance. We also demonstrate the PO-QG algorithm to a dataset of Indian patients with sarcopenia."
1546,679d459debd8ffd557a2b477,cs.LG,https://arxiv.org/pdf/2501.15785,Memorization and Regularization in Generative Diffusion Models,"Ricardo Baptista, Agnimitra Dasgupta, Nikola B. Kovachki, Assad Oberai, Andrew M. Stuart","Machine Learning, Dynamical Systems, Optimization and Control","Diffusion models have emerged as a powerful framework for generative modeling.
At the heart of the methodology is score matching: learning gradients of families of log-densities for noisy versions of the data distribution at different scales. When the loss function adopted in score matching is evaluated using empirical data, rather than the population loss, the minimizer corresponds to the score of a time-dependent Gaussian mixture.
However, use of this analytically tractable minimizer leads to data memorization: in both unconditioned and conditioned settings, the generative model returns the training samples. This paper contains an analysis of the dynamical mechanism underlying memorization. The analysis highlights the need for regularization to avoid reproducing the analytically tractable minimizer; and, in so doing, lays the foundations for a principled
understanding of how to regularize. Numerical experiments investigate the properties of: (i) Tikhonov regularization; (ii) regularization designed to promote asymptotic consistency; and (iii) regularizations induced by under-parameterization of a neural network or by early stopping when training a neural network.
These experiments are evaluated in the context of memorization,
and directions for future development of regularization are highlighted."
1547,679d459debd8ffd557a2b478,cs.LG,https://arxiv.org/pdf/2501.15755,GraphICL: Unlocking Graph Learning Potential in LLMs through Structured Prompt Design,"Yuanfu Sun, Zhengnan Ma, Yi Fang, Jing Ma, Qiaoyu Tan",Machine Learning,"The growing importance of textual and relational systems has driven interest in enhancing large language models (LLMs) for graph-structured data, particularly Text-Attributed Graphs (TAGs), where samples are represented by textual descriptions interconnected by edges. While research has largely focused on developing specialized graph LLMs through task-specific instruction tuning,a comprehensive benchmark for evaluating LLMs solely through prompt designremains surprisingly absent. Without such a carefully crafted evaluation benchmark, most if not all, tailored graph LLMs are compared against general LLMs using simplistic queries (e.g., zero-shot reasoning with LLaMA), which can potentially camouflage many advantages as well as unexpected predicaments of them. To achieve more general evaluations and unveil the true potential of LLMs for graph tasks, we introduceGraph In-context Learning (GraphICL) Benchmark, a comprehensive benchmark comprising novel prompt templates designed to capture graph structure and handle limited label knowledge. Our systematic evaluation shows that general-purpose LLMs equipped with our GraphICL outperform state-of-the-art specialized graph LLMs and graph neural network models in resource-constrained settings and out-of-domain tasks. These findings highlight the significant potential of prompt engineering to enhance LLM performance on graph learning tasks without training and offer a strong baseline for advancing research in graph LLMs."
1548,679d459debd8ffd557a2b479,cs.LG,https://arxiv.org/pdf/2501.15735,Selective Experience Sharing in Reinforcement Learning Enhances Interference Management,"Madan Dahal, Mojtaba Vaezi","Machine Learning, Signal Processing","We propose a novel multi-agent reinforcement learning (RL) approach for inter-cell interference mitigation, in which agents selectively share their experiences with other agents. Each base station is equipped with an agent, which receives signal-to-interference-plus-noise ratio from its own associated users. This information is used to evaluate and selectively share experiences with neighboring agents. The idea is that even a few pertinent experiences from other agents can lead to effective learning.
This approach enables fully decentralized training and execution, minimizes information sharing between agents and significantly reduces communication overhead, which is typically the burden of interference management. The proposed method outperforms state-of-the-art multi-agent RL techniques where training is done in a decentralized manner. Furthermore, with a 75% reduction in experience sharing, the proposed algorithm achieves 98% of the spectral efficiency obtained by algorithms sharing all experiences."
1549,679d459debd8ffd557a2b47a,cs.LG,https://arxiv.org/pdf/2501.15728,Integrating Personalized Federated Learning with Control Systems for Enhanced Performance,"Alice Smith, Bob Johnson, Michael Geller","Machine Learning, Systems and Control","In the expanding field of machine learning, federated learning has emerged as a pivotal methodology for distributed data environments, ensuring privacy while leveraging decentralized data sources. However, the heterogeneity of client data and the need for tailored models necessitate the integration of personalization techniques to enhance learning efficacy and model performance. This paper introduces a novel framework that amalgamates personalized federated learning with robust control systems, aimed at optimizing both the learning process and the control of data flow across diverse networked environments.
Our approach harnesses personalized algorithms that adapt to the unique characteristics of each client’s data, thereby improving the relevance and accuracy of the model for individual nodes without compromising the overall system performance. To manage and control the learning process across the network, we employ a sophisticated control system that dynamically adjusts the parameters based on real-time feedback and system states, ensuring stability and efficiency.
Through rigorous experimentation, we demonstrate that our integrated system not only outperforms standard federated learning models in terms of accuracy and learning speed but also maintains system integrity and robustness in face of varying network conditions and data distributions. The experimental results, obtained from a multi-client simulated environment with non-IID data distributions, underscore the benefits of integrating control systems into personalized federated learning frameworks, particularly in scenarios demanding high reliability and precision.
This study not only paves the way for more adaptive and resilient federated learning architectures but also opens up new avenues for research into the convergence of machine learning and control theory. Future work will focus on scaling the proposed framework to more complex and dynamic environments, exploring the potential of deeper integration with advanced control strategies."
1550,679d459debd8ffd557a2b47b,cs.LG,https://arxiv.org/pdf/2501.15722,INRet: A General Framework for Accurate Retrieval of INRs for Shapes,"Yushi Guan, Daniel Kwan, Ruofan Liang, Selvakumar Panneer, Nilesh Jain, Nilesh Ahuja, Nandita Vijaykumar",Machine Learning,"Implicit neural representations (INRs) have become an important method for encoding various data types, such as 3D objects or scenes, images, and videos. They have proven to be particularly effective at representing 3D content, e.g., 3D scene reconstruction from 2D images, novel 3D content creation, as well as the representation, interpolation and completion of 3D shapes. With the widespread generation of 3D data in an INR format, there is a need to support effective organization and retrieval of INRs saved in a data store. A key aspect of retrieval and clustering of INRs in a data store is the formulation of similarity between INRs that would, for example, enable retrieval of similar INRs using a query INR. In this work, we proposeINRet(INRRetrieve), a method for determining similarity between INRs that represent shapes, thus enabling accurate retrieval of similar shape INRs from an INR data store. INRet flexibly supports different INR architectures such as INRs with octree grids, triplanes, and hash grids, as well as different implicit functions including signed/unsigned distance function and occupancy field. We demonstrate that our method is more general and accurate than the existing INR retrieval method, which only supports simple MLP INRs and requires the same architecture between the query and stored INRs. Furthermore, compared to converting INRs to other representations (e.g., point clouds or multi-view images) for 3D shape retrieval, INRet achieves higher accuracy while avoiding the conversion overhead."
1551,679d459debd8ffd557a2b47c,cs.LG,https://arxiv.org/pdf/2501.15705,Disentanglement Analysis in Deep Latent Variable Models Matching Aggregate Posterior Distributions,"Surojit Saha, Sarang Joshi, Ross Whitaker","Machine Learning, Machine Learning","Deep latent variable models (DLVMs) are designed to learn meaningful representations in an unsupervised manner, such that the hidden explanatory factors are interpretable by independent latent variables (aka disentanglement). The variational autoencoder (VAE)[1,2]is a popular DLVM widely studied in disentanglement analysis due to the modeling of the posterior distribution using a factorized Gaussian distribution[3]that encourages the alignment of the latent factors with the latent axes. Several metrics have been proposed recently, assuming that the latent variables explaining the variation in data are aligned with the latent axes (cardinal directions). However, there are other DLVMs, such as the AAE and WAE-MMD (matching the aggregate posterior to the prior), where the latent variables might not be aligned with the latent axes. In this work, we propose a statistical method toevaluate disentanglementfor any DLVMs in general. The proposed technique discovers the latent vectors representing the generative factors of a dataset thatcan be different from the cardinal latent axes. We empirically demonstrate the advantage of the method on two datasets."
1552,679d459debd8ffd557a2b47d,cs.LG,https://arxiv.org/pdf/2501.15696,Random Walk Guided Hyperbolic Graph Distillation,"Yunbo Long, Liming Xu, Stefan Schoepf, Alexandra Brintrup",Machine Learning,"Graph distillation(GD) is an effective approach to extract useful information from large scale network structures.
However, existing methods, which operate inEuclideanspace to generate condensed graphs, struggle to capture the inherent tree-like geometry of real-world networks, resulting in distilled graphs with limited task-specific information for downstream tasks.
Furthermore, these methods often fail to extract dynamic properties from graphs, which are crucial for understanding information flow and facilitating graph continual learning.
This paper presents theHyperbolic GraphDistillation withRandom WalksOptimization (HyDRO), a novel graph distillation approach that leverages hyperbolic embeddings to capture complex geometric patterns and optimize the spectral gap inHyperbolicspace.
Experiments show that HyDRO demonstrates strong task generalization, consistently outperforming state-of-the-art methods in both node classification and link prediction tasks.
HyDRO also effectively preserves graph random walk properties, producing condensed graphs that achieve enhanced performance in continual graph learning.
Additionally, HyDRO achieves competitive results on mainstream graph distillation benchmarks, while maintaining a strong balance between privacy and utility, and exhibiting robust resistance to noises."
1553,679d459debd8ffd557a2b47e,cs.LG,https://arxiv.org/pdf/2501.15677,Exploring the Feasibility of Deep Learning Models for Long-term Disease Prediction: A Case Study for Wheat Yellow Rust in England,"Zhipeng Yuan, Yu Zhang, Gaoshan Bi, Po Yang",Machine Learning,"Wheat yellow rust, caused by the fungus Puccinia striiformis, is a critical disease affecting wheat crops across Britain, leading to significant yield losses and economic consequences.
Given the rapid environmental changes and the evolving virulence of pathogens, there is a growing need for innovative approaches to predict and manage such diseases over the long term.
This study explores the feasibility of using deep learning models to predict outbreaks of wheat yellow rust in British fields, offering a proactive approach to disease management.
We construct a yellow rust dataset with historial weather information and disease indicator acrossing multiple regions in England.
We employ two poweful deep learning models, including fully connected neural networks and long
short-term memory to develop predictive models capable of recognizing patterns and predicting future disease outbreaks.
The prediction task is defined as a time-series analysis task to forecast the occurrence of wheat yellow rust based on previous weather information.
The models are trained and validated in a randomly sliced datasets.
The performance of these models with different predictive time steps are evaluated based on their accuracy, precision, recall, and F1-score.
Preliminary results indicate that deep learning models can effectively capture the complex interactions between multiple factors influencing disease dynamics, demonstrating a promising capacity to forecast wheat yellow rust with considerable accuracy.
Specifically, the fully-connected neural network achieved 83.65% accuracy in a disease prediction task with 6 month predictive time step setup.
These findings highlight the potential of deep learning to transform disease management strategies, enabling earlier and more precise interventions.
Our study provides a methodological framework for employing deep learning in agricultural settings but also opens avenues for future research to enhance the robustness and applicability of predictive models in combating crop diseases globally."
1554,679d459debd8ffd557a2b47f,cs.LG,https://arxiv.org/pdf/2501.15655,A Machine Learning Approach to Automatic Fall Detection of Combat Soldiers,"Leandro Soares, Rodrigo Parracho, Gustavo Venturini, José Gomes, Jonathan Efigenio, Pablo Rangel, Pedro Gonzalez, Joel dos Santos, Diego Brandão, Eduardo Bezerra","Machine Learning, Neural and Evolutionary Computing","Military personnel and security agents often face significant physical risks during conflict and engagement situations, particularly in urban operations. Ensuring the rapid and accurate communication of incidents involving injuries is crucial for the timely execution of rescue operations. This article presents research conducted under the scope of the Brazilian Navy’s “Soldier of the Future” project, focusing on the development of a Casualty Detection System to identify injuries that could incapacitate a soldier and lead to severe blood loss. The study specifically addresses the detection of soldier falls, which may indicate critical injuries such as hypovolemic hemorrhagic shock. To generate the publicly available dataset, we used smartwatches and smartphones as wearable devices to collect inertial data from soldiers during various activities, including simulated falls. The data were used to train 1D Convolutional Neural Networks (CNN1D) with the objective of accurately classifying falls that could result from life-threatening injuries. We explored different sensor placements—on the wrists and near the center of mass—and various approaches to using inertial variables, including linear and angular accelerations. The neural network models were optimized using Bayesian techniques to enhance their performance. The best-performing model and its results, discussed in this article, contribute to the advancement of automated systems for monitoring soldier safety and improving response times in engagement scenarios."
1555,679d459debd8ffd557a2b480,cs.LG,https://arxiv.org/pdf/2501.15646,Mathematical analysis of the gradients in deep learning,"Steffen Dereich, Thang Do, Arnulf Jentzen, Frederic Weber","Machine Learning, Numerical Analysis",
1556,679d459debd8ffd557a2b481,cs.LG,https://arxiv.org/pdf/2501.15627,HardML: A Benchmark For Evaluating Data Science And Machine Learning knowledge and reasoning in AI,Tidor-Vlad Pricope,"Machine Learning, Artificial Intelligence","We present HardML, a benchmark designed to evaluate the knowledge and reasoning abilities in the fields of data science and machine learning. HardML comprises a diverse set of 100 challenging multiple-choice questions, handcrafted over a period of 6 months, covering the most popular and modern branches of data science and machine learning. These questions are challenging even for a typical Senior Machine Learning Engineer to answer correctly. To minimize the risk of data contamination, HardML uses mostly original content devised by the author. Current state-of-the-art AI models achieve a 30% error rate on this benchmark, which is about 3 times larger than the one achieved on the equivalent, well-known MMLU-ML. While HardML is limited in scope and not aiming to push the frontier—primarily due to its multiple-choice nature—it serves as a rigorous and modern testbed to quantify and track the progress of top AI. While plenty benchmarks and experimentation in LLM evaluation exist in other STEM fields like mathematics, physics and chemistry, the sub-fields of data science and machine learning remain fairly underexplored."
1557,679d459debd8ffd557a2b482,cs.LG,https://arxiv.org/pdf/2501.15615,Deterministic Reservoir Computing for Chaotic Time Series Prediction,"Johannes Viehweg, Constanze Poll, Patrick Mäder",Machine Learning,"Reservoir Computing was shown in recent years to be useful as efficient to learn networks in the field of time series tasks. Their randomized initialization, a computational benefit, results in drawbacks in theoretical analysis of large random graphs, because of which deterministic variations are an still open field of research. Building upon Next-Gen Reservoir Computing and the Temporal Convolution Derived Reservoir Computing, we propose a deterministic alternative to the higher-dimensional mapping therein, TCRC-LM and TCRC-CM, utilizing the parameterized but deterministic Logistic mapping and Chebyshev maps. To further enhance the predictive capabilities in the task of time series forecasting, we propose the novel utilization of the Lobachevsky function as non-linear activation function."
1558,679d459debd8ffd557a2b483,cs.LG,https://arxiv.org/pdf/2501.15592,Information Consistent Pruning: How to Efficiently Search for Sparse Networks?,"Soheil Gharatappeh, Salimeh Yasaei Sekeh","Machine Learning, Information Theory, Neural and Evolutionary Computing","Iterative magnitude pruning methods (IMPs), proven to be successful in reducing the number of insignificant nodes in over-parameterized deep neural networks (DNNs), have been getting an enormous amount of attention with the rapid deployment of DNNs into cutting-edge technologies with computation and memory constraints.
Despite IMPs popularity in pruning networks, a fundamental limitation of existing IMP algorithms is the significant training time required for each pruning iteration.
Our paper introduces a novelstopping criterionfor IMPs that monitors information and gradient flows between networks layers and minimizes the training time.
Information Consistent Pruning (InCoP) eliminates the need to retrain the network to its original performance during intermediate steps while maintaining overall performance at the end of the pruning process.
Through our experiments, we demonstrate that our algorithm is more efficient than current IMPs across multiple dataset-DNN combinations.
We also provide theoretical insights into the core idea of our algorithm alongside mathematical explanations of flow-based IMP. Our code is available athttps://github.com/Sekeh-Lab/InfCoP."
1559,679d459debd8ffd557a2b484,cs.LG,https://arxiv.org/pdf/2501.15590,Assessing and Predicting Air Pollution in Asia: A Regional and Temporal Study (2018-2023),"Anika Rahman, Mst. Taskia Khatun","Machine Learning, Applications",
1560,679d459debd8ffd557a2b485,cs.LG,https://arxiv.org/pdf/2501.15554,BoTier: Multi-Objective Bayesian Optimization with Tiered Composite Objectives,"Mohammad Haddadnia, Leonie Grashoff, Felix Strieth-Kalthoff","Machine Learning, Optimization and Control, Methodology, Machine Learning","Scientific optimization problems are usually concerned with balancing multiple competing objectives, which come as preferences over both the outcomes of an experiment (e.g. maximize the reaction yield) and the corresponding input parameters (e.g. minimize the use of an expensive reagent).
Typically, practical and economic considerations define a hierarchy over these objectives, which must be reflected in algorithms for sample-efficient experiment planning.
Herein, we introduceBoTier, a composite objective that can flexibly represent a hierarchy of preferences over both experiment outcomes and input parameters.
We provide systematic benchmarks on synthetic and real-life surfaces, demonstrating the robust applicability ofBoTieracross a number of use cases.
Importantly,BoTieris implemented in an auto-differentiable fashion, enabling seamless integration with theBoTorchlibrary, thereby facilitating adoption by the scientific community."
1561,679d459debd8ffd557a2b486,cs.LG,https://arxiv.org/pdf/2501.15549,Optimal Transport on Categorical Data for Counterfactuals using Compositional Data and Dirichlet Transport,"Agathe Fernandes Machado, Arthur Charpentier, Ewen Gallic","Machine Learning, Methodology","Recently, optimal transport-based approaches have gained attention for deriving counterfactuals, e.g., to quantify algorithmic discrimination. However, in the general multivariate setting, these methods are often opaque and difficult to interpret. To address this, alternative methodologies have been proposed, using causal graphs combined with iterative quantile regressionsPlečko and Meinshausen(2020)or sequential transportFernandes Machado et al.(2025)to examine fairness at the individual level, often referred to as “counterfactual fairness.” Despite these advancements, transporting categorical variables remains a significant challenge in practical applications with real datasets.
In this paper, we propose a novel approach to address this issue. Our method involves (1) converting categorical variables into compositional data and (2) transporting these compositions within the probabilistic simplex ofℝdsuperscriptℝ𝑑\mathbb{R}^{d}blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. We demonstrate the applicability and effectiveness of this approach through an illustration on real-world data, and discuss limitations."
1562,679d459debd8ffd557a2b487,cs.LG,https://arxiv.org/pdf/2501.15542,Estimating the Optimal Number of Clusters in Categorical Data Clustering by Silhouette Coefficient,"Duy-Tai Dinh, Tsutomu Fujinami, Van-Nam Huynh",Machine Learning,
1563,679d459debd8ffd557a2b488,cs.LG,https://arxiv.org/pdf/2501.15499,One Model to Forecast Them All and in Entity Distributions Bind Them,"Kutay Bölat, Simon Tindemans","Machine Learning, Systems and Control","Probabilistic forecasting in power systems often involves multi-entity datasets like households, feeders, and wind turbines, where generating reliable entity-specific forecasts presents significant challenges. Traditional approaches require training individual models for each entity, making them inefficient and hard to scale. This study addresses this problem using GUIDE-VAE, a conditional variational autoencoder that allows entity-specific probabilistic forecasting using a single model. GUIDE-VAE provides flexible outputs, ranging from interpretable point estimates to full probability distributions, thanks to its advanced covariance composition structure. These distributions capture uncertainty and temporal dependencies, offering richer insights than traditional methods."
1564,679d459debd8ffd557a2b489,cs.LG,https://arxiv.org/pdf/2501.15493,RLER-TTE: An Efficient and Effective Framework for En Route Travel Time Estimation with Reinforcement Learning,"Zhihan Zheng, Haitao Yuan, Minxiao Chen, Shangguang Wang",Machine Learning,"En Route Travel Time Estimation (ER-TTE) aims to learn driving patterns from traveled routes to achieve rapid and accurate real-time predictions. However, existing methods ignore the complexity and dynamism of real-world traffic systems, resulting in significant gaps in efficiency and accuracy in real-time scenarios. Addressing this issue is a critical yet challenging task. This paper proposes a novel framework that redefines the implementation path of ER-TTE to achieve highly efficient and effective predictions. Firstly, we introduce a novel pipeline consisting of a Decision Maker and a Predictor to rectify the inefficient prediction strategies of current methods. The Decision Maker performs efficient real-time decisions to determine whether the high-complexity prediction model in the Predictor needs to be invoked, and the Predictor recalculates the travel time or infers from historical prediction results based on these decisions. Next, to tackle the dynamic and uncertain real-time scenarios, we model the online decision-making problem as a Markov decision process and design an intelligent agent based on reinforcement learning for autonomous decision-making. Moreover, to fully exploit the spatio-temporal correlation between online data and offline data, we meticulously design feature representation and encoding techniques based on the attention mechanism. Finally, to improve the flawed training and evaluation strategies of existing methods, we propose an end-to-end training and evaluation approach, incorporating curriculum learning strategies to manage spatio-temporal data for more advanced training algorithms. Extensive evaluations on three real-world datasets confirm that our method significantly outperforms state-of-the-art solutions in both accuracy and efficiency."
1565,679d459debd8ffd557a2b48a,cs.LG,https://arxiv.org/pdf/2501.15461,Mamba-Based Graph Convolutional Networks: Tackling Over-smoothing with Selective State Space,"Xin He, Yili Wang, Wenqi Fan, Xu Shen, Xin Juan, Rui Miao, Xin Wang",Machine Learning,"Graph Neural Networks (GNNs) have shown great success in various graph-based learning tasks.
However, it often faces the issue of over-smoothing as the model depth increases, which causes all node representations to converge to a single value and become indistinguishable.
This issue stems from the inherent limitations of GNNs, which struggle to distinguish the importance of information from different neighborhoods.
In this paper, we introduce MbaGCN, a novel graph convolutional architecture that draws inspiration from the Mamba paradigm—originally designed for sequence modeling.
MbaGCN presents a new backbone for GNNs, consisting of three key components: theMessage Aggregation Layer, theSelective State Space Transition Layer, and theNode State Prediction Layer.
These components work in tandem to adaptively aggregate neighborhood information, providing greater flexibility and scalability for deep GNN models.
While MbaGCN may not consistently outperform all existing methods on each dataset, it provides a foundational framework that demonstrates the effective integration of the Mamba paradigm into graph representation learning. Through extensive experiments on benchmark datasets, we demonstrate that MbaGCN paves the way for future advancements in graph neural network research.
Our code is inhere."
1566,679d459debd8ffd557a2b48b,cs.LG,https://arxiv.org/pdf/2501.15458,Amortized Safe Active Learning for Real-Time Decision-Making: Pretrained Neural Policies from Simulated Nonparametric Functions,"Cen-You Li, Marc Toussaint, Barbara Rakitsch, Christoph Zimmer",Machine Learning,"Active Learning (AL) is a sequential learning approach aiming at selecting the most informative data for model training.
In many systems, safety constraints appear during data evaluation, requiring the development of safe AL methods. Key challenges of AL are the repeated model training and acquisition optimization required for data selection, which become particularly restrictive under safety constraints.
This repeated effort often creates a bottleneck, especially in physical systems requiring real-time decision-making.
In this paper, we propose a novel amortized safe AL framework. By leveraging a pretrained neural network policy, our method eliminates the need for repeated model training and acquisition optimization, achieving substantial speed improvements while maintaining competitive learning outcomes and safety awareness.
The policy is trained entirely on synthetic data utilizing a novel safe AL objective. The resulting policy is highly versatile and adapts to a wide range of systems, as we demonstrate in our experiments.
Furthermore, our framework is modular and we empirically show that we also achieve superior performance for unconstrained time-sensitive AL tasks if we omit the safety requirement."
1567,679d459debd8ffd557a2b48c,cs.LG,https://arxiv.org/pdf/2501.15403,Scaling of hardware-compatible perturbative training algorithms,"Bakhrom G. Oripov, Andrew Dienstfrey, Adam N. McCaughan, Sonia M. Buckley","Machine Learning, Neural and Evolutionary Computing, Optimization and Control",
1568,679d459debd8ffd557a2b48d,cs.LG,https://arxiv.org/pdf/2501.15388,Guaranteed Multidimensional Time Series Prediction via Deterministic Tensor Completion Theory,"Hao Shu, Jicheng Li, Yu Jin, Hailin Wang",Machine Learning,"In recent years, the prediction of multidimensional time series data has become increasingly important due to its wide-ranging applications. Tensor-based prediction methods have gained attention for their ability to preserve the inherent structure of such data. However, existing approaches, such as tensor autoregression and tensor decomposition, often have consistently failed to provide clear assertions regarding the number of samples that can be exactly predicted. While matrix-based methods using nuclear norms address this limitation, their reliance on matrices limits accuracy and increases computational costs when handling multidimensional data. To overcome these challenges, we reformulate multidimensional time series prediction as a deterministic tensor completion problem and propose a novel theoretical framework. Specifically, we develop a deterministic tensor completion theory and introduce theTemporal Convolutional Tensor Nuclear Norm(TCTNN) model.
By convolving the multidimensional time series along the temporal dimension and applying the tensor nuclear norm, our approach identifies the maximum forecast horizon for exact predictions. Additionally, TCTNN achieves superior performance in prediction accuracy and computational efficiency compared to existing methods across diverse real-world datasets, including climate temperature, network flow, and traffic ride data. Our implementation is publicly available athttps://github.com/HaoShu2000/TCTNN."
1569,679d459debd8ffd557a2b48e,cs.LG,https://arxiv.org/pdf/2501.15365,A Transfer Learning Framework for Anomaly Detection in Multivariate IoT Traffic Data,"Mahshid Rezakhani, Tolunay Seyfi, Fatemeh Afghah","Machine Learning, Cryptography and Security, Networking and Internet Architecture","In recent years, rapid technological advancements and expanded Internet access have led to a significant rise in anomalies within network traffic and time-series data. Prompt detection of these irregularities is crucial for ensuring service quality, preventing financial losses, and maintaining robust security standards. While machine learning algorithms have shown promise in achieving high accuracy for anomaly detection, their performance is often constrained by the specific conditions of their training data.
A persistent challenge in this domain is the scarcity of labeled data for anomaly detection in time-series datasets. This limitation hampers the training efficacy of both traditional machine learning and advanced deep learning models. To address this, unsupervised transfer learning emerges as a viable solution, leveraging unlabeled data from a source domain to identify anomalies in an unlabeled target domain. However, many existing approaches still depend on a small amount of labeled data from the target domain.
To overcome these constraints, we propose a transfer learning-based model for anomaly detection in multivariate time-series datasets. Unlike conventional methods, our approach does not require labeled data in either the source or target domains. Empirical evaluations on novel intrusion detection datasets demonstrate that our model outperforms existing techniques in accurately identifying anomalies within an entirely unlabeled target domain."
1570,679d459debd8ffd557a2b48f,cs.LG,https://arxiv.org/pdf/2501.15361,Decentralized Low-Rank Fine-Tuning of Large Language Models,"Sajjad Ghiasvand, Mahnoosh Alizadeh, Ramtin Pedarsani",Machine Learning,"The emergence of Large Language Models (LLMs) such as GPT-4, LLaMA, and BERT has transformed artificial intelligence, enabling advanced capabilities across diverse applications. While parameter-efficient fine-tuning (PEFT) techniques like LoRA offer computationally efficient adaptations of these models, their practical deployment often assumes centralized data and training environments. However, real-world scenarios frequently involve distributed, privacy-sensitive datasets that require decentralized solutions. Federated learning (FL) addresses data privacy by coordinating model updates across clients, but it is typically based on centralized aggregation through a parameter server, which can introduce bottlenecks and communication constraints. Decentralized learning, in contrast, eliminates this dependency by enabling direct collaboration between clients, improving scalability and efficiency in distributed environments. Despite its advantages, decentralized LLM fine-tuning remains underexplored. In this work, we proposeDec-LoRA, an algorithm for decentralized fine-tuning of LLMs based on low-rank adaptation (LoRA). Through extensive experiments on BERT and LLaMA-2 models, we evaluateDec-LoRA’s performance in handling data heterogeneity and quantization constraints, enabling scalable, privacy-preserving LLM fine-tuning in decentralized settings."
1571,679d459debd8ffd557a2b490,cs.LG,https://arxiv.org/pdf/2501.15356,Federated Class-Incremental Learning: A Hybrid Approach Using Latent Exemplars and Data-Free Techniques to Address Local and Global Forgetting,"Milad Khademi Nori, Il-Min Kim, Guanghui, Wang",Machine Learning,"Federated Class-Incremental Learning (FCIL) refers to a scenario where a dynamically changing number of clients collaboratively learn an ever-increasing number of incoming tasks. FCIL is known to suffer from local forgetting due to class imbalance at each client and global forgetting due to class imbalance across clients. We develop a mathematical framework for FCIL that formulates local and global forgetting. Then, we propose an approach called Hybrid Rehearsal (HR), which utilizes latent exemplars and data-free techniques to address local and global forgetting, respectively. HR employs a customized autoencoder designed for both data classification and the generation of synthetic data. To determine the embeddings of new tasks for all clients in the latent space of the encoder, the server uses the Lennard-Jones Potential formulations. Meanwhile, at the clients, the decoder decodes the stored low-dimensional latent space exemplars back to the high-dimensional input space, used to address local forgetting. To overcome global forgetting, the decoder generates synthetic data. Furthermore, our mathematical framework proves that our proposed approach HR can, in principle, tackle the two local and global forgetting challenges. In practice, extensive experiments demonstrate that while preserving privacy, our proposed approach outperforms the state-of-the-art baselines on multiple FCIL benchmarks with low compute and memory footprints."
1572,679d459debd8ffd557a2b491,cs.LG,https://arxiv.org/pdf/2501.15293,"Deep Learning in Early Alzheimer's disease's Detection: A Comprehensive Survey of Classification, Segmentation, and Feature Extraction Methods","Rubab Hafeez, Sadia Waheed, Syeda Aleena Naqvi, Fahad Maqbool, Amna Sarwar, Sajjad Saleem, Muhammad Imran Sharif, Kamran Siddique, Zahid Akhtar",Machine Learning,
1573,679d459debd8ffd557a2b492,cs.LG,https://arxiv.org/pdf/2501.15282,AutoG: Towards automatic graph construction from tabular data,"Zhikai Chen, Han Xie, Jian Zhang, Xiang song, Jiliang Tang, Huzefa Rangwala, George Karypis",Machine Learning,"Recent years have witnessed significant advancements in graph machine learning (GML), with its applications spanning numerous domains. However, the focus of GML has predominantly been on developing powerful models, often overlooking a crucial initial step: constructing suitable graphs from common data formats, such as tabular data.
This construction process is fundamental to applying graph-based models, yet it remains largely understudied and lacks formalization.
Our research aims to address this gap by formalizing the graph construction problem and proposing an effective solution. We identify two critical challenges to achieve this goal: 1. The absence of dedicated datasets to formalize and evaluate the effectiveness of graph construction methods, and 2. Existing automatic construction methods can only be applied to some specific cases, while tedious human engineering is required to generate high-quality graphs.
To tackle these challenges, we present a two-fold contribution.
First, we introduce a set of datasets to formalize and evaluate graph construction methods.
Second, we propose an LLM-based solution, AutoG, automatically generating high-quality graph schemas without human intervention.
The experimental results demonstrate that the quality of constructed graphs is critical to downstream task performance, and AutoG can generate high-quality graphs that rival those produced by human experts."
1574,679d459debd8ffd557a2b493,cs.LG,https://arxiv.org/pdf/2501.15278,PIP: Perturbation-based Iterative Pruning for Large Language Models,"Yi Cao, Wei-Jie Xu, Yucheng Shen, Weijie Shi, Chi-Min Chan, Jiajie Xu","Machine Learning, Computation and Language","The rapid increase in the parameter counts of Large Language Models (LLMs), reaching billions or even trillions, presents significant challenges for their practical deployment, particularly in resource-constrained environments. To ease this issue, we propose PIP (Perturbation-based Iterative Pruning), a novel double-view structured pruning method to optimize LLMs, which combines information from two different views: the unperturbed view and the perturbed view. With the calculation of gradient differences, PIP iteratively prunes those that struggle to distinguish between these two views. Our experiments show that PIP reduces the parameter count by approximately 20% while retaining over 85% of the original model’s accuracy across varied benchmarks. In some cases, the performance of the pruned model is within 5% of the unpruned version, demonstrating PIP’s ability to preserve key aspects of model effectiveness. Moreover, PIP consistently outperforms existing state-of-the-art (SOTA) structured pruning methods, establishing it as a leading technique for optimizing LLMs in environments with constrained resources. Our code is available at:https://github.com/caoyiiiiii/PIP."
1575,679d459debd8ffd557a2b494,cs.LG,https://arxiv.org/pdf/2501.15273,Into the Void: Mapping the Unseen Gaps in High Dimensional Data,"Xinyu Zhang, Tyler Estro, Geoff Kuenning, Erez Zadok, Klaus Mueller","Machine Learning, Human-Computer Interaction",
1576,679d459debd8ffd557a2b495,cs.LG,https://arxiv.org/pdf/2501.15271,Killing it with Zero-Shot: Adversarially Robust Novelty Detection,"Hossein Mirzaei, Mohammad Jafari, Hamid Reza Dehbashi, Zeinab Sadat Taghavi, Mohammad Sabokrou, Mohammad Hossein Rohban",Machine Learning,"Novelty Detection (ND) plays a crucial role in machine learning by identifying new or unseen data during model inference. This capability is especially important for the safe and reliable operation of automated systems. Despite advances in this field, existing techniques often fail to maintain their performance when subject to adversarial attacks. Our research addresses this gap by marrying the merits of nearest-neighbor algorithms with robust features obtained from models pretrained on ImageNet. We focus on enhancing the robustness and performance of ND algorithms. Experimental results demonstrate that our approach significantly outperforms current state-of-the-art methods across various benchmarks, particularly under adversarial conditions. By incorporating robust pretrained features into the k-NN algorithm, we establish a new standard for performance and robustness in the field of robust ND. This work opens up new avenues for research aimed at fortifying machine learning systems against adversarial vulnerabilities. Our implementation is publicly available athttps://github.com/rohban-lab/ZARND."
1577,679d459debd8ffd557a2b496,cs.LG,https://arxiv.org/pdf/2501.15270,Inductive Biases for Zero-shot Systematic Generalization in Language-informed Reinforcement Learning,"Negin Hashemi Dijujin, Seyed Roozbeh Razavi Rohani, Mohammad Mahdi Samiei, Mahdieh Soleymani Baghshah","Machine Learning, Artificial Intelligence, Computation and Language",
1578,679d459debd8ffd557a2b497,cs.LG,https://arxiv.org/pdf/2501.15266,Enhanced Intrusion Detection in IIoT Networks: A Lightweight Approach with Autoencoder-Based Feature Learning,"Tasnimul Hasan, Abrar Hossain, Mufakir Qamar Ansari, Talha Hussain Syed",Machine Learning,"The rapid expansion of the Industrial Internet of Things (IIoT) has significantly advanced digital technologies and interconnected industrial sys- tems, creating substantial opportunities for growth. However, this growth has also heightened the risk of cyberattacks, necessitating robust security measures to protect IIoT networks. Intrusion Detection Systems (IDS) are essential for identifying and preventing abnormal network behaviors and malicious activities. Despite the potential of Machine Learning (ML)-based IDS solutions, existing models often face challenges with class imbalance and multiclass IIoT datasets, resulting in reduced detection accuracy. This research directly addresses these challenges by implementing six innovative approaches to enhance IDS perfor- mance, including leveraging an autoencoder for di- mensional reduction, which improves feature learning and overall detection accuracy. Our proposed Decision Tree model achieved an exceptional F1 score and accuracy of 99.94% on the Edge-IIoTset dataset. Furthermore, we prioritized lightweight model design, ensuring deployability on resource-constrained edge devices. Notably, we are the first to deploy our model on a Jetson Nano, achieving inference times of 0.185 ms for binary classification and 0.187 ms for multiclass classification. These results highlight the novelty and robustness of our approach, offering a practical and efficient solution to the challenges posed by imbalanced and multiclass IIoT datasets, thereby enhancing the detection and prevention of network intrusions."
1579,679d459debd8ffd557a2b498,cs.LG,https://arxiv.org/pdf/2501.15265,Kernel-Based Anomaly Detection Using Generalized Hyperbolic Processes,"Pauline Bourigault, Danilo P. Mandic",Machine Learning,"We present a novel approach to anomaly detection by integrating Generalized Hyperbolic (GH) processes into kernel-based methods. The GH distribution, known for its flexibility in modeling skewness, heavy tails, and kurtosis, helps to capture complex patterns in data that deviate from Gaussian assumptions. We propose a GH-based kernel function and utilize it within Kernel Density Estimation (KDE) and One-Class Support Vector Machines (OCSVM) to develop anomaly detection frameworks. Theoretical results confirmed the positive semi-definiteness and consistency of the GH-based kernel, ensuring its suitability for machine learning applications. Empirical evaluation on synthetic and real-world datasets showed that our method improves detection performance in scenarios involving heavy-tailed and asymmetric or imbalanced distributions.https://github.com/paulinebourigault/GHKernelAnomalyDetect."
1580,679d459debd8ffd557a2b499,cs.LG,https://arxiv.org/pdf/2501.15259,Scalable Decentralized Learning with Teleportation,"Yuki Takezawa, Sebastian U. Stich","Machine Learning, Optimization and Control, Machine Learning","Decentralized SGD can run with low communication costs, but its sparse communication characteristics deteriorate the convergence rate, especially when the number of nodes is large.
In decentralized learning settings, communication is assumed to occur on only a given topology, while in many practical cases, the topology merely represents a preferred communication pattern, and connecting to arbitrary nodes is still possible.
Previous studies have tried to alleviate the convergence rate degradation in these cases by designing topologies with large spectral gaps.
However, the degradation is still significant when the number of nodes is substantial.
In this work, we proposeTeleportation.Teleportationactivates only a subset of nodes, and the active nodes fetch the parameters from previous active nodes.
Then, the active nodes update their parameters by SGD and perform gossip averaging on a relatively small topology comprising only the active nodes.
We show that by activating only a proper number of nodes,Teleportationcan completely alleviate the convergence rate degradation.
Furthermore, we propose an efficient hyperparameter-tuning method to search for the appropriate number of nodes to be activated.
Experimentally, we showed thatTeleportationcan train neural networks more stably and achieve higher accuracy than Decentralized SGD."
1581,679d459debd8ffd557a2b49a,cs.LG,https://arxiv.org/pdf/2501.15217,Predictive Lagrangian Optimization for Constrained Reinforcement Learning,"Tianqi Zhang, Puzhen Yuan, Guojian Zhan, Ziyu Lin, Yao Lyu, Zhenzhi Qin, Jingliang Duan, Liping Zhang, Shengbo Eben Li","Machine Learning, Systems and Control","Constrained optimization is popularly seen in reinforcement learning (RL) for addressing complex control tasks.
From the perspective of dynamic system, iteratively solving a constrained optimization problem can be framed as the temporal evolution of a feedback control system.
Classical constrained optimization methods, such as penalty and Lagrangian approaches, inherently use proportional and integral feedback controllers.
In this paper, we propose a more generic equivalence framework to build the connection between constrained optimization and feedback control system, for the purpose of developing more effective constrained RL algorithms.
Firstly, we define that each step of the system evolution determines the Lagrange multiplier by solving a multiplier feedback optimal control problem (MFOCP). In this problem, the control input is multiplier, the state is policy parameters, the dynamics is described by policy gradient descent, and the objective is to minimize constraint violations.
Then, we introduce a multiplier guided policy learning (MGPL) module to perform policy parameters updating. And we prove that the resulting optimal policy, achieved through alternating MFOCP and MGPL, aligns with the solution of the primal constrained RL problem, thereby establishing our equivalence framework.
Furthermore, we point out that the existing PID Lagrangian is merely one special case within our framework that utilizes a PID controller. We also accommodate the integration of other various feedback controllers, thereby facilitating the development of new algorithms.
As a representative, we employ model predictive control (MPC) as the feedback controller and consequently propose a new algorithm called predictive Lagrangian optimization (PLO).
Numerical experiments demonstrate its superiority over the PID Lagrangian method, achieving a larger feasible region up to7.2%percent7.27.2\%7.2 %and a comparable average reward."
1582,679d459debd8ffd557a2b49b,cs.LG,https://arxiv.org/pdf/2501.15203,Reinforcement Learning Controlled Adaptive PSO for Task Offloading in IIoT Edge Computing,"Minod Perera, Sheik Mohammad Mostakim Fattah, Sajib Mistry, Aneesh Krishna","Machine Learning, Distributed, Parallel, and Cluster Computing","Industrial Internet of Things (IIoT) applications demand efficient task offloading to handle heavy data loads with minimal latency. Mobile Edge Computing (MEC) brings computation closer to devices to reduce latency and server load, optimal performance requires advanced optimization techniques. We propose a novel solution combining Adaptive Particle Swarm Optimization (APSO) with Reinforcement Learning, specifically Soft Actor Critic (SAC), to enhance task offloading decisions in MEC environments. This hybrid approach leverages swarm intelligence and predictive models to adapt to dynamic variables such as human interactions and environmental changes. Our method improves resource management and service quality, achieving optimal task offloading and resource distribution in IIoT edge computing."
1583,679d459debd8ffd557a2b49c,cs.LG,https://arxiv.org/pdf/2501.15194,Reliable Pseudo-labeling via Optimal Transport with Attention for Short Text Clustering,"Zhihao Yao, Jixuan Yin, Bo Li","Machine Learning, Computation, Machine Learning","Short text clustering has gained significant attention in the data mining community. However, the limited valuable information contained in short texts often leads to low-discriminative representations, increasing the difficulty of clustering.
This paper proposes a novel short text clustering framework, called ReliablePseudo-labeling viaOptimalTransport withAttention for Short Text Clustering (POTA), that generate reliable pseudo-labels to aid discriminative representation learning for clustering.
Specially,POTAfirst implements an instance-level attention mechanism to capture the semantic relationships among samples, which are then incorporated as a regularization term into an optimal transport problem. By solving this OT problem, we can yield reliable pseudo-labels that simultaneously account for sample-to-sample semantic consistency and sample-to-cluster global structure information.
Additionally, the proposed OT can adaptively estimate cluster distributions, makingPOTAwell-suited for varying degrees of imbalanced datasets.
Then, we utilize the pseudo-labels to guide contrastive learning to generate discriminative representations and achieve efficient clustering.
Extensive experiments demonstratePOTAoutperforms state-of-the-art methods. The code is available at:https://github.com/YZH0905/POTA-STC/tree/main."
1584,679d459debd8ffd557a2b49d,cs.LG,https://arxiv.org/pdf/2501.15190,A Floating Normalization Scheme for Deep Learning-Based Custom-Range Parameter Extraction in BSIM-CMG Compact Models,"Aasim Ashai, Aakash Jadhav, Biplab Sarkar","Machine Learning, Signal Processing",
1585,679d459debd8ffd557a2b49e,cs.LG,https://arxiv.org/pdf/2501.15189,Extracting Forward Invariant Sets from Neural Network-Based Control Barrier Functions,"Goli Vaisi, James Ferlez, Yasser Shoukry","Machine Learning, Robotics, Systems and Control, Machine Learning","Training Neural Networks (NNs) to serve as Barrier Functions (BFs) is a popular
way to improve the safety of autonomous dynamical systems. Despite significant
practical success, these methods are not generally guaranteed to produce true
BFs in a provable sense, which undermines their intended use as safety
certificates. In this paper, we consider the problem of formally certifying a
learned NN as a BF with respect to state avoidance for an autonomous system:
viz. computing a region of the state space on which the candidate NN is
provably a BF. In particular, we propose a sound algorithm that
efficiently produces such a certificate set for a shallow NN. Our algorithm
combines two novel approaches: it first uses NN reachability tools to identify
a subset of states for which the output of the NN does not increase along
system trajectories;
then, it uses a novel enumeration algorithm for hyperplane arrangements to find
the intersection of the NN’s zero-sub-level set with the first set of states.
In this way, our algorithm soundly finds a subset of states on which the NN is
certified as a BF. We further demonstrate the effectiveness of our algorithm at
certifying for real-world NNs as BFs in two case studies. We complemented these
with scalability experiments that demonstrate the efficiency of our
algorithm."
1586,679d459debd8ffd557a2b49f,cs.LG,https://arxiv.org/pdf/2501.15163,Learning with Noisy Labels: the Exploration of Error Bounds in Classification,"Haixia Liu, Boxiao Li, Can Yang, Yang Wang","Machine Learning, Machine Learning","Numerous studies have shown that label noise can lead to poor generalization performance, negatively affecting classification accuracy. Therefore, understanding the effectiveness of classifiers trained using deep neural networks in the presence of noisy labels is of considerable practical significance. In this paper, we focus on the error bounds of excess risks for classification problems with noisy labels within deep learning frameworks. We begin by exploring loss functions with noise-tolerant properties, ensuring that the empirical minimizer on noisy data aligns with that on the true data. Next, we estimate the error bounds of the excess risks, expressed as a sum of statistical error and approximation error. We estimate the statistical error on a dependent (mixing) sequence, bounding it with the help of the associated independent block sequence. For the approximation error, we first express the classifiers as the composition of the softmax function and a continuous function from[0,1]dsuperscript01𝑑[0,1]^{d}[ 0 , 1 ] start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPTtoℝKsuperscriptℝ𝐾{\mathbb{R}}^{K}blackboard_R start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT. The main task is then to estimate the approximation error for the continuous function from[0,1]dsuperscript01𝑑[0,1]^{d}[ 0 , 1 ] start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPTtoℝKsuperscriptℝ𝐾{\mathbb{R}}^{K}blackboard_R start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT. Finally, we focus on the curse of dimensionality based on the low-dimensional manifold assumption."
1587,679d459debd8ffd557a2b4a0,cs.LG,https://arxiv.org/pdf/2501.15125,FreqMoE: Enhancing Time Series Forecasting through Frequency Decomposition Mixture of Experts,Ziqi Liu,Machine Learning,"Long-term time series forecasting is essential in areas like finance and weather prediction. Besides traditional methods that operate in the time domain, many recent models transform time series data into the frequency domain to better capture complex patterns. However, these methods often use filtering techniques to remove certain frequency signals as noise, which may unintentionally discard important information and reduce prediction accuracy. To address this, we propose the Frequency Decomposition Mixture of Experts (FreqMoE) model, which dynamically decomposes time series data into frequency bands, each processed by a specialized expert. A gating mechanism adjusts the importance of each output of expert based on frequency characteristics, and the aggregated results are fed into a prediction module that iteratively refines the forecast using residual connections. Our experiments demonstrate that FreqMoE outperforms state-of-the-art models, achieving the best performance on 51 out of 70 metrics across all tested datasets, while significantly reducing the number of required parameters to under 50k, providing notable efficiency advantages."
1588,679d459debd8ffd557a2b4a1,cs.LG,https://arxiv.org/pdf/2501.15070,Unifying Prediction and Explanation in Time-Series Transformers via Shapley-based Pretraining,"Qisen Cheng, Jinming Xing, Chang Xue, Xiaoran Yang",Machine Learning,"In this paper, we propose ShapTST, a framework that enables time-series transformers to efficiently generate Shapley-value-based explanations alongside predictions in a single forward pass. Shapley values are widely used to evaluate the contribution of different time-steps and features in a test sample, and are commonly generated through repeatedly inferring on each sample with different parts of information removed. Therefore, it requires expensive inference-time computations that occur at every request for model explanations. In contrast, our framework unifies the explanation and prediction in training through a novel Shapley-based pre-training design, which eliminates the undesirable test-time computation and replaces it with a single-time pre-training. Moreover, this specialized pre-training benefits the prediction performance by making the transformer model more effectively weigh different features and time-steps in the time-series, particularly improving the robustness against data noise that is common to raw time-series data. We experimentally validated our approach on eight public datasets, where our time-series model achieved competitive results in both classification and regression tasks, while providing Shapley-based explanations similar to those obtained with post-hoc computation. Our work offers an efficient and explainable solution for time-series analysis tasks in the safety-critical applications."
1589,679d459debd8ffd557a2b4a2,cs.LG,https://arxiv.org/pdf/2501.15062,Exact Fit Attention in Node-Holistic Graph Convolutional Network for Improved EEG-Based Driver Fatigue Detection,"Meiyan Xu, Qingqing Chen, Duo Chen, Yi Ding, Jingyuan Wang, Peipei Gu, Yijie Pan, Deshuang Huang, Xun Zhang, Jiayang Guo",Machine Learning,"EEG-based fatigue monitoring can effectively reduce the incidence of related traffic accidents. In the past decade, with the advancement of deep learning, convolutional neural networks (CNN) have been increasingly used for EEG signal processing.
However, due to the data’s non-Euclidean characteristics, existing CNNs may lose important spatial information from EEG, specifically channel correlation.
Thus, we propose the node-holistic graph convolutional network (NHGNet), a model that uses graphic convolution to dynamically learn each channel’s features. With exact fit attention optimization, the network captures inter-channel correlations through a trainable adjacency matrix. The interpretability is enhanced by revealing critical areas of brain activity and their interrelations in various mental states.
In validations on two public datasets, NHGNet outperforms the SOTAs. Specifically, in the intra-subject, NHGNet improved detection accuracy by at least 2.34% and 3.42%, and in the inter-subjects, it improved by at least 2.09% and 15.06%.
Visualization research on the model revealed that the central parietal area plays an important role in detecting fatigue levels, whereas the frontal and temporal lobes are essential for maintaining vigilance."
1590,679d459debd8ffd557a2b4a3,cs.LG,https://arxiv.org/pdf/2501.15057,Predictive Modeling and Uncertainty Quantification of Fatigue Life in Metal Alloys using Machine Learning,"Jiang Chang, Deekshith Basvoju, Aleksandar Vakanski, Indrajit Charit, Min Xian","Machine Learning, Materials Science",
1591,679d459debd8ffd557a2b4a4,cs.LG,https://arxiv.org/pdf/2501.15035,Semi-supervised Anomaly Detection with Extremely Limited Labels in Dynamic Graphs,"Jiazhen Chen, Sichao Fu, Zheng Ma, Mingbin Feng, Tony S. Wirjanto, Qinmu Peng",Machine Learning,"Semi-supervised graph anomaly detection (GAD) has recently received increasing attention, which aims to distinguish anomalous patterns from graphs under the guidance of a moderate amount of labeled data and a large volume of unlabeled data. Although these proposed semi-supervised GAD methods have achieved great success, their superior performance will be seriously degraded when the provided labels are extremely limited due to some unpredictable factors. Besides, the existing methods primarily focus on anomaly detection in static graphs, and little effort was paid to consider the continuous evolution characteristic of graphs over time (dynamic graphs). To address these challenges, we propose a novel GAD framework (EL2-DGAD) to tackle anomaly detection problem in dynamic graphs with extremely limited labels. Specifically, a transformer-based graph encoder model is designed to more effectively preserve evolving graph structures beyond the local neighborhood. Then, we incorporate an ego-context hypersphere classification loss to classify temporal interactions according to their structure and temporal neighborhoods while ensuring the normal samples are mapped compactly against anomalous data. Finally, the above loss is further augmented with an ego-context contrasting module which utilizes unlabeled data to enhance model generalization. Extensive experiments on four datasets and three label rates demonstrate the effectiveness of the proposed method in comparison to the existing GAD methods."
1592,679d459debd8ffd557a2b4a5,cs.LG,https://arxiv.org/pdf/2501.15019,Utilizing Graph Neural Networks for Effective Link Prediction in Microservice Architectures,"Ghazal Khodabandeh, Alireza Ezaz, Majid Babaei, Naser Ezzati-Jivan",Machine Learning,
1593,679d459debd8ffd557a2b4a6,cs.LG,https://arxiv.org/pdf/2501.15005,Towards Distributed Backdoor Attacks with Network Detection in Decentralized Federated Learning,"Bohan Liu, Yang Xiao, Ruimeng Ye, Zinan Ling, Xiaolong Ma, Bo Hui",Machine Learning,"Distributed backdoor attacks (DBA) have shown a higher attack success rate than centralized attacks in centralized federated learning (FL). However, it has not been investigated in the decentralized FL. In this paper, we experimentally demonstrate that, while directly applying DBA to decentralized FL, the attack success rate depends on the distribution of attackers in the network architecture. Considering that the attackers can not decide their location, this paper aims to achieve a high attack success rate regardless of the attackers’ location distribution. Specifically, we first design a method to detect the network by predicting the distance between any two attackers on the network. Then, based on the distance, we organize the attackers in different clusters. Lastly, we propose an algorithm todynamicallyembed local patterns decomposed from a global pattern into the different attackers in each cluster. We conduct a thorough empirical investigation and find that our method can, in benchmark datasets,
outperform both centralized attacks and naive DBA in different decentralized frameworks."
1594,679d459debd8ffd557a2b4a7,cs.LG,https://arxiv.org/pdf/2501.14997,Causal Discovery via Bayesian Optimization,"Bao Duong, Sunil Gupta, Thin Nguyen","Machine Learning, Machine Learning","Existing score-based methods for directed acyclic graph (DAG) learning
from observational data struggle to recover the causal graph accurately
and sample-efficiently. To overcome this, in this study, we propose𝐃𝐫𝐁𝐎𝐃𝐫𝐁𝐎\mathbf{DrBO}bold_DrBO(DAGrecovery viaBayesianOptimization)—a novel DAG learning framework leveraging
Bayesian optimization (BO) to find high-scoring DAGs. We show that,
by sophisticatedly choosing the promising DAGs to explore, we can
find higher-scoring ones much more efficiently. To address the scalability
issues of conventional BO in DAG learning, we replace Gaussian Processes
commonly employed in BO with dropout neural networks, trained in a
continual manner, which allows for (i) flexibly modeling the DAG scores
without overfitting, (ii) incorporation of uncertainty into the estimated
scores, and (iii) scaling with the number of evaluations. As a result,𝐃𝐫𝐁𝐎𝐃𝐫𝐁𝐎\mathbf{DrBO}bold_DrBOis computationally efficient and can find the accurate DAG
in fewer trials and less time than existing state-of-the-art methods.
This is demonstrated through an extensive set of empirical evaluations
on many challenging settings with both synthetic and real data. Our
implementation is available athttps://github.com/baosws/DrBO."
1595,679d459debd8ffd557a2b4a8,cs.LG,https://arxiv.org/pdf/2501.14995,GreenAuto: An Automated Platform for Sustainable AI Model Design on Edge Devices,"Xiaolong Tu, Dawei Chen, Kyungtae Han, Onur Altintas, Haoxin Wang",Machine Learning,"We presentGreenAuto, an end-to-end automated platform designed for sustainable AI model exploration, generation, deployment, and evaluation.GreenAutoemploys a Pareto front-based search method within an expanded neural architecture search (NAS) space, guided by gradient descent to optimize model exploration. Pre-trained kernel-level energy predictors estimate energy consumption across all models, providing a global view that directs the search toward more sustainable solutions. By automating performance measurements and iteratively refining the search process,GreenAutodemonstrates the efficient identification of sustainable AI models without the need for human intervention."
1596,679d459debd8ffd557a2b4a9,cs.LG,https://arxiv.org/pdf/2501.14992,Extensive Exploration in Complex Traffic Scenarios using Hierarchical Reinforcement Learning,"Zhihao Zhang, Ekim Yurtsever, Keith A. Redmill","Machine Learning, Robotics","Developing an automated driving system capable of navigating complex traffic environments remains a formidable challenge. Unlike rule-based or supervised learning-based methods, Deep Reinforcement Learning (DRL) based controllers eliminate the need for domain-specific knowledge and datasets, thus providing adaptability to various scenarios. Nonetheless, a common limitation of existing studies on DRL-based controllers is their focus on driving scenarios with simple traffic patterns, which hinders their capability to effectively handle complex driving environments with delayed, long-term rewards, thus compromising the generalizability of their findings.
In response to these limitations, our research introduces a pioneering hierarchical framework that efficiently decomposes intricate decision-making problems into manageable and interpretable subtasks. We adopt a two step training process that trains the high-level controller and low-level controller separately. The high-level controller exhibits an enhanced exploration potential with long-term delayed rewards, and the low-level controller provides longitudinal and lateral control ability using short-term instantaneous rewards. Through simulation experiments, we demonstrate the superiority of our hierarchical controller in managing complex highway driving situations."
1597,679d459debd8ffd557a2b4aa,cs.LG,https://arxiv.org/pdf/2501.14991,Advances in Set Function Learning: A Survey of Techniques and Applications,"Jiahao Xie, Guangmo Tong",Machine Learning,"Set function learning has emerged as a crucial area in machine learning, addressing the challenge of modeling functions that take sets as inputs. Unlike traditional machine learning that involves fixed-size input vectors where the order of features matters, set function learning demands methods that are invariant to permutations of the input set, presenting a unique and complex problem. This survey provides a comprehensive overview of the current development in set function learning, covering foundational theories, key methodologies, and diverse applications. We categorize and discuss existing approaches, focusing on deep learning approaches, such as DeepSets and Set Transformer based methods, as well as other notable alternative methods beyond deep learning, offering a complete view of current models. We also introduce various applications and relevant datasets, such as point cloud processing and multi-label classification, highlighting the significant progress achieved by set function learning methods in these domains. Finally, we conclude by summarizing the current state of set function learning approaches and identifying promising future research directions, aiming to guide and inspire further advancements in this promising field."
1598,679d459debd8ffd557a2b4ab,cs.LG,https://arxiv.org/pdf/2501.14985,DepressionX: Knowledge Infused Residual Attention for Explainable Depression Severity Assessment,"Yusif Ibrahimov, Tarique Anwar, Tommy Yuan",Machine Learning,"In today’s interconnected society, social media platforms have become an important part of our lives, where individuals virtually express their thoughts, emotions, and moods. These expressions offer valuable insights into their mental health. This paper explores the use of platforms like Facebook,𝕏𝕏\mathbb{X}blackboard_X(formerly Twitter), and Reddit for mental health assessments. We propose a domain knowledge-infused residual attention model calledDepressionXfor explainable depression severity detection. Existing deep learning models on this problem have shown considerable performance, but they often lack transparency in their decision-making processes. In healthcare, where decisions are critical, the need for explainability is crucial. In our model, we address the critical gap by focusing on the explainability of depression severity detection while aiming for a high performance accuracy. In addition to being explainable, our model consistently outperforms the state-of-the-art models by over 7% in terms ofF1subscriptF1\text{F}_{1}F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTscore on balanced as well as imbalanced datasets. Our ultimate goal is to establish a foundation for trustworthy and comprehensible analysis of mental disorders via social media."
1599,679d459debd8ffd557a2b4ac,cs.LG,https://arxiv.org/pdf/2501.14964,Personalized Layer Selection for Graph Neural Networks,"Kartik Sharma, Vineeth Rakesh Mohan, Yingtong Dou, Srijan Kumar, Mahashweta Das",Machine Learning,"Graph Neural Networks (GNNs) combine node attributes over a fixed granularity of the local graph structure around a node to predict its label. However, different nodes may relate to a node-level property with a different granularity of its local neighborhood, and using the same level of smoothing for all nodes can be detrimental to their classification. In this work, we challenge the common fact that a single GNN layer can classify all nodes of a graph by training GNNs with a distinctpersonalizedlayer for each node. Inspired by metric learning, we propose a novel algorithm,MetSelect111Due to IP restrictions, we are still working on getting the code approval and plan to release it upon acceptance, to select the optimal representation layer to classify each node.
In particular, we identify a prototype
representation of each class in a transformed GNN layer and then, classify using the layer where the distance is smallest to a class prototype after normalizing with that layer’s variance. Results on10101010datasets
and3333different GNNs show that we significantly improve the node classification accuracy of GNNs in a plug-and-play manner. We also find that using variable layers for prediction enables GNNs to be deeper and more robust to poisoning attacks.
We hope this work can inspire future works to learn more adaptive and personalized graph representations."
1600,679d459debd8ffd557a2b4ad,cs.LG,https://arxiv.org/pdf/2501.14926,Interpretability in Parameter Space: Minimizing Mechanistic Description Length with Attribution-based Parameter Decomposition,"Dan Braun, Lucius Bushnaq, Stefan Heimersheim, Jake Mendel, Lee Sharkey","Machine Learning, Machine Learning","Mechanistic interpretability aims to understand the internal mechanisms learned by neural networks. Despite recent progress toward this goal, it remains unclear how best to decompose neural network parameters into mechanistic components.We introduceAttribution-based Parameter Decomposition(APD), a method that directly decomposes a neural network’s parameters into components that (i) are faithful to the parameters of the original network, (ii) require a minimal number of components to process any input, and (iii) are maximally simple. Our approach thus optimizes for a minimal length description of the network’s mechanisms. We demonstrate APD’s effectiveness by successfully identifying ground truth mechanisms in multiple toy experimental settings: Recovering features from superposition; separating compressed computations; and identifying cross-layer distributed representations. While challenges remain to scaling APD to non-toy models, our results suggest solutions to several open problems in mechanistic interpretability, including identifying minimal circuits in superposition, offering a conceptual foundation for ‘features’, and providing an architecture-agnostic framework for neural network decomposition."
1601,679d459debd8ffd557a2b4ae,cs.LG,https://arxiv.org/pdf/2501.14889,Iterative Feature Space Optimization through Incremental Adaptive Evaluation,"Yanping Wu, Yanyong Huang, Zhengzhang Chen, Zijun Yao, Yanjie Fu, Kunpeng Liu, Xiao Luo, Dongjie Wang",Machine Learning,"Iterative feature space optimization involves systematically evaluating and adjusting the feature space to improve downstream task performance. However, existing works suffer from three key limitations: 1) overlooking differences among data samples leads to evaluation bias; 2) tailoring feature spaces to specific machine learning models results in overfitting and poor generalization; 3) requiring the evaluator to be retrained from scratch during each optimization iteration significantly reduces the overall efficiency of the optimization process.
To bridge these gaps, we propose a gEneralizedAdaptive featureSpaceEvaluator (EASE) to efficiently produce optimal and generalized feature spaces.
This framework consists of two key components: Feature-Sample Subspace Generator and Contextual Attention Evaluator.
The first component aims to decouple the information distribution within the feature space to mitigate evaluation bias.
To achieve this, we first identify features most relevant to prediction tasks and samples most challenging for evaluation based on feedback from the subsequent evaluator.
These identified feature and samples are then used to construct feature subspaces for next optimization iteration.
This decoupling strategy makes the evaluator consistently target the most challenging aspects of the feature space.
The second component intends to incrementally capture evolving patterns of the feature space for efficient evaluation.
We propose a weighted-sharing multi-head attention mechanism to encode key characteristics of the feature space into an embedding vector for evaluation.
Moreover, the evaluator is updated incrementally, retaining prior evaluation knowledge while incorporating new insights, as consecutive feature spaces during the optimization process share partial information.
Extensive experiments on fourteen real-world datasets demonstrate the effectiveness of the proposed framework.
Our code and data are publicly available111Https://anonymous.4open.science/r/EASE-1C51."
1602,679d459debd8ffd557a2b4af,cs.LG,https://arxiv.org/pdf/2501.14817,A Cutting Mechanics-based Machine Learning Modeling Method to Discover Governing Equations of Machining Dynamics,"Alisa Ren, Mason Ma, Jiajie Wu, Jaydeep Karandikar, Chris Tyler, Tony Shi, Tony Schmitz","Machine Learning, Computational Engineering, Finance, and Science",
1603,679d459debd8ffd557a2b4b0,cs.LG,https://arxiv.org/pdf/2501.16306,Graph Neural Network Based Hybrid Beamforming Design in Wideband Terahertz MIMO-OFDM Systems,"Beier Li, Mai Vu","Signal Processing, Machine Learning, Networking and Internet Architecture","6G wireless technology is projected to adopt higher and wider frequency bands, enabled by highly directional beamforming. However, the vast bandwidths available also make the impact of beam squint in massive multiple input and multiple output (MIMO) systems non-negligible. Traditional approaches such as adding a true-time-delay line (TTD) on each antenna are costly due to the massive antenna arrays required. This paper puts forth a signal processing alternative, specifically adapted to the multicarrier structure of OFDM systems, through an innovative application of Graph Neural Networks (GNNs) to optimize hybrid beamforming. By integrating two types of graph nodes to represent the analog and the digital beamforming matrices efficiently, our approach not only reduces the computational and memory burdens but also achieves high spectral efficiency performance, approaching that of all digital beamforming. The GNN runtime and memory requirement are at a fraction of the processing time and resource consumption of traditional signal processing methods, hence enabling real-time adaptation of hybrid beamforming. Furthermore, the proposed GNN exhibits strong resiliency to beam squinting, achieving almost constant spectral efficiency even as the system bandwidth increases at higher carrier frequencies."
1604,679d459debd8ffd557a2b4b1,cs.LG,https://arxiv.org/pdf/2501.16226,The Effect of Optimal Self-Distillation in Noisy Gaussian Mixture Model,"Kaito Takanami, Takashi Takahashi, Ayaka Sakata","Machine Learning, Disordered Systems and Neural Networks, Machine Learning",
1605,679d459debd8ffd557a2b4b2,cs.LG,https://arxiv.org/pdf/2501.16212,An FPGA-Based Neuro-Fuzzy Sensor for Personalized Driving Assistance,"Óscar Mata-Carballeira, Jon Gutiérrez-Zaballa, Inés del Campo, Victoria Martínez","Robotics, Machine Learning","Advanced driving-assistance systems (ADAS) are intended to automatize driver tasks, as well as improve driving and vehicle safety. This work proposes an intelligent neuro-fuzzy sensor for driving style (DS) recognition, suitable for ADAS enhancement. The development of the driving style intelligent sensor uses naturalistic driving data from the SHRP2 study, which includes data from a CAN bus, inertial measurement unit, and front radar. The system has been successfully implemented using a field-programmable gate array (FPGA) device of the Xilinx Zynq programmable system-on-chip (PSoC). It can mimic the typical timing parameters of a group of drivers as well as tune these typical parameters to model individual DSs. The neuro-fuzzy intelligent sensor provides high-speed real-time active ADAS implementation and is able to personalize its behavior into safe margins without driver intervention. In particular, the personalization procedure of the time headway (THW) parameter for an ACC in steady car following was developed, achieving a performance of 0.53 microseconds. This performance fulfilled the requirements of cutting-edge active ADAS specifications."
1606,679d459debd8ffd557a2b4b3,cs.LG,https://arxiv.org/pdf/2501.16209,Solving Turbulent Rayleigh-B\'enard Convection using Fourier Neural Operators,"Michiel Straat, Thorben Markmann, Barbara Hammer","Fluid Dynamics, Machine Learning","We trainFourier Neural Operator(FNO) surrogate models forRayleigh-Bénard Convection(RBC), a model for convection processes that occur in nature and industrial settings. We compare the prediction accuracy and model properties ofFNOsurrogates to two popular surrogates used in fluid dynamics:Dynamic Mode Decomposition(DMD) and theLinearly-Recurrent Autoencoder Network(LRAN). We regardDirect Numerical Simulationsof theRBCequations as the ground truth on which the models are trained and evaluated in different settings. TheFNOperforms favorably when compared to theDMDandLRANand its predictions are fast and highly accurate for this task. Additionally, we show its zero-shot super-resolution ability for the convection dynamics. TheFNOmodel has a high potential to be used in downstream tasks such as flow control inRBC."
1607,679d459debd8ffd557a2b4b4,cs.LG,https://arxiv.org/pdf/2501.16171,"Separate This, and All of these Things Around It: Music Source Separation via Hyperellipsoidal Queries","Karn N. Watcharasupat, Alexander Lerch","Audio and Speech Processing, Information Retrieval, Machine Learning, Sound","Music source separation is an audio-to-audio retrieval task of extracting one or more constituent components, or composites thereof, from a musical audio mixture.
Each of these constituent components is often referred to as a “stem” in literature. Historically, music source separation has been dominated by a stem-based paradigm, leading to most state-of-the-art systems being either a collection of single-stem extraction models, or a tightly coupled system with a fixed, difficult-to-modify, set of supported stems. Combined with the limited data availability, advances in music source separation have thus been mostly limited to the “VDBO” set of stems:vocals,drum,bass, and the catch-allothers. Recent work in music source separation has begun to challenge the fixed-stem paradigm, moving towards models able to extract any musical sound as long as this target type of sound could be specified to the model as an additional query input.
We generalize this idea to aquery-by-regionsource separation system, specifying the target based on the query regardless of how many sound sources or which sound classes are contained within it. To do so, we propose the use of hyperellipsoidal regions as queries to allow for an intuitive yet easily parametrizable approach to specifying both the target (location) as well as its spread. Evaluation of the proposed system on the MoisesDB dataset demonstrated state-of-the-art performance of the proposed system both in terms of signal-to-noise ratios and retrieval metrics."
1608,679d459debd8ffd557a2b4b5,cs.LG,https://arxiv.org/pdf/2501.16120,Copyright and Competition: Estimating Supply and Demand with Unstructured Data,"Sukjin Han, Kyungho Lee","Econometrics, Machine Learning, Applications, Machine Learning","Copyright policies play a pivotal role in protecting the intellectual property of creators and companies in creative industries. The advent of cost-reducing technologies, such as generative AI, in these industries calls for renewed attention to the role of these policies. This paper studies product positioning and competition in a market of creatively differentiated products and the competitive and welfare effects of copyright protection. A common feature of products with creative elements is that their key attributes (e.g., images and text) areunstructuredand thus high-dimensional. We focus on a stylized design product, fonts, and use data from the world’s largest online marketplace for fonts. We use neural network embeddings to quantify unstructured attributes and measure the visual similarity. We show that this measure closely aligns with actual human perception. Based on this measure, we empirically find that competitions occur locally in the visual characteristics space. We then develop a structural model for supply and demand that integrate the embeddings. Through counterfactual analyses, we find that local copyright protection can enhance consumer welfare when products are relocated, and the interplay between copyright and cost-reducing technologies is essential in determining an optimal policy for social welfare. We believe that the embedding analysis and empirical models introduced in this paper can be applicable to a range of industries where unstructured data captures essential features of products and markets."
1609,679d459debd8ffd557a2b4b6,cs.LG,https://arxiv.org/pdf/2501.16110,Using Generative Models to Produce Realistic Populations of UK Windstorms,"Yee Chun Tsoi, Kieran M. R. Hunt, Len Shaffrey, Atta Badii, Richard Dixon, Ludovico Nicotina","Atmospheric and Oceanic Physics, Machine Learning",
1610,679d459debd8ffd557a2b4b7,cs.LG,https://arxiv.org/pdf/2501.16086,Value-oriented forecast reconciliation for renewables in electricity markets,"Honglin Wen, Pierre Pinson","Machine Learning, Machine Learning","Forecast reconciliation is considered an effective method for achieving coherence and improving forecast accuracy. However, the value of reconciled forecasts in downstream decision-making tasks has been mostly overlooked. In a multi-agent setup with heterogeneous loss functions, this oversight may lead to unfair outcomes, hence resulting in conflicts during the reconciliation process. To address this, we propose a value-oriented forecast reconciliation approach that focuses on the forecast value for individual agents. Fairness is ensured through the use of a Nash bargaining framework. Specifically, we model this problem as a cooperative bargaining game, where each agent aims to optimize their own gain while contributing to the overall reconciliation process. We then present a primal-dual algorithm for parameter estimation based on empirical risk minimization. From an application perspective, we consider an aggregated wind energy trading problem, where profits are distributed using a weighted allocation rule. We demonstrate the effectiveness of our approach through several numerical experiments, showing that it consistently results in increased profits for all agents involved."
1611,679d459debd8ffd557a2b4b8,cs.LG,https://arxiv.org/pdf/2501.16008,Gaussian credible intervals in Bayesian nonparametric estimation of the unseen,"Claudia Contardi, Emanuele Dolera, Stefano Favaro","Methodology, Machine Learning, Machine Learning, Other Statistics","The unseen-species problem assumesn≥1𝑛1n\geq 1italic_n ≥ 1samples from a population of individuals belonging to different species, possibly infinite, and calls for estimating the numberKn,msubscript𝐾𝑛𝑚K_{n,m}italic_K start_POSTSUBSCRIPT italic_n , italic_m end_POSTSUBSCRIPTof hitherto unseen species that would be observed ifm≥1𝑚1m\geq 1italic_m ≥ 1new samples were collected from the same population. This is a long-standing problem in statistics, which has gained renewed relevance in biological and physical sciences, particularly in settings with large values ofn𝑛nitalic_nandm𝑚mitalic_m. In this paper, we adopt a Bayesian nonparametric approach to the unseen-species problem under the Pitman-Yor prior, and propose a novel methodology to derive largem𝑚mitalic_masymptotic credible intervals forKn,msubscript𝐾𝑛𝑚K_{n,m}italic_K start_POSTSUBSCRIPT italic_n , italic_m end_POSTSUBSCRIPT, for anyn≥1𝑛1n\geq 1italic_n ≥ 1. By leveraging a Gaussian central limit theorem for the posterior distribution ofKn,msubscript𝐾𝑛𝑚K_{n,m}italic_K start_POSTSUBSCRIPT italic_n , italic_m end_POSTSUBSCRIPT, our method improves upon competitors in two key aspects: firstly, it enables the full parameterization of the Pitman-Yor prior, including the Dirichlet prior; secondly, it avoids the need of Monte Carlo sampling, enhancing computational efficiency. We validate the proposed method on synthetic and real data, demonstrating that it improves the empirical performance of competitors by significantly narrowing the gap between asymptotic and exact credible intervals for anym≥1𝑚1m\geq 1italic_m ≥ 1."
1612,679d459debd8ffd557a2b4b9,cs.LG,https://arxiv.org/pdf/2501.15941,SAPPHIRE: Preconditioned Stochastic Variance Reduction for Faster Large-Scale Statistical Learning,"Jingruo Sun, Zachary Frangella, Madeleine Udell","Machine Learning, Machine Learning","Regularized empirical risk minimization (rERM) has become important in data-intensive fields such as genomics and advertising,
with stochastic gradient methods typically used to solve the largest problems.
However, ill-conditioned objectives and non-smooth regularizers undermine the performance of traditional stochastic gradient methods, leading to slow convergence and significant computational costs.
To address these challenges, we propose theSAPPHIRE(Sketching-basedApproximations forProximalPreconditioning andHessianInexactness with Variance-REeduced Gradients) algorithm,
which integrates sketch-based preconditioning to tackle ill-conditioning
and uses a scaled proximal mapping to minimize the non-smooth regularizer.
This stochastic variance-reduced algorithm achieves condition-number-free linear convergence to the optimum,
delivering an efficient and scalable solution for
ill-conditioned composite large-scale convex machine learning problems.
Extensive experiments on lasso and logistic regression demonstrate thatSAPPHIREoften converges20202020times faster than other common choices such asCatalyst,SAGA, andSVRG.
This advantage persists even when the objective is non-convex or the preconditioner is infrequently updated, highlighting its robust and practical effectiveness."
1613,679d459debd8ffd557a2b4ba,cs.LG,https://arxiv.org/pdf/2501.15922,SkillScope: A Tool to Predict Fine-Grained Skills Needed to Solve Issues on GitHub,"Benjamin C. Carter, Jonathan Rivas Contreras, Carlos A. Llanes Villegas, Pawan Acharya, Jack Utzerath, Adonijah O. Farner, Hunter Jenkins, Dylan Johnson, Jacob Penney, Igor Steinmacher, Marco A. Gerosa, Fabio Santos","Software Engineering, Machine Learning","New contributors often struggle to find tasks that they can tackle when onboarding onto a new Open Source Software (OSS) project. One reason for this difficulty is that issue trackers lack explanations about the knowledge or skills needed to complete a given task successfully. These explanations can be complex and time-consuming to produce. Past research has partially addressed this problem by labeling issues with issue types, issue difficulty level, and issue skills. However, current approaches are limited to a small set of labels and lack in-depth details about their semantics, which may not sufficiently help contributors identify suitable issues. To surmount this limitation, this paper explores large language models (LLMs) and Random Forest (RF) to predict the multilevel skills required to solve the open issues. We introduce a novel tool,SkillScope, which retrieves current issues from Java projects hosted on GitHub and predicts the multilevel programming skills required to resolve these issues. In a case study, we demonstrate thatSkillScopecould predict 217 multilevel skills for tasks with 91% precision, 88% recall, and 89% F-measure on average. Practitioners can use this tool to better delegate or choose tasks to solve in OSS projects."
1614,679d459debd8ffd557a2b4bb,cs.LG,https://arxiv.org/pdf/2501.15893,Benchmarking Quantum Reinforcement Learning,"Nico Meyer, Christian Ufrecht, George Yammine, Georgios Kontes, Christopher Mutschler, Daniel D. Scherer","Quantum Physics, Machine Learning","Benchmarking and establishing proper statistical validation metrics for reinforcement learning (RL) remain ongoing challenges, where no consensus has been established yet. The emergence of quantum computing and its potential applications in quantum reinforcement learning (QRL) further complicate benchmarking efforts. To enable valid performance comparisons and to streamline current research in this area, we propose a novel benchmarking methodology, which is based on a statistical estimator for sample complexity and a definition of statistical outperformance. Furthermore, considering QRL, our methodology casts doubt on some previous claims regarding its superiority. We conducted experiments on a novel benchmarking environment with flexible levels of complexity. While we still identify possible advantages, our findings are more nuanced overall. We discuss the potential limitations of these results and explore their implications for empirical research on quantum advantage in QRL."
1615,679d459debd8ffd557a2b4bc,cs.LG,https://arxiv.org/pdf/2501.15849,Gaussian Process-Based Prediction and Control of Hammerstein-Wiener Systems,"Mingzhou Yin, Matthias A. Müller","Systems and Control, Machine Learning","This work investigates data-driven prediction and control of Hammerstein-Wiener systems using physics-informed Gaussian process models. Data-driven prediction algorithms have been developed for structured nonlinear systems based on Willems’ fundamental lemma. However, existing frameworks cannot treat output nonlinearities and require a dictionary of basis functions for Hammerstein systems. In this work, an implicit predictor structure is considered, leveraging the multi-step-ahead ARX structure for the linear part of the model. This implicit function is learned by Gaussian process regression with kernel functions designed from Gaussian process priors for the nonlinearities. The linear model parameters are estimated as hyperparameters by assuming a stable spline hyperprior. The implicit Gaussian process model provides explicit output prediction by optimizing selected optimality criteria. The model is also applied to receding horizon control with the expected control cost and chance constraint satisfaction guarantee. Numerical results demonstrate that the proposed prediction and control algorithms are superior to black-box Gaussian process models."
1616,679d459debd8ffd557a2b4bd,cs.LG,https://arxiv.org/pdf/2501.15828,Hybrid Quantum Neural Networks with Amplitude Encoding: Advancing Recovery Rate Predictions,"Ying Chen, Paul Griffin, Paolo Recchia, Zhou Lei, Hongrui Chang","Computational Finance, Machine Learning, Quantum Physics","Recovery rate prediction plays a pivotal role in bond investment strategies, enhancing risk assessment, optimizing portfolio allocation, improving pricing accuracy, and supporting effective credit risk management. However, forecasting faces challenges like high-dimensional features, small sample sizes, and overfitting. We propose a hybrid Quantum Machine Learning model incorporating Parameterized Quantum Circuits (PQC) within a neural network framework. PQCs inherently preserve unitarity, avoiding computationally costly orthogonality constraints, while amplitude encoding enables exponential data compression, reducing qubit requirements logarithmically. Applied to a global dataset of 1,725 observations (1996–2023), our method achieved superior accuracy (RMSE 0.228) compared to classical neural networks (0.246) and quantum models with angle encoding (0.242), with efficient computation times. This work highlights the potential of hybrid quantum-classical architectures in advancing recovery rate forecasting."
1617,679d459debd8ffd557a2b4be,cs.LG,https://arxiv.org/pdf/2501.15799,Can Molecular Evolution Mechanism Enhance Molecular Representation?,"Kun Li, Longtao Hu, Xiantao Cai, Jia Wu, Wenbin Hu","Biomolecules, Machine Learning, Neural and Evolutionary Computing","Molecular evolution is the process of simulating the natural evolution of molecules in chemical space to explore potential molecular structures and properties. The relationships between similar molecules are often described through transformations such as adding, deleting, and modifying atoms and chemical bonds, reflecting specific evolutionary paths. Existing molecular representation methods mainly focus on mining data, such as atomic-level structures and chemical bonds directly from the molecules, often overlooking their evolutionary history. Consequently, we aim to explore the possibility of enhancing molecular representations by simulating the evolutionary process. We extract and analyze the changes in the evolutionary pathway and explore combining it with existing molecular representations. Therefore, this paper proposes the molecular evolutionary network (MEvoN) for molecular representations. First, we construct the MEvoN using molecules with a small number of atoms and generate evolutionary paths utilizing similarity calculations. Then, by modeling the atomic-level changes, MEvoN reveals their impact on molecular properties. Experimental results show that the MEvoN-based molecular property prediction method significantly improves the performance of traditional end-to-end algorithms on several molecular datasets. The code is available at https://anonymous.4open.science/r/MEvoN-7416/."
1618,679d459debd8ffd557a2b4bf,cs.LG,https://arxiv.org/pdf/2501.15753,Scale-Insensitive Neural Network Significance Tests,Hasan Fallahgoul,"Machine Learning, Machine Learning, Econometrics","This paper develops a scale-insensitive framework for neural network significance testing, substantially generalizing existing approaches through three key innovations. First, we replace metric entropy calculations with Rademacher complexity bounds, enabling the analysis of neural networks without requiring bounded weights or specific architectural constraints. Second, we weaken the regularity conditions on the target function to require only Sobolev space membershipHs⁢([−1,1]d)superscript𝐻𝑠superscript11𝑑H^{s}([-1,1]^{d})italic_H start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ( [ - 1 , 1 ] start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT )withs>d/2𝑠𝑑2s>d/2italic_s > italic_d / 2, significantly relaxing previous smoothness assumptions while maintaining optimal approximation rates. Third, we introduce a modified sieve space construction based on moment bounds rather than weight constraints, providing a more natural theoretical framework for modern deep learning practices. Our approach achieves these generalizations while preserving optimal convergence rates and establishing valid asymptotic distributions for test statistics. The technical foundation combines localization theory, sharp concentration inequalities, and scale-insensitive complexity measures to handle unbounded weights and general Lipschitz activation functions. This framework better aligns theoretical guarantees with contemporary deep learning practice while maintaining mathematical rigor."
1619,679d459debd8ffd557a2b4c0,cs.LG,https://arxiv.org/pdf/2501.15739,Automatic Machine Learning Framework to Study Morphological Parameters of AGN Host Galaxies within $z < 1.4$ in the Hyper Supreme-Cam Wide Survey,"Chuan Tian, C. Megan Urry, Aritra Ghosh, Daisuke Nagai, Tonima T. Ananna, Meredith C. Powell, Connor Auge, Aayush Mishra, David B. Sanders, Nico Cappelluti, Kevin Schawinski","Astrophysics of Galaxies, Instrumentation and Methods for Astrophysics, Machine Learning","We present a composite machine learning framework to estimate posterior probability distributions of bulge-to-total light ratio, half-light radius, and flux for Active Galactic Nucleus (AGN) host galaxies withinz<1.4𝑧1.4z<1.4italic_z < 1.4andm<23𝑚23m<23italic_m < 23in the Hyper Supreme-Cam Wide survey.
We divide the data into five redshift bins:low(0<z<0.250𝑧0.250<z<0.250 < italic_z < 0.25),mid(0.25<z<0.50.25𝑧0.50.25<z<0.50.25 < italic_z < 0.5),high(0.5<z<0.90.5𝑧0.90.5<z<0.90.5 < italic_z < 0.9),extra(0.9<z<1.10.9𝑧1.10.9<z<1.10.9 < italic_z < 1.1) andextreme(1.1<z<1.41.1𝑧1.41.1<z<1.41.1 < italic_z < 1.4), and train our models independently in each bin.
We use PSFGAN to decompose the AGN point source light from its host galaxy, and invoke the Galaxy Morphology Posterior Estimation Network (GaMPEN) to estimate morphological parameters of the recovered host galaxy.
We first trained our models on simulated data, and then fine-tuned our algorithm via transfer learning using labeled real data.
To create training labels for transfer learning, we used GALFIT to fit∼20,000similar-toabsent20000\sim 20,000∼ 20 , 000real HSC galaxies in each redshift bin.
We comprehensively examined that the predicted values from our final models agree well with the GALFIT values for the vast majority of cases.
Our PSFGAN + GaMPEN framework runs at least three orders of magnitude faster than traditional light-profile fitting methods, and can be easily retrained for other morphological parameters or on other datasets with diverse ranges of resolutions, seeing conditions, and signal-to-noise ratios, making it an ideal tool for analyzing AGN host galaxies from large surveys coming soon from the Rubin-LSST, Euclid, and Roman telescopes."
1620,679d459debd8ffd557a2b4c1,cs.LG,https://arxiv.org/pdf/2501.15737,Geometric Deep Learning for Automated Landmarking of Maxillary Arches on 3D Oral Scans from Newborns with Cleft Lip and Palate,"Artur Agaronyan, HyeRan Choo, Marius Linguraru, Syed Muhammad Anwar","Image and Video Processing, Machine Learning",
1621,679d459debd8ffd557a2b4c2,cs.LG,https://arxiv.org/pdf/2501.15694,A Statistical Learning Approach to Mediterranean Cyclones,"L. Roveri, L. Fery, L. Cavicchia, F. Grotto","Atmospheric and Oceanic Physics, Machine Learning","Mediterranean cyclones are extreme meteorological events of which much less is known compared to their tropical, oceanic counterparts.
The raising interest in such phenomena is due to their impact on a region increasingly more affected by climate change, but a precise characterization remains a non trivial task.
In this work we showcase how a Bayesian algorithm (Latent Dirichlet Allocation) can classify Mediterranean cyclones relying on wind velocity data, leading to a drastic dimensional reduction that allows the use of supervised statistical learning techniques for detecting and tracking new cyclones."
1622,679d459debd8ffd557a2b4c3,cs.LG,https://arxiv.org/pdf/2501.15690,Refined climatologies of future precipitation over High Mountain Asia using probabilistic ensemble learning,"Kenza Tazi, Sun Woo P. Kim, Marc Girona-Mata, Richard E. Turner","Atmospheric and Oceanic Physics, Machine Learning, Machine Learning","High Mountain Asia holds the largest concentration of frozen water outside the polar regions, serving as a crucial water source for more than 1.9 billion people. In the face of climate change, precipitation represents the largest source of uncertainty for hydrological modelling in this area. Future precipitation predictions remain challenging due to complex orography, lack of in situ hydrological observations, and limitations in climate model resolution and parametrisation for this region. To address the uncertainty posed by these challenges, climate models are often aggregated into multi-model ensembles. While multi-model ensembles are known to improve the predictive accuracy and analysis of future climate projections, consensus regarding how models are aggregated is lacking. In this study, we propose a probabilistic machine learning framework to systematically combine 13 regional climate models from the Coordinated Regional Downscaling Experiment (CORDEX) over High Mountain Asia. Our approach accounts for seasonal and spatial biases within the models, enabling the prediction of more faithful precipitation distributions. The framework is validated against gridded historical precipitation data and is used to generate projections for the near-future (2036–2065) and far-future (2066–2095) under RCP4.5 and RCP8.5 scenarios."
1623,679d459debd8ffd557a2b4c4,cs.LG,https://arxiv.org/pdf/2501.15634,"Be Intentional About Fairness!: Fairness, Size, and Multiplicity in the Rashomon Set","Gordon Dai, Pavan Ravishankar, Rachel Yuan, Daniel B. Neill, Emily Black","Computers and Society, Machine Learning","When selecting a model from a set of equally performant models, how much unfairness can you really reduce? Is it important to be intentional about fairness when choosing among this set, or is arbitrarily choosing among the set of “good” models good enough?
Recent work has highlighted that the phenomenon of model multiplicity—where multiple models with nearly identical predictive accuracy exist for the same task—has both positive and negative implications for fairness, from strengthening the enforcement of civil rights law in AI systems to showcasing arbitrariness in AI decision-making.
Despite the enormous implications of model multiplicity, there is little work that explores the properties of sets of equally accurate models, or Rashomon sets, in general. In this paper, we present five main theoretical and methodological contributions which help us to understand the relatively unexplored properties of the Rashomon set, in particular with regards to fairness. Our contributions include methods for efficiently sampling models from this set and techniques for identifying the fairest models according to key fairness metrics such as statistical parity. We also derive the probability that an individual’s prediction will be flipped within the Rashomon set, as well as expressions for the set’s size and the distribution of error tolerance used across models. These results lead to
policy-relevant takeaways, such as the importance of intentionally looking for fair models within the Rashomon set, and understanding which individuals or groups may be more susceptible to arbitrary decisions."
1624,679d459debd8ffd557a2b4c5,cs.LG,https://arxiv.org/pdf/2501.15631,BoKDiff: Best-of-K Diffusion Alignment for Target-Specific 3D Molecule Generation,"Ali Khodabandeh Yalabadi, Mehdi Yazdani-Jahromi, Ozlem Ozmen Garibay","Biomolecules, Machine Learning","Structure-based drug design (SBDD) leverages the 3D structure of biomolecular targets to guide the creation of new therapeutic agents. Recent advances in generative models, including diffusion models and geometric deep learning, have demonstrated promise in optimizing ligand generation. However, the scarcity of high-quality protein-ligand complex data and the inherent challenges in aligning generated ligands with target proteins limit the effectiveness of these methods. We propose BoKDiff, a novel framework that enhances ligand generation by combining multi-objective optimization and Best-of-K alignment methodologies. Built upon the DecompDiff model, BoKDiff generates diverse candidates and ranks them using a weighted evaluation of molecular properties such as QED, SA, and docking scores. To address alignment challenges, we introduce a method that relocates the center of mass of generated ligands to their docking poses, enabling accurate sub-component extraction. Additionally, we integrate a Best-of-N (BoN) sampling approach, which selects the optimal ligand from multiple generated candidates without requiring fine-tuning. BoN achieves exceptional results, with QED values exceeding 0.6, SA scores above 0.75, and a success rate surpassing 35%, demonstrating its efficiency and practicality. BoKDiff achieves state-of-the-art results on the CrossDocked2020 dataset, including a -8.58 average Vina docking score and a 26% success rate in molecule generation. This study is the first to apply Best-of-K alignment and Best-of-N sampling to SBDD, highlighting their potential to bridge generative modeling with practical drug discovery requirements. The code is provided at thisGitHub URL."
1625,679d459debd8ffd557a2b4c6,cs.LG,https://arxiv.org/pdf/2501.15617,I-trustworthy Models. A framework for trustworthiness evaluation of probabilistic classifiers,"Ritwik Vashistha, Arya Farahi","Machine Learning, Machine Learning, Methodology","As probabilistic models continue to permeate various facets of our society and contribute to scientific advancements, it becomes a necessity to go beyond traditional metrics such as predictive accuracy and error rates and assess their trustworthiness. Grounded in the competence-based theory of trust, this work formalizesℐℐ\mathcal{I}caligraphic_I-trustworthy framework – a novel framework for assessing the trustworthiness of probabilistic classifiers for inference tasks by linking local calibration to trustworthiness. To assessℐℐ\mathcal{I}caligraphic_I-trustworthiness, we use the local calibration error (LCE) and develop a method of hypothesis-testing. This method utilizes a kernel-based test statistic, Kernel Local Calibration Error (KLCE), to test local calibration of a probabilistic classifier. This study provides theoretical guarantees by offering convergence bounds for an unbiased estimator of KLCE. Additionally, we present a diagnostic tool designed to identify and measure biases in cases of miscalibration. The effectiveness of the proposed test statistic is demonstrated through its application to both simulated and real-world datasets. Finally, LCE of related recalibration methods is studied, and we provide evidence of insufficiency of existing methods to achieveℐℐ\mathcal{I}caligraphic_I-trustworthiness."
1626,679d459debd8ffd557a2b4c7,cs.LG,https://arxiv.org/pdf/2501.15559,Towards Sharper Information-theoretic Generalization Bounds for Meta-Learning,"Wen Wen, Tieliang Gong, Yuxin Dong, Yong-Jin Liu, Weizhan Zhang","Machine Learning, Machine Learning","In recent years, information-theoretic generalization bounds have emerged as a promising approach for analyzing the generalization capabilities of meta-learning algorithms. However, existing results are confined to two-step bounds, failing to provide a sharper characterization of the meta-generalization gap that simultaneously accounts for environment-level and task-level dependencies. This paper addresses this fundamental limitation by establishing novel single-step information-theoretic bounds for meta-learning. Our bounds exhibit substantial advantages over prior MI- and CMI-based bounds, especially in terms of tightness, scaling behavior associated with sampled tasks and samples per task, and computational tractability. Furthermore, we provide novel theoretical insights into the generalization behavior of two classes of noise and iterative meta-learning algorithms via gradient covariance analysis, where the meta-learner uses either the entire meta-training data (e.g., Reptile), or separate training and test data within the task (e.g., model agnostic meta-learning (MAML)). Numerical results validate the effectiveness of the derived bounds in capturing the generalization dynamics of meta-learning."
1627,679d459debd8ffd557a2b4c8,cs.LG,https://arxiv.org/pdf/2501.15522,Estimating Committor Functions via Deep Adaptive Sampling on Rare Transition Paths,"Yueyang Wang, Kejun Tang, Xili Wang, Xiaoliang Wan, Weiqing Ren, Chao Yang","Machine Learning, Machine Learning, Quantitative Methods","The committor functions are central to investigating rare but important events in molecular simulations. It is known that computing the committor function suffers from the curse of dimensionality. Recently, using neural networks to estimate the committor function has gained attention due to its potential for high-dimensional problems. Training neural networks to approximate the committor function needs to sample transition data from straightforward simulations of rare events, which is very inefficient. The scarcity of transition data makes it challenging to approximate the committor function. To address this problem, we propose an efficient framework to generate data points in the transition state region that helps train neural networks to approximate the committor function. We design a Deep Adaptive Sampling method for TRansition paths (DASTR), where deep generative models are employed to generate samples to capture the information of transitions effectively. In particular, we treat a non-negative function in the integrand of the loss functional as an unnormalized probability density function and approximate it with the deep generative model. The new samples from the deep generative model are located in the transition state region and fewer samples are located in the other region. This distribution provides effective samples for approximating the committor function and significantly improves the accuracy. We demonstrate the effectiveness of the proposed method through both simulations and realistic examples."
1628,679d459debd8ffd557a2b4c9,cs.LG,https://arxiv.org/pdf/2501.15351,Fairness in LLM-Generated Surveys,"Andrés Abeliuk, Vanessa Gaete, Naim Bro","Computers and Society, Machine Learning","Large Language Models (LLMs) excel in text generation and understanding, especially in simulating socio-political and economic patterns, serving as an alternative to traditional surveys. However, their global applicability remains questionable due to unexplored biases across socio-demographic and geographic contexts.
This study examines how LLMs perform across diverse populations by analyzing public surveys from Chile and the United States, focusing on predictive accuracy and fairness metrics. The results show performance disparities, with LLM consistently outperforming on U.S. datasets. This bias originates from the U.S.-centric training data, remaining evident after accounting for socio-demographic differences.
In the U.S., political identity and race significantly influence prediction accuracy, while in Chile, gender, education, and religious affiliation play more pronounced roles.
Our study presents a novel framework for measuring socio-demographic biases in LLMs, offering a path toward ensuring fairer and more equitable model performance across diverse socio-cultural contexts."
1629,679d459debd8ffd557a2b4ca,cs.LG,https://arxiv.org/pdf/2501.15338,Fairness-aware Contextual Dynamic Pricing with Strategic Buyers,"Pangpang Liu, Will Wei Sun","Computer Science and Game Theory, Machine Learning, Machine Learning","Contextual pricing strategies are prevalent in online retailing, where the seller adjusts prices based on products’ attributes and buyers’ characteristics. Although such strategies can enhance seller’s profits, they raise concerns about fairness when significant price disparities emerge among specific groups, such as gender or race. These disparities can lead to adverse perceptions of fairness among buyers and may even violate the law and regulation. In contrast, price differences can incentivize disadvantaged buyers to strategically manipulate their group identity to obtain a lower price. In this paper, we investigate contextual dynamic pricing with fairness constraints, taking into account buyers’ strategic behaviors when their group status is private and unobservable from the seller. We propose a dynamic pricing policy that simultaneously achieves price fairness and discourages strategic behaviors. Our policy achieves an upper bound ofO⁢(T+H⁢(T))𝑂𝑇𝐻𝑇O(\sqrt{T}+H(T))italic_O ( square-root start_ARG italic_T end_ARG + italic_H ( italic_T ) )regret overT𝑇Titalic_Ttime horizons, where the termH⁢(T)𝐻𝑇H(T)italic_H ( italic_T )arises from buyers’ assessment of the fairness of the pricing policy based on their learned price difference. When buyers are able to learn the fairness of the price policy, this upper bound reduces toO⁢(T)𝑂𝑇O(\sqrt{T})italic_O ( square-root start_ARG italic_T end_ARG ). We also prove anΩ⁢(T)Ω𝑇\Omega(\sqrt{T})roman_Ω ( square-root start_ARG italic_T end_ARG )regret lower bound of any pricing policy under our problem setting. We support our findings with extensive experimental evidence, showcasing our policy’s effectiveness. In our real data analysis, we observe the existence of price discrimination against race in the loan application even after accounting for other contextual information. Our proposed pricing policy demonstrates a significant improvement, achieving 35.06% reduction in regret compared to the benchmark policy."
1630,679d459debd8ffd557a2b4cb,cs.LG,https://arxiv.org/pdf/2501.15328,Physiologically-Informed Predictability of a Teammate's Future Actions Forecasts Team Performance,"Yinuo Qin, Richard T. Lee, Weijia Zhang, Xiaoxiao Sun, Paul Sajda","Neurons and Cognition, Machine Learning","In collaborative environments, a deep understanding of multi-human teaming dynamics is essential for optimizing performance. However, the relationship between individuals’ behavioral and physiological markers and their combined influence on overall team performance remains poorly understood. To explore this, we designed a triadic human collaborative sensorimotor task in virtual reality (VR) and introduced a novel predictability metric to examine team dynamics and performance. Our findings reveal a strong connection between team performance and the predictability of a team member’s future actions based on other team members’ behavioral and physiological data. Contrary to conventional wisdom that high-performing teams are highly synchronized, our results suggest that physiological and behavioral synchronizations among team members have a limited correlation with team performance. These insights provide a new quantitative framework for understanding multi-human teaming, paving the way for deeper insights into team dynamics and performance."
1631,679d459debd8ffd557a2b4cc,cs.LG,https://arxiv.org/pdf/2501.15301,Separable Computation of Information Measures,"Xiangxiang Xu, Lizhong Zheng","Information Theory, Machine Learning, Machine Learning","We study a separable design for computing information measures, where the information measure is computed from learned feature representations instead of raw data. Under mild assumptions on the feature representations, we demonstrate that a class of information measures admit such separable computation, including mutual information,f𝑓fitalic_f-information, Wyner’s common information, Gács–Körner common information, and Tishby’s information bottleneck. Our development establishes several new connections between information measures and the statistical dependence structure. The characterizations also provide theoretical guarantees of practical designs for estimating information measures through representation learning."
1632,679d459debd8ffd557a2b4cd,cs.LG,https://arxiv.org/pdf/2501.15196,A Review on Self-Supervised Learning for Time Series Anomaly Detection: Recent Advances and Open Challenges,"Aitor Sánchez-Ferrera, Borja Calvo, Jose A. Lozano","Machine Learning, Machine Learning","Time series anomaly detection presents various challenges due to the sequential and dynamic nature of time-dependent data. Traditional unsupervised methods frequently encounter difficulties in generalization, often overfitting to known normal patterns observed during training and struggling to adapt to unseen normality. In response to this limitation, self-supervised techniques for time series have garnered attention as a potential solution to undertake this obstacle and enhance the performance of anomaly detectors. This paper presents a comprehensive review of the recent methods that make use of self-supervised learning for time series anomaly detection. A taxonomy is proposed to categorize these methods based on their primary characteristics, facilitating a clear understanding of their diversity within this field. The information contained in this survey, along with additional details that will be periodically updated, is available on the following GitHub repository:https://github.com/Aitorzan3/Awesome-Self-Supervised-Time-Series-Anomaly-Detection."
1633,679d459debd8ffd557a2b4ce,cs.LG,https://arxiv.org/pdf/2501.15186,An Iterative Deep Ritz Method for Monotone Elliptic Problems,"Tianhao Hu, Bangti Jin, Fengru Wang","Numerical Analysis, Machine Learning","In this work, we present a novel iterative deep Ritz method (IDRM) for solving a general class of elliptic problems. It is inspired by the iterative procedure for minimizing the loss during the training of the neural network, but at each step encodes the geometry of the underlying function space and incorporates a convex penalty to enhance the performance of the algorithm. The algorithm is applicable to elliptic problems involving a monotone operator (not necessarily of variational form) and does not impose any stringent regularity assumption on the solution. It improves several existing neural PDE solvers, e.g., physics informed neural network and deep Ritz method, in terms of the accuracy for the concerned class of elliptic problems. Further, we establish a convergence rate for the method using tools from geometry of Banach spaces and theory of monotone operators, and also analyze the learning error. To illustrate the effectiveness of the method, we present several challenging examples, including a comparative study with existing techniques.Key words: deep Ritz method, deep neural network, iterative method, monotone elliptic problem, convergence"
1634,679d459debd8ffd557a2b4cf,cs.LG,https://arxiv.org/pdf/2501.15157,Median of Forests for Robust Density Estimation,"Hongwei Wen, Annika Betken, Tao Huang","Machine Learning, Machine Learning","Robust density estimation refers to the consistent estimation of the density function even when the data is contaminated by outliers. We find that existing forest density estimation at a certain point is inherently resistant to the outliers outside the cells containing the point, which we callnon-local outliers, but not resistant to the restlocal outliers.
To achieve robustness against all outliers, we propose an ensemble learning algorithm calledmedians of forests for robust density estimation(MFRDE), which adopts a pointwise median operation on forest density estimators fitted on subsampled datasets.
Compared to exsiting robust kernel-based methods, MFRDE enables us to choose larger subsampling sizes, sacrificing less accuracy for density estimation while achieving robustness.
On the theoretical side, we introduce the local outlier exponent to quantify the number of local outliers. Under this exponent, we show that even if the number of outliers reaches a certain polynomial order in the sample size, MFRDE is able to achieve almost the same convergence rate as the same algorithm on uncontaminated data, whereas robust kernel-based methods fail.
On the practical side, real data experiments show that MFRDE outperforms existing robust kernel-based methods.
Moreover, we apply MFRDE to anomaly detection to showcase a further application."
1635,679d459debd8ffd557a2b4d0,cs.LG,https://arxiv.org/pdf/2501.15106,In-Context Operator Learning for Linear Propagator Models,"Tingwei Meng, Moritz Voß, Nils Detering, Giulio Farolfi, Stanley Osher, Georg Menz","Trading and Market Microstructure, Machine Learning, Optimization and Control, Computational Finance","We study operator learning in the context of linear propagator models for optimal order execution problems with transient price impact à la Bouchaud et al. (2004) and Gatheral (2010). Transient price impact persists and decays over time according to some propagator kernel. Specifically, we propose to use In-Context Operator Networks (ICON), a novel transformer-based neural network architecture introduced by Yang et al. (2023), which facilitates data-driven learning of operators by merging offline pre-training with an online few-shot prompting inference. First, we train ICON to learn the operator from various propagator models that maps the trading rate to the induced transient price impact. The inference step is then based on in-context prediction, where ICON is presented only with a few examples. We illustrate that ICON is capable of accurately inferring the underlying price impact model from the data prompts, even with propagator kernels not seen in the training data. In a second step, we employ the pre-trained ICON model provided with context as a surrogate operator in solving an optimal order execution problem via a neural network control policy, and demonstrate that the exact optimal execution strategies from Abi Jaber and Neuman (2022) for the models generating the context are correctly retrieved. Our introduced methodology is very general, offering a new approach to solving optimal stochastic control problems with unknown state dynamics, inferred data-efficiently from a limited number of examples by leveraging the few-shot and transfer learning capabilities of transformer networks."
1636,679d459debd8ffd557a2b4d1,cs.LG,https://arxiv.org/pdf/2501.15079,Salvaging Forbidden Treasure in Medical Data: Utilizing Surrogate Outcomes and Single Records for Rare Event Modeling,"Xiaohui Yin, Shane Sacco, Robert H. Aseltine, Fei Wang, Kun Chen","Methodology, Machine Learning, Applications, Machine Learning","The vast repositories of Electronic Health Records (EHR) and medical claims hold untapped potential for studying rare but critical events, such as suicide attempt. Conventional setups often model suicide attempt as a univariate outcome and also exclude any “single-record” patients with a single documented encounter due to a lack of historical information. However, patients who were diagnosed with suicide attempts at the only encounter could, to some surprise, represent a substantial proportion of all attempt cases in the data, as high as 70–80%. We innovate a hybrid & integrative learning framework to leverage concurrent outcomes as surrogates and harness the “forbidden” yet precious information from single-record data. Our approach employs a supervised learning component to learn the latent variables that connect primary (e.g., suicide) and surrogate outcomes (e.g., mental disorders) to historical information. It simultaneously employs an unsupervised learning component to utilize the single-record data, through the shared latent variables. As such, our approach offers a general strategy for information integration that is crucial to modeling rare conditions and events.
With hospital inpatient data from Connecticut, we demonstrate that single-record data and concurrent diagnoses indeed carry valuable information, and utilizing them can substantially improve suicide risk modeling."
1637,679d459debd8ffd557a2b4d2,cs.LG,https://arxiv.org/pdf/2501.15067,CG-RAG: Research Question Answering by Citation Graph Retrieval-Augmented LLMs,"Yuntong Hu, Zhihan Lei, Zhongjie Dai, Allen Zhang, Abhinav Angirekula, Zheng Zhang, Liang Zhao","Information Retrieval, Machine Learning","Research question answering requires accurate retrieval and contextual understanding of scientific literature. However, current Retrieval-Augmented Generation (RAG) methods often struggle to balance complex document relationships with precise information retrieval.
In this paper, we introduce Contextualized Graph Retrieval-Augmented Generation (CG-RAG), a novel framework that integrates sparse and dense retrieval signals within graph structures to enhance retrieval efficiency and subsequently improve generation quality for research question answering. First, we propose a contextual graph representation for citation graphs, effectively capturing both explicit and implicit connections within and across documents. Next, we introduce Lexical-Semantic Graph Retrieval (LeSeGR), which seamlessly integrates sparse and dense retrieval signals with graph encoding. It bridges the gap between lexical precision and semantic understanding in citation graph retrieval, demonstrating generalizability to existing graph retrieval and hybrid retrieval methods. Finally, we present a context-aware generation strategy that utilizes the retrieved graph-structured information to generate precise and contextually enriched responses using large language models (LLMs). Extensive experiments on research question answering benchmarks across multiple domains demonstrate that our CG-RAG framework significantly outperforms RAG methods combined with various state-of-the-art retrieval approaches, delivering superior retrieval accuracy and generation quality."
1638,679d459debd8ffd557a2b4d3,cs.LG,https://arxiv.org/pdf/2501.14933,Conformal Inference of Individual Treatment Effects Using Conditional Density Estimates,"Baozhen Wang, Xingye Qiao","Machine Learning, Machine Learning","In an era where diverse and complex data are increasingly accessible, the precise prediction of individual treatment effects (ITE) becomes crucial across fields such as healthcare, economics, and public policy. Current state-of-the-art approaches, while providing valid prediction intervals through Conformal Quantile Regression (CQR) and related techniques, often yield overly conservative prediction intervals. In this work, we introduce a conformal inference approach to ITE using the conditional density of the outcome given the covariates. We leverage the reference distribution technique to efficiently estimate the conditional densities as the score functions under a two-stage conformal ITE framework. We show that our prediction intervals are not only marginally valid but are narrower than existing methods. Experimental results further validate the usefulness of our method."
1639,679d459debd8ffd557a2b4d4,cs.LG,https://arxiv.org/pdf/2501.14837,A Semiparametric Bayesian Method for Instrumental Variable Analysis with Partly Interval-Censored Time-to-Event Outcome,"Elvis Han Cui, Xuyang Lu, Jin Zhou, Hua Zhou, Gang Li","Methodology, Machine Learning, Applications, Computation, Machine Learning","This paper develops a semiparametric Bayesian instrumental variable analysis method for estimating the causal effect of an endogenous variable when dealing with unobserved confounders and measurement errors with partly interval-censored time-to-event data, where event times are observed exactly for some subjects but left-censored, right-censored, or interval-censored for others. Our method is based on a two-stage Dirichlet process mixture instrumental variable (DPMIV) model which simultaneously models the first-stage random error term for the exposure variable and the second-stage random error term for the time-to-event outcome using a bivariate Gaussian mixture of the Dirichlet process (DPM) model. The DPM model can be broadly understood as a mixture model with an unspecified number of Gaussian components, which relaxes the normal error assumptions and allows the number of mixture components to be determined by the data.
We develop an MCMC algorithm for the DPMIV model tailored for partly interval-censored data and conduct extensive simulations to assess the performance of our DPMIV method in comparison with some competing methods.
Our simulations revealed that our proposed method is robust under different error distributions and can have superior performance over its parametric counterpart under various scenarios. We further demonstrate the effectiveness of our approach on an UK Biobank data to investigate the causal effect of systolic blood pressure on time-to-development of cardiovascular disease from the onset of diabetes mellitus."
1640,679d459debd8ffd557a2b4d5,cs.LG,https://arxiv.org/pdf/2501.14824,A causal learning approach to in-orbit inertial parameter estimation for multi-payload deployers,"Konstantinos Platanitis, Miguel Arana-Catania, Saurabh Upadhyay, Leonard Felicetti","Systems and Control, Instrumentation and Methods for Astrophysics, Machine Learning, Robotics",
1641,679d459debd8ffd557a2b4d6,cs.LG,https://arxiv.org/pdf/2501.14813,Dissertation Machine Learning in Materials Science -- A case study in Carbon Nanotube field effect transistors,Shulin Tan,"Applied Physics, Mesoscale and Nanoscale Physics, Machine Learning, Data Analysis, Statistics and Probability",
1642,679d459debd8ffd557a2b4d7,cs.LG,https://arxiv.org/pdf/2501.14787,Matrix Calculus (for Machine Learning and Beyond),"Paige Bright, Alan Edelman, Steven G. Johnson","History and Overview, Machine Learning, Numerical Analysis, Machine Learning",
1643,679d459debd8ffd557a2b4d8,cs.LG,https://arxiv.org/pdf/2501.14750,"Engineering Carbon Credits Towards A Responsible FinTech Era: The Practices, Implications, and Future","Qingwen Zeng, Hanlin Xu, Nanjun Xu, Flora Salim, Junbin Gao, Huaming Chen","Computers and Society, Machine Learning","Carbon emissions are a key factor to serious climate change, and carbon credits have emerged as an effective tool for mitigating climate deterioration and environmental damage while assisting organisations in managing their carbon footprint. In recent years, carbon credits have garnered significant attention across various economic sectors due to their substantial impact. However, fully utilizing them to address these issues remains a challenge. This study aims to enhance understanding of the engineering practices for carbon credits to develop responsible fintech solutions, thereby providing stronger incentives and insights for carbon emission management. To achieve this goal, wefirst conduct a comprehensive review of the negative impacts associated with organisations’ strategy of evading carbon management through non-disclosure of carbon emissions. Our findings reveal that both non-disclosure of carbon emissions and high carbon emissions negatively affect a organisation’s financial stability and market value. Therefore, it is suggested that organisations should actively manage their carbon emissions and transparently share relevant information to mitigate potential risks. This study highlights the importance of carbon credits for businesses, emphasizing their role in improving corporate sustainability. With the insights from the carbon emission, wethen investigate the engineering methods that assist organisations in managing carbon emissions more cost-effectively, in particular for the data-driven computing solutions. Specifically, we analyse the factors influencing carbon prices and review the state-of-the-art carbon price prediction algorithms. These tools help organisations optimize their carbon credit purchasing strategies, thereby reducing costs and improving operational efficiency. Furthermore, we review the most recent corporate carbon emission prediction algorithms, which not only provide more accurate environmental performance assessments for investors and governments when carbon emissions data is not disclosed, but also help organisations estimate their future carbon credits needs, supporting better budget planning and strategy optimization. Finally, by integrating carbon price and carbon emission predictions, this paperproposes future research directions, including the prediction of corporate-level carbon management costs. We anticipate this will lay a foundation for future quantitative research on how carbon management practices impact corporate market value and financial performance. To the best of our knowledge, this is the first systematic review covering the various aspects of carbon credits, with a particular focus on computing solutions and engineering practices."
1644,679d459debd8ffd557a2b4d9,cs.LG,https://arxiv.org/pdf/2501.14746,Neuromorphic Spiking Neural Network Based Classification of COVID-19 Spike Sequences,"Taslim Murad, Prakash Chourasia, Sarwan Ali, Imdad Ullah Khan, Murray Patterson","Neural and Evolutionary Computing, Machine Learning","The availability of SARS-CoV-2 (severe acute respiratory syndrome coronavirus 2) virus data post-COVID has reached exponentially to an enormous magnitude, opening research doors to analyze its behavior. Various studies are conducted by researchers to gain a deeper understanding of the virus, like genomic surveillance, etc, so that efficient prevention mechanisms can be developed. However, the unstable nature of the virus (rapid mutations, multiple hosts, etc) creates challenges in designing analytical systems for it. Therefore, we propose a neural network-based (NN) mechanism to perform an efficient analysis of the SARS-CoV-2 data, as NN portrays generalized behavior upon training. Moreover, rather than using the full-length genome of the virus, we apply our method to its spike region, as this region is known to have predominant mutations and is used to attach to the host cell membrane. In this paper, we introduce a pipeline that first converts the spike protein sequences into a fixed-length numerical representation and then uses Neuromorphic Spiking Neural Network to classify those sequences. We compare the performance of our method with various baselines using real-world SARS-CoV-2 spike sequence data and show that our method is able to achieve higher predictive accuracy compared to the recent baselines."
1645,679d459debd8ffd557a2b4da,cs.LG,https://arxiv.org/pdf/2501.14741,On Design Choices in Similarity-Preserving Sparse Randomized Embeddings,"Denis Kleyko, Dmitri A. Rachkovskij","Neural and Evolutionary Computing, Machine Learning, Neurons and Cognition","Expand & Sparsify is a principle that is observed in anatomically similar neural circuits found in the mushroom body (insects) and the cerebellum (mammals).
Sensory data are projected randomly to much higher-dimensionality (expand part) where only few the most strongly excited neurons are activated (sparsify part). This principle has been leveraged to design aFlyHashalgorithm that forms similarity-preserving sparse embeddings, which have been found useful for such tasks as novelty detection, pattern recognition, and similarity search.
Despite its simplicity,FlyHashhas a number of design choices to be set such as preprocessing of the input data, choice of sparsifying activation function, and formation of the random projection matrix.
In this paper, we explore the effect of these choices on the performance of similarity search withFlyHashembeddings.
We find that the right combination of design choices can lead to drastic difference in the search performance."
1646,679d459debd8ffd557a2b4db,cs.LG,https://arxiv.org/pdf/2501.14736,NEAT Algorithm-based Stock Trading Strategy with Multiple Technical Indicators Resonance,Li-Chun Huang,"Neural and Evolutionary Computing, Machine Learning, Portfolio Management","In this study, we applied the NEAT (NeuroEvolution of Augmenting Topologies) algorithm to stock trading using multiple technical indicators. Our approach focused on maximizing earning, avoiding risk, and outperforming the Buy & Hold strategy. We used progressive training data and a multi-objective fitness function to guide the evolution of the population towards these objectives. The results of our study showed that the NEAT model achieved similar returns to the Buy & Hold strategy, but with lower risk exposure and greater stability. We also identified some challenges in the training process, including the presence of a large number of unused nodes and connections in the model architecture. In future work, it may be worthwhile to explore ways to improve the NEAT algorithm and apply it to shorter interval data in order to assess the potential impact on performance."
1647,679d459debd8ffd557a2b4dc,cs.LG,https://arxiv.org/pdf/2501.14724,MLPs at the EOC: Concentration of the NTK,"Dávid Terjék, Diego González-Sánchez","Machine Learning, Machine Learning",
1648,679d459debd8ffd557a2b4dd,cs.LG,https://arxiv.org/pdf/2501.14723,CodeMonkeys: Scaling Test-Time Compute for Software Engineering,"Ryan Ehrlich, Bradley Brown, Jordan Juravsky, Ronald Clark, Christopher Ré, Azalia Mirhoseini",Machine Learning,"Scaling test-time compute is a promising axis for improving LLM capabilities.
However, test-time compute can be scaled in a variety of ways, and effectively combining different approaches remains an active area of research.
Here, we explore this problem in the context of solving real-world GitHub issues from the SWE-bench dataset.
Our system, named CodeMonkeys, allows models to iteratively edit a codebase by jointly generating and running a testing script alongside their draft edit.
We sample many of these multi-turn trajectories for every issue to generate a collection of candidate edits.
This approach lets us scale “serial” test-time compute by increasing the number of iterations per trajectory and “parallel” test-time compute by increasing the number of trajectories per problem.
With parallel scaling, we can amortize up-front costs across multiple downstream samples, allowing us to identify relevant codebase context using the simple method of letting an LLM read every file.
In order to select between candidate edits, we combine voting using model-generated tests with a final multi-turn trajectory dedicated to selection.
Overall, CodeMonkeys resolves 57.4% of issues from SWE-bench Verified using a budget of approximately 2300 USD.
Our selection method can also be used to combine candidates from different sources. Selecting over an ensemble of edits from existing top SWE-bench Verified submissions obtains a score of 66.2% and outperforms the best member of the ensemble on its own.
We fully release our code and data athttps://scalingintelligence.stanford.edu/pubs/codemonkeys/."
1649,679d459debd8ffd557a2b4de,cs.LG,https://arxiv.org/pdf/2501.14652,Decoupled SGDA for Games with Intermittent Strategy Communication,"Ali Zindari, Parham Yazdkhasti, Anton Rodomanov, Tatjana Chavdarova, Sebastian U. Stich",Machine Learning,"We focus on reducing communication overhead in multiplayer games, where frequently exchanging strategies between players is not feasible and players have noisy or outdated strategies of the other players.
We introduceDecoupled SGDA, a novel adaptation of Stochastic Gradient Descent Ascent (SGDA).
In this approach, players independently update their strategies based on outdated opponent strategies, with periodic synchronization to align strategies.
For Strongly-Convex-Strongly-Concave (SCSC) games, we demonstrate that Decoupled SGDA achieves near-optimal communication complexity comparable to the best-known GDA rates.
Forweakly coupledgames where the interaction between players is lower relative to the non-interactive part of the game, Decoupled SGDA significantly reduces communication costs compared to standard SGDA.
Our findings extend to multi-player games. To provide insights into the effect of communication frequency and convergence, we extensively study the convergence of Decoupled SGDA for quadratic minimax problems.
Lastly, in settings where the noise over the players is imbalanced, Decoupled SGDA significantly outperforms federated minimax methods."
1650,679d459debd8ffd557a2b4df,cs.LG,https://arxiv.org/pdf/2501.14641,Towards Scalable Topological Regularizers,"Hiu-Tung Wong, Darrick Lee, Hong Yan","Machine Learning, Algebraic Topology","Latent space matching, which consists of matching distributions of features in latent space, is a crucial component for tasks such as adversarial attacks and defenses, domain adaptation, and generative modelling.
Metrics for probability measures, such as Wasserstein and maximum mean discrepancy, are commonly used to quantify the differences between such distributions.
However, these are often costly to compute, or do not appropriately take the geometric and topological features of the distributions into consideration.
Persistent homology is a tool from topological data analysis which quantifies the multi-scale topological structure of point clouds, and has recently been used as a topological regularizer in learning tasks.
However, computation costs preclude larger scale computations, and discontinuities in the gradient lead to unstable training behavior such as in adversarial tasks.
We propose the use ofprincipal persistence measures, based on computing the persistent homology of a large number of small subsamples, as a topological regularizer.
We provide a parallelized GPU implementation of this regularizer, and prove that gradients are continuous for smooth densities.
Furthermore, we demonstrate the efficacy of this regularizer on shape matching, image generation, and semi-supervised learning tasks, opening the door towards a scalable
regularizer for topological features."
1651,679d459debd8ffd557a2b4e0,cs.LG,https://arxiv.org/pdf/2501.14636,A Paired Autoencoder Framework for Inverse Problems via Bayes Risk Minimization,"Emma Hart, Julianne Chung, Matthias Chung","Machine Learning, Numerical Analysis","In this work, we describe a new data-driven approach for inverse problems that exploits technologies from machine learning, in particular autoencoder network structures. We consider a paired autoencoder framework, where two autoencoders are used to efficiently represent the input and target spaces separately and optimal mappings are learned between latent spaces, thus enabling forward and inverse surrogate mappings. We focus on interpretations using Bayes risk and empirical Bayes risk minimization, and we provide various theoretical results and connections to existing works on low-rank matrix approximations. Similar to end-to-end approaches, our paired approach creates a surrogate model for forward propagation and regularized inversion. However, our approach outperforms existing approaches in scenarios where training data for unsupervised learning are readily available but training pairs for supervised learning are scarce. Furthermore, we show that cheaply computable evaluation metrics are available through this framework and can be used to predict whether the solution for a new sample should be predicted well."
1652,679d459debd8ffd557a2b4e1,cs.LG,https://arxiv.org/pdf/2501.14604,Inverse Evolution Data Augmentation for Neural PDE Solvers,"Chaoyu Liu, Chris Budd, Carola-Bibiane Schönlieb",Machine Learning,"Neural networks have emerged as promising tools for solving partial differential equations (PDEs), particularly through the application of neural operators. Training neural operators typically requires a large amount of training data to ensure accuracy and generalization. In this paper, we propose a novel data augmentation method specifically designed for training neural operators on evolution equations. Our approach utilizes insights from inverse processes of these equations to efficiently generate data from random initialization that are combined with original data. To further enhance the accuracy of the augmented data, we introduce high-order inverse evolution schemes. These schemes consist of only a few explicit computation steps, yet the resulting data pairs can be proven to satisfy the corresponding implicit numerical schemes. In contrast to traditional PDE solvers that require small time steps or implicit schemes to guarantee accuracy, our data augmentation method employs explicit schemes with relatively large time steps, thereby significantly reducing computational costs. Accuracy and efficacy experiments confirm the effectiveness of our approach. Additionally, we validate our approach through experiments with the Fourier Neural Operator and UNet on three common evolution equations that are Burgers’ equation, the Allen-Cahn equation and the Navier-Stokes equation. The results demonstrate a significant improvement in the performance and robustness of the Fourier Neural Operator when coupled with our inverse evolution data augmentation method."
1653,679d459debd8ffd557a2b4e2,cs.LG,https://arxiv.org/pdf/2501.14588,Data Assetization via Resources-decoupled Federated Learning,"Jianzhe Zhao, Feida Zhu, Lingyan He, Zixin Tang, Mingce Gao, Shiyu Yang, Guibing Guo",Machine Learning,"With the development of the digital economy, data is increasingly recognized as an essential resource for both work and life. However, due to privacy concerns, data owners tend to maximize the value of data through information flow rather than direct data transfer. Federated learning (FL) provides an effective approach to collaborative training models while preserving privacy. However, different data owners not only have variations in the quantity and quality of their data resources but also face mismatches between data and computing resources as model parameters and training data grow. These challenges hinder data owners’ willingness to participate and reduce the effectiveness of data assetization. In this work, we first identify the resource-decoupled FL environment, which includes model owners, data owners, and computing centers. We design a Tripartite Stackelberg Model and theoretically analyze the Stackelberg-Nash Equilibrium (SNE) for participants to optimize global utility. We propose the Quality-aware Dynamic Resources-decoupled FL algorithm (QD-RDFL), in which we derive and solve the optimal strategies of all parties to achieve SHE using backward induction, and a dynamic optimization mechanism is designed to improve the optimal strategy profile by evaluating the contribution of data quality from data owners to the global model during real training. Our comprehensive experiments demonstrate that our method effectively encourages the linkage of the three parties involved, maximizing global utility and data asset value."
1654,679d459debd8ffd557a2b4e3,cs.LG,https://arxiv.org/pdf/2501.14551,Fairness of Deep Ensembles: On the interplay between per-group task difficulty and under-representation,"Estanislao Claucich, Sara Hooker, Diego H. Milone, Enzo Ferrante, Rodrigo Echeveste",Machine Learning,"Ensembling is commonly regarded as an effective way to improve the general performance of models in machine learning, while also increasing the robustness of predictions. When it comes to algorithmic fairness, heterogeneous ensembles, composed of multiple model types, have been employed to mitigate biases in terms of demographic attributes such as sex, age or ethnicity. Moreover, recent work has shown how in multi-class problems even simple homogeneous ensembles may favor performance of the worst-performing target classes. While homogeneous ensembles are simpler to implement in practice, it is not yet clear whether their benefits translate to groups defined not in terms of their target class, but in terms of demographic or protected attributes, hence improving fairness. In this work we show how this simple and straightforward method is indeed able to mitigate disparities, particularly benefiting under-performing subgroups. Interestingly, this can be achieved without sacrificing overall performance, which is a common trade-off observed in bias mitigation strategies. Moreover, we analyzed the interplay between two factors which may result in biases: sub-group under-representation and the inherent difficulty of the task for each group. These results revealed that, contrary to popular assumptions, having balanced datasets may be suboptimal if the task difficulty varies between subgroups. Indeed, we found that a perfectly balanced dataset may hurt both the overall performance and the gap between groups. This highlights the importance of considering the interaction between multiple forces at play in fairness."
1655,679d459debd8ffd557a2b4e4,cs.LG,https://arxiv.org/pdf/2501.14543,Reducing Action Space for Deep Reinforcement Learning via Causal Effect Estimation,"Wenzhang Liu, Lianjun Jin, Lu Ren, Chaoxu Mu, Changyin Sun",Machine Learning,"Intelligent decision-making within large and redundant action spaces remains challenging in deep reinforcement learning. Considering similar but ineffective actions at each step can lead to repetitive and unproductive trials. Existing methods attempt to improve agent exploration by reducing or penalizing redundant actions, yet they fail to provide quantitative and reliable evidence to determine redundancy. In this paper, we propose a method to improve exploration efficiency by estimating the causal effects of actions. Unlike prior methods, our approach offers quantitative results regarding the causality of actions for one-step transitions. We first pre-train an inverse dynamics model to serve as prior knowledge of the environment. Subsequently, we classify actions across the entire action space at each time step and estimate the causal effect of each action to suppress redundant actions during exploration. We provide a theoretical analysis to demonstrate the effectiveness of our method and present empirical results from simulations in environments with redundant actions to evaluate its performance. Our implementation is available athttps://github.com/agi-brain/cee.git."
1656,679d459debd8ffd557a2b4e5,cs.LG,https://arxiv.org/pdf/2501.14531,On Hardening DNNs against Noisy Computations,"Xiao Wang, Hendrik Borras, Bernhard Klein, Holger Fröning",Machine Learning,"The success of deep learning has sparked significant interest in designing computer hardware optimized for the high computational demands of neural network inference.
As further miniaturization of digital CMOS processors becomes increasingly challenging, alternative computing paradigms, such as analog computing, are gaining consideration.
Particularly for compute-intensive tasks such as matrix multiplication, analog computing presents a promising alternative due to its potential for significantly higher energy efficiency compared to conventional digital technology.
However, analog computations are inherently noisy, which makes it challenging to maintain high accuracy on deep neural networks.
This work investigates the effectiveness of training neural networks with quantization to increase the robustness against noise.
Experimental results across various network architectures show that quantization-aware training with constant scaling factors enhances robustness.
We compare these methods with noisy training, which incorporates a noise injection during training that mimics the noise encountered during inference.
While both two methods increase tolerance against noise, noisy training emerges as the superior approach for achieving robust neural network performance, especially in complex neural architectures."
1657,679d459debd8ffd557a2b4e6,cs.LG,https://arxiv.org/pdf/2501.14499,Automated Assignment Grading with Large Language Models: Insights From a Bioinformatics Course,"Pavlin G. Poličar, Martin Špendl, Tomaž Curk, Blaž Zupan","Machine Learning, Computers and Society","Motivation:Providing students with individualized feedback through assignments is a cornerstone of education that supports their learning and development. Studies have shown that timely, high-quality feedback plays a critical role in improving learning outcomes. However, providing personalized feedback on a large scale in classes with large numbers of students is often impractical due to the significant time and effort required. Recent advances in natural language processing and large language models (LLMs) offer a promising solution by enabling the efficient delivery of personalized feedback. These technologies can reduce the workload of course staff while improving student satisfaction and learning outcomes. Their successful implementation, however, requires thorough evaluation and validation in real classrooms.Results:We present the results of a practical evaluation of LLM-based graders for written assignments in the 2024/25 iteration of the Introduction to Bioinformatics course at the University of Ljubljana. Over the course of the semester, more than 100 students answered 36 text-based questions, most of which were automatically graded using LLMs. In a blind study, students received feedback from both LLMs and human teaching assistants without knowing the source, and later rated the quality of the feedback. We conducted a systematic evaluation of six commercial and open-source LLMs and compared their grading performance with human teaching assistants. Our results show that with well-designed prompts, LLMs can achieve grading accuracy and feedback quality comparable to human graders. Our results also suggest that open-source LLMs perform as well as commercial LLMs, allowing schools to implement their own grading systems while maintaining privacy."
1658,679d459debd8ffd557a2b4e7,cs.LG,https://arxiv.org/pdf/2501.14460,MLMC: Interactive multi-label multi-classifier evaluation without confusion matrices,"Aleksandar Doknic, Torsten Möller",Machine Learning,"Machine learning-based classifiers are commonly evaluated by metrics like accuracy, but deeper analysis is required to understand their strengths and weaknesses. MLMC is a visual exploration tool that tackles the challenge of multi-label classifier comparison and evaluation. It offers a scalable alternative to confusion matrices which are commonly used for such tasks, but don’t scale well with a large number of classes or labels. Additionally, MLMC allows users to view classifier performance from an instance perspective, a label perspective, and a classifier perspective. Our user study shows that the techniques implemented by MLMC allow for a powerful multi-label classifier evaluation while preserving user friendliness."
1659,679d459debd8ffd557a2b4e8,cs.LG,https://arxiv.org/pdf/2501.14453,Optimal Strategies for Federated Learning Maintaining Client Privacy,"Uday Bhaskar, Varul Srivastava, Avyukta Manjunatha Vummintala, Naresh Manwani, Sujit Gujar",Machine Learning,"Federated Learning (FL) emerged as a learning method to enable the server to train models over data distributed among various clients. These clients are protective about their data being leaked to the server, any other client, or an external adversary, and hence, locally train the model and share it with the server rather than sharing the data. The introduction of sophisticated inferencing attacks enabled the leakage of information about data through access to model parameters. To tackle this challenge, privacy-preserving federated learning aims to achieve differential privacy through learning algorithms like DP-SGD. However, such methods involve adding noise to the model, data, or gradients, reducing the model’s performance."
1660,679d459debd8ffd557a2b4e9,cs.LG,https://arxiv.org/pdf/2501.14441,Impact of Batch Normalization on Convolutional Network Representations,"Hermanus L. Potgieter, Coenraad Mouton, Marelie H. Davel",Machine Learning,"Batch normalization (BatchNorm) is a popular layer normalization technique used when training deep neural networks.
It has been shown to enhance the training speed and accuracy of deep learning models. However, the mechanics by which BatchNorm achieves these benefits is an active area of research, and different perspectives have been proposed.
In this paper, we investigate the effect of BatchNorm on the resulting hidden representations, that is, the vectors of activation values formed as samples are processed at each hidden layer.
Specifically, we consider the sparsity of these representations, as well as their implicit clustering – the creation of groups of representations that are similar to some extent.
We contrast image classification models trained with and without batch normalization and highlight consistent differences observed.
These findings highlight that BatchNorm’s effect on representational sparsity isnota significant factor affecting generalization, while the representations of models trained with BatchNorm tend to show more advantageous clustering characteristics."
1661,679d459debd8ffd557a2b4ea,cs.LG,https://arxiv.org/pdf/2501.14440,Convergence of gradient based training for linear Graph Neural Networks,"Dhiraj Patel, Anton Savostianov, Michael T. Schaub","Machine Learning, Discrete Mathematics, Social and Information Networks, Numerical Analysis","Graph Neural Networks (GNNs) are powerful tools for addressing learning problems on graph structures, with a wide range of applications in molecular biology and social networks. However, the theoretical foundations underlying their empirical performance are not well understood. In this article, we examine the convergence of gradient dynamics in the training of linear GNNs. Specifically, we prove that the gradient flow training of a linear GNN with mean squared loss converges to the global minimum at an exponential rate. The convergence rate depends explicitly on the initial weights and the graph shift operator, which we validate on synthetic datasets from well-known graph models and real-world datasets. Furthermore, we discuss the gradient flow that minimizes the total weights at the global minimum. In addition to the gradient flow, we study the convergence of linear GNNs under gradient descent training, an iterative scheme viewed as a discretization of gradient flow."
1662,679d459debd8ffd557a2b4eb,cs.LG,https://arxiv.org/pdf/2501.14427,GraphBC: Improving LLMs for Better Graph Data Processing,"Xu Chu, Hanlin Xue, Zhijie Tan, Bingce Wang, Tong Mo, Weiping Li",Machine Learning,"The success of Large Language Models (LLMs) in various domains has led researchers to apply them to graph-related problems by converting graph data into natural language text. However, unlike graph data, natural language inherently has sequential order. We observe that when the order of nodes or edges in the natural language description of a graph is shuffled, despite describing the same graph, model performance fluctuates between high performance and random guessing. Additionally, due to the limited input context length of LLMs, current methods typically randomly sample neighbors of target nodes as representatives of their neighborhood, which may not always be effective for accurate reasoning. To address these gaps, we introduce GraphBC. This novel model framework features an Order Selector Module to ensure proper serialization order of the graph and a Subgraph Sampling Module to sample subgraphs with better structure for better reasoning. Furthermore, we propose Graph CoT obtained through distillation, and enhance LLM’s reasoning and zero-shot learning capabilities for graph tasks through instruction tuning. Experiments on multiple datasets for node classification and graph question-answering demonstrate that GraphBC improves LLMs’ performance and generalization ability on graph tasks."
1663,679d459debd8ffd557a2b4ec,cs.LG,https://arxiv.org/pdf/2501.14426,CENTS: Generating synthetic electricity consumption time series for rare and unseen scenarios,"Michael Fuest, Alfredo Cuesta, Kalyan Veeramachaneni",Machine Learning,"Recent breakthroughs in large-scale generative modeling have demonstrated the potential of foundation models in domains such as natural language, computer vision, and protein structure prediction. However, their application in the energy and smart grid sector remains limited due to the scarcity and heterogeneity of high-quality data. In this work, we propose a method for creating high-fidelity electricity consumption time series data for rare and unseen context variables (e.g. location, building type, photovoltaics). Our approach,ContextEncoding andNormalizingTimeSeries Generation, orCENTS, includes three key innovations: (i) A context normalization approach that enables inverse transformation for time series context variables unseen during training, (ii) a novel context encoder to condition any state-of-the-art time-series generator on arbitrary numbers and combinations of context variables, (iii) a framework for training this context encoder jointly with a time-series generator using an auxiliary context classification loss designed to increase expressivity of context embeddings and improve model performance. We further provide a comprehensive overview of different evaluation metrics for generative time series models. Our results highlight the efficacy of the proposed method in generating realistic household-level electricity consumption data, paving the way for training larger foundation models in the energy domain on synthetic as well as real-world data."
1664,679d459debd8ffd557a2b4ed,cs.LG,https://arxiv.org/pdf/2501.14394,Reinforcement Learning for Efficient Returns Management,"Pascal Linden, Nathalie Paul, Tim Wirtz, Stefan Wrobel",Machine Learning,"In retail warehouses, returned products are typically placed in an intermediate storage until a decision regarding further shipment to stores is made.
The longer products are held in storage, the higher the inefficiency and costs of the returns management process, since enough storage area has to be provided and maintained while the products are not placed for sale.
To reduce the average product storage time, we consider an alternative solution where re-allocation decisions for products can be made instantly upon their arrival in the warehouse allowing only a limited number of products to still be stored simultaneously. We transfer the problem to an online multiple knapsack problem and propose a novel reinforcement learning approach to pack the items (products) into the knapsacks (stores) such that the overall value (expected revenue) is maximized. Empirical evaluations on simulated data demonstrate that, compared to the usual offline decision procedure, our approach comes with a performance gap of only3%percent33\%3 %while significantly reducing the average storage time of a product by96%percent9696\%96 %."
1665,679d459debd8ffd557a2b4ee,cs.LG,https://arxiv.org/pdf/2501.14390,Distinguishing Parkinson's Patients Using Voice-Based Feature Extraction and Classification,"Burak Çelik, Ayhan Akbal",Machine Learning,
1666,679d459debd8ffd557a2b4ef,cs.LG,https://arxiv.org/pdf/2501.14373,Fat-to-Thin Policy Optimization: Offline RL with Sparse Policies,"Lingwei Zhu, Han Wang, Yukie Nagai",Machine Learning,"Sparse policies have important real-world implications, e.g. in modelling safety-critical tasks where some dangerous actions should have strictly zero probability.
However, sparse policies can cause many difficulties with existing algorithms which require evaluating off-policy/offline actions that may fall outside the current support.
In this paper, we propose the first policy optimization algorithm: Fat-to-Thin Policy Optimization (FtTPO) for learning sparse policies.
Specifically, we maintain a Fat (heavy-tailed) proposal policy that is exploited as regularization for learning the thin (sparse) actor to enforce vicinity to the data generating policy.
We instantiate FtTPO with the generalq𝑞qitalic_q-Gaussian family which contains both heavy-tailed and sparse members."
1667,679d459debd8ffd557a2b4f0,cs.LG,https://arxiv.org/pdf/2501.14351,Facies Classification with Copula Entropy,Jian Ma,"Machine Learning, Geophysics, Applications","In this paper we propose to apply copula entropy (CE) to facies classification. In our method, the correlations between geological variables and facies classes are measured with CE and then the variables associated with large negative CEs are selected for classification. We verified the proposed method on a typical facies dataset for facies classification and the experimental results show that the proposed method can select less geological variables for facies classification without sacrificing classification performance. The geological variables such selected are also interpretable to geologists with geological meanings due to the rigorous definition of CE."
1668,679d459debd8ffd557a2b4f1,cs.LG,https://arxiv.org/pdf/2501.14349,"Online Inverse Linear Optimization: Improved Regret Bound, Robustness to Suboptimality, and Toward Tight Regret Analysis","Shinsaku Sakaue, Taira Tsuchiya, Han Bao, Taihei Oki",Machine Learning,"We study an online learning problem where, overT𝑇Titalic_Trounds, a learner observes both time-varying sets of feasible actions and an agent’s optimal actions, selected by solving linear optimization over the feasible actions.
The learner sequentially makes predictions of the agent’s underlying linear objective function, and their quality is measured by theregret, the cumulative gap between optimal objective values and those achieved by following the learner’s predictions.
A seminal work by Bärmann et al. (ICML 2017) showed that online learning methods can be applied to this problem to achieve regret bounds ofO⁢(T)𝑂𝑇O(\sqrt{T})italic_O ( square-root start_ARG italic_T end_ARG ).
Recently, Besbes et al. (COLT 2021, Oper. Res. 2023) significantly improved the result by achieving anO⁢(n4⁢ln⁡T)𝑂superscript𝑛4𝑇O(n^{4}\ln T)italic_O ( italic_n start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT roman_ln italic_T )regret bound, wheren𝑛nitalic_nis the dimension of the ambient space of objective vectors.
Their method, based on the ellipsoid method, runs in polynomial time but is inefficient for largen𝑛nitalic_nandT𝑇Titalic_T.
In this paper, we obtain anO⁢(n⁢ln⁡T)𝑂𝑛𝑇O(n\ln T)italic_O ( italic_n roman_ln italic_T )regret bound, improving upon the previous bound ofO⁢(n4⁢ln⁡T)𝑂superscript𝑛4𝑇O(n^{4}\ln T)italic_O ( italic_n start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT roman_ln italic_T )by a factor ofn3superscript𝑛3n^{3}italic_n start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT.
Our method is simple and efficient: we apply the online Newton step (ONS) to appropriate exp-concave loss functions.
Moreover, for the case where the agent’s actions are possibly suboptimal, we establish anO⁢(n⁢ln⁡T+ΔT⁢n⁢ln⁡T)𝑂𝑛𝑇subscriptΔ𝑇𝑛𝑇O(n\ln T+\sqrt{\Delta_{T}n\ln T})italic_O ( italic_n roman_ln italic_T + square-root start_ARG roman_Δ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT italic_n roman_ln italic_T end_ARG )regret bound, whereΔTsubscriptΔ𝑇\Delta_{T}roman_Δ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPTis the cumulative suboptimality of the agent’s actions.
This bound is achieved by using MetaGrad, which runs ONS withΘ⁢(ln⁡T)Θ𝑇\Theta(\ln T)roman_Θ ( roman_ln italic_T )different learning rates in parallel.
We also provide a simple instance that implies anΩ⁢(n)Ω𝑛\Omega(n)roman_Ω ( italic_n )lower bound, showing that ourO⁢(n⁢ln⁡T)𝑂𝑛𝑇O(n\ln T)italic_O ( italic_n roman_ln italic_T )bound is tight up to anO⁢(ln⁡T)𝑂𝑇O(\ln T)italic_O ( roman_ln italic_T )factor.
This gives rise to a natural question: can theO⁢(ln⁡T)𝑂𝑇O(\ln T)italic_O ( roman_ln italic_T )factor in the upper bound be removed?
For the special case ofn=2𝑛2n=2italic_n = 2, we show that anO⁢(1)𝑂1O(1)italic_O ( 1 )regret bound is possible, while we delineate challenges in extending this result to higher dimensions."
1669,679d459debd8ffd557a2b4f2,cs.LG,https://arxiv.org/pdf/2501.14321,Domain Expansion: Parameter-Efficient Modules as Building Blocks for Composite Domains,"Mann Patel, Divyajyoti Panda, Hilay Mehta, Parth Patel, Dhruv Parikh",Machine Learning,"Parameter-Efficient Fine-Tuning (PEFT) is an efficient alternative to full scale fine-tuning, gaining popularity recently. With pre-trained model sizes growing exponentially, PEFT can be effectively utilized to fine-tune compact modules, Parameter-Efficient Modules (PEMs), trained to be domain experts over diverse domains. In this project, we explore composing such individually fine-tuned PEMs for distribution generalization over the composite domain. To compose PEMs, simple composing functions are used that operate purely on the weight space of the individually fine-tuned PEMs, without requiring any additional fine-tuning. The proposed method is applied to the task of representing the 16 Myers-Briggs Type Indicator (MBTI) composite personalities via 4 building block dichotomies, comprising of 8 individual traits which can be merged (composed) to yield a unique personality. We evaluate the individual trait PEMs and the composed personality PEMs via an online MBTI personality quiz questionnaire, validating the efficacy of PEFT to fine-tune PEMs and merging PEMs without further fine-tuning for domain composition. Code is available here.111https://github.com/manncodes/domain-expansion"
1670,679d459debd8ffd557a2b4f3,cs.LG,https://arxiv.org/pdf/2501.14314,Graph Feedback Bandits on Similar Arms: With and Without Graph Structures,"Han Qi, Fei Guo, Li Zhu, Qiaosheng Zhang, Xuelong Li",Machine Learning,"In this paper, we study the stochastic multi-armed bandit problem with graph feedback. Motivated by applications in clinical trials and recommendation systems, we assume that two arms are connected if and only if they are similar (i.e., their means are close to each other). We establish a regret lower bound for this problem under the novel feedback structure and introduce two upper confidence bound (UCB)-based algorithms: Double-UCB, which has problem-independent regret upper bounds, and Conservative-UCB, which has problem-dependent upper bounds. Leveraging the similarity structure, we also explore a scenario where the number of arms increases over time (referred to as theballooning setting). Practical applications of this scenario include Q&A platforms (e.g., Reddit, Stack Overflow, Quora) and product reviews on platforms like Amazon and Flipkart, where answers (or reviews) continuously appear, and the goal is to display the best ones at the top.
We extend these two UCB-based algorithms to the ballooning setting. Under mild assumptions, we provide regret upper bounds for both algorithms and discuss their sub-linearity. Furthermore, we propose a new version of the corresponding algorithms that do not rely on prior knowledge of the graph’s structural information and provide regret upper bounds.
Finally, we conduct experiments to validate the theoretical results."
1671,679d459debd8ffd557a2b4f4,cs.LG,https://arxiv.org/pdf/2501.14311,An Efficient Real Time DDoS Detection Model Using Machine Learning Algorithms,Debashis Kar Suvra,Machine Learning,"Distributed Denial of Service attacks have become a significant threat to industries and governments leading to substantial financial losses. With the growing reliance on internet services, DDoS attacks can disrupt services by overwhelming servers with false traffic causing downtime and data breaches. Although various detection techniques exist, selecting an effective method remains challenging due to trade-offs between time efficiency and accuracy. This research focuses on developing an efficient real-time DDoS detection system using machine learning algorithms leveraging the UNB CICDDoS2019 dataset including various traffic features. The study aims to classify DDoS and non-DDoS traffic through various ML classifiers including Logistic Regression, K-Nearest Neighbors, Random Forest, Support Vector Machine, Naive Bayes. The dataset is preprocessed through data cleaning, standardization and feature selection techniques using Principal Component Analysis. The research explores the performance of these algorithms in terms of precision, recall and F1-score as well as time complexity to create a reliable system capable of real-time detection and mitigation of DDoS attacks. The findings indicate that RF, AdaBoost and XGBoost outperform other algorithms in accuracy and efficiency, making them ideal candidates for real-time applications."
1672,679d459debd8ffd557a2b4f5,cs.LG,https://arxiv.org/pdf/2501.14291,"Advances in Temporal Point Processes: Bayesian, Deep, and LLM Approaches","Feng Zhou, Quyu Kong, Yixuan Zhang","Machine Learning, Machine Learning","Temporal point processes (TPPs) are stochastic process models used to characterize event sequences occurring in continuous time. Traditional statistical TPPs have a long-standing history, with numerous models proposed and successfully applied across diverse domains. In recent years, advances in deep learning have spurred the development of neural TPPs, enabling greater flexibility and expressiveness in capturing complex temporal dynamics. The emergence of large language models (LLMs) has further sparked excitement, offering new possibilities for modeling and analyzing event sequences by leveraging their rich contextual understanding.
This survey presents a comprehensive review of recent research on TPPs from three perspectives: Bayesian, deep learning, and LLM approaches. We begin with a review of the fundamental concepts of TPPs, followed by an in-depth discussion of model design and parameter estimation techniques in these three frameworks. We also revisit classic application areas of TPPs to highlight their practical relevance. Finally, we outline challenges and promising directions for future research."
1673,679d459debd8ffd557a2b4f6,cs.LG,https://arxiv.org/pdf/2501.14271,TLXML: Task-Level Explanation of Meta-Learning via Influence Functions,"Yoshihiro Mitsuka, Shadan Golestan, Zahin Sufiyan, Sheila Schoepp, Shotaro Miwa, Osmar R. Zaïane",Machine Learning,"The scheme of adaptation via meta-learning is seen as an ingredient for solving the problem of data shortage or distribution shift in real-world applications, but it also brings the new risk of inappropriate updates of the model in the user environment, which increases the demand for explainability. Among the various types of XAI methods, establishing a method of explanation based on past experience in meta-learning requires special consideration due to its bi-level structure of training, which has been left unexplored. In this work, we propose influence functions for explaining meta-learning that measure the sensitivities of training tasks to adaptation and inference. We also argue that the approximation of the Hessian using the Gauss-Newton matrix resolves computational barriers peculiar to meta-learning. We demonstrate the adequacy of the method through experiments on task distinction and task distribution distinction using image classification tasks with MAML and Prototypical Network."
1674,679d459debd8ffd557a2b4f7,cs.LG,https://arxiv.org/pdf/2501.14266,TrajFlow: A Generative Framework for Occupancy Density Estimation Using Normalizing Flows,"Mitch Kosieradzki, Seongjin Choi",Machine Learning,"In transportation systems and autonomous vehicles, intelligent agents must understand the future motion of traffic participants to effectively plan motion trajectories.
At the same time, the motion of traffic participants is inherently uncertain.
In this paper, we proposeTrajFlow, a generative framework for estimating the occupancy density of traffic participants.
Our framework utilizes a causal encoder to extract semantically meaningful embeddings of the observed trajectory, as well as a normalizing flow to decode these embeddings and determine the most likely future location of traffic participants at some time point in the future.
Our formulation differs from existing approaches because we model the marginal distribution of spatial locations instead of the joint distribution of unobserved trajectories. The advantages of a marginal formulation are numerous. First, we demonstrate that the marginal formulation produces higher accuracy on challenging trajectory forecasting benchmarks. Second, the marginal formulation allows for a fully continuous sampling of future locations. Finally, marginal densities are better suited for downstream tasks as they allow for the computation of per-agent motion trajectories and occupancy grids, the two most commonly used representations for motion forecasting. We present a novel architecture based entirely on neural differential equations as an implementation of this framework and provide ablations to demonstrate the advantages of a continuous implementation over a more traditional discrete neural network based approach. The code is available athttps://github.com/kosieram21/TrajFlow."
1675,679d459debd8ffd557a2b4f8,cs.LG,https://arxiv.org/pdf/2501.14256,Revisiting Applicable and Comprehensive Knowledge Tracing in Large-Scale Data,"Yiyun Zhou, Wenkang Han, Jingyuan Chen","Machine Learning, Information Retrieval","Knowledge Tracing (KT) is a fundamental component of Intelligent Tutoring Systems (ITS), enabling the modeling of students’ knowledge states to predict future performance. The introduction of Deep Knowledge Tracing (DKT), the first deep learning-based KT (DLKT) model, has brought significant advantages in terms of applicability and comprehensiveness. However, recent DLKT models, such as Attentive Knowledge Tracing (AKT), have often prioritized predictive performance at the expense of these benefits. While deep sequential models like DKT have shown potential, they face challenges related to parallel computing, storage decision modification, and limited storage capacity.
To address these limitations, we propose DKT2, a novel KT model that leverages the recently developed xLSTM architecture. DKT2 enhances input representation using the Rasch model and incorporates Item Response Theory (IRT) for interpretability, allowing for the decomposition of learned knowledge into familiar and unfamiliar knowledge. By integrating this knowledge with predicted questions, DKT2 generates comprehensive knowledge states. Extensive experiments conducted across three large-scale datasets demonstrate that DKT2 consistently outperforms 17 baseline models in various prediction tasks, underscoring its potential for real-world educational applications. This work bridges the gap between theoretical advancements and practical implementation in KT.
Our code and datasets will be available athttps://github.com/codebase-2025/DKT2."
1676,679d459debd8ffd557a2b4f9,cs.LG,https://arxiv.org/pdf/2501.14233,A Data-driven Dynamic Temporal Correlation Modeling Framework for Renewable Energy Scenario Generation,"Xiaochong Dong, Yilin Liu, Xuemin Zhang, Shengwei Mei",Machine Learning,
1677,679d459debd8ffd557a2b4fa,cs.LG,https://arxiv.org/pdf/2501.14211,When GNNs meet symmetry in ILPs: an orbit-based feature augmentation approach,"Qian Chen, Lei Li, Qian Li, Jianghua Wu, Akang Wang, Ruoyu Sun, Xiaodong Luo, Tsung-Hui Chang, Qingjiang Shi","Machine Learning, Optimization and Control","A common characteristic in integer linear programs (ILPs) is symmetry, allowing variables to be permuted without altering the underlying problem structure. Recently, GNNs have emerged as a promising approach for solving ILPs.
However, a significant challenge arises when applying GNNs to ILPs with symmetry: classic GNN architectures struggle to differentiate between symmetric variables, which limits their predictive accuracy. In this work, we investigate the properties of permutation equivariance and invariance in GNNs, particularly in relation to the inherent symmetry of ILP formulations. We reveal that the interaction between these two factors contributes to the difficulty of distinguishing between symmetric variables.
To address this challenge, we explore the potential of feature augmentation and propose several guiding principles for constructing augmented features. Building on these principles, we develop an orbit-based augmentation scheme that first groups symmetric variables and then samples augmented features for each group from a discrete uniform distribution. Empirical results demonstrate that our proposed approach significantly enhances both training efficiency and predictive performance."
1678,679d459debd8ffd557a2b4fb,cs.LG,https://arxiv.org/pdf/2501.14197,Bi-directional Curriculum Learning for Graph Anomaly Detection: Dual Focus on Homogeneity and Heterogeneity,"Yitong Hao, Enbo He, Yue Zhang, Guisheng Yin","Machine Learning, Social and Information Networks, Machine Learning","Graph anomaly detection (GAD) aims to identify nodes from a graph that are significantly different from normal patterns. Most previous studies are model-driven, focusing on enhancing the detection effect by improving the model structure. However, these approaches often treat all nodes equally, neglecting the different contributions of various nodes to the training. Therefore, we introduce graph curriculum learning as a simple and effective plug-and-play module to optimize GAD methods. The existing graph curriculum learning mainly focuses on the homogeneity of graphs and treats nodes with high homogeneity as easy nodes. In fact, GAD models can handle not only graph homogeneity but also heterogeneity, which leads to the unsuitability of these existing methods. To address this problem, we propose an innovativeBi-directionalCurriculumLearning strategy (BCL), which considers nodes with higher and lower similarity to neighbor nodes as simple nodes in the direction of focusing on homogeneity and focusing on heterogeneity, respectively, and prioritizes their training. Extensive experiments show that BCL can be quickly integrated into existing detection processes and significantly improves the performance of ten GAD anomaly detection models on seven commonly used datasets."
1679,679d459debd8ffd557a2b4fc,cs.LG,https://arxiv.org/pdf/2501.14152,Multimodal Prescriptive Deep Learning,"Dimitris Bertsimas, Lisa Everest, Vasiliki Stoumpou","Machine Learning, Machine Learning","We introduce a multimodal deep learning framework, Prescriptive Neural Networks (PNNs), that combines ideas from optimization and machine learning, and is, to the best of our knowledge, the first prescriptive method to handle multimodal data. The PNN is a feedforward neural network trained on embeddings to output an outcome-optimizing prescription. In two real-world multimodal datasets, we demonstrate that PNNs prescribe treatments that are able to significantly improve estimated outcomes in transcatheter aortic valve replacement (TAVR) procedures by reducing estimated postoperative complication rates by 32% and in liver trauma injuries by reducing estimated mortality rates by over 40%. In four real-world, unimodal tabular datasets, we demonstrate that PNNs outperform or perform comparably to other well-known, state-of-the-art prescriptive models; importantly, on tabular datasets, we also recover interpretability through knowledge distillation, fitting interpretable Optimal Classification Tree models onto the PNN prescriptions as classification targets, which is critical for many real-world applications. Finally, we demonstrate that our multimodal PNN models achieve stability across randomized data splits comparable to other prescriptive methods and produce realistic prescriptions across the different datasets."
1680,679d459debd8ffd557a2b4fd,cs.LG,https://arxiv.org/pdf/2501.14143,"An Extensive and Methodical Review of Smart Grids for Sustainable Energy Management-Addressing Challenges with AI, Renewable Energy Integration and Leading-edge Technologies","Parag Biswas, Abdur Rashid, abdullah al masum, MD Abdullah Al Nasim, A.S.M Anas Ferdous, Kishor Datta Gupta, Angona Biswas","Machine Learning, Computers and Society","Energy management decreases energy expenditures and consumption while simultaneously increasing energy efficiency, reducing carbon emissions, and enhancing operational performance. Smart grids are a type of sophisticated energy infrastructure that increase the generation and distribution of electricity’s sustainability, dependability, and efficiency by utilizing digital communication technologies. They combine a number of cutting-edge techniques and technology to improve energy resource management. A large amount of research study on the topic of smart grids for energy management has been completed in the last several years. The authors of the present study want to cover a number of topics, including smart grid benefits and components, technical developments, integrating renewable energy sources, using artificial intelligence and data analytics, cybersecurity, and privacy. Smart Grids for Energy Management are an innovative field of study aiming at tackling various difficulties and magnifying the efficiency, dependability, and sustainability of energy systems, including: 1) Renewable sources of power like solar and wind are intermittent and unpredictable 2) Defending smart grid system from various cyber-attacks 3) Incorporating an increasing number of electric vehicles into the system of power grid without overwhelming it. Additionally, it is proposed to use AI and data analytics for better performance on the grid, reliability, and energy management. It also looks into how AI and data analytics can be used to optimize grid performance, enhance reliability, and improve energy management. The authors will explore these significant challenges and ongoing research. Lastly, significant issues in this field are noted, and recommendations for further work are provided."
1681,679d459debd8ffd557a2b4fe,cs.LG,https://arxiv.org/pdf/2501.14136,Saliency Maps are Ambiguous: Analysis of Logical Relations on First and Second Order Attributions,"Leonid Schwenke, Martin Atzmueller",Machine Learning,"Recent work uncovered potential flaws in e. g., attribution or heatmap based saliency methods.
A typical flaw is a confirmations bias, where the scores are compared to human expectation. Since measuring the quality of saliency methods is hard due to missing ground truth model reasoning, finding general limitations is also hard. This is further complicated, because masking-based evaluation on complex data can easily introduce a bias, as most methods cannot fully ignore inputs. In this work, we extend our previous analysis on the logical dataset framework ANDOR, where we showed that all analysed saliency methods fail to grasp all needed classification information for all possible scenarios. Specifically, this paper extends our previous work using analysis on more datasets, in order to better understand in which scenarios the saliency methods fail. Further, we apply the Global Coherence Representation as an additional evaluation method in order to enable actual input omission."
1682,679d459debd8ffd557a2b4ff,cs.LG,https://arxiv.org/pdf/2501.14118,Selecting Critical Scenarios of DER Adoption in Distribution Grids Using Bayesian Optimization,"Olivier Mulkin, Miguel Heleno, Mike Ludkovski","Machine Learning, Applications, Machine Learning","We develop a new methodology to select scenarios of DER adoption most critical for distribution grids. Anticipating risks of future voltage and line flow violations due to additional PV adopters is central for utility investment planning but continues to rely on deterministic or ad hoc scenario selection. We propose a highly efficient search framework based on multi-objective Bayesian Optimization. We treat underlying grid stress metrics as computationally expensive black-box functions, approximated via Gaussian Process surrogates and design an acquisition function based on probability of scenarios being Pareto-critical across a collection of line- and bus-based violation objectives. Our approach provides a statistical guarantee and offers an order of magnitude speed-up relative to a conservative exhaustive search. Case studies on realistic feeders with 200-400 buses demonstrate the effectiveness and accuracy of our approach."
1683,679d459debd8ffd557a2b500,cs.LG,https://arxiv.org/pdf/2501.14103,Personalized Interpolation: An Efficient Method to Tame Flexible Optimization Window Estimation,"Xin Zhang, Weiliang Li, Rui Li, Zihang Fu, Tongyi Tang, Zhengyu Zhang, Wen-Yen Chen, Nima Noorshams, Nirav Jasapara, Xiaowen Ding, Ellie Wen, Xue Feng",Machine Learning,"In the realm of online advertising, optimizing conversions is crucial for delivering relevant products to users and enhancing business outcomes(Rosales et al.,2012; Lee et al.,2012; Lu et al.,2017; Jannach and Jugovac,2019). Predicting conversion events is challenging due to variable delays between user interactions, such as impressions or clicks, and the actual conversions. These delays differ significantly across various advertisers and products, necessitating distinct optimization time windows for targeted conversions. To address this, we introduce a novel approach named thePersonalized Interpolationmethod, which innovatively builds upon existing fixed conversion window models to estimate flexible conversion windows. This method allows for the accurate estimation of conversions across a variety of delay ranges, thus meeting the diverse needs of advertisers without increasing system complexity. To validate the efficacy of our proposed method, we conducted comprehensive experiments using ads conversion model. Our experiments demonstrate that this method not only achieves high prediction accuracy but also does so more efficiently than other existing solutions. This validation underscores the potential of our Personalized Interpolation method to significantly enhance conversion optimization in real-world online advertising systems, promising improved targeting and effectiveness in advertising strategies."
1684,679d459debd8ffd557a2b501,cs.LG,https://arxiv.org/pdf/2501.14102,5G LDPC Linear Transformer for Channel Decoding,"Mario Hernandez, Fernando Pinero","Machine Learning, Information Theory","This work introduces a novel, fully differentiable linear-time complexity transformer decoder and a transformer decoder to correct 5G New Radio (NR) LDPC codes111All code is available at:https://github.com/pollyjuice74/5G-Decoder.. We propose a scalable approach to decode linear block codes withO⁢(n)𝑂𝑛O(n)italic_O ( italic_n )complexity rather thanO⁢(n2)𝑂superscript𝑛2O(n^{2})italic_O ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )for regular transformers. The architectures’ performances are compared to Belief Propagation (BP), the production-level decoding algorithm used for 5G New Radio (NR) LDPC codes. We achieve bit error rate performance that matches a regular Transformer decoder and surpases one iteration BP, also achieving competitive time performance against BP, even for larger block codes. We utilize Sionna[11], Nvidia’s 5G & 6G physical layer research software, for reproducible results."
1685,679d459debd8ffd557a2b502,cs.LG,https://arxiv.org/pdf/2501.14094,Datasheets for AI and medical datasets (DAIMS): a data validation and documentation framework before machine learning analysis in medical research,"Ramtin Zargari Marandi, Anne Svane Frahm, Maja Milojevic",Machine Learning,
1686,679d459debd8ffd557a2b503,cs.LG,https://arxiv.org/pdf/2501.14090,Making Reliable and Flexible Decisions in Long-tailed Classification,"Bolian Li, Ruqi Zhang","Machine Learning, Machine Learning","Long-tailed classification is challenging due to its heavy imbalance in class probabilities. While existing methods often focus on overall accuracy or accuracy for tail classes, they overlook a critical aspect: certain types of errors can carry greater risks than others in real-world long-tailed problems. For example, misclassifying patients (a tail class) as healthy individuals (a head class) entails far more serious consequences than the reverse scenario. To address this critical issue, we introduce MakingReliable andFlexibleDecisions inLong-tailedClassification (RF-DLC), a novel framework aimed at reliable predictions in long-tailed problems. Leveraging Bayesian Decision Theory, we introduce an integrated gain to seamlessly combine long-tailed data distributions and the decision-making procedure. We further propose an efficient variational optimization strategy for the decision risk objective. Our method adapts readily to diverse utility matrices, which can be designed for specific tasks, ensuring its flexibility for different problem settings. In empirical evaluation, we design a new metric, False Head Rate, to quantify tail-sensitivity risk, along with comprehensive experiments on multiple real-world tasks, including large-scale image classification and uncertainty quantification, to demonstrate the reliability and flexibility of our method.111https://github.com/lblaoke/RF-DLC."
1687,679d459debd8ffd557a2b504,cs.LG,https://arxiv.org/pdf/2501.14036,Efficient Precision Control in Object Detection Models for Enhanced and Reliable Ovarian Follicle Counting,"Vincent Blot, Alexandra Lorenzo de Brionne, Ines Sellami, Olivier Trassard, Isabelle Beau, Charlotte Sonigo, Nicolas J-B. Brunel",Machine Learning,"Image analysis is a key tool for describing the detailed mechanisms of folliculogenesis, such as evaluating the quantity of mouse Primordial ovarian Follicles (PMF) in the ovarian reserve. The development of high-resolution virtual slide scanners offers the possibility of quantifying, robustifying and accelerating the histopathological procedure. A major challenge for machine learning is to control the precision of predictions while enabling a high recall, in order to provide reproducibility. We use a multiple testing procedure that gives an overperforming way to solve the standard Precision-Recall trade-off that gives probabilistic guarantees on the precision. In addition, we significantly improve the overall performance of the models (increase of F1-score) by selecting the decision threshold using contextual biological information or using an auxiliary model. As it is model-agnostic, this contextual selection procedure paves the way to the development of a strategy that can improve the performance of any model without the need of retraining it."
1688,679d459debd8ffd557a2b505,cs.LG,https://arxiv.org/pdf/2501.14710,Overcoming Fairness Trade-offs via Pre-processing: A Causal Perspective,"Charlotte Leininger, Simon Rittel, Ludwig Bothmann","Machine Learning, Machine Learning","Training machine learning models for fair decisions faces two key challenges: Thefairness-accuracy trade-offresults from enforcing fairness which weakens its predictive performance in contrast to an unconstrained model. The incompatibility of different fairness metrics poses
another trade-off – also known as theimpossibility theorem.
Recent work
identifies the bias within the observed data as a possible root cause and shows that fairness and predictive performance are in fact in accord when predictive performance is measured on unbiased data. We offer a causal explanation for these findings using the framework of the
FiND (fictitious and normatively desired) world, a “fair” world, where protected attributes have no causal effects on the target variable.
We show theoretically that (i) classical fairness metrics deemed to be incompatible are naturally satisfied in the FiND world, while (ii) fairness aligns with high predictive performance.
We extend our analysis by suggesting how one can benefit from these theoretical insights in practice, using causal pre-processing methods that approximate the FiND world.
Additionally, we propose a method for evaluating the approximation of the FiND world via pre-processing in practical use cases where we do not have access to the FiND world.
In simulations and empirical studies, we demonstrate that these pre-processing methods are successful in approximating the FiND world
and resolve both trade-offs.
Our results provide actionable solutions for practitioners to achieve fairness and high predictive performance simultaneously."
1689,679d459debd8ffd557a2b506,cs.LG,https://arxiv.org/pdf/2501.14708,Decision-Focused Learning for Complex System Identification: HVAC Management System Application,"Pietro Favaro, Jean-François Toubeau, François Vallée, Yury Dvorkin","Systems and Control, Machine Learning","As opposed to conventional training methods tailored to minimize a given statistical metric or task-agnostic loss (e.g., mean squared error), Decision-Focused Learning (DFL) trains machine learning models for optimal performance in downstream decision-making tools.
We argue thatDFLcan be leveraged to learn the parameters of system dynamics, expressed as constraint of the convex optimization control policy, while the system control signal is being optimized, thus creating an end-to-end learning framework. This is particularly relevant for systems in which behavior changes once the control policy is applied, hence rendering historical data less applicable. The proposed approach can perform system identification — i.e., determine appropriate parameters for the system analytical model — and control simultaneously to ensure that the model’s accuracy is focused on areas most relevant to control.
Furthermore, because black-box systems are non-differentiable, we design a loss function that requires solely to measure the system response. We propose pre-training on historical data and constraint relaxation to stabilize theDFLand deal with potential infeasibilities in learning.
We demonstrate the usefulness of the method on a building Heating, Ventilation, and Air Conditioning day-ahead management system for a realistic 15-zone building located in Denver, US. The results show that the conventionalRCbuilding model, with the parameters obtained from historical data using supervised learning, underestimates HVAC electrical power consumption. For our case study, the ex-post cost is on average six times higher than the expected one. Meanwhile, the same RC model with parameters obtained viaDFLunderestimates the ex-post cost only by 3%."
1690,679d459debd8ffd557a2b507,cs.LG,https://arxiv.org/pdf/2501.14663,End-to-end workflow for machine learning-based qubit readout with QICK and hls4ml,"Giuseppe Di Guglielmo, Botao Du, Javier Campos, Alexandra Boltasseva, Akash V. Dixit, Farah Fahim, Zhaxylyk Kudyshev, Santiago Lopez, Ruichao Ma, Gabriel N. Perdue, Nhan Tran, Omer Yesilyurt, Daniel Bowring","Quantum Physics, Machine Learning","We present an end-to-end workflow for superconducting qubit readout that embeds co-designed Neural Networks (NNs) into the Quantum Instrumentation Control Kit (QICK). Capitalizing on the custom firmware and software of the QICK platform, which is built on Xilinx RFSoC FPGAs, we aim to leverage machine learning (ML) to address critical challenges in qubit readout accuracy and scalability. The workflow utilizes thehls4mlpackage and employs quantization-aware training to translate ML models into hardware-efficient FPGA implementations via user-friendly Python APIs. We experimentally demonstrate the design, optimization, and integration of an ML algorithm for single transmon qubit readout, achieving 96% single-shot fidelity with a latency of 32 ns and less than 16% FPGA look-up table resource utilization. Our results offer the community an accessible workflow to advance ML-driven readout and adaptive control in quantum information processing applications."
1691,679d459debd8ffd557a2b508,cs.LG,https://arxiv.org/pdf/2501.14660,Mean-field limit from general mixtures of experts to quantum neural networks,"Anderson Melchor Hernandez, Davide Pastorello, Giacomo De Palma","Mathematical Physics, Machine Learning, Probability","In this work, we study the asymptotic behavior of Mixture of Experts (MoE) trained via gradient flow on supervised learning problems. Our main result establishes the propagation of chaos for a MoE as the number of experts diverges. We demonstrate that the corresponding empirical measure of their parameters is close to a probability measure that solves a nonlinear continuity equation, and we provide an explicit convergence rate that depends solely on the number of experts. We apply our results to a MoE generated by a quantum neural network."
1692,679d459debd8ffd557a2b509,cs.LG,https://arxiv.org/pdf/2501.14635,Optimal Transport Barycenter via Nonconvex-Concave Minimax Optimization,"Kaheon Kim, Rentian Yao, Changbo Zhu, Xiaohui Chen","Machine Learning, Machine Learning","The optimal transport barycenter (a.k.a. Wasserstein barycenter) is a fundamental notion of averaging that extends from the Euclidean space to the Wasserstein space of probability distributions. Computation of theunregularizedbarycenter for discretized probability distributions on point clouds is a challenging task when the domain dimensiond>1𝑑1d>1italic_d > 1. Most practical algorithms for approximating the barycenter problem are based on entropic regularization. In this paper, we introduce a nearly linear timeO⁢(m⁢log⁡m)𝑂𝑚𝑚O(m\log{m})italic_O ( italic_m roman_log italic_m )and linear space complexityO⁢(m)𝑂𝑚O(m)italic_O ( italic_m )primal-dual algorithm, theWasserstein-Descentℍ˙1superscript˙ℍ1\dot{\mathbb{H}}^{1}over˙ start_ARG blackboard_H end_ARG start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT-Ascent(WDHA) algorithm, for computing theexactbarycenter when the input probability density functions are discretized on anm𝑚mitalic_m-point grid. The key success of the WDHA algorithm hinges on alternating between two different yet closely related Wasserstein and Sobolev optimization geometries for the primal barycenter and dual Kantorovich potential subproblems. Under reasonable assumptions, we establish the convergence rate and iteration complexity of WDHA to its stationary point when the step size is appropriately chosen.
Superior computational efficacy, scalability, and accuracy over the existing Sinkhorn-type algorithms are demonstrated on high-resolution (e.g.,1024×1024102410241024\times 10241024 × 1024images) 2D synthetic and real data."
1693,679d459debd8ffd557a2b50a,cs.LG,https://arxiv.org/pdf/2501.14625,Accelerated Preference Elicitation with LLM-Based Proxies,"David Huang, Francisco Marmolejo-Cossío, Edwin Lock, David Parkes","Computer Science and Game Theory, Machine Learning","Bidders in combinatorial auctions face significant challenges when describing their preferences to an auctioneer. Classical work on preference elicitation focuses on query-based techniques inspired from proper learning—often viaproxiesthat interface between bidders and an auction mechanism—to incrementally learn bidder preferences as needed to compute efficient allocations. Although such elicitation mechanisms enjoy theoretical query efficiency, the amount of communication required may still be too cognitively taxing in practice."
1694,679d459debd8ffd557a2b50b,cs.LG,https://arxiv.org/pdf/2501.14615,Single-neuron deep generative model uncovers underlying physics of neuronal activity in Ca imaging data,"Jordi Abante, Angelo Piga, Berta Ros, Clara F López-León, Josep M Canals, Jordi Soriano","Neurons and Cognition, Machine Learning","Calcium imaging has become a powerful alternative to electrophysiology for studying neuronal activity, offering spatial resolution and the ability to measure large populations of neurons in a minimally invasive manner. This technique has broad applications in neuroscience, neuroengineering, and medicine, enabling researchers to explore the relationship between neuron location and activity. Recent advancements in deep generative models (DGMs) have facilitated the modeling of neuronal population dynamics, uncovering latent representations that provide insights into behavior prediction and neuronal variance. However, these models often rely on spike inference algorithms and primarily focus on population-level dynamics, limiting their applicability for single-neuron analyses. To address this gap, we propose a novel framework for single-neuron representation learning using autoregressive variational autoencoders (AVAEs). Our approach embeds individual neurons’ spatiotemporal signals into a reduced-dimensional space without the need for spike inference algorithms. The AVAE excels over traditional linear methods by generating more informative and discriminative latent representations, improving tasks such as visualization, clustering, and the understanding of neuronal activity. Additionally, the reconstruction performance of the AVAE outperforms the state of the art, demonstrating its ability to accurately recover the original fluorescence signal from the learned representation. Using realistic simulations, we show that our model captures underlying physical properties and connectivity patterns, enabling it to distinguish between different firing and connectivity types. These findings position the AVAE as a versatile and powerful tool for advancing single-neuron analysis and lays the groundwork for future integration of multimodal single-cell datasets in neuroscience."
1695,679d459debd8ffd557a2b50c,cs.LG,https://arxiv.org/pdf/2501.14570,coverforest: Conformal Predictions with Random Forest in Python,"Panisara Meehinkong, Donlapark Ponnoprat","Machine Learning, Machine Learning, Computation","Conformal prediction provides a framework for uncertainty quantification, specifically in the forms of prediction intervals and sets with distribution-free guaranteed coverage. While recent cross-conformal techniques such as CV+ and Jackknife+-after-bootstrap achieve better data efficiency than traditional split conformal methods, they incur substantial computational costs due to required pairwise comparisons between training and test samples’ out-of-bag scores. Observing that these methods naturally extend from ensemble models, particularly random forests, we leverage existing optimized random forest implementations to enable efficient cross-conformal predictions."
1696,679d459debd8ffd557a2b50d,cs.LG,https://arxiv.org/pdf/2501.14539,A Recurrent Spiking Network with Hierarchical Intrinsic Excitability Modulation for Schema Learning,"Yingchao Yu, Yaochu Jin, Yuchen Xiao, Yuping Yan","Neural and Evolutionary Computing, Machine Learning","Schema, a form of structured knowledge that promotes transfer learning, is attracting growing attention in both neuroscience and artificial intelligence (AI). Current schema research in neural computation is largely constrained to a single behavioral paradigm and relies heavily on recurrent neural networks (RNNs) which lack the neural plausibility and biological interpretability. To address these limitations, this work first constructs a generalized behavioral paradigm framework for schema learning and introduces three novel cognitive tasks, thus supporting a comprehensive schema exploration. Second, we propose a new model using recurrent spiking neural networks with hierarchical intrinsic excitability modulation (HM-RSNNs). The top level of the model selects excitability properties for task-specific demands, while the bottom level fine-tunes these properties for intra-task problems. Finally, extensive visualization analyses of HM-RSNNs are conducted to showcase their computational advantages, track the intrinsic excitability evolution during schema learning, and examine neural coordination differences across tasks. Biologically inspired lesion studies further uncover task-specific distributions of intrinsic excitability within schemas. Experimental results show that HM-RSNNs significantly outperform RSNN baselines across all tasks and exceed RNNs in three novel cognitive tasks. Additionally, HM-RSNNs offer deeper insights into neural dynamics underlying schema learning."
1697,679d459debd8ffd557a2b50e,cs.LG,https://arxiv.org/pdf/2501.14434,Remining Hard Negatives for Generative Pseudo Labeled Domain Adaptation,"Goksenin Yuksel, David Rau, Jaap Kamps","Information Retrieval, Machine Learning","Dense retrievers have demonstrated significant potential for neural information retrieval; however, they exhibit a lack of robustness to domain shifts, thereby limiting their efficacy in zero-shot settings across diverse domains. A state-of-the-art domain adaptation technique is Generative Pseudo Labeling (GPL). GPL uses synthetic query generation and initially mined hard negatives to distill knowledge from cross-encoder to dense retrievers in the target domain. In this paper, we analyze the documents retrieved by the domain-adapted model and discover that these are more relevant to the target queries than those of the non-domain-adapted model. We then propose refreshing the hard-negative index during the knowledge distillation phase to mine better hard negatives. Our remining R-GPL approach boosts ranking performance in 13/14 BEIR datasets and 9/12 LoTTe datasets. Our contributions are (i) analyzing hard negatives returned by domain-adapted and non-domain-adapted models and (ii) applying the GPL training with and without hard-negative re-mining in LoTTE and BEIR datasets."
1698,679d459debd8ffd557a2b50f,cs.LG,https://arxiv.org/pdf/2501.14430,Statistical Verification of Linear Classifiers,"Anton Zhiyanov, Alexander Shklyaev, Alexey Galatenko, Vladimir Galatenko, Alexander Tonevitsky","Machine Learning, Machine Learning, Probability, Statistics Theory, Applications","We propose a homogeneity test closely related to the concept of linear separability between two samples.
Using the test one can answer the question whether a linear classifier is merely ‘‘random’’ or effectively captures differences between two classes.
We focus on establishing upper bounds for the test’sp-value when applied to two-dimensional samples.
Specifically, for normally distributed samples we experimentally demonstrate that the upper bound is highly accurate.
Using this bound, we evaluate classifiers designed to detect ER-positive breast cancer recurrence based on gene pair expression.
Our findings confirm significance of IGFBP6 and ELOVL5 genes in this process."
1699,679d459debd8ffd557a2b510,cs.LG,https://arxiv.org/pdf/2501.14253,Distributionally Robust Coreset Selection under Covariate Shift,"Tomonari Tanaka, Hiroyuki Hanada, Hanting Yang, Tatsuya Aoyama, Yu Inatsu, Satoshi Akahane, Yoshito Okura, Noriaki Hashimoto, Taro Murayama, Hanju Lee, Shinya Kojima, Ichiro Takeuchi","Machine Learning, Machine Learning","Coreset selection, which involves selecting a small subset from an existing training dataset, is an approach to reducing training data, and various approaches have been proposed for this method.
In practical situations where these methods are employed, it is often the case that the data distributions differ between the development phase and the deployment phase, with the latter being unknown.
Thus, it is challenging to select an effective subset of training data that performs well across all deployment scenarios.
We therefore propose Distributionally Robust Coreset Selection (DRCS).
DRCS theoretically derives an estimate of the upper bound for the worst-case test error, assuming that the future covariate distribution may deviate within a defined range from the training distribution.
Furthermore, by selecting instances in a way that suppresses the estimate of the upper bound for the worst-case test error, DRCS achieves distributionally robust training instance selection.
This study is primarily applicable to convex training computation, but we demonstrate that it can also be applied to deep learning under appropriate approximations.
In this paper, we focus on covariate shift, a type of data distribution shift, and demonstrate the effectiveness of DRCS through experiments."
1700,679d459debd8ffd557a2b511,cs.LG,https://arxiv.org/pdf/2501.14246,Adaptive Progressive Attention Graph Neural Network for EEG Emotion Recognition,"Tianzhi Feng, Chennan Wu, Yi Niu, Fu Li, Boxun Fu, Zhifu Zhao, Xiaotian Wang, Guangming Shi","Signal Processing, Machine Learning","In recent years, numerous neuroscientific studies have shown that human emotions are closely linked to specific brain regions, with these regions exhibiting variability across individuals and emotional states. To fully leverage these neural patterns, we propose an Adaptive Progressive Attention Graph Neural Network (APAGNN), which dynamically captures the spatial relationships among brain regions during emotional processing. The APAGNN employs three specialized experts that progressively analyze brain topology. The first expert captures global brain patterns, the second focuses on region-specific features, and the third examines emotion-related channels. This hierarchical approach enables increasingly refined analysis of neural activity. Additionally, a weight generator integrates the outputs of all three experts, balancing their contributions to produce the final predictive label. Extensive experiments on three publicly available datasets (SEED, SEED-IV and MPED) demonstrate that the proposed method enhances EEG emotion recognition performance, achieving superior results compared to baseline methods."
1701,679d459debd8ffd557a2b512,cs.LG,https://arxiv.org/pdf/2501.14155,Learning to Price with Resource Constraints: From Full Information to Machine-Learned Prices,"Ruicheng Ao, Jiashuo Jiang, David Simchi-Levi","Optimization and Control, Machine Learning","We study the dynamic pricing problem with knapsack, addressing the challenge of balancing exploration and exploitation under resource constraints. We introduce three algorithms tailored to different informational settings: a Boundary Attracted Re-solve Method for full information, an online learning algorithm for scenarios with no prior information, and an estimate-then-select re-solve algorithm that leverages machine-learned informed prices with known upper bound of estimation errors. The Boundary Attracted Re-solve Method achieves logarithmic regret without requiring the non-degeneracy condition, while the online learning algorithm attains an optimalO⁢(T)𝑂𝑇O(\sqrt{T})italic_O ( square-root start_ARG italic_T end_ARG )regret. Our estimate-then-select approach bridges the gap between these settings, providing improved regret bounds when reliable offline data is available. Numerical experiments validate the effectiveness and robustness of our algorithms across various scenarios. This work advances the understanding of online resource allocation and dynamic pricing, offering practical solutions adaptable to different informational structures."
1702,679d459debd8ffd557a2b513,cs.LG,https://arxiv.org/pdf/2501.14107,EFiGP: Eigen-Fourier Physics-Informed Gaussian Process for Inference of Dynamic Systems,"Jianhong Chen, Shihao Yang","Machine Learning, Machine Learning","Parameter estimation and trajectory reconstruction for data-driven dynamical systems governed by ordinary differential equations (ODEs) are essential tasks in fields such as biology, engineering, and physics. These inverse problems – estimating ODE parameters from observational data – are particularly challenging when the data are noisy, sparse, and the dynamics are nonlinear. We propose the Eigen-Fourier Physics-Informed Gaussian Process (EFiGP), an algorithm that integrates Fourier transformation and eigen-decomposition into a physics-informed Gaussian Process framework. This approach eliminates the need for numerical integration, significantly enhancing computational efficiency and accuracy. Built on a principled Bayesian framework, EFiGP incorporates the ODE system through probabilistic conditioning, enforcing governing equations in the Fourier domain while truncating high-frequency terms to achieve denoising and computational savings. The use of eigen-decomposition further simplifies Gaussian Process covariance operations, enabling efficient recovery of trajectories and parameters even in dense-grid settings. We validate the practical effectiveness of EFiGP on three benchmark examples, demonstrating its potential for reliable and interpretable modeling of complex dynamical systems while addressing key challenges in trajectory recovery and computational cost."
1703,679d459debd8ffd557a2b514,cs.LG,https://arxiv.org/pdf/2501.14095,Improved subsample-and-aggregate via the private modified winsorized mean,"Kelly Ramsay, Dylan Spicker","Methodology, Machine Learning","We develop a univariate, differentially private mean estimator, called the privatemodified winsorized mean, designed to be used as the aggregator in subsample-and-aggregate.
We demonstrate, via real data analysis, that common differentially private multivariate mean estimators may not perform well as the aggregator, even with a dataset with 8000 observations, motivating our developments.
We show that the modified winsorized mean is minimax optimal for several, large classes of distributions, even under adversarial contamination.
We also demonstrate that, empirically, the modified winsorized mean performs well compared to other private mean estimates.
We consider the modified winsorized mean as the aggregator in subsample-and-aggregate, deriving a finite sample deviations bound for a subsample-and-aggregate estimate generated with the new aggregator.
This result yields two important insights: (i) the optimal choice of subsamples depends on the bias of the estimator computed on the subsamples, and (ii) the rate of convergence of the subsample-and-aggregate estimator depends on the robustness of the estimator computed on the subsamples."
1704,679d459debd8ffd557a2b515,cs.LG,https://arxiv.org/pdf/2501.13935,Low rank matrix completion and realization of graphs: results and problems,"S. Dzhenzher, T. Garaev, O. Nikitenko, A. Petukhov, A. Skopenkov, A. Voropaev","History and Overview, Discrete Mathematics, Machine Learning, Combinatorics, Geometric Topology","The Netflix problem (from machine learning) asks the following.
Given a ratings matrix in which each entry(i,j)𝑖𝑗(i,j)( italic_i , italic_j )represents the rating of moviej𝑗jitalic_jby customeri𝑖iitalic_i, if customeri𝑖iitalic_ihas watched moviej𝑗jitalic_j, and is otherwise missing, we would like to predict the remaining entries in order to make good recommendations to customers on what to watch next.
The remaining entries are predicted so as to minimize therankof the completed matrix."
1705,679d459debd8ffd557a2b516,cs.NE,https://arxiv.org/pdf/2501.18479,Transformer Semantic Genetic Programming for Symbolic Regression,"Philipp Anthes, Dominik Sobania, Franz Rothlauf",Neural and Evolutionary Computing,"In standard genetic programming (stdGP), solutions are varied by modifying their syntax, with uncertain effects on their semantics. Geometric-semantic genetic programming (GSGP), a popular variant of GP, effectively searches the semantic solution space using variation operations based on linear combinations, although it results in significantly larger solutions.
This paper presents Transformer Semantic Genetic Programming (TSGP), a novel and flexible semantic approach that uses a generative transformer model as search operator.
The transformer is trained on synthetic test problems and learns semantic similarities between solutions. Once the model is trained, it can be used to create offspring solutions with high semantic similarity also for unseen and unknown problems. Experiments on several symbolic regression problems show that TSGP generates solutions with comparable or even significantly better prediction quality than stdGP, SLIM_GSGP, DSR, and DAE-GP. Like SLIM_GSGP, TSGP is able to create new solutions that are semantically similar without creating solutions of large size. An analysis of the search dynamic reveals that the solutions generated by TSGP are semantically more similar than the solutions generated by the benchmark approaches allowing a better exploration of the semantic solution space."
1706,679d459debd8ffd557a2b517,cs.NE,https://arxiv.org/pdf/2501.17172,Towards spiking analog hardware implementation of a trajectory interpolation mechanism for smooth closed-loop control of a spiking robot arm,"Daniel Casanueva-Morato, Chenxi Wu, Giacomo Indiveri, Juan P. Dominguez-Morales, Alejandro Linares-Barranco","Neural and Evolutionary Computing, Robotics","Neuromorphic engineering aims to incorporate the computational principles found in animal brains, into modern technological systems.
Following this approach, in this work we propose a closed-loop neuromorphic control system for an event-based robotic arm.
The proposed system consists of a shifted Winner-Take-All spiking network for interpolating a reference trajectory and a spiking comparator network responsible for controlling the flow continuity of the trajectory, which is fed back to the actual position of the robot.
The comparator model is based on a differential position comparison neural network, which governs the execution of the next trajectory points to close the control loop between both components of the system.
To evaluate the system, we implemented and deployed the model on a mixed-signal analog-digital neuromorphic platform, the DYNAP-SE2, to facilitate integration and communication with the ED-Scorbot robotic arm platform.
Experimental results on one joint of the robot validate the use of this architecture and pave the way for future neuro-inspired control of the entire robot."
1707,679d459debd8ffd557a2b518,cs.NE,https://arxiv.org/pdf/2501.17166,Optimizing Carbon Footprint in ICT through Swarm Intelligence with Algorithmic Complexity,"Vasileios Alevizos, Nikitas Gerolimos, Sabrina Edralin, Clark Xu, Akebu Simasiku, Georgios Priniotakis, George Papakostas, Zongliang Yue","Neural and Evolutionary Computing, Computational Physics","Global emissions from fossil fuel combustion and cement production were recorded in 2022, signaling a resurgence to pre-pandemic levels and providing an apodictic indication that emission peaks have not yet been achieved. Significant contributions to this upward trend are made by the Information and Communication Technology (ICT) industry due to its substantial energy consumption. This shows the need for further exploration of swarm intelligence applications to measure and optimize the carbon footprint within ICT. All causative factors are evaluated based on the quality of data collection; variations from each source are quantified; and an objective function related to carbon footprint in ICT energy management is optimized. Emphasis is placed on the asyndetic integration of data sources to construct a convex optimization problem. An apodictic necessity to prevent the erosion of accuracy in carbon footprint assessments is addressed. Complexity percentages ranged from 5.25% for the Bat Algorithm to 7.87% for Fast Bacterial Swarming, indicating significant fluctuations in resource intensity among algorithms. These findings suggest that we were able to quantify the environmental impact of various swarm algorithms."
1708,679d459debd8ffd557a2b519,cs.NE,https://arxiv.org/pdf/2501.16745,Toward Relative Positional Encoding in Spiking Transformers,"Changze Lv, Yansen Wang, Dongqi Han, Yifei Shen, Xiaoqing Zheng, Xuanjing Huang, Dongsheng Li",Neural and Evolutionary Computing,"Spiking neural networks (SNNs) are bio-inspired networks that model how neurons in the brain communicate through discrete spikes, which have great potential in various tasks due to their energy efficiency and temporal processing capabilities.
SNNs with self-attention mechanisms (Spiking Transformers) have recently shown great advancements in various tasks such as sequential modeling and image classifications.
However, integrating positional information, which is essential for capturing sequential relationships in data, remains a challenge in Spiking Transformers.
In this paper, we introduce an approximate method for relative positional encoding (RPE) in Spiking Transformers, leveraging Gray Code as the foundation for our approach.
We provide comprehensive proof of the method’s effectiveness in partially capturing relative positional information for sequential tasks.
Additionally, we extend our RPE approach by adapting it to a two-dimensional form suitable for image patch processing.
We evaluate the proposed RPE methods on several tasks, including time series forecasting, text classification, and patch-based image classification.
Our experimental results demonstrate that the incorporation of RPE significantly enhances performance by effectively capturing relative positional information."
1709,679d459debd8ffd557a2b51a,cs.NE,https://arxiv.org/pdf/2501.16735,Stochastic Population Update Provably Needs An Archive in Evolutionary Multi-objective Optimization,"Shengjie Ren, Zimin Liang, Miqing Li, Chao Qian",Neural and Evolutionary Computing,"Evolutionary algorithms (EAs) have been widely applied to multi-objective optimization, due to their nature of population-based search. Population update, a key component in multi-objective EAs (MOEAs), is usually performed in a greedy, deterministic manner. However, recent studies have questioned this practice and shown that stochastic population update (SPU), which allows inferior solutions have a chance to be preserved, can help MOEAs jump out of local optima more easily. While introducing randomness in the population update process boosts the exploration of MOEAs, there is a drawback that the population may not always preserve the very best solutions found, thus entailing a large population. Intuitively, a possible solution to this issue is to introduce an archive that stores the best solutions ever found. In this paper, we theoretically show that using an archive can allow a small population and accelerate the search of SPU-based MOEAs substantially. Specifically, we analyze the expected running time of two well-established MOEAs, SMS-EMOA and NSGA-II, with SPU for solving a commonly studied bi-objective problem OneJumpZeroJump, and prove that using an archive can bring (even exponential) speedups. The comparison between SMS-EMOA and NSGA-II also suggests that the(μ+μ)𝜇𝜇(\mu+\mu)( italic_μ + italic_μ )update mode may be more suitable for SPU than the(μ+1)𝜇1(\mu+1)( italic_μ + 1 )update mode. Furthermore, our derived running time bounds for using SPU alone are significantly tighter than previously known ones. Our theoretical findings are also empirically validated on a well-known practical problem, the multi-objective traveling salesperson problem. We hope this work may provide theoretical support to explore different ideas of designing algorithms in evolutionary multi-objective optimization."
1710,679d459debd8ffd557a2b51b,cs.NE,https://arxiv.org/pdf/2501.16250,Runtime Analysis of the Compact Genetic Algorithm on the LeadingOnes Benchmark,"Marcel Chwiałkowski, Benjamin Doerr, Martin S. Krejca",Neural and Evolutionary Computing,"The compact genetic algorithm (cGA) is one of the simplest estimation-of-distribution algorithms (EDAs).
Next to the univariate marginal distribution algorithm (UMDA)—another simple EDA—, the cGA has been subject to extensive mathematical runtime analyses, often showcasing a similar or even superior performance to competing approaches.
Surprisingly though, up to date and in contrast to the UMDA and many other heuristics, we lack a rigorous runtime analysis of the cGA on theLeadingOnesbenchmark—one of the most studied theory benchmarks in the domain of evolutionary computation."
1711,679d459debd8ffd557a2b51c,cs.NE,https://arxiv.org/pdf/2501.16159,Comprehensive Benchmarking Environment for Worker Flexibility in Flexible Job Shop Scheduling Problems,"David Hutter, Thomas Steinberger, Michael Hellwig",Neural and Evolutionary Computing,"In Production Scheduling, the Flexible Job Shop Scheduling Problem (FJSSP) aims to optimize a sequence of operations and assign each to an eligible machine with varying processing times. To be even closer to real production environments, the integration of the workforce may be considered. Hence, each machine also requires a worker to be present to process an operation which additionally affects the processing times. The resulting problem is called Flexible Job Shop Scheduling Problem with Worker Flexibility (FJSSP-W)."
1712,679d459debd8ffd557a2b51d,cs.NE,https://arxiv.org/pdf/2501.15319,PSO and the Traveling Salesman Problem: An Intelligent Optimization Approach,"Kael Silva Araújo, Francisco Márcio Barboza","Neural and Evolutionary Computing, Optimization and Control","The Traveling Salesman Problem (TSP) is a well-known combinatorial optimization problem that aims to find the shortest possible route that visits each city exactly once and returns to the starting point. This paper explores the application of Particle Swarm Optimization (PSO), a population-based optimization algorithm, to solve TSP. Although PSO was originally designed for continuous optimization problems, this work adapts PSO for the discrete nature of TSP by treating the order of cities as a permutation. A local search strategy, including 2-opt and 3-opt techniques, is applied to improve the solution after updating the particle positions. The performance of the proposed PSO algorithm is evaluated using benchmark TSP instances and compared to other popular optimization algorithms, such as Genetic Algorithms (GA) and Simulated Annealing (SA). Results show that PSO performs well for small to medium-sized problems, though its performance diminishes for larger instances due to difficulties in escaping local optima. This paper concludes that PSO is a promising approach for solving TSP, with potential for further improvement through hybridization with other optimization techniques."
1713,679d459debd8ffd557a2b51e,cs.NE,https://arxiv.org/pdf/2501.15129,EvoRL: A GPU-accelerated Framework for Evolutionary Reinforcement Learning,"Bowen Zheng, Ran Cheng, Kay Chen Tan",Neural and Evolutionary Computing,"Evolutionary Reinforcement Learning (EvoRL) has emerged as a promising approach to overcoming the limitations of traditional reinforcement learning (RL) by integrating the Evolutionary Computation (EC) paradigm with RL. However, the population-based nature of EC significantly increases computational costs, thereby restricting the exploration of algorithmic design choices and scalability in large-scale settings.
To address this challenge, we introduceEvoRL111To distinguish the framework from the general term EvoRL, we useEvoRLto refer specifically to the proposed framework., the first end-to-end EvoRL framework optimized for GPU acceleration. The framework executes the entire training pipeline on accelerators, including environment simulations and EC processes, leveraging hierarchical parallelism through vectorization and compilation techniques to achieve superior speed and scalability. This design enables the efficient training of large populations on a single machine.
In addition to its performance-oriented design,EvoRLoffers a comprehensive platform for EvoRL research, encompassing implementations of traditional RL algorithms (e.g., A2C, PPO, DDPG, TD3, SAC), Evolutionary Algorithms (e.g., CMA-ES, OpenES, ARS), and hybrid EvoRL paradigms such as Evolutionary-guided RL (e.g., ERL, CEM-RL) and Population-Based AutoRL (e.g., PBT).
The framework’s modular architecture and user-friendly interface allow researchers to seamlessly integrate new components, customize algorithms, and conduct fair benchmarking and ablation studies.
The project is open-source and available at:https://github.com/EMI-Group/evorl."
1714,679d459debd8ffd557a2b51f,cs.NE,https://arxiv.org/pdf/2501.14759,LPBSA: Enhancing Optimization Efficiency through Learner Performance-based Behavior and Simulated Annealing,"Dana R. Hamad, Tarik A. Rashid","Neural and Evolutionary Computing, Optimization and Control",
1715,679d459debd8ffd557a2b520,cs.NE,https://arxiv.org/pdf/2501.14751,Optimizing LPB Algorithms using Simulated Annealing,"Dana Rasul Hamad, Tarik A. Rashid",Neural and Evolutionary Computing,
1716,679d459debd8ffd557a2b521,cs.NE,https://arxiv.org/pdf/2501.14742,"Evaluating the effectiveness, reliability and efficiency of a multi-objective sequential optimization approach for building performance design","Riccardo Talami, Jonathan Wright, Bianca Howard","Neural and Evolutionary Computing, Optimization and Control",
1717,679d459debd8ffd557a2b522,cs.NE,https://arxiv.org/pdf/2501.14503,Benchmarking global optimization techniques for unmanned aerial vehicle path planning,"Mhd Ali Shehadeh, Jakub Kudela","Neural and Evolutionary Computing, Robotics, Optimization and Control","The Unmanned Aerial Vehicle (UAV) path planning problem is a complex optimization problem in the field of robotics. In this paper, we investigate the possible utilization of this problem in benchmarking global optimization methods. We devise a problem instance generator and pick 56 representative instances, which we compare to established benchmarking suits through Exploratory Landscape Analysis to show their uniqueness. For the computational comparison, we select twelve well-performing global optimization techniques from both subfields of stochastic algorithms (evolutionary computation methods) and deterministic algorithms (Dividing RECTangles, or DIRECT-type methods). The experiments were conducted in settings with varying dimensionality and computational budgets. The results were analyzed through several criteria (number of best-found solutions, mean relative error, Friedman ranks) and utilized established statistical tests. The best-ranking methods for the UAV problems were almost universally the top-performing evolutionary techniques from recent competitions on numerical optimization at the Institute of Electrical and Electronics Engineers Congress on Evolutionary Computation. Lastly, we discussed the variable dimension characteristics of the studied UAV problems that remain still largely under-investigated."
1718,679d459debd8ffd557a2b523,cs.NE,https://arxiv.org/pdf/2501.14490,Channel-wise Parallelizable Spiking Neuron with Multiplication-free Dynamics and Large Temporal Receptive Fields,"Peng Xue, Wei Fang, Zhengyu Ma, Zihan Huang, Zhaokun Zhou, Yonghong Tian, Timothée Masquelier, Huihui Zhou",Neural and Evolutionary Computing,"Spiking Neural Networks (SNNs) are distinguished from Artificial Neural Networks (ANNs) for their sophisticated neuronal dynamics and sparse binary activations (spikes) inspired by the biological neural system. Traditional neuron models use iterative step-by-step dynamics, resulting in serial computation and slow training speed of SNNs. Recently, parallelizable spiking neuron models have been proposed to fully utilize the massive parallel computing ability of graphics processing units to accelerate the training of SNNs. However, existing parallelizable spiking neuron models involve dense floating operations and can only achieve high long-term dependencies learning ability with a large order at the cost of huge computational and memory costs. To solve the dilemma of performance and costs, we propose the mul-free channel-wise Parallel Spiking Neuron, which is hardware-friendly and suitable for SNNs’ resource-restricted application scenarios. The proposed neuron imports the channel-wise convolution to enhance the learning ability, induces the sawtooth dilations to reduce the neuron order, and employs the bit shift operation to avoid multiplications. The algorithm for design and implementation of acceleration methods is discussed meticulously.
Our methods are validated in neuromorphic Spiking Heidelberg Digits voices, sequential CIFAR images, and neuromorphic DVS-Lip vision datasets, achieving the best accuracy among SNNs.
Training speed results demonstrate the effectiveness of our acceleration methods, providing a practical reference for future research."
1719,679d459debd8ffd557a2b524,cs.NE,https://arxiv.org/pdf/2501.14484,$SpikePack$: Enhanced Information Flow in Spiking Neural Networks with High Hardware Compatibility,"Guobin Shen, Jindong Li, Tenglong Li, Dongcheng Zhao, Yi Zeng",Neural and Evolutionary Computing,"Spiking Neural Networks (SNNs) hold promise for energy-efficient,
biologically inspired computing. We identify substantial information loss during
spike transmission, linked to temporal dependencies in traditional Leaky
Integrate-and-Fire (LIF) neurons—a key factor potentially limiting SNN
performance. Existing SNN architectures also underutilize modern GPUs,
constrained by single-bit spike storage and isolated weight-spike operations
that restrict computational efficiency. We introduceSpikePack, a neuron
model designed to reduce transmission loss while preserving essential features
like membrane potential reset and leaky integration.SpikePackachieves
constant𝒪⁢(1)𝒪1\mathcal{O}(1)caligraphic_O ( 1 )time and space complexity, enabling efficient parallel
processing on GPUs and also supporting serial inference on existing SNN
hardware accelerators. Compatible with standard Artificial Neural Network (ANN)
architectures,SpikePackfacilitates near-lossless ANN-to-SNN conversion
across various networks. Experimental results on tasks such as image
classification, detection, and segmentation showSpikePackachieves
significant gains in accuracy and efficiency for both directly trained and converted
SNNs over state-of-the-art models. Tests on FPGA-based platforms further
confirm cross-platform flexibility, delivering high performance and enhanced
sparsity. By enhancing information flow and rethinking SNN-ANN integration,SpikePackadvances efficient SNN deployment across diverse hardware platforms."
1720,679d459debd8ffd557a2b525,cs.NE,https://arxiv.org/pdf/2501.14285,Cascaded Large-Scale TSP Solving with Unified Neural Guidance: Bridging Local and Population-based Search,"Shengcai Liu, Haoze Lv, Zhiyuan Wang, Ke Tang",Neural and Evolutionary Computing,"The traveling salesman problem (TSP) is a fundamental NP-hard optimization problem.
This work presents UNiCS, a novel unified neural-guided cascaded solver for solving large-scale TSP instances.
UNiCS comprises a local search (LS) phase and a population-based search (PBS) phase, both guided by a learning component called unified neural guidance (UNG).
Specifically, UNG guides solution generation across both phases and determines appropriate phase transition timing to effectively combine the complementary strengths of LS and PBS.
While trained only on simple distributions with relatively small-scale TSP instances, UNiCS generalizes effectively to challenging TSP benchmarks containing much larger instances (10,000-71,009 nodes) with diverse node distributions entirely unseen during training.
Experimental results on the large-scale TSP instances demonstrate that UNiCS consistently outperforms state-of-the-art methods, with its advantage remaining consistent across various runtime budgets."
1721,679d459debd8ffd557a2b526,cs.NE,https://arxiv.org/pdf/2501.14284,Feature-based Evolutionary Diversity Optimization of Discriminating Instances for Chance-constrained Optimization Problems,"Saba Sadeghi Ahouei, Denis Antipov, Aneta Neumann, Frank Neumann","Neural and Evolutionary Computing, Optimization and Control","Algorithm selection is crucial in the field of optimization, as no single algorithm performs perfectly across all types of optimization problems. Finding the best algorithm among a given set of algorithms for a given problem requires a detailed analysis of the problem’s features. To do so, it is important to have a diverse set of benchmarking instances highlighting the difference in algorithms’ performance.
In this paper, we evolve diverse benchmarking instances for chance-constrained optimization problems that contain stochastic components characterized by their expected values and variances. These instances clearly differentiate the performance of two given algorithms, meaning they are easy to solve by one algorithm and hard to solve by the other.
We introduce a(μ+1)⁢E⁢A𝜇1𝐸𝐴(\mu+1)~{}EA( italic_μ + 1 ) italic_E italic_Afor feature-based diversity optimization to evolve such differentiating instances. We study the chance-constrained maximum coverage problem with stochastic weights on the vertices as an example of chance-constrained optimization problems.
The experimental results demonstrate that our method successfully generates diverse instances based on different features while effectively distinguishing the performance between a pair of algorithms."
1722,679d459debd8ffd557a2b527,cs.HC,https://arxiv.org/pdf/2501.18506,Design and Validation of Learning Aware HMI For Learning-Enabled Increasingly Autonomous Systems,"Parth Ganeriwala, Michael Matessa, Siddhartha Bhattacharyya, Randolph M. Jones, Jennifer Davis, Parneet Kaur, Simone Fulvio Rollini, Natasha Neogi","Human-Computer Interaction, Emerging Technologies, Software Engineering","With the rapid advancements in Artificial Intelligence (AI), autonomous agents are increasingly expected to manage complex situations where learning-enabled algorithms are vital. However, the integration of these advanced algorithms poses significant challenges, especially concerning safety and reliability. This research emphasizes the importance of incorporating human-machine collaboration into the systems engineering process to design learning-enabled increasingly autonomous systems (LEIAS). Our proposed LEIAS architecture emphasizes communication representation and pilot preference learning to boost operational safety. Leveraging the Soar cognitive architecture, the system merges symbolic decision logic with numeric decision preferences enhanced through reinforcement learning. A core aspect of this approach is transparency; the LEIAS provides pilots with a comprehensive, interpretable view of the system’s state, encompassing detailed evaluations of sensor reliability, including GPS, IMU, and LIDAR data. This multi-sensor assessment is critical for diagnosing discrepancies and maintaining trust. Additionally, the system learns and adapts to pilot preferences, enabling responsive, context-driven decision-making. Autonomy is incrementally escalated based on necessity, ensuring pilots retain control in standard scenarios and receive assistance only when required. Simulation studies conducted in Microsoft’s XPlane simulation environment to validate this architecture’s efficacy, showcasing its performance in managing sensor anomalies and enhancing human-machine collaboration, ultimately advancing safety in complex operational environments."
1723,679d459debd8ffd557a2b528,cs.HC,https://arxiv.org/pdf/2501.18493,Examining the Expanding Role of Synthetic Data Throughout the AI Development Pipeline,"Shivani Kapania, Stephanie Ballard, Alex Kessler, Jennifer Wortman Vaughan",Human-Computer Interaction,"Alongside the growth of generative AI, we are witnessing a surge in the use of synthetic data across all stages of the AI development pipeline. It is now common practice for researchers and practitioners to use one large generative model (which we refer to as anauxiliary model) to generate synthetic data that is used to train or evaluate another, reconfiguring AI workflows and reshaping the very nature of data.
While scholars have raised concerns over the risks of synthetic data, policy guidance and best practices for its responsible use have not kept up with these rapidly evolving industry trends, in part because we lack a clear picture of current practices and challenges. Our work aims to address this gap. Through 29 interviews with AI practitioners and responsible AI experts, we examine the expanding role of synthetic data in AI development.
Our findings reveal how auxiliary models are now widely used across the AI development pipeline.
Practitioners describe synthetic data as crucial for addressing data scarcity and providing a competitive edge, noting that evaluation of generative AI systems at scale would be infeasible without auxiliary models.
However, they face challenges controlling the outputs of auxiliary models, generating data that accurately depict underrepresented groups, and scaling data validation practices that are based primarily on manual inspection. We detail general limitations of and ethical considerations for synthetic data and conclude with a proposal of concrete steps towards the development of best practices for its responsible use."
1724,679d459debd8ffd557a2b529,cs.HC,https://arxiv.org/pdf/2501.18462,Massive Online Course on Entrepreneurship. Case Study,"Manuela Petrescu, Tudor Dan Mihoc","Human-Computer Interaction, Social and Information Networks","Entrepreneurship is a key component of society, and universities and major political structures have tried to support its development in recent years. The present study aims to check the perception of students (based on gender) about entrepreneurial intentions after participating in a course that had a large number of undergraduate students. There were 970 students enrolled from different faculties with various specializations. We conducted a gender-based survey on the unconventional entrepreneurial fundamentals course, where each course was delivered by a different speaker. We also compared the responses provided by computer science students with the overall responses to find differences in their perceptions related to the feasibility of teaching entrepreneurship online, determining the entrepreneurial intention of the students taking this course, and analyzing the perceptions related to the business environment and the ease of starting a business. We found that students, regardless of gender or field of study, prefer interactive online presentations based on the manner in which lectures on this subject were conducted."
1725,679d459debd8ffd557a2b52a,cs.HC,https://arxiv.org/pdf/2501.18210,Hashtag Re-Appropriation for Audience Control on Recommendation-Driven Social Media Xiaohongshu (rednote),"Ruyuan Wan, Lingbo Tong, Tiffany Knearem, Toby Jia-Jun Li, Ting-Hao 'Kenneth' Huang, Qunfang Wu","Human-Computer Interaction, Computers and Society, Information Retrieval, Social and Information Networks","Algorithms have played a central role in personalized recommendations on social media. However, they also present significant obstacles for content creators trying to predict and manage their audience reach. This issue is particularly challenging for marginalized groups seeking to maintain safe spaces. Our study explores how women on Xiaohongshu (rednote), a recommendation-driven social platform, proactively re-appropriate hashtags (e.g., #{CJK}UTF8gbsn宝宝辅食, Baby Supplemental Food) by using them in posts unrelated to their literal meaning. The hashtags were strategically chosen from topics that would be uninteresting to the male audience they wanted to block.
Through a mixed-methods approach, we analyzed the practice of hashtag re-appropriation based on 5,800 collected posts and interviewed 24 active users from diverse backgrounds to uncover users’ motivations and reactions towards the re-appropriation. This practice highlights how users can reclaim agency over content distribution on recommendation-driven platforms, offering insights into self-governance within algorithmic-centered power structures."
1726,679d459debd8ffd557a2b52b,cs.HC,https://arxiv.org/pdf/2501.18002,Agentic Workflows for Conversational Human-AI Interaction Design,"Arthur Caetano, Kavya Verma, Atieh Taheri, Radha Kumaran, Zichen Chen, Jiaao Chen, Tobias Höllerer, Misha Sra",Human-Computer Interaction,"Conversational human-AI interaction (CHAI) have recently driven mainstream adoption of AI. However, CHAI poses two key challenges for designers and researchers: users frequently have ambiguous goals and an incomplete understanding of AI functionalities, and the interactions are brief and transient, limiting opportunities for sustained engagement with users. AI agents can help address these challenges by suggesting contextually relevant prompts, by standing in for users during early design testing, and by helping users better articulate their goals. Guided by research-through-design, we explored agentic AI workflows through the development and testing of a probe over four iterations with 10 users. We present our findings through an annotated portfolio of design artifacts, and through thematic analysis of user experiences, offering solutions to the problems of ambiguity and transient in CHAI. Furthermore, we examine the limitations and possibilities of these AI agent workflows, suggesting that similar collaborative approaches between humans and AI could benefit other areas of design."
1727,679d459debd8ffd557a2b52c,cs.HC,https://arxiv.org/pdf/2501.18148,The Dilemma of Building Do-It-Yourself (DIY) Solutions for Workplace Accessibility,"Yoonha Cha, Victoria Jackson, Karina Kohl, Rafael Prikladnicki, André van der Hoek, Stacy M. Branham","Software Engineering, Human-Computer Interaction","Existing commercial and in-house software development tools are often inaccessible to Blind and Low Vision Software Professionals (BLVSPs), hindering their participation and career growth at work. Building on existing research on Do-It-Yourself (DIY) Assistive Technologies and customized tools made by programmers, we shed light on the currently unexplored intersection of how DIY tools built and used by BLVSPs support accessible software development. Through semi-structured interviews with 30 BLVSPs, we found that such tools serve many different purposes and are driven by motivations such as desiring to maintain a professional image and a sense of dignity at work. These tools had significant impacts on workplace accessibility and revealed a need for a more centralized community for sharing tools, tips, and tricks. Based on our findings, we introduce the “Double Hacker Dilemma” and highlight a need for developing more effective peer and organizational platforms that support DIY tool sharing."
1728,679d459debd8ffd557a2b52d,cs.HC,https://arxiv.org/pdf/2501.17942,"""I Would Never Trust Anything Western"": Kumu (Educator) Perspectives on Use of LLMs for Culturally Revitalizing CS Education in Hawaiian Schools","Manas Mhasakar, Rachel Baker-Ramos, Ben Carter, Evyn-Bree Helekahi-Kaiwi, Josiah Hester","Computers and Society, Human-Computer Interaction","As large language models (LLMs) become increasingly integrated into educational technology, their potential to assist in developing curricula has gained interest among educators.
Despite this growing attention, their applicability in culturally responsive Indigenous educational settings like Hawai‘i’s public schools and Kaiapuni (immersion language) programs, remains understudied.
Additionally,‘Ōlelo Hawai‘i, the Hawaiian language, as a low-resource language, poses unique challenges and concerns about cultural sensitivity and the reliability of generated content.
Through surveys and interviews withkumu(educators), this study explores the perceived benefits and limitations of using LLMs for culturally revitalizing computer science (CS) education in Hawaiian public schools with Kaiapuni programs.
Our findings highlight AI’s time-saving advantages while exposing challenges such as cultural misalignment and reliability concerns.
We conclude with design recommendations for future AI tools to better align with Hawaiian cultural values and pedagogical practices, towards the broader goal of trustworthy, effective, and culturally grounded AI technologies."
1729,679d459debd8ffd557a2b52e,cs.HC,https://arxiv.org/pdf/2501.17819,eaSEL: Promoting Social-Emotional Learning and Parent-Child Interaction through AI-Mediated Content Consumption,"Jocelyn Shen, Jennifer King Chen, Leah Findlater, Griffin Dietz Smith",Human-Computer Interaction,"As children increasingly consume media on devices, parents look for ways this usage can support learning and growth, especially in domains like social-emotional learning. We introduce eaSEL, a system that (a) integrates social-emotional learning (SEL) curricula into children’s video consumption by generating reflection activities and (b) facilitates parent-child discussions around digital media without requiring co-consumption of videos. We present a technical evaluation of our system’s ability to detect social-emotional moments within a transcript and to generate high-quality SEL-based activities for both children and parents. Through a user study withN=20𝑁20N=20italic_N = 20parent-child dyads, we find that after completing an eaSEL activity, children reflect more on the emotional content of videos. Furthermore, parents find that the tool promotes meaningful active engagement and could scaffold deeper conversations around content. Our work paves directions in how AI can support children’s social-emotional reflection of media and family connections in the digital age."
1730,679d459debd8ffd557a2b52f,cs.HC,https://arxiv.org/pdf/2501.17799,Leveraging Multimodal LLM for Inspirational User Interface Search,"Seokhyeon Park, Yumin Song, Soohyun Lee, Jaeyoung Kim, Jinwook Seo","Human-Computer Interaction, Information Retrieval","Inspirational search, the process of exploring designs to inform and inspire new creative work, is pivotal in mobile user interface (UI) design.
However, exploring the vast space of UI references remains a challenge.
Existing AI-based UI search methods often miss crucial semantics like target users or the mood of apps.
Additionally, these models typically require metadata like view hierarchies, limiting their practical use.
We used a multimodal large language model (MLLM) to extract and interpret semantics from mobile UI images.
We identified key UI semantics through a formative study and developed a semantic-based UI search system.
Through computational and human evaluations, we demonstrate that our approach significantly outperforms existing UI retrieval methods, offering UI designers a more enriched and contextually relevant search experience.
We enhance the understanding of mobile UI design semantics and highlight MLLMs’ potential in inspirational search, providing a rich dataset of UI semantics for future studies."
1731,679d459debd8ffd557a2b530,cs.HC,https://arxiv.org/pdf/2501.17768,TeamPortal: Exploring Virtual Reality Collaboration Through Shared and Manipulating Parallel Views,"Xian Wang, Luyao Shen, Lei Chen, Mingming Fan, Lik-Hang Lee",Human-Computer Interaction,"Virtual Reality (VR) offers a unique collaborative experience, with parallel views playing a pivotal role in Collaborative Virtual Environments by supporting the transfer and delivery of items. Sharing and manipulating partners’ views provides users with a broader perspective that helps them identify the targets and partner actions. We proposed TeamPortal accordingly and conducted two user studies with 72 participants (36 pairs) to investigate the potential benefits of interactive, shared perspectives in VR collaboration. Our first study compared ShaView and TeamPortal against a baseline in a collaborative task that encompassed a series of searching and manipulation tasks. The results show that TeamPortal significantly reduced movement and increased collaborative efficiency and social presence in complex tasks. Following the results, the second study evaluated three variants: TeamPortal+, SnapTeamPortal+, and DropTeamPortal+. The results show that both SnapTeamPortal+ and DropTeamPortal+ improved task efficiency and willingness to further adopt these technologies, though SnapTeamPortal+ reduced co-presence. Based on the findings, we proposed three design implications to inform the development of future VR collaboration systems."
1732,679d459debd8ffd557a2b531,cs.HC,https://arxiv.org/pdf/2501.17585,Tapor: 3D Hand Pose Reconstruction with Fully Passive Thermal Sensing for Around-device Interactions,"Xie Zhang, Chenxiao Li, Chenshu Wu",Human-Computer Interaction,"This paper presents the design and implementation ofTapor, a privacy-preserving, non-contact, and fully passive sensing system for accurate and robust 3D hand pose reconstruction for around-device interaction using a single low-cost thermal array sensor.
Thermal sensing using inexpensive and miniature thermal arrays emerges with an excellent utility-privacy balance, offering an imaging resolution significantly lower than cameras but far superior to RF signals like radar or WiFi.
The design ofTapor, however, is challenging, mainly because the captured temperature maps are low-resolution and textureless.
To overcome the challenges, we investigate the thermo-depth and thermo-pose properties and present a novel physics-inspired neural network design that learns effective 3D spatial representations of potential hand poses.
We then formulate the 3D pose reconstruction problem as a distinct retrieval task, enabling precise determination of the hand pose corresponding to the input temperature map.
To deployTaporon IoT devices, we introduce an effective heterogeneous knowledge distillation method that reduces the computation by 377×\times×.
We fully implementTaporand conduct comprehensive experiments in various real-world scenarios.
The results demonstrate the remarkable performance ofTapor, which is further illustrated by four case studies of gesture control and finger tracking.
We envisionTaporto be a ubiquitous interface for around-device control and have released the dataset, software, firmware, and demo videos athttps://github.com/IOT-Tapor/TAPOR."
1733,679d459debd8ffd557a2b532,cs.HC,https://arxiv.org/pdf/2501.17475,EMD-Fuzzy: An Empirical Mode Decomposition Based Fuzzy Model for Cross-Stimulus Transfer Learning of SSVEP,"Beining Cao, Xiaowei Jiang, Daniel Leong, Charlie Li-Ting Tsai, Yu-Cheng Chang, Thomas Do, Chin-Teng",Human-Computer Interaction,"The Brain-Computer Interface (BCI) enables direct brain-to-device communication, with the Steady-State Visual Evoked Potential (SSVEP) paradigm favored for its stability and high accuracy across various fields. In SSVEP BCI systems, supervised learning models significantly enhance performance over unsupervised models, achieving higher accuracy in less time. However, prolonged data collection can cause user fatigue and even trigger photosensitive epilepsy, creating a negative user experience. Thus, reducing calibration time is crucial. To address this, Cross-Stimulus transfer learning (CSTL) can shorten calibration by utilizing only partial frequencies. Traditional CSTL methods, affected by time-domain impulse response variations, are suitable only for adjacent frequency transfers, limiting their general applicability. We introduce an Empirical Mode Decomposition (EMD) Based Fuzzy Model (EMD-Fuzzy), which employs EMD to extract crucial frequency information and achieves stimulus transfer in the frequency domain through Fast Fourier Transform (FFT) to mitigate time-domain differences. Combined with a Fuzzy Decoder that uses fuzzy logic for representation learning, our approach delivers promising preliminary results in offline tests and state-of-the-art performance. With only 4 frequencies, our method achieved an accuracy of82.75%±16.30%plus-or-minuspercent82.75percent16.3082.75\%\pm 16.30\%82.75 % ± 16.30 %and an information transfer rate (ITR) of186.56±52.09plus-or-minus186.5652.09186.56\pm 52.09186.56 ± 52.09bits/min on the 40-target Benchmark dataset.
In online tests, our method demonstrates robust efficacy, achieving an averaged accuracy of86.30%±6.18%plus-or-minuspercent86.30percent6.1886.30\%\pm 6.18\%86.30 % ± 6.18 %across 7 subjects. This performance underscores the effectiveness of integrating EMD and fuzzy logic into EEG decoding for CSTL and highlights our method’s potential in real-time applications where consistent and reliable decoding is crucial."
1734,679d459debd8ffd557a2b533,cs.HC,https://arxiv.org/pdf/2501.17430,Throwaway Accounts and Moderation on Reddit,"Cheng Guo, Kelly Caine","Human-Computer Interaction, Computers and Society","Social media platforms (SMPs) facilitate information sharing across varying levels of sensitivity. A crucial design decision for SMP administrators is the platform’s identity policy, with some opting for real-name systems while others allow anonymous participation. Content moderation on these platforms is conducted by both humans and automated bots. This paper examines the relationship between anonymity, specifically through the use of “throwaway” accounts, and the extent and nature of content moderation on Reddit. Our findings indicate that content originating from anonymous throwaway accounts is more likely to violate rules on Reddit. Thus, they are more likely to be removed by moderation than standard pseudonymous accounts. However, the moderation actions applied to throwaway accounts are consistent with those applied to ordinary accounts, suggesting that the use of anonymous accounts does not necessarily necessitate increased human moderation. We conclude by discussing the implications of these findings for identity policies and content moderation strategies on SMPs."
1735,679d459debd8ffd557a2b534,cs.HC,https://arxiv.org/pdf/2501.17375,Self-Guided Virtual Reality Therapy for Anxiety: A Systematic Review,"Winona Graham, Russell Drinkwater, Joshua Kelson, Muhammad Ashad Kabir","Human-Computer Interaction, Multimedia",
1736,679d459debd8ffd557a2b535,cs.HC,https://arxiv.org/pdf/2501.17258,Controlling AI Agent Participation in Group Conversations: A Human-Centered Approach,"Stephanie Houde, Kristina Brimijoin, Michael Muller, Steven I. Ross, Dario Andres Silva Moran, Gabriel Enrique Gonzalez, Siya Kunde, Morgan A. Foreman, Justin D. Weisz",Human-Computer Interaction,"Conversational AI agents are commonly applied within single-user, turn-taking scenarios. The interaction mechanics of these scenarios are trivial: when the user enters a message, the AI agent produces a response. However, the interaction dynamics are more complex within group settings. How should an agent behave in these settings? We report on two experiments aimed at uncovering users’ experiences of an AI agent’s participation within a group, in the context of group ideation (brainstorming). In the first study, participants benefited from and preferred having the AI agent in the group, but participants disliked when the agent seemed to dominate the conversation and they desired various controls over its interactive behaviors. In the second study, we created functional controls over the agent’s behavior, operable by group members, to validate their utility and probe for additional requirements. Integrating our findings across both studies, we developed a taxonomy of controls for when, what, and where a conversational AI agent in a group should respond, who can control its behavior, and how those controls are specified and implemented. Our taxonomy is intended to aid AI creators to think through important considerations in the design of mixed-initiative conversational agents."
1737,679d459debd8ffd557a2b536,cs.HC,https://arxiv.org/pdf/2501.17247,"""It makes you think"": Provocations Help Restore Critical Thinking to AI-Assisted Knowledge Work","Ian Drosos, Advait Sarkar, Xiaotong, Neil Toronto",Human-Computer Interaction,"Recent research suggests that the use of Generative AI tools may result in diminished critical thinking during knowledge work. We study the effect on knowledge work of provocations: brief textual prompts that offer critiques for and propose alternatives to AI suggestions. We conduct a between-subjects study (n=24𝑛24n=24italic_n = 24) in which participants completed AI-assisted shortlisting tasks with and without provocations. We find that provocations can induce critical and metacognitive thinking. We derive five dimensions that impact the user experience of provocations: task urgency, task importance, user expertise, provocation actionability, and user responsibility. We connect our findings to related work on design frictions, microboundaries, and distributed cognition. We draw design implications for critical thinking interventions in AI-assisted knowledge work."
1738,679d459debd8ffd557a2b537,cs.HC,https://arxiv.org/pdf/2501.17747,In-IDE Programming Courses: Learning Software Development in a Real-World Setting,"Anastasiia Birillo, Ilya Vlasov, Katsiaryna Dzialets, Hieke Keuning, Timofey Bryksin","Software Engineering, Computers and Society, Human-Computer Interaction","While learning programming languages is crucial for software engineers, mastering the necessary tools is equally important. To facilitate this, JetBrains recently released the JetBrains Academy plugin, which customizes the IDE for learners, allowing tutors to create courses entirely within IDE."
1739,679d459debd8ffd557a2b538,cs.HC,https://arxiv.org/pdf/2501.16885,"""My Whereabouts, my Location, it's Directly Linked to my Physical Security"": An Exploratory Qualitative Study of Location-Dependent Security and Privacy Perceptions among Activist Tech Users","Christian Eichenmüller, Lisa Kuhn, Zinaida Benenson","Human-Computer Interaction, Computers and Society","Digital-safety research with at-risk users is particularly urgent. At-risk users are more likely to be digitally attacked or targeted by surveillance and could be disproportionately harmed by attacks that facilitate physical assaults. One group of such at-risk users are activists and politically active individuals. For them, as for other at-risk users, the rise of smart environments harbors new risks. Since digitization and datafication are no longer limited to a series of personal devices that can be switched on and off, but increasingly and continuously surround users, granular geolocation poses new safety challenges. Drawing on eight exploratory qualitative interviews of an ongoing research project, this contribution highlights what activists with powerful adversaries think about evermore data traces, including location data, and how they intend to deal with emerging risks. Responses of activists include attempts to control one’s immediate technological surroundings and to more carefully manage device-related location data. For some activists, threat modeling has also shaped provider choices based on geopolitical considerations. Since many activists have not enough digital-safety knowledge for effective protection, feelings of insecurity and paranoia are widespread. Channeling the concerns and fears of our interlocutors, we call for more research on how activists can protect themselves against evermore fine-grained location data tracking."
1740,679d459debd8ffd557a2b539,cs.HC,https://arxiv.org/pdf/2501.16864,A methodology and a platform for high-quality rich personal data collection,"Ivan Kayongo, Leonardo Malcotti, Haonan Zhao, Fausto Giunchiglia",Human-Computer Interaction,"In the last years the pervasive use of sensors, as they exist in smart devices, e.g., phones, watches, medical devices, has increased dramatically the availability of personal data. However, existing research on data collection primarily focuses on the objective view of reality, as provided, for instance, by sensors, often neglecting the integration of subjective human input, as provided, for instance, by user answers to questionnaires. This limits substantially the exploitability of the collected data. In this paper we present a methodology and a platform specifically designed for the collection of a combination of large-scale sensor data and qualitative human feedback. The methodology has been designed to be deployed on top, and enriches the functionalities of, an existing data collection APP, callediLog, which has been used in large scale, worldwide data collection experiments. The main goal is to put the key actors involved in an experiment, i.e., theresearcherin charge, theparticipant, andiLogin better control of the experiment itself, thus enabling a much improved quality and richness of the data collected. The novel functionalities of the resulting platform are: (i) a time-wise representation of the situational context within which the data collection is performed, (ii) an explicit representation of the temporal context within which the data collection is performed, (iii) a calendar-based dashboard for the real-time monitoring of the data collection context(s), and, finally, (iv) a mechanism for the run-time revision of the data collection plan. The practicality and utility of the proposed functionalities are demonstrated by showing how they apply to a case study involving 350 University students."
1741,679d459debd8ffd557a2b53a,cs.HC,https://arxiv.org/pdf/2501.16693,"Explainability and AI Confidence in Clinical Decision Support Systems: Effects on Trust, Diagnostic Performance, and Cognitive Load in Breast Cancer Care","Olya Rezaeian, Alparslan Emrah Bayrak, Onur Asan",Human-Computer Interaction,"Artificial Intelligence (AI) has demonstrated potential in healthcare, particularly in enhancing diagnostic accuracy and decision-making through Clinical Decision Support Systems (CDSSs). However, the successful implementation of these systems relies on user trust and reliance, which can be influenced by explainable AI. This study explores the impact of varying explainability levels on clinicians’ trust, cognitive load, and diagnostic performance in breast cancer detection. Utilizing an interrupted time series design, we conducted a web-based experiment involving 28 healthcare professionals. The results revealed that high confidence scores substantially increased trust but also led to overreliance, reducing diagnostic accuracy. In contrast, low confidence scores decreased trust and agreement while increasing diagnosis duration, reflecting more cautious behavior. Some explainability features influenced cognitive load by increasing stress levels. Additionally, demographic factors such as age, gender, and professional role shaped participants’ perceptions and interactions with the system. This study provides valuable insights into how explainability impact clinicians’ behavior and decision-making. The findings highlight the importance of designing AI-driven CDSSs that balance transparency, usability, and cognitive demands to foster trust and improve integration into clinical workflows."
1742,679d459debd8ffd557a2b53b,cs.HC,https://arxiv.org/pdf/2501.16674,Demonstration of picoRing mouse: an ultra-low-powered wireless mouse ring with ring-to-wristband coil-based sensitive impedance sensing,"Yifan Li, Masaaki Fukumoto, Mohamed Kari, Tomoyuki Yokota, Takao Someya, Yoshihiro Kawahara, Ryo Takahashi",Human-Computer Interaction,"Wireless mouse rings offer subtle, reliable pointing interactions for wearable computing platforms.
However, the small battery below 27 mAh in the miniature rings restricts the ring’s continuous lifespan to just 1-2 hours, because current low-powered wireless communication such as BLE is power-consuming for ring’s continuous use.
The ring’s short lifespan persistently disrupts users’ mouse use with the need for frequent charging.
This interactivity demonstrates picoRingmouse, enabling a continuous ring-based mouse interaction with ultra-low-powered ring-to-wristband wireless communication.
picoRingmouseemploys a coil-based impedance sensing named semi-passive inductive telemetry, allowing a wristband coil to capture a unique frequency response of a nearby ring coil via a sensitive inductive coupling between the coils.
The ring coil converts the corresponding user’s mouse input into the unique frequency response via an 820 uW mouse-driven modulation module.
Therefore, picoRingmousecan operate continuously over92929292hours on a single charge of a20202020mAh battery while supporting subtle scrolling and pressing interactions."
1743,679d459debd8ffd557a2b53c,cs.HC,https://arxiv.org/pdf/2501.16661,Jupybara: Operationalizing a Design Space for Actionable Data Analysis and Storytelling with LLMs,"Huichen Will Wang, Larry Birnbaum, Vidya Setlur",Human-Computer Interaction,"Mining and conveying actionable insights from complex data is a key challenge of exploratory data analysis (EDA) and storytelling. To address this challenge, we present a design space for actionable EDA and storytelling. Synthesizing theory and expert interviews, we highlight howsemantic precision,rhetorical persuasion, andpragmatic relevanceunderpin effective EDA and storytelling. We also show how this design space subsumes common challenges in actionable EDA and storytelling, such as identifying appropriate analytical strategies and leveraging relevant domain knowledge. Building on the potential of LLMs to generate coherent narratives with commonsense reasoning, we contributeJupybara, an AI-enabled assistant for actionable EDA and storytelling implemented as a Jupyter Notebook extension.Jupybaraemploys two strategies—design-space-aware prompting and multi-agent architectures—to operationalize our design space. An expert evaluation confirmsJupybara’s usability, steerability, explainability, and reparability, as well as the effectiveness of our strategies in operationalizing the design space framework with LLMs."
1744,679d459debd8ffd557a2b53d,cs.HC,https://arxiv.org/pdf/2501.16566,"AffectGPT: A New Dataset, Model, and Benchmark for Emotion Understanding with Multimodal Large Language Models","Zheng Lian, Haoyu Chen, Lan Chen, Haiyang Sun, Licai Sun, Yong Ren, Zebang Cheng, Bin Liu, Rui Liu, Xiaojiang Peng, Jiangyan Yi, Jianhua Tao",Human-Computer Interaction,"The emergence of multimodal large language models (MLLMs) advances multimodal emotion recognition (MER) to the next level—from naive discriminative tasks to complex emotion understanding with advanced video understanding abilities and natural language description. However, the current community suffers from a lack of large-scale datasets with intensive, descriptive emotion annotations, as well as a multimodal-centric framework to maximize the potential of MLLMs for emotion understanding. To address this, we establish a new benchmark for MLLM-based emotion understanding with a novel dataset (MER-Caption), and a new model (AffectGPT). Utilizing our model-based crowd-sourcing data collection strategy, we construct the largest descriptive emotion dataset to date (by far), featuring over 2K fine-grained emotion categories across 115K samples. We also introduce the AffectGPT model, designed with pre-fusion operations to enhance multimodal integration. Finally, we present MER-UniBench, a unified benchmark with evaluation metrics tailored for both typical MER tasks and the free-form, natural language output style of MLLMs. Extensive experimental results demonstrate AffectGPT’s robust performance across various MER tasks. We are publicly releasing both the AffectGPT model and the MER-Caption dataset to foster further research and development in emotion understanding."
1745,679d459debd8ffd557a2b53e,cs.HC,https://arxiv.org/pdf/2501.16557,CARING-AI: Towards Authoring Context-aware Augmented Reality INstruction through Generative Artificial Intelligence,"Jingyu Shi, Rahul Jain, Seungguen Chi, Hyungjun Doh, Hyunggun Chi, Alexander J. Quinn, Karthik Ramani",Human-Computer Interaction,"Context-aware AR instruction enables adaptive and in-situ learning experiences.
However, hardware limitations and expertise requirements constrain the creation of such instructions.
With recent developments in Generative Artificial Intelligence (Gen-AI), current research tries to tackle these constraints by deploying AI-generated content (AIGC) in AR applications.
However, our preliminary study with six AR practitioners revealed that the current AIGC lacks contextual information to adapt to varying application scenarios and is therefore limited in authoring.
To utilize the strong generative power of GenAI to ease the authoring of AR instruction while capturing the context, we developed CARING-AI, an AR system to author context-aware humanoid-avatar-based instructions with GenAI.
By navigating in the environment, users naturally provide contextual information to generate humanoid-avatar animation as AR instructions that blend in the context spatially and temporally.
We showcased three application scenarios of CARING-AI: Asynchronous Instructions, Remote Instructions, and Ad Hoc Instructions based on a design space of AIGC in AR Instructions.
With two user studies (N=12), we assessed the system usability of CARING-AI and demonstrated the easiness and effectiveness of authoring with Gen-AI."
1746,679d459debd8ffd557a2b53f,cs.HC,https://arxiv.org/pdf/2501.16518,Generative AI as a Playful yet Offensive Tourist: Exploring Tensions Between Playful Features and Citizen Concerns in Designing Urban Play,"Peng-Kai Hung, Janet Yi-Ching Huang, Stephan Wensveen, Rung-Huei Liang",Human-Computer Interaction,"Play is pivotal in fostering the emotional, social, and cultural dimensions of urban spaces. While generative AI (GAI) potentially supports playful urban interaction, a balanced and critical approach to the design opportunities and challenges is needed. This work develops iWonder, an image-to-image GAI tool engaging fourteen designers in urban explorations to identify GAI’s playful features and create design ideas. Fourteen citizens then evaluated these ideas, providing expectations and critical concerns from a bottom-up perspective. Our findings reveal the dynamic interplay between users, GAI, and urban contexts, highlighting GAI’s potential to facilitate playful urban experiences throughgenerative agency,meaningful unpredictability,social performativity, and the associated offensive qualities. We propose design considerations to address citizen concerns and the ‘tourist metaphor’ to deepen our understanding of GAI’s impact, offering insights to enhance cities’ socio-cultural fabric. Overall, this research contributes to the effort to harness GAI’s capabilities for urban enrichment."
1747,679d459debd8ffd557a2b540,cs.HC,https://arxiv.org/pdf/2501.16515,SimulataR: Rapid Assisted Reality Prototyping using Design-Blended Videos,"Ashwin Ram, Yue Gu, Bowen Wang, Sneha Jaikumar, Youqi Wu, Benjamin Tan Kuan Wei, Qingyang Xu, Haiming Liu, Shengdong Zhao",Human-Computer Interaction,"Assisted Reality (aR) is a subfield of Augmented Reality (AR) that overlays information onto a user’s immediate view via see-through head-mounted displays (OST-HMDs). This technology has proven to be effective and energy-efficient to support the user and information interaction for everyday wearable intelligent systems. The aR viewing experience, however, is affected by varying real-world backgrounds, lighting, and user movements, which makes designing for aR challenging. Designers have to test their designs in-situ across multiple real-world settings, which can be time-consuming and labor-intensive. We propose SimulataR, a cost-effective desktop-based approach for rapid aR prototyping using first-person-view context videos blended with design prototypes to simulate an aR expereince. A field study involving 12 AR users comparing SimulataR to real OST-HMDs found that SimulataR can approximate the aR experience, particularly for indoors and in low-to-moderate lit outdoor environments. Case studies with two designers who used SimulataR in their design process demonstrates the potential of design-blended videos for rapid aR prototyping."
1748,679d459debd8ffd557a2b541,cs.HC,https://arxiv.org/pdf/2501.16505,Just stop doing everything for now!: Understanding security attacks in remote collaborative mixed reality,"Maha Sajid, Syed Ibrahim Mustafa Shah Bukhari, Bo Ji, Brendan David-John",Human-Computer Interaction,"Mixed Reality (MR) devices are being increasingly adopted across a wide range of real-world applications, ranging from education and healthcare to remote work and entertainment. However, the unique immersive features of MR devices, such as 3D spatial interactions and the encapsulation of virtual objects by invisible elements, introduce new vulnerabilities leading to interaction obstruction and misdirection. We implemented latency, click redirection, object occlusion, and spatial occlusion attacks within a remote collaborative MR platform using the Microsoft HoloLens 2 and evaluated user behavior and mitigations through a user study. We compared responses to MR-specific attacks, which exploit the unique characteristics of remote collaborative immersive environments, and traditional security attacks implemented in MR. Our findings indicate that users generally exhibit lower recognition rates for immersive attacks (e.g., spatial occlusion) compared to attacks inspired by traditional ones (e.g., click redirection). Our results demonstrate a clear gap in user awareness and responses when collaborating remotely in MR environments. Our findings emphasize the importance of training users to recognize potential threats and enhanced security measures to maintain trust in remote collaborative MR systems."
1749,679d459debd8ffd557a2b542,cs.HC,https://arxiv.org/pdf/2501.16929,Giving Sense to Inputs: Toward an Accessible Control Framework for Shared Autonomy,"Shalutha Rajapakshe, Jean-Marc Odobez, Emmanuel Senft","Robotics, Human-Computer Interaction","This document is a model and instructions forLaTeX.
This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes,
or Math in Paper Title or Abstract."
1750,679d459debd8ffd557a2b543,cs.HC,https://arxiv.org/pdf/2501.16780,"AVE Speech Dataset: A Comprehensive Benchmark for Multi-Modal Speech Recognition Integrating Audio, Visual, and Electromyographic Signals","Dongliang Zhou, Yakun Zhang, Jinghan Wu, Xingyu Zhang, Liang Xie, Erwei Yin","Sound, Human-Computer Interaction, Multimedia, Audio and Speech Processing","The global aging population faces considerable challenges, particularly in communication, due to the prevalence of hearing and speech impairments. To address these, we introduce the AVE speech dataset, a comprehensive multi-modal benchmark for speech recognition tasks. The dataset includes a 100-sentence Mandarin Chinese corpus with audio signals, lip-region video recordings, and six-channel electromyography (EMG) data, collected from 100 participants. Each subject read the entire corpus ten times, with each sentence averaging approximately two seconds in duration, resulting in over 55 hours of multi-modal speech data per modality. Experiments demonstrate that combining these modalities significantly improves recognition performance, particularly in cross-subject and high-noise environments. To our knowledge, this is the first publicly available sentence-level dataset integrating these three modalities for large-scale Mandarin speech recognition. We expect this dataset to drive advancements in both acoustic and non-acoustic speech recognition research, enhancing cross-modal learning and human-machine interaction."
1751,679d459debd8ffd557a2b544,cs.HC,https://arxiv.org/pdf/2501.16668,Examining Online Social Support for Countering QAnon Conspiracies,"Michael Robert Haupt, Meng Zhen Larsen, Michelle Strayer, Luning Yang, Tim K. Mackey","Social and Information Networks, Human-Computer Interaction, Physics and Society",
1752,679d459debd8ffd557a2b545,cs.HC,https://arxiv.org/pdf/2501.16641,SCDiar: a streaming diarization system based on speaker change detection and speech recognition,"Naijun Zheng, Xucheng Wan, Kai Liu, Zhou Huan","Audio and Speech Processing, Human-Computer Interaction, Sound","In hours-long meeting scenarios, real-time speech stream often struggles with achieving accurate speaker diarization, commonly leading to speaker identification and speaker count errors.
To address this challenge, we propose SCDiar, a system that operates on speech segments, split at the token level by a speaker change detection (SCD) module. Building on these segments, we introduce several enhancements to efficiently select the best available segment for each speaker. These improvements lead to significant gains across various benchmarks. Notably, on real-world meeting data involving more than ten participants, SCDiar outperforms previous systems by up to 53.6% in accuracy, substantially narrowing the performance gap between online and offline systems."
1753,679d459debd8ffd557a2b546,cs.HC,https://arxiv.org/pdf/2501.16601,Non-Western Perspectives on Web Inclusivity: A Study of Accessibility Practices in the Global South,"Masudul Hasan Masud Bhuiyan, Matteo Varvello, Cristian-Alexandru Staicu, Yasir Zaki","Software Engineering, Computers and Society, Human-Computer Interaction","The Global South faces unique challenges in achieving digital inclusion due to a heavy reliance on mobile devices for internet access and the prevalence of slow or unreliable networks. While numerous studies have investigated web accessibility within specific sectors such as education, healthcare, and government services, these efforts have been largely constrained to individual countries or narrow contexts, leaving a critical gap in cross-regional, large-scale analysis. This paper addresses this gap by conducting the first large-scale comparative study of mobile web accessibility across the Global South. In this work, we evaluate 100,000 websites from 10 countries in the Global South to provide a comprehensive understanding of accessibility practices in these regions. Our findings reveal that websites from countries with strict accessibility regulations and enforcement tend to adhere better to Web Content Accessibility Guidelines (WCAG) guidelines. However, accessibility violations impact different disability groups in varying ways. Blind and low-vision individuals in the Global South are disproportionately affected, as only 40% of the evaluated websites meet critical accessibility guidelines. This significant shortfall is largely due to developers frequently neglecting to implement valid alt text for images and ARIA descriptions, which are essential specification mechanisms in the HTML standard for the effective operation of screen readers."
1754,679d459debd8ffd557a2b547,cs.HC,https://arxiv.org/pdf/2501.16305,Exploring Data-Driven Advocacy in Home Health Care Work,"Joy Ming, Hawi H Tolera, Jiamin Tu, Ella Yitzhaki, Chit Sum Eunice Ngai, Madeline Sterling, Ariel C Avgar, Aditya Vashistha, Nicola Dell",Human-Computer Interaction,"This paper explores opportunities and challenges for data-driven advocacy to support home care workers, an often overlooked group of low-wage, frontline health workers. First, we investigate what data to collect and how to collect it in ways that preserve privacy and avoid burdening workers. Second, we examine how workers and advocates could use collected data to strengthen individual and collective advocacy efforts. Our qualitative study with 11 workers and 15 advocates highlights tensions between workers’ desires for individual and immediate benefits and advocates’ preferences to prioritize more collective and long-term benefits. We also uncover discrepancies between participants’ expectations for how data might transform advocacy and their on-the-ground experiences collecting and using real data. Finally, we discuss future directions for data-driven worker advocacy, including combining different kinds of data to ameliorate challenges, leveraging advocates as data stewards, and accounting for workers’ and organizations’ heterogeneous goals."
1755,679d459debd8ffd557a2b548,cs.HC,https://arxiv.org/pdf/2501.16240,AiGet: Transforming Everyday Moments into Hidden Knowledge Discovery with AI Assistance on Smart Glasses,"Runze Cai, Nuwan Janaka, Hyeongcheol Kim, Yang Chen, Shengdong Zhao, Yun Huang, David Hsu",Human-Computer Interaction,"Unlike the free exploration of childhood, the demands of daily life reduce our motivation to explore our surroundings, leading to missed opportunities for informal learning. Traditional tools for knowledge acquisition are reactive, relying on user initiative and limiting their ability to uncover hidden interests. Through formative studies, we introduceAiGet, a proactive AI assistant integrated with AR smart glasses, designed to seamlessly embed informal learning into low-demand daily activities (e.g., casual walking and shopping).AiGetanalyzes real-time user gaze patterns, environmental context, and user profiles, leveraging large language models to deliver personalized, context-aware knowledge with low disruption to primary tasks. In-lab evaluations and real-world testing, including continued use over multiple days, demonstrateAiGet’s effectiveness in uncovering overlooked yet surprising interests, enhancing primary task enjoyment, reviving curiosity, and deepening connections with the environment. We further propose design guidelines for AI-assisted informal learning, focused on transforming everyday moments into enriching learning experiences."
1756,679d459debd8ffd557a2b549,cs.HC,https://arxiv.org/pdf/2501.16230,MIND-EEG: Multi-granularity Integration Network with Discrete Codebook for EEG-based Emotion Recognition,"Yuzhe Zhang, Chengxi Xie, Huan Liu, Yuhan Shi, Dalin Zhang",Human-Computer Interaction,"Emotion recognition using electroencephalogram (EEG) signals has broad potential across various domains. EEG signals have ability to capture rich spatial information related to brain activity, yet effectively modeling and utilizing these spatial relationships remains a challenge. Existing methods struggle with simplistic spatial structure modeling, failing to capture complex node interactions, and lack generalizable spatial connection representations, failing to balance the dynamic nature of brain networks with the need for discriminative and generalizable features. To address these challenges, we propose the Multi-granularity Integration Network with Discrete Codebook for EEG-based Emotion Recognition (MIND-EEG). The framework employs a multi-granularity approach, integrating global and regional spatial information through a Global State Encoder, an Intra-Regional Functionality Encoder, and an Inter-Regional Interaction Encoder to comprehensively model brain activity. Additionally, we introduce a discrete codebook mechanism for constructing network structures via vector quantization, ensuring compact and meaningful brain network representations while mitigating over-smoothing and enhancing model generalization. The proposed framework effectively captures the dynamic and diverse nature of EEG signals, enabling robust emotion recognition. Extensive comparisons and analyses demonstrate the effectiveness of MIND-EEG, and the source code is publicly available athttps://anonymous.4open.science/r/MIND_EEG."
1757,679d459debd8ffd557a2b54a,cs.HC,https://arxiv.org/pdf/2501.16084,The Shiny Scary Future of Automated Research Synthesis in HCI,Katja Rogers,Human-Computer Interaction,"Automation and semi-automation through computational tools like LLMs are also making their way to deployment in research synthesis and secondary research, such as systematic reviews. In some steps of research synthesis, this has the opportunity to provide substantial benefits by saving time that previously was spent on repetitive tasks. The screening stages in particular may benefit from carefully vetted computational support. However, this position paper argues for additional caution when bringing in such tools to the analysis and synthesis phases, where human judgement and expertise should be paramount throughout the process."
1758,679d459debd8ffd557a2b54b,cs.HC,https://arxiv.org/pdf/2501.16010,MaRginalia: Enabling In-person Lecture Capturing and Note-taking Through Mixed Reality,"Leping Qiu, Erin Seongyoon Kim, Sangho Suh, Ludwig Sidenmark, Tovi Grossman",Human-Computer Interaction,"Students often take digital notes during live lectures, but current methods can be slow when capturing information from lecture slides or the instructor’s speech, and require them to focus on their devices, leading to distractions and missing important details. This paper explores supporting live lecture note-taking with mixed reality (MR) to quickly capture lecture information and take notes while staying engaged with the lecture. A survey and interviews with university students revealed common note-taking behaviors and challenges to inform the design. We present MaRginalia to provide digital note-taking with a stylus tablet and MR headset. Students can take notes with an MR representation of the tablet, lecture slides, and audio transcript without looking down at their device. When preferred, students can also perform detailed interactions by looking at the physical tablet. We demonstrate the feasibility and usefulness of MaRginalia and MR-based note-taking in a user study with 12 students."
1759,679d459debd8ffd557a2b54c,cs.HC,https://arxiv.org/pdf/2501.15931,MetaGadget: An Accessible Framework for IoT Integration into Commercial Metaverse Platforms,"Ryutaro Kurai, Hikari Yanagawa, Yuichi Hiroi, Takefumi Hiraki",Human-Computer Interaction,"While the integration of IoT devices in virtual spaces is becoming increasingly common, technical barriers to controlling custom devices in multi-user Virtual Reality (VR) environments remain high, particularly limiting new applications in educational and prototyping settings. We propose MetaGadget, a framework for connecting IoT devices to commercial metaverse platforms that implements device control through HTTP-based event triggers without requiring persistent client connections. Through two workshops focused on smart home control and custom device integration, we explored the potential application of IoT connectivity in multi-user metaverse environments. Participants successfully implemented new interactions unique to the metaverse, such as environmental sensing and remote control systems that support simultaneous operation by multiple users, and reported positive feedback on the ease of system development. We verified that our framework provides a new approach to controlling IoT devices in the metaverse while reducing technical requirements, and provides a foundation for creative practice that connects multi-user VR environments and physical spaces."
1760,679d459debd8ffd557a2b54d,cs.HC,https://arxiv.org/pdf/2501.15885,"A Low-Cost, High-Precision Human-Machine Interaction Solution Based on Multi-Coil Wireless Charging Pads",Bojun Zhang,Human-Computer Interaction,"Wireless charging pads are common, yet their functionality is mainly restricted to charging. Existing gesture recognition techniques, such as those based on machine vision and WiFi, have drawbacks like high costs and poor precision. This paper presents a new human - machine interaction solution using multi - coil wireless charging pads.
The proposed approach leverages the pads’ existing modules without additional wearable sensors. It determines gestures by monitoring current and power changes in different coils. The data processing includes noise removal, sorting, high - pass filtering, and slicing. A Bayesian network and particle filtering are employed for motion tracking.
Through experiments, this solution proves to have wide applications, high recognition accuracy, and low cost. It can effectively identify diverse gestures, increasing the value of wireless charging pads. It outperforms traditional methods, with a 0.73 improvement in recognition accuracy and better environmental adaptability."
1761,679d459debd8ffd557a2b54e,cs.HC,https://arxiv.org/pdf/2501.15864,Explaining Facial Expression Recognition,"Sanjeev Nahulanthran, Leimin Tian, Dana Kulić, Mor Vered",Human-Computer Interaction,"Facial expression recognition (FER) has emerged as a promising approach to the development of emotion-aware intelligent agents and systems.
However, key challenges remain in utilizing FER in real-world contexts, including ensuring user understanding and establishing a suitable level of user trust. We developed a novel explanation method utilizing Facial Action Units (FAUs) to explain the output of a FER model through both textual and visual modalities. We conducted an empirical user study evaluating user understanding and trust, comparing our approach to state-of-the-art eXplainable AI (XAI) methods.
Our results indicate that visual AND textual as well as textual-only FAU-based explanations resulted in better user understanding of the FER model.
We also show that all modalities of FAU-based methods improved appropriate trust of the users towards the FER model."
1762,679d459debd8ffd557a2b54f,cs.HC,https://arxiv.org/pdf/2501.15819,Navigation Framework for Blind and Visually Impaired Persons based on Sensor Fusion,"Chathurika S. Silva, Prasad Wimalaratne",Human-Computer Interaction,
1763,679d459debd8ffd557a2b550,cs.HC,https://arxiv.org/pdf/2501.15770,Walk in Their Shoes to Navigate Your Own Path: Learning About Procrastination Through A Serious Game,"Runhua Zhang, Jiaqi Gan, Shangyuan Gao, Siyi Chen, Xinyu Wu, Dong Chen, Yulin Tian, Qi Wang, Pengcheng An",Human-Computer Interaction,"Procrastination, the voluntary delay of tasks despite potential negative consequences, has prompted numerous time and task management interventions in the HCI community. While these interventions have shown promise in addressing specific behaviors, psychological theories suggest that learning about procrastination itself may help individuals develop their own coping strategies and build mental resilience. However, little research has explored how to support this learning process through HCI approaches. We presentProcrastiMate, a text adventure game where players learn about procrastination’s causes and experiment with coping strategies by guiding in-game characters in managing relatable scenarios. Our field study with 27 participants revealed thatProcrastiMatefacilitated learning and self-reflection while maintaining psychological distance, motivating players to integrate newly acquired knowledge in daily life. This paper contributes empirical insights on leveraging serious games to facilitate learning about procrastination and offers design implications for addressing psychological challenges through HCI approaches."
1764,679d459debd8ffd557a2b551,cs.HC,https://arxiv.org/pdf/2501.15721,On Parallelism in Music and Language: A Perspective from Symbol Emergence Systems based on Probabilistic Generative Models,Tadahiro Taniguchi,"Human-Computer Interaction, Multimedia","Music and language are structurally similar. Such structural similarity is often explained by generative processes. This paper describes the recent development of probabilistic generative models (PGMs) for language learning and symbol emergence in robotics. Symbol emergence in robotics aims to develop a robot that can adapt to real-world environments and human linguistic communications and acquire language from sensorimotor information alone (i.e., in an unsupervised manner).
This is regarded as a constructive approach to symbol emergence systems.
To this end, a series of PGMs have been developed, including those for simultaneous phoneme and word discovery, lexical acquisition, object and spatial concept formation, and the emergence of a symbol system. By extending the models, a symbol emergence system comprising a multi-agent system in which a symbol system emerges is revealed to be modeled using PGMs. In this model, symbol emergence can be regarded as collective predictive coding. This paper expands on this idea by combining the theory that ”emotion is based on the predictive coding of interoceptive signals” and ”symbol emergence systems,” and describes the possible hypothesis of the emergence of meaning in music."
1765,679d459debd8ffd557a2b552,cs.HC,https://arxiv.org/pdf/2501.15711,DanmuA11y: Making Time-Synced On-Screen Video Comments (Danmu) Accessible to Blind and Low Vision Users via Multi-Viewer Audio Discussions,"Shuchang Xu, Xiaofu Jin, Huamin Qu, Yukang Yan",Human-Computer Interaction,"By overlaying time-synced user comments on videos, Danmu creates a co-watching experience for online viewers. However, its visual-centric design poses significant challenges for blind and low vision (BLV) viewers. Our formative study identified three primary challenges that hinder BLV viewers’ engagement with Danmu: the lack of visual context, the speech interference between comments and videos, and the disorganization of comments. To address these challenges, we present DanmuA11y, a system that makes Danmu accessible by transforming it into multi-viewer audio discussions. DanmuA11y incorporates three core features: (1)Augmenting Danmu with visual context, (2)Seamlessly integrating Danmu into videos, and (3)Presenting Danmu via multi-viewer discussions. Evaluation with twelve BLV viewers demonstrated that DanmuA11y significantly improved Danmu comprehension, provided smooth viewing experiences, and fostered social connections among viewers. We further highlight implications for enhancing commentary accessibility in video-based social media and live-streaming platforms."
1766,679d459debd8ffd557a2b553,cs.HC,https://arxiv.org/pdf/2501.15628,Understanding Attitudes and Trust of Generative AI Chatbots for Social Anxiety Support,"Yimeng Wang, Yinzhou Wang, Kelly Crace, Yixuan Zhang",Human-Computer Interaction,"Social anxiety (SA) has become increasingly prevalent. Traditional coping strategies often face accessibility challenges. Generative AI (GenAI), known for their knowledgeable and conversational capabilities, are emerging as alternative tools for mental well-being. With the increased integration of GenAI, it is important to examine individuals’ attitudes and trust in GenAI chatbots’ support for SA. Through a mixed-method approach that involved surveys (n=159𝑛159n=159italic_n = 159) and interviews (n=17𝑛17n=17italic_n = 17), we found that individuals with severe symptoms tended to trust and embrace GenAI chatbots more readily, valuing their non-judgmental support and perceived emotional comprehension. However, those with milder symptoms prioritized technical reliability. We identified factors influencing trust, such as GenAI chatbots’ ability to generate empathetic responses and its context-sensitive limitations, which were particularly important among individuals with SA. We also discuss the design implications and use of GenAI chatbots in fostering cognitive and emotional trust, with practical and design considerations."
1767,679d459debd8ffd557a2b554,cs.HC,https://arxiv.org/pdf/2501.15608,Engage and Mobilize! Understanding Evolving Patterns of Social Media Usage in Emergency Management,"Hemant Purohit, Cody Buntain, Amanda Lee Hughes, Steve Peterson, Valerio Lorini, Carlos Castillo",Human-Computer Interaction,"The work of Emergency Management (EM) agencies requires timely collection of relevant data to inform decision-making for operations and public communication before, during, and after a disaster. However, the limited human resources available to deploy for field data collection is a persistent problem for EM agencies.
Thus, over the last decade, many of these agencies have started leveraging social media as a supplemental data source and a new venue to engage with the public.
Such uses present both opportunities and challenges. While prior research has analyzed the potential benefits and attitudes of practitioners and the public when leveraging social media during disasters, a gap exists in the critical analysis of the actual practices and uses of social media among EM agencies, across both geographical regions and phases of the
EM lifecycle - typically mitigation, preparedness, response, and recovery.
In this paper, we conduct a mixed-method analysis to update and fill this gap on how EM practitioners in the U.S. and Europe use social media, building on a survey study of about 150 professionals and a follow-up interview study with 11 participants.
The results indicate that using social media is no longer a non-traditional practice in operational and informational processes for the decision-making of EM agenciesworking at both the local level (e.g., county or town) and non-local level (e.g., state/province, federal/national) for emergency management.
Especially, the practitioners affiliated with agencies working at the local level have a very high perceived value of social media for situational awareness (e.g., analyzing disaster extent and impact) and public communication (e.g., disseminating timely information and correcting errors in crisis coverage).Further, practitioners now engage with the public during the preparedness phase to mobilize them during the response phase.
We present a model to understand the current practices of communication between agencies and the public, as well as among practitioners while leveraging social media.We also discuss novel challenges,including public fragmentation caused by the increasing use of multiple social media platforms, information integrity, and social listening expectations.
We conclude with the policy, technological, and socio-technical needs to design future social media analytics systems to support the work of EM agencies in such communication."
1768,679d459debd8ffd557a2b555,cs.HC,https://arxiv.org/pdf/2501.15599,Evaluating an LLM-Powered Chatbot for Cognitive Restructuring: Insights from Mental Health Professionals,"Yinzhou Wang, Yimeng Wang, Ye Xiao, Liabette Escamilla, Bianca Augustine, Kelly Crace, Gang Zhou, Yixuan Zhang",Human-Computer Interaction,"Recent advancements in large language models (LLMs) promise to expand mental health interventions by emulating therapeutic techniques, potentially easing barriers to care. Yet there is a lack of real-world empirical evidence evaluating the strengths and limitations of LLM-enabled psychotherapy interventions. In this work, we evaluate an LLM-powered chatbot, designed via prompt engineering to deliver cognitive restructuring (CR), with 19 users. Mental health professionals then examined the resulting conversation logs to uncover potential benefits and pitfalls. Our findings indicate that an LLM-based CR approach has the capability to adhere to core CR protocols, prompt Socratic questioning, and provide empathetic validation. However, issues of power imbalances, advice-giving, misunderstood cues, and excessive positivity reveal deeper challenges, including the potential to erode therapeutic rapport and ethical concerns. We also discuss design implications for leveraging LLMs in psychotherapy and underscore the importance of expert oversight to mitigate these concerns—critical steps toward safer, more effective AI-assisted interventions."
1769,679d459debd8ffd557a2b556,cs.HC,https://arxiv.org/pdf/2501.15583,Cognitive Performance Measurements and the Impact of Sleep Quality Using Wearable and Mobile Sensors,"Aku Visuri, Heli Koskimäki, Niels van Berkel, Andy Alorwu, Ella Peltonen, Saeed Abdullah, Simo Hosio",Human-Computer Interaction,"Human cognitive performance is an underlying factor in most of our daily lives, and numerous factors influence cognitive performance. In this work, we investigate how changes in sleep quality influence cognitive performance, measured from a dataset collected during a 2-month field study. We collected cognitive performance data (alertness) with the Psychomotor Vigilance Task (PVT), mobile keyboard typing metrics from participants’ smartphones, and sleep quality metrics through a wearable sleep tracking ring. Our findings highlight that specific sleep metrics like night-time heart rate, sleep latency, sleep timing, sleep restfulness, and overall sleep quantity significantly influence cognitive performance. To strengthen the current research on cognitive measurements, we introduce smartphone typing metrics as a proxy or a complementary method for continuous passive measurement of cognitive performance. Together, our findings contribute to ubiquitous computing via a longitudinal case study with a novel wearable device, the resulting findings on the association between sleep and cognitive function, and the introduction of smartphone keyboard typing as a proxy of cognitive function."
1770,679d459debd8ffd557a2b557,cs.HC,https://arxiv.org/pdf/2501.15456,"""See What I Imagine, Imagine What I See"": Human-AI Co-Creation System for 360$^\circ$ Panoramic Video Generation in VR",Yunge Wen,Human-Computer Interaction,"The emerging field of panoramic video generation from text and image prompts unlocks new creative possibilities in virtual reality (VR), addressing the limitations of current immersive experiences, which are constrained by pre-designed environments that restrict user creativity. To advance this frontier, we present Imagine360, a proof-of-concept prototype that integrates co-creation principles with AI agents. This system enables refined speech-based text prompts, egocentric perspective adjustments, and real-time customization of virtual surroundings based on user perception and intent. An eight-participant pilot study comparing non-AI and linear AI-driven workflows demonstrates that Imagine360’s co-creative approach effectively integrates temporal and spatial creative controls. This introduces a transformative VR paradigm, allowing users to seamlessly transition between ’seeing’ and ’imagining,’ thereby shaping virtual reality through the creations of their minds."
1771,679d459debd8ffd557a2b558,cs.HC,https://arxiv.org/pdf/2501.15413,XR-penter: Material-Aware and In Situ Design of Scrap Wood Assemblies,"Ramya Iyer, Mustafa Doga Dogan, Maria Larsson, Takeo Igarashi",Human-Computer Interaction,"Woodworkers have to navigate multiple considerations when planning a project, including available resources, skill-level, and intended effort.Do it yourself(DIY) woodworkers face these challenges most acutely because of tight material constraints and a desire for custom designs tailored to specific spaces. To address these needs, we presentXR-penter, an extended reality (XR) application that supports in situ, material-aware woodworking for casual makers. Our system enables users to design virtual scrap wood assemblies directly in their workspace, encouragingsustainablepractices through the use of discarded materials. Users register physical material as virtualtwins, manipulate these twins into an assembly in XR, and preview cuts needed for fabrication.We conducted a case study and feedback sessions to demonstrate howXR-pentersupports improvisational workflows in practice, the type of woodworker who would benefit most from our system, and insights on integrating similar spatial and material considerations into future work."
1772,679d459debd8ffd557a2b559,cs.HC,https://arxiv.org/pdf/2501.15408,Memory Reviver: Supporting Photo-Collection Reminiscence for People with Visual Impairment via a Proactive Chatbot,"Shuchang Xu, Chang Chen, Zichen Liu, Xiaofu Jin, Linping Yuan, Yukang Yan, Huamin Qu",Human-Computer Interaction,"Reminiscing with photo collections offers significant psychological benefits but poses challenges for people with visual impairment (PVI). Their current reliance on sighted help restricts the flexibility of this activity. In response, we explored using a chatbot in a preliminary study. We identified two primary challenges that hinder effective reminiscence with a chatbot: the scattering of information and a lack of proactive guidance. To address these limitations, we present Memory Reviver, a proactive chatbot that helps PVI reminisce with a photo collection through natural language communication.
Memory Reviver incorporates two novel features: (1) aMemory Tree, which uses a hierarchical structure to organize the information in a photo collection; and (2) aProactive Strategy, which actively delivers information to users at proper conversation rounds. Evaluation with twelve PVI demonstrated that Memory Reviver effectively facilitated engaging reminiscence, enhanced understanding of photo collections, and delivered natural conversational experiences. Based on our findings, we distill implications for supporting photo reminiscence and designing chatbots for PVI."
1773,679d459debd8ffd557a2b55a,cs.HC,https://arxiv.org/pdf/2501.15347,Heterogeneous Population Encoding for Multi-joint Regression using sEMG Signals,"Farah Baracat, Luca Manneschi, Elisa Donati","Human-Computer Interaction, Signal Processing","Regression-based decoding of continuous movements is essential forhuman-machine interfaces, such as prosthetic control. This study explores a feature-based approach to encodingSurface Electromyography(sEMG) signals, focusing on the role of variability in neural-inspired population encoding. By employing heterogeneous populations ofLeaky Integrate-and-Fire(LIF) neurons with varying sizes and diverse parameter distributions, we investigate how population size and variability in encoding parameters, such as membrane time constants and thresholds, influence decoding performance. Using a simple linear readout, we demonstrate that variability improves robustness and generalizability compared to single-neuron encoders. These findings emphasize the importance of optimizing variability and population size for efficient and scalable regression tasks inspiking neural networks, paving the way for robust, low-powerHMIimplementations."
1774,679d459debd8ffd557a2b55b,cs.HC,https://arxiv.org/pdf/2501.15346,Between Puppet and Actor: Reframing Authorship in this Age of AI Agents,"Yuqian Sun, Stefano Gualeni",Human-Computer Interaction,
1775,679d459debd8ffd557a2b55c,cs.HC,https://arxiv.org/pdf/2501.15332,"Perception of an AI Teammate in an Embodied Control Task Affects Team Performance, Reflected in Human Teammates' Behaviors and Physiological Responses","Yinuo Qin, Richard T. Lee, Paul Sajda","Human-Computer Interaction, Neurons and Cognition","The integration of artificial intelligence (AI) into human teams is widely expected to enhance performance and collaboration. However, our study reveals a striking and counterintuitive result: human-AI teams performed worse than human-only teams, especially when task difficulty increased. Using a virtual reality-based sensorimotor task, we observed that the inclusion of an active human-like AI teammate disrupted team dynamics, leading to elevated arousal, reduced engagement, and diminished communication intensity among human participants. These effects persisted even as the human teammates’ perception of the AI teammate improved over time. These findings challenge prevailing assumptions about the benefits of AI in team settings and highlight the critical need for human-centered AI design to mitigate adverse physiological and behavioral impacts, ensuring more effective human-AI collaboration."
1776,679d459debd8ffd557a2b55d,cs.HC,https://arxiv.org/pdf/2501.15047,"Evaluating the Effectiveness of Mobile Game-Based Learning for Raising Adolescent Health Awareness: The Case of ""AHlam Na 2.0""",Noel P. Caliston,Human-Computer Interaction,
1777,679d459debd8ffd557a2b55e,cs.HC,https://arxiv.org/pdf/2501.15028,Mining Evidence about Your Symptoms: Mitigating Availability Bias in Online Self-Diagnosis,"Junti Zhang, Zicheng Zhu, Jingshu Li, Yi-Chieh Lee",Human-Computer Interaction,
1778,679d459debd8ffd557a2b55f,cs.HC,https://arxiv.org/pdf/2501.14903,The Many Tendrils of the Octopus Map,"Eduardo Puerta, Shani Spivak, Michael Correll",Human-Computer Interaction,"Conspiratorial thinking can connect many distinct or distant ills to a central cause. This belief has visual form in theoctopus map: a map where a central force (for instance a nation, an ideology, or an ethnicity) is depicted as a literal or figurative octopus, with extending tendrils. In this paper, we explore how octopus maps function as visual arguments through an analysis of historical examples as well as a through a crowd-sourced study on how the underlying data and the use of visual metaphors contribute to specific negative or conspiratorial interpretations. We find that many features of the data or visual style can lead to “octopus-like” thinking in visualizations, even without the use of an explicit octopus motif. We conclude with a call for a deeper analysis of visual rhetoric, and an acknowledgment of the potential for the design of data visualizations to contribute to harmful or conspiratorial thinking."
1779,679d459debd8ffd557a2b560,cs.HC,https://arxiv.org/pdf/2501.14648,Moving Towards Epistemic Autonomy: A Paradigm Shift for Centering Participant Knowledge,"Leah Hope Ajmani, Talia Bhatt, Michael Ann Devito",Human-Computer Interaction,"Justice, epistemology, and marginalization are rich areas of study in HCI. And yet, we repeatedly find platforms and algorithms that push communities further into the margins. In this paper, we propose epistemic autonomy–—one’s ability to govern knowledge about themselves—as a necessary HCI paradigm for working with marginalized communities. We establish epistemic autonomy by applying the transfeminine principle of autonomy to the problem of epistemic injustice. To articulate the harm of violating one’s epistemic autonomy, we present six stories from two trans women: (1) a transfem online administrator and (2) a transfem researcher. We then synthesize our definition of epistemic autonomy in research into a research paradigm. Finally, we present two variants of common HCI methods, autoethnography and asynchronous remote communities, that stem from these beliefs. We discuss how CHI is uniquely situated to champion this paradigm and, thereby, the epistemic autonomy of our research participants."
1780,679d459debd8ffd557a2b561,cs.HC,https://arxiv.org/pdf/2501.14327,Characterizing Visual Intents for People with Low Vision through Eye Tracking,"Ru Wang, Ruijia Chen, Anqiao Erica Cai, Zhiyuan Li, Sanbrita Mondal, Yuhang Zhao",Human-Computer Interaction,"Accessing visual information is crucial yet challenging for people with low vision due to their visual conditions (e.g., low visual acuity, limited visual field). However, unlike blind people, low vision people have and prefer using their functional vision in daily tasks. Gaze patterns thus become an important indicator to uncover their visual challenges and intents, inspiring more adaptive visual support. We seek to deeply understand low vision users’ gaze behaviors in different image viewing tasks, characterizing typical visual intents and the unique gaze patterns exhibited by people with different low vision conditions. We conducted a retrospective think-aloud study using eye tracking with 14 low vision participants and nine sighted controls. Participants completed various image viewing tasks and watched the playback of their gaze trajectories to reflect on their visual experiences. Based on the study, we derived a visual intent taxonomy with five intents characterized by participants’ gaze behaviors and demonstrated how low vision conditions affect gaze patterns across visual intents. Our findings underscore the importance of combining visual ability information, image context, and eye tracking data in visual intent recognition, setting up a foundation for intent-aware assistive technologies for low vision."
1781,679d459debd8ffd557a2b562,cs.HC,https://arxiv.org/pdf/2501.14179,AI Chatbots as Professional Service Agents: Developing a Professional Identity,"Wenwen Li, Kangwei Shi, Yidong Chai",Human-Computer Interaction,"With the rapid expansion of large language model (LLM) applications, there is an emerging shift in the role of LLM-based AI chatbots from serving merely as general inquiry tools to acting as professional service agents. However, current studies often overlook a critical aspect of professional service agents: the act of communicating in a manner consistent with their professional identities. This is of particular importance in the healthcare sector, where effective communication with patients is essential for achieving professional goals, such as promoting patient well-being by encouraging healthy behaviors. To bridge this gap, we propose LAPI (LLM-based Agent with a Professional Identity), a novel framework for designing professional service agent tailored for medical question-and-answer (Q&A) services, ensuring alignment with a specific professional identity. Our method includes a theory-guided task planning process that decomposes complex professional tasks into manageable subtasks aligned with professional objectives and a pragmatic entropy method designed to generate professional and ethical responses with low uncertainty. Experiments on various LLMs show that the proposed approach outperforms baseline methods, including few-shot prompting, chain-of-thought prompting, across key metrics such as fluency, naturalness, empathy, patient-centricity, and ROUGE-L scores. Additionally, the ablation study underscores the contribution of each component to the overall effectiveness of the approach."
1782,679d459debd8ffd557a2b563,cs.HC,https://arxiv.org/pdf/2501.14133,Development of a Validation and Inspection Tool for Armband-based Lifelog Data (VITAL) to Facilitate the Clinical Use of Wearable Data: A Prototype and Usability Evaluation,"Im Eunyoung, Kang Sunghoon, Kim Hyeoneui","Human-Computer Interaction, Systems and Control",
1783,679d459debd8ffd557a2b564,cs.HC,https://arxiv.org/pdf/2501.14110,Developing a Fair Online Recruitment Framework Based on Job-seekers' Fairness Concerns,"Changyang He, Yue Deng, Alessandro Fabris, Bo Li, Asia Biega",Human-Computer Interaction,"The susceptibility to biases and discrimination is a pressing issue in today’s labor markets. Though digital recruitment systems play an increasingly significant role in human resources management, thus far we lack a systematic understanding of human-centered design principles for fair online hiring. This work proposes a fair recruitment framework based on job-seekers’ fairness concerns shared in an online forum. Through qualitative analysis, we uncover four overarching themes of job-seekers’ fairness concerns, including discrimination against sensitive attributes, interaction biases, improper interpretations of qualifications, and power imbalance. Based on these findings, we derive design implications for algorithms and interfaces in recruitment systems, integrating them into a fair recruitment framework spanning different hiring stages and fairness considerations."
1784,679d459debd8ffd557a2b565,cs.HC,https://arxiv.org/pdf/2501.14092,Capital and CHI: Technological Capture and How It Structures CHI Research,Eric Gilbert,Human-Computer Interaction,"This paper advances a theoretical argument about the role capital plays in structuring CHI research. We introduce the concept oftechnological captureto theorize the mechanism by which this happens. Using this concept, we decompose the effect on CHI into four broad forms: technological capture createsmarket-creating, market-expanding, market-aligned,andexternality-reducingCHI research. We place differentCHIsubcommunities into these forms—arguing that many of their values are inherited from capital underlying the field. Rather than a disciplinary- or conference-oriented conceptualization of the field, this work theorizes CHI as tightly-coupled with capital via technological capture. The paper concludes by discussing some implications for CHI."
1785,679d459debd8ffd557a2b566,cs.HC,https://arxiv.org/pdf/2501.14530,Design and Implementation of a Psychiatry Resident Training System Based on Large Language Models,"Zhenguang Zhong, Jia Tang","Computers and Society, Human-Computer Interaction, Neurons and Cognition",
1786,679d459debd8ffd557a2b567,cs.HC,https://arxiv.org/pdf/2501.14163,Reddit Rules and Rulers: Quantifying the Link Between Rules and Perceptions of Governance across Thousands of Communities,"Leon Leibmann, Galen Weld, Amy X. Zhang, Tim Althoff","Social and Information Networks, Computers and Society, Human-Computer Interaction",
1787,679d459debd8ffd557a2b568,cs.HC,https://arxiv.org/pdf/2501.14098,"Exploring User Perspectives on Data Collection, Data Sharing Preferences, and Privacy Concerns with Remote Healthcare Technology","Daniela Napoli, Heather Molyneaux, Helene Fournier, Sonia Chiasson","Computers and Society, Human-Computer Interaction","Remote healthcare technology can help tackle societal issues by improving access to quality healthcare services and enhancing diagnoses through in-place monitoring. These services can be implemented through a combination of mobile devices, applications, wearable sensors, and other smart technology. It is paramount to handle sensitive data that is collected in ways that meet users’ privacy expectations. We surveyed 384 people in Canada aged 20 to 93 years old to explore participants’ comfort with data collection, sharing preferences, and potential privacy concerns related to remote healthcare technology. We explore these topics within the context of various healthcare scenarios including health emergencies and managing chronic health conditions."
1788,679d459debd8ffd557a2b569,cs.IR,https://arxiv.org/pdf/2501.18536,"Illusions of Relevance: Using Content Injection Attacks to Deceive Retrievers, Rerankers, and LLM Judges","Manveer Singh Tamber, Jimmy Lin",Information Retrieval,"Consider a scenario in which a user searches for information, only to encounter texts flooded with misleading or non-relevant content.
This scenario exemplifies a simple yet potent vulnerability in neural Information Retrieval (IR) pipelines:content injection attacks.
We find that embedding models for retrieval, rerankers, and large language model (LLM) relevance judges are vulnerable to these attacks, in which adversaries insert misleading text into passages to manipulate model judgements.
We identify two primary threats: (1) inserting unrelated or harmful content within passages that still appear deceptively “relevant”, and (2) inserting entire queries or key query terms into passages to boost their perceived relevance.
While the second tactic has been explored in prior research, we present, to our knowledge, the first empirical analysis of the first threat, demonstrating how state-of-the-art models can be easily misled.
Our study systematically examines the factors that influence an attack’s success, such as the placement of injected content and the balance between relevant and non-relevant material.
Additionally, we explore various defense strategies, including adversarial passage classifiers, retriever fine-tuning to discount manipulated content, and prompting LLM judges to adopt a more cautious approach.
However, we find that these countermeasures often involve trade-offs, sacrificing effectiveness for attack robustness and sometimes penalizing legitimate documents in the process.
Our findings highlight the need for stronger defenses against these evolving adversarial strategies to maintain the trustworthiness of IR systems.
We release our code and scripts to facilitate further research111https://github.com/manveertamber/content_injection_attacks."
1789,679d459debd8ffd557a2b56a,cs.IR,https://arxiv.org/pdf/2501.18216,Behavior Modeling Space Reconstruction for E-Commerce Search,"Yejing Wang, Chi Zhang, Xiangyu Zhao, Qidong Liu, Maolin Wang, Xuewei Tao, Zitao Liu, Xing Shi, Xudong Yang, Ling Zhong, Wei Lin",Information Retrieval,"Delivering superior search services is crucial for enhancing customer experience and driving revenue growth in e-commerce. Conventionally, search systems model user behaviors by combining user preference and query-item relevance statically, often through a fixed logical ‘and’ relationship.
This paper reexamines existing approaches through a unified lens using both causal graphs and Venn diagrams, uncovering two prevalent yet significant issues: entangled preference and relevance effects, and a collapsed modeling space.
To surmount these challenges, our research introduces a novel framework, DRP, which enhances search accuracy through two components to reconstruct the behavior modeling space. Specifically, we implement preference editing to proactively remove the relevance effect from preference predictions, yielding untainted user preferences. Additionally, we employ adaptive fusion, which dynamically adjusts fusion criteria to align with the varying patterns of relevance and preference, facilitating more nuanced and tailored behavior predictions within the reconstructed modeling space.
Empirical validation on two public datasets and a proprietary e-commerce search dataset underscores the superiority of our proposed methodology, demonstrating marked improvements in performance over existing approaches. The code is available athttps://github.com/Applied-Machine-Learning-Lab/DRP."
1790,679d459debd8ffd557a2b56b,cs.IR,https://arxiv.org/pdf/2501.18177,Investigating Tax Evasion Emergence Using Dual Large Language Model and Deep Reinforcement Learning Powered Agent-based Simulation,"Teddy Lazebnik, Labib Shami","Information Retrieval, Computers and Society, Multiagent Systems","Tax evasion, usually the largest component of an informal economy, is a persistent challenge over history with significant socio-economic implications. Many socio-economic studies investigate its dynamics, including influencing factors, the role and influence of taxation policies, and the prediction of the tax evasion volume over time. These studies assumed such behavior is given, as observed in the real world, neglecting the\saybig bang of such activity in a population. To this end, computational economy studies adopted developments in computer simulations, in general, and recent innovations in artificial intelligence (AI), in particular, to simulate and study informal economy appearance in various socio-economic settings.
This study presents a novel computational framework to examine the dynamics of tax evasion and the emergence of informal economic activity. Employing an agent-based simulation powered by Large Language Models and Deep Reinforcement Learning, the framework is uniquely designed to allow informal economic behaviors to emerge organically, without presupposing their existence or explicitly signaling agents about the possibility of evasion. This provides a rigorous approach for exploring the socio-economic determinants of compliance behavior. The experimental design, comprising model validation and exploratory phases, demonstrates the framework’s robustness in replicating theoretical economic behaviors. Findings indicate that individual personality traits, external narratives, enforcement probabilities, and the perceived efficiency of public goods provision significantly influence both the timing and extent of informal economic activity. The results underscore that efficient public goods provision and robust enforcement mechanisms are complementary; neither alone is sufficient to curtail informal activity effectively. By modeling the emergence of informal economic behavior without assumptions, this research advances the theoretical and practical understanding of tax compliance, offering critical policy insights for designing equitable tax systems and fostering sustainable economic governance."
1791,679d459debd8ffd557a2b56c,cs.IR,https://arxiv.org/pdf/2501.18117,Improving Minimax Group Fairness in Sequential Recommendation,"Krishna Acharya, David Wardrope, Timos Korres, Aleksandr Petrov, Anders Uhrenholt",Information Retrieval,"Training sequential recommenders such as SASRec with uniform sample weights achieves good overall performance but can fall short on specific user groups. One such example is popularity bias, where mainstream users receive better recommendations than niche content viewers. To improve recommendation quality across diverse user groups, we explore three Distributionally Robust Optimization(DRO) methods: Group DRO, Streaming DRO, and Conditional Value at Risk (CVaR) DRO. While Group and Streaming DRO rely on group annotations and struggle with users belonging to multiple groups, CVaR does not require such annotations and can naturally handle overlapping groups. In experiments on two real-world datasets, we show that the DRO methods outperform standard training, with CVaR delivering the best results. Additionally, we find that Group and Streaming DRO are sensitive to the choice of group used for loss computation. Our contributions include (i) a novel application of CVaR to recommenders, (ii) showing that the DRO methods improve group metrics as well as overall performance, and (iii) demonstrating CVaR’s effectiveness in the practical scenario of intersecting user groups. Our code is available athttps://github.com/krishnacharya/sequentialrec-fairness"
1792,679d459debd8ffd557a2b56d,cs.IR,https://arxiv.org/pdf/2501.18056,RL-based Query Rewriting with Distilled LLM for online E-Commerce Systems,"Duy A. Nguyen, Rishi Kesav Mohan, Van Yang, Pritom Saha Akash, Kevin Chen-Chuan Chang",Information Retrieval,"Query rewriting (QR) is a critical technique in e-commerce search, addressing the lexical gap between user queries and product descriptions to enhance search performance. Existing QR approaches typically fall into two categories: discriminative models and generative methods leveraging large language models (LLMs). Discriminative models often struggle with natural language understanding and offer limited flexibility in rewriting, while generative LLMs, despite producing high-quality rewrites, face high inference latency and cost in online settings. These limitations force offline deployment, making them vulnerable to issues like information staleness and semantic drift.
To overcome these challenges, we propose a novel hybrid pipeline for QR that balances efficiency and effectiveness. Our approach combinesoffline knowledge distillationto create a lightweight but efficient student model withonline reinforcement learning (RL)to refine query rewriting dynamically using real-time feedback. A key innovation is the use of LLMs assimulated human feedback, enabling scalable reward signals and cost-effective evaluation without manual annotations. Experimental results on Amazon ESCI dataset demonstrate significant improvements in query relevance, diversity, and adaptability, as well as positive feedback from the LLM simulation.
This work contributes to advancing LLM capabilities for domain-specific applications, offering a robust solution for dynamic and complex e-commerce search environments."
1793,679d459debd8ffd557a2b56e,cs.IR,https://arxiv.org/pdf/2501.17981,Can Generative LLMs Create Query Variants for Test Collections? An Exploratory Study,"Marwah Alaofi, Luke Gallagher, Mark Sanderson, Falk Scholer, Paul Thomas",Information Retrieval,"This paper explores the utility of a Large Language Model (LLM) to automatically generate queries and query variants from a description of an information need.
Given a set of information needs described as backstories, we explore how similar the queries generated by the LLM are to those generated by humans. We quantify the similarity using different metrics and examine how the use of each set would contribute to document pooling when building test collections. Our results show potential in using LLMs to generate query variants. While they may not fully capture the wide variety of human-generated variants, they generate similar sets of relevant documents, reaching up to 71.1% overlap at a pool depth of 100."
1794,679d459debd8ffd557a2b56f,cs.IR,https://arxiv.org/pdf/2501.17969,LLMs can be Fooled into Labelling a Document as Relevant (best caf\'e near me; this paper is perfectly relevant),"Marwah Alaofi, Paul Thomas, Falk Scholer, Mark Sanderson",Information Retrieval,"Large Language Modelsare increasingly being used to assess the relevance of information objects. This work reports on experiments to study the labelling of short texts (i.e., passages) for relevance, using multiple open-source and proprietaryLLMs. While the overall agreement of someLLMswith human judgements is comparable to human-to-human agreement measured in previous research,LLMsare more likely to label passages as relevant compared to human judges, indicating thatLLMlabels denoting non-relevance are more reliable than those indicating relevance."
1795,679d459debd8ffd557a2b570,cs.IR,https://arxiv.org/pdf/2501.17788,WARP: An Efficient Engine for Multi-Vector Retrieval,"Jan Luca Scheerer, Matei Zaharia, Christopher Potts, Gustavo Alonso, Omar Khattab",Information Retrieval,"We study the efficiency of multi-vector retrieval methods like ColBERT and its recent variant XTR. We introduceWARP, a retrieval engine that drastically improves the efficiency of XTR-based ColBERT retrievers through three key innovations: (1) WARPSELECTSELECT{}_{\text{SELECT}}start_FLOATSUBSCRIPT SELECT end_FLOATSUBSCRIPTfor dynamic similarity imputation, (2) implicit decompression during retrieval, and (3) a two-stage reduction process for efficient scoring. Thanks also to highly-optimized C++ kernels and to the adoption of specialized inference runtimes,WARPcan reduce end-to-end query latency relative to XTR’s reference implementation by 41x, and thereby achieves a 3x speedup over the official ColBERTv2 PLAID engine, while preserving retrieval quality."
1796,679d459debd8ffd557a2b571,cs.IR,https://arxiv.org/pdf/2501.17670,Distinguished Quantized Guidance for Diffusion-based Sequence Recommendation,"Wenyu Mao, Shuchang Liu, Haoyang Liu, Haozhe Liu, Xiang Li, Lanatao Hu",Information Retrieval,
1797,679d459debd8ffd557a2b572,cs.IR,https://arxiv.org/pdf/2501.17409,Value Function Decomposition in Markov Recommendation Process,"Xiaobei Wang, Shuchang Liu, Qingpeng Cai, Xiang Li, Lantao Hu, Han li, Guangming Xie",Information Retrieval,"Recent advances in recommender systems have shown that user-system interaction essentially formulates long-term optimization problems, and online reinforcement learning can be adopted to improve recommendation performance.
The general solution framework incorporates a value function that estimates the user’s expected cumulative rewards in the future and guides the training of the recommendation policy.
To avoid local maxima, the policy may explore potential high-quality actions during inference to increase the chance of finding better future rewards.
To accommodate the stepwise recommendation process, one widely adopted approach to learning the value function is learning from the difference between the values of two consecutive states of a user.
However, we argue that this paradigm involves an incorrect approximation in the stochastic process.
Specifically, between the current state and the next state in each training sample, there exist two separate random factors from the stochastic policy and the uncertain user environment.
Original temporal difference (TD) learning under these mixed random factors may result in a suboptimal estimation of the long-term rewards.
As a solution, we show that these two factors can be separately approximated by decomposing the original temporal difference loss.
The disentangled learning framework can achieve a more accurate estimation with faster learning and improved robustness against action exploration.
As empirical verification of our proposed method, we conduct offline experiments with online simulated environments built based on public datasets."
1798,679d459debd8ffd557a2b573,cs.IR,https://arxiv.org/pdf/2501.17039,Enhanced Retrieval of Long Documents: Leveraging Fine-Grained Block Representations with Large Language Models,"Minghan Li, Eric Gaussier, Guodong Zhou",Information Retrieval,"In recent years, large language models (LLMs) have demonstrated exceptional power in various domains, including information retrieval. Most of the previous practices involve leveraging these models to create a single embedding for each query, each passage, or each document individually, a strategy exemplified and used by the Retrieval-Augmented Generation (RAG) framework. While this method has proven effective, we argue that it falls short in fully capturing the nuanced intricacies of document-level texts due to its reliance on a relatively coarse-grained representation. To address this limitation, we introduce a novel, fine-grained approach aimed at enhancing the accuracy of relevance scoring for long documents. Our methodology firstly segments a long document into blocks, each of which is embedded using an LLM, for matching with the query representation. When calculating the relevance score, we aggregate the query-block relevance scores through a weighted sum method, yielding a comprehensive score for the query with the entire document. Despite its apparent simplicity, our experimental findings reveal that this approach outperforms standard representation methods and achieves a significant reduction in embedding generation latency. Moreover, by carefully optimizing pairwise loss functions, superior performances have been achieved."
1799,679d459debd8ffd557a2b574,cs.IR,https://arxiv.org/pdf/2501.16902,Document Screenshot Retrievers are Vulnerable to Pixel Poisoning Attacks,"Shengyao Zhuang, Ekaterina Khramtsova, Xueguang Ma, Bevan Koopman, Jimmy Lin, Guido Zuccon",Information Retrieval,"Recent advancements in dense retrieval have introduced vision-language model (VLM)-based retrievers, such as DSE and ColPali, which leverage document screenshots embedded as vectors to enable effective search and offer a simplified pipeline over traditional text-only methods.
In this study, we propose three pixel poisoning attack methods designed to compromise VLM-based retrievers and evaluate their effectiveness under various attack settings and parameter configurations. Our empirical results demonstrate that injecting even a single adversarial screenshot into the retrieval corpus can significantly disrupt search results, poisoning the top-10 retrieved documents for 41.9% of queries in the case of DSE and 26.4% for ColPali. These vulnerability rates notably exceed those observed with equivalent attacks on text-only retrievers. Moreover, when targeting a small set of known queries, the attack success rate raises, achieving complete success in certain cases.
By exposing the vulnerabilities inherent in vision-language models, this work highlights the potential risks associated with their deployment."
1800,679d459debd8ffd557a2b575,cs.IR,https://arxiv.org/pdf/2501.16125,SampleLLM: Optimizing Tabular Data Synthesis in Recommendations,"Jingtong Gao, Zhaocheng Du, Xiaopeng Li, Xiangyu Zhao, Yichao Wang, Xiangyang Li, Huifeng Guo, Ruiming Tang",Information Retrieval,"Tabular data synthesis is crucial in machine learning, yet existing general methods-primarily based on statistical or deep learning models-are highly data-dependent and often fall short in recommender systems. This limitation arises from their difficulty in capturing complex distributions and understanding complicated feature relations from sparse and limited data, along with their inability to grasp semantic feature relations. Recently, Large Language Models (LLMs) have shown potential in generating synthetic data through few-shot learning and semantic understanding. However, they often suffer from inconsistent distribution and lack of diversity due to their inherent distribution disparity with the target dataset. To address these challenges and enhance tabular data synthesis for recommendation tasks, we propose a novel two-stage framework named SampleLLM to improve the quality of LLM-based tabular data synthesis for recommendations by ensuring better distribution alignment. In the first stage, SampleLLM employs LLMs with Chain-of-Thought prompts and diverse exemplars to generate data that closely aligns with the target dataset distribution, even when input samples are limited. The second stage uses an advanced feature attribution-based importance sampling method to refine feature relationships within the synthetic data, reducing any distribution biases introduced by the LLM. Experimental results on three recommendation datasets, two general datasets, and online deployment illustrate that SampleLLM significantly surpasses existing methods for recommendation tasks and holds promise for a broader range of tabular data scenarios."
1801,679d459debd8ffd557a2b576,cs.IR,https://arxiv.org/pdf/2501.16112,Survey: Understand the challenges of MachineLearning Experts using Named EntityRecognition Tools,"Florian Freund, Philippe Tamla, Matthias Hemmje",Information Retrieval,
1802,679d459debd8ffd557a2b577,cs.IR,https://arxiv.org/pdf/2501.16111,Options-Aware Dense Retrieval for Multiple-Choice query Answering,"Manish Singh, Manish Shrivastava",Information Retrieval,"Long-context multiple-choice question answering tasks require robust reasoning over extensive text sources. Since most of the pre-trained transformer models are restricted to processing only a few hundred words at a time, successful completion of such tasks often relies on the identification of evidence spans, such as sentences, that provide supporting evidence for selecting the correct answer. Prior research in this domain has predominantly utilized pre-trained dense retrieval models, given the absence of supervision to fine-tune the retrieval process. This paper proposes a novel method called Options Aware Dense Retrieval (OADR) to address these challenges. ORDA uses an innovative approach to fine-tuning retrieval by leveraging query-options embeddings, which aim to mimic the embeddings of the oracle query (i.e., the query paired with the correct answer) for enhanced identification of supporting evidence. Through experiments conducted on the QuALITY benchmark dataset, we demonstrate that our proposed model surpasses existing baselines in terms of performance and accuracy.Keywords:Dense Retrieval, Sentence Transformer, Contrastive Training, Multiple-Choice Question Answering"
1803,679d459debd8ffd557a2b578,cs.IR,https://arxiv.org/pdf/2501.15470,Unveiling the Potential of Multimodal Retrieval Augmented Generation with Planning,"Xiaohan Yu, Zhihan Yang, Chong Chen","Information Retrieval, Multiagent Systems","Multimodal Retrieval Augmented Generation (MRAG) systems, while promising for enhancing Multimodal Large Language Models (MLLMs), often rely on rigid, single-step retrieval methods. This limitation hinders their ability to effectively address real-world scenarios that demand adaptive information acquisition and query refinement. To overcome this, we introduce the novel task of Multimodal Retrieval Augmented Generation Planning (MRAG Planning), focusing on optimizing MLLM performance while minimizing computational overhead. We present CogPlanner, a versatile framework inspired by human cognitive processes. CogPlanner iteratively refines queries and selects retrieval strategies, enabling both parallel and sequential modeling approaches. To rigorously evaluate MRAG Planning, we introduce CogBench, a new benchmark specifically designed for this task. CogBench facilitates the integration of lightweight CogPlanner with resource-efficient MLLMs. Our experimental findings demonstrate that CogPlanner surpasses existing MRAG baselines, achieving significant improvements in both accuracy and efficiency with minimal computational overhead."
1804,679d459debd8ffd557a2b579,cs.IR,https://arxiv.org/pdf/2501.15429,An Aspect Performance-aware Hypergraph Neural Network for Review-based Recommendation,"Junrui Liu, Tong Li, Di Wu, Zifang Tang, Yuan Fang, Zhen Yang",Information Retrieval,"Online reviews allow consumers to provide detailed feedback on various aspects of items.
Existing methods utilize these aspects to model users’ fine-grained preferences for specific item features through graph neural networks.
We argue that the performance of items on different aspects is important for making precise recommendations, which has not been taken into account by existing approaches, due to lack of data.
In this paper, we propose an aspect performance-aware hypergraph neural network (APH) for the review-based recommendation, which learns the performance of items from the conflicting sentiment polarity of user reviews.
Specifically, APH comprehensively models the relationships among users, items, aspects, and sentiment polarity by systematically constructing an aspect hypergraph based on user reviews.
In addition, APH aggregates aspects representing users and items by employing an aspect performance-aware hypergraph aggregation method.
It aggregates the sentiment polarities from multiple users by jointly considering user preferences and the semantics of their sentiments, determining the weights of sentiment polarities to infer the performance of items on various aspects.
Such performances are then used as weights to aggregate neighboring aspects.
Experiments on six real-world datasets demonstrate that APH improves MSE, Precision@5, and Recall@5 by an average of2.30%percent2.302.30\%2.30 %,4.89%percent4.894.89\%4.89 %, and1.60%percent1.601.60\%1.60 %over the best baseline.
The source code and data are available athttps://github.com/dianziliu/APH."
1805,679d459debd8ffd557a2b57a,cs.IR,https://arxiv.org/pdf/2501.15425,An Empirically-parametrized Spatio-Temporal Extended-SIR Model for Combined Dilution and Vaccination Mitigation for Rabies Outbreaks in Wild Jackals,"Teddy Lazebnik, Yehuda Samuel, Jonathan Tichon, Roi Lapid, Roni King, Tomer Nissimian, Orr Spiegel","Information Retrieval, Physics and Society","The transmission of zoonotic diseases between animals and humans poses an increasing threat. Rabies is a prominent example with various instances globally, facilitated by a surplus of meso-predators (commonly, facultative synanthropic species e.g., golden jackals [Canis aureus, hereafter jackals]) thanks to the abundance of anthropogenic resources leading to dense populations close to human establishments. To mitigate rabies outbreaks and prevent human infections, authorities target the jackal which is the main rabies vector in many regions, through the dissemination of oral vaccines in known jackals’ activity centers, as well as opportunistic culling to reduce population density. Because dilution (i.e., culling) is not selective towards sick or un-vaccinated individuals, these two complementary epizootic intervention policies (EIPs) can interfere with each other. Nonetheless, there is only limited examination of the interactive effectiveness of these EIPs and their potential influence on rabies epizootic spread dynamics, highlighting the need to understand these measures and the spread of rabies in wild jackals. In this study, we introduce a novel spatio-temporal extended-SIR (susceptible-infected-recovered) model with a graph-based spatial framework for evaluating mitigation efficiency. We implement the model in a case study using a jackal population in northern Israel, and using spatial and movement data collected by Advanced Tracking and Localization of Animals in real-life Systems (ATLAS) telemetry. An agent-based simulation approach allows us to explore various biologically-realistic scenarios, and assess the impact of different EIPs configurations. Our model suggests that under biologically-realistic underlying assumptions and scenarios, the effectiveness of both EIPs is not influenced much by the jackal population size but is sensitive to their dispersal between activity centers. Furthermore, we show both theoretically and empirically that counter-intuitively, there are cases in which under the practice of EIPs, the epizootic (or endemic) spread is increasing (due to interference among them). Our findings emphasize the importance of accurately capturing the local jackal movement dynamics to obtain and predict the desired outcome from an applied EIP configuration."
1806,679d459debd8ffd557a2b57b,cs.IR,https://arxiv.org/pdf/2501.15183,Generating Negative Samples for Multi-Modal Recommendation,"Yanbiao Ji, Yue Ding, Dan Luo, Chang Liu, Jing Tong, Shaokai Wu, Hongtao Lu",Information Retrieval,
1807,679d459debd8ffd557a2b57c,cs.IR,https://arxiv.org/pdf/2501.15118,ABXI: Invariant Interest Adaptation for Task-Guided Cross-Domain Sequential Recommendation,"Qingtian Bian, Marcus Vinícius de Carvalho, Tieying Li, Jiaxing Xu, Hui Fang, Yiping Ke",Information Retrieval,"Cross-Domain Sequential Recommendation (CDSR) has recently gained attention for countering data sparsity by transferring knowledge across domains. A common approach merges domain-specific sequences into cross-domain sequences, serving as bridges to connect domains. One key challenge is to correctly extract the shared knowledge among these sequences and appropriately transfer it. Most existing works directly transfer unfiltered cross-domain knowledge rather than extracting domain-invariant components and adaptively integrating them into domain-specific modelings. Another challenge lies in aligning the domain-specific and cross-domain sequences. Existing methods align these sequences based on timestamps, but this approach can cause prediction mismatches when the current tokens and their targets belong to different domains. In such cases, the domain-specific knowledge carried by the current tokens may degrade performance. To address these challenges, we propose the A-B-Cross-to-Invariant Learning Recommender (ABXI). Specifically, leveraging LoRA’s effectiveness for efficient adaptation, ABXI incorporates two types of LoRAs to facilitate knowledge adaptation. First, all sequences are processed through a shared encoder that employs a domain LoRA for each sequence, thereby preserving unique domain characteristics. Next, we introduce an invariant projector that extracts domain-invariant interests from cross-domain representations, utilizing an invariant LoRA to adapt these interests into modeling each specific domain. Besides, to avoid prediction mismatches, all domain-specific sequences are aligned to match the domains of the cross-domain ground truths. Experimental results on three datasets demonstrate that our approach outperforms other CDSR counterparts by a large margin. The codes are available inhttps://github.com/DiMarzioBian/ABXI."
1808,679d459debd8ffd557a2b57d,cs.IR,https://arxiv.org/pdf/2501.15087,PatchRec: Multi-Grained Patching for Efficient LLM-based Sequential Recommendation,"Jiayi Liao, Ruobing Xie, Sihang Li, Xiang Wang, Xingwu Sun, Zhanhui Kang, Xiangnan He",Information Retrieval,"Large Language Models for sequential recommendation (LLM4SR), which transform user-item interactions into language modeling, have shown promising results.
However, due to the limitations of context window size and the computational costs associated with Large Language Models (LLMs), current approaches primarily truncate user history by only considering the textual information of items from the most recent interactions in the input prompt.
This truncation fails to fully capture the long-term behavioral patterns of users.
To address this, we propose a multi-grained patching framework — PatchRec.
It compresses the textual tokens of an item title into a compact item patch, and further compresses multiple item patches into a denser session patch, with earlier interactions being compressed to a greater degree.
The framework consists of two stages: (1) Patch Pre-training, which familiarizes LLMs with item-level compression patterns, and (2) Patch Fine-tuning, which teaches LLMs to model sequences at multiple granularities.
Through this simple yet effective approach, empirical results demonstrate that PatchRec outperforms existing methods, achieving significant performance gains with fewer tokens fed to the LLM.
Specifically, PatchRec shows up to a 32% improvement in HR@20 on the Goodreads dataset over uncompressed baseline, while using only 7% of the tokens.
This multi-grained sequence modeling paradigm, with an adjustable compression ratio, enables LLMs to be efficiently deployed in real-world recommendation systems that handle extremely long user behavior sequences."
1809,679d459debd8ffd557a2b57e,cs.IR,https://arxiv.org/pdf/2501.14922,Search results diversification in competitive search,"Tommy Mordo, Itamar Reinman, Moshe Tennenholtz, Oren Kurland","Information Retrieval, Computer Science and Game Theory","In Web retrieval, there are many cases of competition between authors of Web documents: their incentive is to have their documents highly ranked for queries of interest. As such, the Web is a prominent example of a competitive search setting. Past work on competitive search focused on ranking functions based solely on relevance estimation. We study ranking functions that integrate a results-diversification aspect. We show that the competitive search setting with diversity-based ranking has an equilibrium. Furthermore, we theoretically and empirically show that the phenomenon of authors mimicking content in documents highly ranked in the past, which was demonstrated in previous work, is mitigated when search results diversification is applied."
1810,679d459debd8ffd557a2b57f,cs.IR,https://arxiv.org/pdf/2501.14579,Knowledge Graphs Construction from Criminal Court Appeals: Insights from the French Cassation Court,"Alexander V. Belikov, Sacha Raoult",Information Retrieval,
1811,679d459debd8ffd557a2b580,cs.IR,https://arxiv.org/pdf/2501.14466,On Correlating Factors for Domain Adaptation Performance,"Goksenin Yuksel, Jaap Kamps","Information Retrieval, Applications","Dense retrievers have demonstrated significant potential for neural information retrieval; however, they lack robustness to domain shifts, limiting their efficacy in zero-shot settings across diverse domains. In this paper, we set out to analyze the possible factors that lead to successful domain adaptation of dense retrievers. We include domain similarity proxies between generated queries to test and source domains. Furthermore, we conduct a case study comparing two powerful domain adaptation techniques. We find that generated query type distribution is an important factor, and generating queries that share a similar domain to the test documents improves the performance of domain adaptation methods. This study further emphasizes the importance of domain-tailored generated queries."
1812,679d459debd8ffd557a2b581,cs.IR,https://arxiv.org/pdf/2501.14296,Multi-stage Large Language Model Pipelines Can Outperform GPT-4o in Relevance Assessment,"Julian A. Schnabel, Johanne R. Trippas, Falk Scholer, Danula Hettiachchi",Information Retrieval,"The effectiveness of search systems is evaluated using relevance labels that indicate the usefulness of documents for specific queries and users. While obtaining these relevance labels from real users is ideal, scaling such data collection is challenging. Consequently, third-party annotators are employed, but their inconsistent accuracy demands costly auditing, training, and monitoring. We propose an LLM-based modular classification pipeline that divides the relevance assessment task into multiple stages, each utilising different prompts and models of varying sizes and capabilities. Applied to TREC Deep Learning (TREC-DL), one of our approaches showed an 18.4% Krippendorff’sα𝛼\alphaitalic_αaccuracy increase over OpenAI’s GPT-4o mini while maintaining a cost of about 0.2 USD per million input tokens, offering a more efficient and scalable solution for relevance assessment. This approach beats the baseline performance of GPT-4o (5 USD). With a pipeline approach, even the accuracy of the GPT-4o flagship model, measured inα𝛼\alphaitalic_α, could be improved by 9.7%."
1813,679d459debd8ffd557a2b582,cs.IT,https://arxiv.org/pdf/2501.18572,Optimum Monitoring and Job Assignment with Multiple Markov Machines,"Sahan Liyanaarachchi, Sennur Ulukus","Information Theory, Systems and Control","We study a class of systems termed Markov Machines (MM) which process job requests with exponential service times. Assuming a Poison job arrival process, these MMs oscillate between two states, free and busy. We consider the problem of sampling the states of these MMs so as to track their states, subject to a total sampling budget, with the goal of allocating external job requests effectively to them. For this purpose, we leverage thebinary freshness metricto quantify the quality of our ability to track the states of the MMs, and introduce two new metrics termedfalse acceptance ratio(FAR) andfalse rejection ratio(FRR) to evaluate the effectiveness of our job assignment strategy. We provide optimal sampling rate allocation schemes for jointly monitoring a system ofN𝑁Nitalic_Nheterogeneous MMs."
1814,679d459debd8ffd557a2b583,cs.IT,https://arxiv.org/pdf/2501.18502,One-Bit Distributed Mean Estimation with Unknown Variance,"Ritesh Kumar, Shashank Vatedka","Information Theory, Statistics Theory","In this work, we study the problem of distributed mean estimation with1111-bit communication constraints when the variance is unknown. We focus on the specific case where each user has access to onei.i.d.sample drawn from a distribution that belongs to a scale-location family, and is limited to sending just a single bit of information to a central server whose goal is to estimate the mean. We propose non-adaptive and adaptive estimators that are shown to be asymptotically normal. We derive bounds on the asymptotic (in the number of users) Mean Squared Error (MSE) achieved by these estimators. For a class of symmetric log-concave distributions, we derive matching lower bounds for the MSE achieved by adaptive estimators, proving the optimality of our scheme. We show that non-adaptive estimators can be strictly suboptimal by deriving a lower bound on the MSE achieved by any non-adaptive estimator for Gaussian distributions and demonstrating a positive gap between this and the MSE achieved by our adaptive scheme."
1815,679d459debd8ffd557a2b584,cs.IT,https://arxiv.org/pdf/2501.18374,Proofs for Folklore Theorems on the Radon-Nikodym Derivative,"Yaiza Bermudez, Gaetan Bisson, Iñaki Esnaola, Samir M. Perlaza","Information Theory, History and Overview, Statistics Theory, Machine Learning",Rigorous statements and formal proofs are presented for both foundational and advanced folklore theorems on the Radon-Nikodym derivative. The cases of product and marginal measures are carefully considered; and the hypothesis under which the statements hold are rigorously enumerated.
1816,679d459debd8ffd557a2b585,cs.IT,https://arxiv.org/pdf/2501.18308,Zero Estimation Cost Strategy for Witsenhausen Counterexample with Causal Encoder,"Mengyuan Zhao, Tobias J. Oechtering, Maël Le Treust",Information Theory,
1817,679d459debd8ffd557a2b586,cs.IT,https://arxiv.org/pdf/2501.18250,Dynamic Model Fine-Tuning For Extreme MIMO CSI Compression,"Mehdi Sattari, Deniz Gündüz, Tommmy Svensson","Information Theory, Signal Processing","Efficient channel state information (CSI) compression is crucial in frequency division duplexing (FDD) massive multiple-input multiple-output (MIMO) systems due to excessive feedback overhead. Recently, deep learning-based compression techniques have demonstrated superior performance across various data types, including CSI. However, these approaches often experience performance degradation when the data distribution changes due to their limited generalization capabilities. To address this challenge, we propose a model fine-tuning approach for CSI feedback in massive MIMO systems. The idea is to fine-tune the encoder/decoder network models in a dynamic fashion using the recent CSI samples.
First, we explore encoder-only fine-tuning, where only the encoder parameters are updated, leaving the decoder and latent parameters unchanged. Next, we consider full-model fine-tuning, where the encoder and decoder models are jointly updated. Unlike encoder-only fine-tuning, full-model fine-tuning requires the updated decoder and latent parameters to be transmitted to the decoder side. To efficiently handle this, we propose different prior distributions for model updates, such as uniform and truncated Gaussian to entropy code them together with the compressed CSI and account for additional feedback overhead imposed by conveying the model updates. Moreover, we incorporate quantized model updates during fine-tuning to reflect the impact of quantization in the deployment phase.
Our results demonstrate that full-model fine-tuning significantly enhances the rate-distortion (RD) performance of neural CSI compression. Furthermore, we analyze how often the full-model fine-tuning should be applied in a new wireless environment and identify an optimal period interval for achieving the best RD trade-off."
1818,679d459debd8ffd557a2b587,cs.IT,https://arxiv.org/pdf/2501.18236,RIS-assisted Physical Layer Security,"Xudong Li, Matthias Frey, Ehsan Tohidi, Igor Bjelaković, Sławomir Stańczak","Information Theory, Signal Processing","We propose a reconfigurable intelligent surface (RIS)-assisted wiretap channel, where the RIS is strategically deployed to provide a spatial separation to the transmitter, and orthogonal combiners are employed at the legitimate receiver to extract the data streams from the direct and RIS-assisted links. Then we derive the achievable secrecy rate under semantic security for the RIS-assisted channel and design an algorithm for the secrecy rate optimization problem. The simulation results show the effects of total transmit power, the location and number of eavesdroppers on the security performance."
1819,679d459debd8ffd557a2b588,cs.IT,https://arxiv.org/pdf/2501.18080,PAC Codes Meet CRC-Polar Codes,"Xinyi Gu, Mohammad Rowshan, Jinhong Yuan",Information Theory,"CRC-Polar codes under SC list decoding are well-regarded for their competitive error performance. This paper examines these codes by focusing on minimum weight codewords and breaking them down into the rows of the polar transform. Inspired by the significant impact of parity check bits and their positions, we apply a shifted rate-profile for polarization-adjusted convolutional (PS-PAC) codes, thereby achieving similar improvements in the weight distribution of polar codes through precoding. The results demonstrate a significant improvement in error performance, achieving up to a 0.5 dB power gain with short PS-PAC codes.
Additionally, leveraging convolutional precoding in PAC codes, we adopt a continuous deployment (masking) of parity check bits derived from the remainder of continuous division of the partial message polynomial and the CRC polynomial over frozen positions in the rate-profile. This approach enhances performance for long codes, with an overall improvement of 0.12 dB."
1820,679d459debd8ffd557a2b589,cs.IT,https://arxiv.org/pdf/2501.18058,Power-Efficient Over-the-Air Aggregation with Receive Beamforming for Federated Learning,"Faeze Moradi Kalarde, Min Dong, Ben Liang, Yahia A. Eldemerdash Ahmed, Ho Ting Cheng","Information Theory, Signal Processing","This paper studies power-efficient uplink transmission design for federated learning (FL) that employs over-the-air analog aggregation and multi-antenna beamforming at the server. We jointly optimize device transmit weights and receive beamforming at each FL communication round to minimize the total device transmit power while ensuring convergence in FL training. Through our convergence analysis, we establish sufficient conditions on the aggregation error to guarantee FL training convergence. Utilizing these conditions, we reformulate the power minimization problem into a unique bi-convex structure that contains a transmit beamforming optimization subproblem and a receive beamforming feasibility subproblem. Despite this unconventional structure, we propose a novel alternating optimization approach that guarantees monotonic decrease of the objective value, to allow convergence to a partial optimum.
We further consider imperfect channel state information (CSI),
which requires accounting for the channel estimation errors in the power minimization problem and FL convergence analysis.
We propose a CSI-error-aware joint beamforming algorithm, which can substantially outperform one that does not account for channel estimation errors.
Simulation with canonical classification datasets demonstrates that our proposed methods achieve significant power reduction compared to existing benchmarks across a wide range of parameter settings, while attaining the same target accuracy under the same convergence rate."
1821,679d459debd8ffd557a2b58a,cs.IT,https://arxiv.org/pdf/2501.18587,Entropy functionals and equilibrium states in mixed quantum-classical dynamics,"Cesare Tronci, David Martínez-Crespo, François Gay-Balmaz","Quantum Physics, Statistical Mechanics, Information Theory, Mathematical Physics, Chemical Physics","The computational challenges posed by many-particle quantum systems are often overcome by mixed quantum-classical (MQC) models in which certain degrees of freedom are treated as classical while others are retained as quantum. One of the fundamental questions raised by this hybrid picture involves the characterization of the information associated to MQC systems. Based on the theory of dynamical invariants in Hamiltonian systems, here we propose a family of hybrid entropy functionals that consistently specialize to the usual Rényi and Shannon entropies. Upon considering the MQC Ehrenfest model for the dynamics of quantum and classical probabilities, we apply the hybrid Shannon entropy to characterize equilibrium configurations for simple Hamiltonians. The present construction also applies beyond Ehrenfest dynamics."
1822,679d459debd8ffd557a2b58b,cs.IT,https://arxiv.org/pdf/2501.18382,Rydberg Atomic Quantum Receivers for the Multi-User MIMO Uplink,"Tierui Gong, Chau Yuen, Chong Meng Samson See, Mérouane Debbah, Lajos Hanzo","Signal Processing, Information Theory","Rydberg atomic quantum receivers exhibit great potential in assisting classical wireless communications due to their outstanding advantages in detecting radio frequency signals. To realize this potential, we integrate a Rydberg atomic quantum receiver into a classical multi-user multiple-input multiple-output (MIMO) scheme to form a multi-user Rydberg atomic quantum MIMO (RAQ-MIMO) system for the uplink.
To study this system, we first construct an equivalent baseband signal model, which facilitates convenient system design, signal processing and optimizations. We then study the ergodic achievable rates under both the maximum ratio combining (MRC) and zero-forcing (ZF) schemes by deriving their tight lower bounds. We next compare the ergodic achievable rates of the RAQ-MIMO and the conventional massive MIMO schemes by offering a closed-form expression for the difference of their ergodic achievable rates, which allows us to directly compare the two systems. Our results show that RAQ-MIMO allows the average transmit power of users to be∼20similar-toabsent20\sim 20∼ 20dBm lower than that of the conventional massive MIMO.
Viewed from a different perspective, an extra∼7similar-toabsent7\sim 7∼ 7bits/s/Hz/user rate becomes achievable by ZF RAQ-MIMO, when equipping50∼500similar-to5050050\sim 50050 ∼ 500receive elements for receiving1∼100similar-to11001\sim 1001 ∼ 100user signals at an enough transmit power (e.g.,≥20absent20\geq 20≥ 20dBm)."
1823,679d459debd8ffd557a2b58c,cs.IT,https://arxiv.org/pdf/2501.17876,SCDM: Score-Based Channel Denoising Model for Digital Semantic Communications,"Hao Mo, Yaping Sun, Shumin Yao, Hao Chen, Zhiyong Chen, Xiaodong Xu, Meixia Tao, Shuguang Cui","Signal Processing, Information Theory","Score-based diffusion models represent a significant variant within the diffusion model family and have seen extensive application
in the increasingly popular domain of generative tasks. Recent investigations have explored the denoising potential of diffusion models in
semantic communications. However, in previous paradigms, noise distortion in the diffusion process
does not match precisely with digital channel noise characteristics. In this work, we introduce the Score-Based Channel Denoising Model (SCDM) for
Digital Semantic Communications (DSC). SCDM views the distortion of constellation symbol sequences in digital transmission as a score-based forward diffusion process.
We design a tailored forward noise corruption to align digital channel noise properties in the training phase. During the inference stage, the well-trained SCDM can
effectively denoise received semantic symbols under various SNR conditions, reducing the difficulty for the semantic decoder in extracting semantic information
from the received noisy symbols and thereby enhancing the robustness of the reconstructed semantic information. Experimental results show that SCDM outperforms the
baseline model in PSNR, SSIM, and MSE metrics, particularly at low SNR levels. Moreover, SCDM reduces storage requirements by a factor of 7.8.
This efficiency in storage, combined with its robust denoising capability, makes SCDM a practical solution for DSC across diverse channel conditions."
1824,679d459debd8ffd557a2b58d,cs.IT,https://arxiv.org/pdf/2501.17777,On decoding hyperbolic codes,"Eduardo Camps-Moreno, Ignacio García-Marco, Hiram H. López, Irene Márquez-Corbella, Edgar Martínez-Moro, Eliseo Sarmiento","Information Theory, Combinatorics","This work studies several decoding algorithms for hyperbolic codes. We use some previous ideas to describe how to decode a hyperbolic code using the largest Reed-Muller code contained in it or using the smallest Reed-Muller code that contains it. A combination of these two algorithms is proposed when hyperbolic codes are defined by polynomials in two variables. Then, we compare hyperbolic codes and Cube codes (tensor product of Reed-Solomon codes) and propose decoding algorithms of hyperbolic codes based on their closest Cube codes. Finally, we adapt to hyperbolic codes the Geil and Matsumoto’s generalization of Sudan’s list decoding algorithm.††footnotetext:E. Camps-Moreno, H. H. López, E. Martínez-Moro and I. Márquez-Corbella were partially supported by Grant TED2021-130358B-I00 funded by MCIU/AEI/10.13039/501100011033 and by the ”European Union NextGenerationEU/PRTR”.H. H. López was partially supported by the NSF grants DMS-2201094 and DMS-2401558.I. García-Marco and I. Márquez-Corbella were partially supported by the Spanish MICINN PID2019-105896GB-I00"
1825,679d459debd8ffd557a2b58e,cs.IT,https://arxiv.org/pdf/2501.17706,Source-Channel Separation Theorems for Distortion Perception Coding,"Chao Tian, Jun Chen, Krishna Narayanan",Information Theory,"It is well known that separation between lossy source coding and channel coding is asymptotically optimal under classical additive distortion measures. Recently, coding under a new class of quality considerations, often referred to as perception or realism, has attracted significant attention due to its close connection to neural generative models and semantic communications. In this work, we revisit source-channel separation under the consideration of distortion-perception. We show that when the perception quality is measured on the block level, i.e., in the strong-sense, the optimality of separation still holds when common randomness is shared between the encoder and the decoder; however, separation is no longer optimal when such common randomness is not available. In contrast, when the perception quality is the average per-symbol measure, i.e., in the weak-sense, the optimality of separation holds regardless of the availability of common randomness."
1826,679d459debd8ffd557a2b58f,cs.IT,https://arxiv.org/pdf/2501.17644,Efficient Stochastic Polar Decoder With Correlated Stochastic Computing,"Jiaxing Li, Shuwen Zhang, Zhisong Bie","Information Theory, Signal Processing","Polar codes have gained significant attention in channel coding for their ability to approach the capacity of binary input discrete memoryless channels (B-DMCs), thanks to their reliability and efficiency in transmission. However, existing decoders often struggle to balance hardware area and performance. Stochastic computing offers a way to simplify circuits, and previous work has implemented decoding using this approach. A common issue with these methods is performance degradation caused by the introduction of correlation. This paper presents an Efficient Correlated Stochastic Polar Decoder (ECS-PD) that fundamentally addresses the issue of the ‘hold-state’, preventing it from increasing as correlation computation progresses. We propose two optimization strategies aimed at reducing iteration latency, increasing throughput, and simplifying the circuit to improve hardware efficiency. The optimization can reduce the number of iterations by 25.2% at𝑬𝒃/𝑵𝟎subscript𝑬𝒃subscript𝑵0\boldsymbol{E_{b}/N_{0}}bold_italic_E start_POSTSUBSCRIPT bold_italic_b end_POSTSUBSCRIPT bold_/ bold_italic_N start_POSTSUBSCRIPT bold_0 end_POSTSUBSCRIPT= 3 dB. Compared to other efficient designs, the proposed ECS-PD achieves higher throughput and is 2.7 times more hardware-efficient than the min-sum decoder."
1827,679d459debd8ffd557a2b590,cs.IT,https://arxiv.org/pdf/2501.17554,Information Theory for Expectation Measures,Peter Harremoës,"Information Theory, Probability","Shannon based his information theory on the notion of probability
measures as it we developed by Kolmogorov. In this paper we study
some fundamental problems in information theory based on expectation
measures. In the theory of expectation measures it is natural to study
data sets where no randomness is present and it is also natural to
study information theory for point processes as well as sampling where
the sample size is not fixed. Expectation measures in combination
with Kraft’s Inequality can be used to clarify in which cases probability
measures can be used to quantify randomness."
1828,679d459debd8ffd557a2b591,cs.IT,https://arxiv.org/pdf/2501.17476,Hybrid Channel- and Coding-Based Challenge-Response Physical-Layer Authentication,"Laura Crosara, Mahtab Mirmohseni, Stefano Tomasin","Information Theory, Signal Processing","This letter proposes a new physical layer authentication mechanism operating at the physical layer of a communication system where the receiver has partial control of the channel conditions (e.g., using an intelligent reflecting surface). We aim to exploit both instantaneous channel state information (CSI) and a secret shared key for authentication. This is achieved by both transmitting an identifying key by wiretap coding (to conceal the key from the attacker) and checking that the instantaneous CSI corresponds to the channel configuration randomly selected by the receiver. We investigate the trade-off between the pilot signals used for CSI estimation and the coding rate (or key length) to improve the overall security of the authentication procedure."
1829,679d459debd8ffd557a2b592,cs.IT,https://arxiv.org/pdf/2501.17473,Remote State Estimation over a Wearing Channel: Information Freshness vs. Channel Aging,"Jiping Luo, George Stamatakis, Osvaldo Simeone, Nikolaos Pappas","Information Theory, Systems and Control","We study the remote estimation of a linear Gaussian system over anonstationarychannel thatwears out over time and with every use. The sensor can either transmit a fresh measurement in the current time slot, restore the channel quality at the cost of downtime, or remain silent. More frequent transmissions yield accurate estimates but incur significant wear on the channel. Renewing the channel too often improves channel conditions but results in poor estimation quality. What is the optimal timing to transmit measurements and restore the channel? We formulate the problem as a Markov decision process (MDP) and show the monotonicity properties of an optimal policy. A structured policy iteration algorithm is proposed to find the optimal policy."
1830,679d459debd8ffd557a2b593,cs.IT,https://arxiv.org/pdf/2501.17412,Randomized Scheduling for Periodic Multi-Source Systems with PAoI Violation Guarantees,"Kuan-Yu Lin, Wei-Lun Lu, Yu-Pin Hsu, Yu-Chih Huang",Information Theory,"The Age of Information (AoI) has been recognized as a critical metric for assessing the freshness of information in modern communication systems.
In this work, we examine an information update system where multiple information sources transmit updates to their respective destinations via a shared base station. Our main contribution is the proposal of a randomized scheduling algorithm that offers distinct statistical AoI guarantees for heterogeneous sources. Specifically, we rigorously derive an analytical upper bound on peak age of information (PAoI) violation probability by leveraging properties of the multivariate noncentral hypergeometric Wallenius distribution. Building on these analytical results, two designs of coefficients for the randomized policy are proposed to meet the outage constraints for all sources, tailored to the long and short sampling delay cases, respectively.
Simulation results demonstrate the accuracy of our analysis on PAoI violation probability and also show that our proposed design always provides a feasible solution in most cases."
1831,679d459debd8ffd557a2b594,cs.IT,https://arxiv.org/pdf/2501.17746,Predictive Beamforming with Distributed MIMO,"Hasret Taha Akçalı, Özlem Tuğfe Demir, Tolga Girici, Emil Björnson","Signal Processing, Information Theory","In vehicle-to-everything (V2X) applications, roadside units (RSUs) can be tasked with both sensing and communication functions to enable sensing-assisted communications. Recent studies have demonstrated that distance, angle, and velocity information obtained through sensing can be leveraged to reduce the overhead associated with communication beam tracking. In this work, we extend this concept to scenarios involving multiple distributed RSUs and distributed MIMO (multiple-input multiple-output) systems. We derive the state evolution model, formulate the extended Kalman-filter equations, and implement predictive beamforming for distributed MIMO. Simulation results indicate that, when compared with a co-located massive MIMO antenna array, distributed antennas lead to more uniform and robust sensing performance, coverage, and data rates, while the vehicular user is in motion."
1832,679d459debd8ffd557a2b595,cs.IT,https://arxiv.org/pdf/2501.17736,"Winning Rates of $(n,k)$ Quantum Coset Monogamy Games","Michael Schleppy, Emina Soljanin","Quantum Physics, Information Theory","We formulate the(n,k)𝑛𝑘(n,k)( italic_n , italic_k )Coset Monogamy Game, in which two players must extract complementary information of unequal size (k𝑘kitalic_kbits vs.n−k𝑛𝑘n-kitalic_n - italic_kbits) from a random coset state without communicating. The complementary information takes the form of random Pauli-X and Pauli-Z errors on subspace states. Our game generalizes those considered in previous works that deal with the case of equal information size(k=n2)𝑘𝑛2(k=\frac{n}{2})( italic_k = divide start_ARG italic_n end_ARG start_ARG 2 end_ARG ). We prove a convex upper bound of the information-theoretic winning rate of the(n,k)𝑛𝑘(n,k)( italic_n , italic_k )Coset Monogamy Game in terms of the subspace rateR=kn∈[0,1]𝑅𝑘𝑛01R=\frac{k}{n}\in[0,1]italic_R = divide start_ARG italic_k end_ARG start_ARG italic_n end_ARG ∈ [ 0 , 1 ]. This bound improves upon previous results for the case ofR=12𝑅12R=\frac{1}{2}italic_R = divide start_ARG 1 end_ARG start_ARG 2 end_ARG. We also prove the achievability of an optimal winning probability upper bound for the class of unentangled strategies of the(n,k)𝑛𝑘(n,k)( italic_n , italic_k )Coset Monogamy Game."
1833,679d459debd8ffd557a2b596,cs.IT,https://arxiv.org/pdf/2501.17303,Measurement-Based Modeling and Analysis of UAV Air-Ground Channels at 1 and 4 GHz,"Zhuangzhuang Cui, Cesar Briso-Rodriguez, Ke Guan, Cesar Calvo-Ramirez, Bo Ai, Zhangdui Zhong","Signal Processing, Information Theory, Instrumentation and Detectors",
1834,679d459debd8ffd557a2b597,cs.IT,https://arxiv.org/pdf/2501.17059,Channel Estimation for XL-MIMO Systems with Decentralized Baseband Processing: Integrating Local Reconstruction with Global Refinement,"Anzheng Tang, Jun-Bo Wang, Yijin Pan, Cheng Zeng, Yijian Chen, Hongkang Yu, Ming Xiao, Rodrigo C. de Lamare, Jiangzhou Wang","Information Theory, Signal Processing","In this paper, we investigate the channel estimation problem for extremely large-scale multiple-input
multiple-output (XL-MIMO) systems with a hybrid analog-digital architecture, implemented within a decentralized baseband processing (DBP) framework with a star topology. Existing centralized and fully decentralized channel estimation methods face limitations due to excessive computational complexity or degraded performance. To overcome these challenges, we propose a novel two-stage channel estimation scheme that integrates local sparse reconstruction with global fusion and refinement.
Specifically, in the first stage, by exploiting the sparsity of channels in the angular-delay domain, the local reconstruction task is formulated as a sparse signal recovery problem. To solve it, we develop a graph neural networks-enhanced sparse Bayesian learning (SBL-GNNs) algorithm, which effectively captures dependencies among channel coefficients, significantly improving estimation accuracy.
In the second stage, the local estimates from the local processing units (LPUs) are aligned into a global angular domain for fusion at the central processing unit (CPU).
Based on the aggregated observations, the channel refinement is modeled as a Bayesian denoising problem.
To efficiently solve it, we devise a variational message passing algorithm that incorporates a Markov chain-based hierarchical sparse prior, effectively leveraging both the sparsity and the correlations of the channels in the global angular-delay domain.
Simulation results validate the effectiveness and superiority of the proposed SBL-GNNs algorithm over existing methods, demonstrating improved estimation performance and reduced computational complexity."
1835,679d459debd8ffd557a2b598,cs.IT,https://arxiv.org/pdf/2501.17010,New Quantum MDS Codes with Flexible Parameters from Hermitian Self-Orthogonal GRS Codes,"Oisin Campion, Fernando Hernando, Gary McGuire",Information Theory,"Letq𝑞qitalic_qbe a prime power.
Letλ>1𝜆1\lambda>1italic_λ > 1be a divisor ofq−1𝑞1q-1italic_q - 1, and
letτ>1𝜏1\tau>1italic_τ > 1andρ>1𝜌1\rho>1italic_ρ > 1be divisors ofq+1𝑞1q+1italic_q + 1.
Under certain conditions we prove that there exists an MDS stabilizer quantum code with lengthn=λ⁢τ⁢σ𝑛𝜆𝜏𝜎n=\lambda\tau\sigmaitalic_n = italic_λ italic_τ italic_σwhere2≤σ≤ρ2𝜎𝜌2\leq\sigma\leq\rho2 ≤ italic_σ ≤ italic_ρ.
This is a flexible construction, which
includes new MDS parameters not known before."
1836,679d459debd8ffd557a2b599,cs.IT,https://arxiv.org/pdf/2501.17002,Covert Adversarial Actuators in Finite MDPs,"Edoardo David Santi, Gongpu Chen, Deniz Gündüz, Asaf Cohen",Information Theory,"We consider aMarkov decision process (MDP)in which actions prescribed by the controller are executed by a separate actuator, which may behave adversarially. At each time step, the controller selects and transmits an action to the actuator; however, the actuator may deviate from the intended action to degrade the control reward. Given that the controller observes only the sequence of visited states, we investigate whether the actuator can covertly deviate from the controller’s policy to minimize its reward without being detected. We establish conditions for covert adversarial behavior over an infinite time horizon and formulate an optimization problem to determine the optimal adversarial policy under these conditions. Additionally, we derive the asymptotic error exponents for detection in two scenarios: (1) a binary hypothesis testing framework, where the actuator either follows the prescribed policy or a known adversarial strategy, and (2) a composite hypothesis testing framework, where the actuator may employ any stationary policy. For the latter case, we also propose an optimization problem to maximize the adversary’s performance."
1837,679d459debd8ffd557a2b59a,cs.IT,https://arxiv.org/pdf/2501.16838,Spread Codes from Abelian non-cyclic groups,"Joan-Josep Climent, Veronica Requena, Xaro Soler-Escrivà",Information Theory,"Given the finite field𝔽qsubscript𝔽𝑞\mathbb{F}_{q}blackboard_F start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT, for a prime powerq𝑞qitalic_q, in this paper we present a way of constructing spreads of𝔽qnsuperscriptsubscript𝔽𝑞𝑛\mathbb{F}_{q}^{n}blackboard_F start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT.
They will arise as orbits under the action of an Abelian non-cyclic group.
First, we construct a family of orbit codes of maximum distance using this group, and then we complete each of these codes to achieve a spread of the whole space
having an orbital structure."
1838,679d459debd8ffd557a2b59b,cs.IT,https://arxiv.org/pdf/2501.16823,Phase Noise Resilient Codebook Design for Sparse Code Multiple Access,"Haibo Liu, Qu Luo, Zilong Liu, Shan Luo, Pei Xiao, Xiaojun Yuan",Information Theory,"Sparse code multiple access (SCMA) is a promising technique for future machine type communication systems due to its superior spectral efficiency and capability for supporting massive connectivity.
This paper proposes a novel class of sparse codebooks to improve the error rate performance of SCMA in the presence of phase noise (PN). Specifically, we first analyze the error rate performance of SCMA impaired by looking into the pair-wise error probability. Then, a novel codebook design metric, called minimum PN metric (MPNM), is proposed. In addition, to design PN resilient codebooks, we propose a novel pulse-amplitude modulation (PAM)-based low projection mother constellation (LP-MC), called LP-PAM. The codebooks for different users are obtained by rotating and scaling the MC, where the phase rotation angles and scaling factors for different users are optimized by maximizing the proposed MPNM. Numerical results show that the proposed PNCBs have larger MPNM values and achieve improved error rate performance than the-state-of-the-art codebooks."
1839,679d459debd8ffd557a2b59c,cs.IT,https://arxiv.org/pdf/2501.16762,Rate-Distortion under Neural Tracking of Speech: A Directed Redundancy Approach,"Jan Østergaard, Sangeeth Geetha Jayaprakash, Rodrigo Ordoñez",Information Theory,"The data acquired at different scalp EEG electrodes when human subjects are exposed to speech stimuli are highly redundant. The redundancy is partly due to volume conduction effects and partly due to localized regions of the brain synchronizing their activity in response to the stimuli. In a competing talker scenario, we use a recent measure of directed redundancy to assess the amount of redundant information that is causally conveyed from the attended stimuli to the left temporal region of the brain. We observe that for the attended stimuli, the transfer entropy as well as the directed redundancy is proportional to the correlation between the speech stimuli and the reconstructed signal from the EEG signals.
This demonstrates that both the rate as well as the rate-redundancy are inversely proportional to the distortion in neural speech tracking. Thus, a greater rate indicates a greater redundancy between the electrode signals, and a greater correlation between the reconstructed signal and the attended stimuli. A similar relationship is not observed for the distracting stimuli."
1840,679d459debd8ffd557a2b59d,cs.IT,https://arxiv.org/pdf/2501.16610,Embracing Reconfigurable Antennas in the Tri-hybrid MIMO Architecture for 6G,"Miguel Rodrigo Castellanos, Siyun Yang, Chan-Byoung Chae, Robert W. Heath Jr","Information Theory, Emerging Technologies, Networking and Internet Architecture","Multiple-input multiple-output (MIMO) communication has led to immense enhancements in data rates and efficient spectrum management.The evolution of MIMO has been accompaniedby increased hardware complexity and array sizes, causing system power consumption to rise as a result. Despite past advances in power-efficient hybrid architectures, new solutions are needed to enable extremely large-scale MIMO deployments for 6G and beyond. In this paper, we introduce a novel architecture that integrates low-power reconfigurable antennas with both digital and analog precoding. Thistri-hybridapproach addresses key limitations in traditional and hybrid MIMO systems by improving power consumptionand adding new layer for signal processing. We provide a comprehensive analysis of the proposed architecture and compare its performance with existing solutions, including fully-digital and hybrid MIMO systems. The results demonstrate significant improvements in energy efficiency, highlighting the potential of the tri-hybrid system to meet the growing demands of future wireless networks.We also discuss several design and implementation challenges, including the need for technological advancements in reconfigurable array hardware and tunable antenna parameters."
1841,679d459debd8ffd557a2b59e,cs.IT,https://arxiv.org/pdf/2501.16343,Self-orthogonal and self-dual codes from maximal curves,"Puyin Wang, Jinquan Luo","Information Theory, Algebraic Geometry",
1842,679d459debd8ffd557a2b59f,cs.IT,https://arxiv.org/pdf/2501.16690,Quantum advantage in decentralized control of POMDPs: A control-theoretic view of the Mermin-Peres square,Venkat Anantharam,"Optimization and Control, Information Theory, Systems and Control, Quantum Physics",
1843,679d459debd8ffd557a2b5a0,cs.IT,https://arxiv.org/pdf/2501.16333,"A New Proof for the Linear Filtering and Smoothing Equations, and Asymptotic Expansion of Nonlinear Filtering",Masahiro Kurisaki,"Signal Processing, Information Theory, Probability, Statistics Theory",
1844,679d459debd8ffd557a2b5a1,cs.IT,https://arxiv.org/pdf/2501.16296,Entanglement-Assisted Coding for Arbitrary Linear Computations Over a Quantum MAC,"Lei Hu, Mohamed Nomeir, Alptug Aytekin, Yu Shi, Sennur Ulukus, Saikat Guha","Information Theory, Networking and Internet Architecture, Signal Processing, Quantum Physics","We study a linear computation problem over a quantum multiple access channel (LC-QMAC), whereS𝑆Sitalic_Sservers share an entangled state and separately store classical data streamsW1,⋯,WSsubscript𝑊1⋯subscript𝑊𝑆W_{1},\cdots,W_{S}italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ⋯ , italic_W start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPTover a finite field𝔽dsubscript𝔽𝑑\mathbb{F}_{d}blackboard_F start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT. A user aims to computeK𝐾Kitalic_Klinear combinations of these data streams, represented asY=𝐕1⁢W1+𝐕2⁢W2+⋯+𝐕S⁢WS∈𝔽dK×1𝑌subscript𝐕1subscript𝑊1subscript𝐕2subscript𝑊2⋯subscript𝐕𝑆subscript𝑊𝑆superscriptsubscript𝔽𝑑𝐾1Y=\mathbf{V}_{1}W_{1}+\mathbf{V}_{2}W_{2}+\cdots+\mathbf{V}_{S}W_{S}\in\mathbb%
{F}_{d}^{K\times 1}italic_Y = bold_V start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + bold_V start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + ⋯ + bold_V start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ∈ blackboard_F start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K × 1 end_POSTSUPERSCRIPT. To this end, each server encodes its classical information into its local quantum subsystem and transmits it to the user, who retrieves the desired computations via quantum measurements. In this work, we propose an achievable scheme for LC-QMAC based on the stabilizer formalism and the ideas from entanglement-assisted quantum error–correcting codes (EAQECC). Specifically, given any linear computation matrix, we construct a self-orthogonal matrix that can be implemented using the stabilizer formalism. Also, we apply precoding matrices to minimize the number of auxiliary qudits required. Our scheme achieves more computations per qudit, i.e., a higher computation rate, compared to the best-known methods in the literature, and attains the capacity in certain cases."
1845,679d459debd8ffd557a2b5a2,cs.IT,https://arxiv.org/pdf/2501.16287,A Unified Representation of Density-Power-Based Divergences Reducible to M-Estimation,Masahiro Kobayashi,"Information Theory, Statistics Theory, Machine Learning","Density-power-based divergences are known to provide robust inference procedures against outliers, and their extensions have been widely studied.
A characteristic of successful divergences is that the estimation problem can be reduced to M-estimation.
In this paper, we define a norm-based Bregman density power divergence (NB-DPD)—density-power-based divergence with functional flexibility within the framework of Bregman divergences that can be reduced to M-estimation.
We show that, by specifying the functionϕγsubscriptitalic-ϕ𝛾\phi_{\gamma}italic_ϕ start_POSTSUBSCRIPT italic_γ end_POSTSUBSCRIPT, NB-DPD reduces to well-known divergences, such as the density power divergence and theγ𝛾\gammaitalic_γ-divergence.
Furthermore, by examining the combinations of functionsϕγsubscriptitalic-ϕ𝛾\phi_{\gamma}italic_ϕ start_POSTSUBSCRIPT italic_γ end_POSTSUBSCRIPTcorresponding to existing divergences, we show that a new divergence connecting these existing divergences can be derived.
Finally, we show that the redescending property, one of the key indicators of robustness, holds only for theγ𝛾\gammaitalic_γ-divergence."
1846,679d459debd8ffd557a2b5a3,cs.IT,https://arxiv.org/pdf/2501.16145,Capacity-Achieving Input Distribution of the Additive Uniform Noise Channel With Peak Amplitude and Cost Constraint,"Jonas Stapmanns, Catarina Dias, Luke Eilers, Jean-Pascal Pfister",Information Theory,"Under which condition is quantization optimal? We address this question in the context of the additive uniform noise channel under peak amplitude and power constraints. We compute analytically the capacity-achieving input distribution as a function of the noise level, the average power constraint and the exponent of the power constraint. We found that when the cost constraint is tight and the cost function is concave, the capacity-achieving input distribution is discrete, whereas when the cost function is convex, the support of the capacity-achieving input distribution spans the entire interval."
1847,679d459debd8ffd557a2b5a4,cs.IT,https://arxiv.org/pdf/2501.16081,Combating Interference for Over-the-Air Federated Learning: A Statistical Approach via RIS,"Wei Shi, Jiacheng Yao, Wei Xu, Jindan Xu, Xiaohu You, Yonina C. Eldar, Chunming Zhao","Information Theory, Signal Processing","Over-the-air computation (AirComp) integrates analog communication with task-oriented computation, serving as a key enabling technique for communication-efficient federated learning (FL) over wireless networks. However, owing to its analog characteristics, AirComp-enabled FL (AirFL) is vulnerable to both unintentional and intentional interference. In this paper, we aim to attain robustness in AirComp aggregation against interference via reconfigurable intelligent surface (RIS) technology to artificially reconstruct wireless environments. Concretely, we establish performance objectives tailored for interference suppression in wireless FL systems, aiming to achieve unbiased gradient estimation and reduce its mean square error (MSE). Oriented at these objectives, we introduce the concept of phase-manipulated favorable propagation and channel hardening for AirFL, which relies on the adjustment of RIS phase shifts to realize statistical interference elimination and reduce the error variance of gradient estimation. Building upon this concept, we propose two robust aggregation schemes of power control and RIS phase shifts design, both ensuring unbiased gradient estimation in the presence of interference. Theoretical analysis of the MSE and FL convergence affirms the anti-interference capability of the proposed schemes. It is observed that computation and interference errors diminish by an order of𝒪⁢(1N)𝒪1𝑁\mathcal{O}\left(\frac{1}{N}\right)caligraphic_O ( divide start_ARG 1 end_ARG start_ARG italic_N end_ARG )whereN𝑁Nitalic_Nis the number of RIS elements, and the ideal convergence rate without interference can be asymptotically achieved by increasingN𝑁Nitalic_N. Numerical results confirm the analytical results and validate the superior performance of the proposed schemes over existing baselines."
1848,679d459debd8ffd557a2b5a5,cs.IT,https://arxiv.org/pdf/2501.15880,Movable Antennas Meet Intelligent Reflecting Surface: Friends or Foes?,"Xin Wei, Weidong Mei, Qingqing Wu, Qiaoran Jia, Boyu Ning, Zhi Chen, Jun Fang","Information Theory, Signal Processing","Movable antenna (MA) and intelligent reflecting surface (IRS) are considered promising technologies for the next-generation wireless communication systems due to their shared capabilities of reconfiguring and improving wireless channel conditions. This, however, raises a fundamental question: Does the performance gain of MAs over conventional fixed-position antennas (FPAs) still exist in the presence of the IRS passive beamforming? To answer this question, we investigate in this paper an IRS-assisted multi-user multiple-input single-output (MISO) MA system, where a multi-MA base station (BS) transmits to multiple single-FPA users. We formulate a sum-rate maximization problem by jointly optimizing the active/passive beamforming of the BS/IRS and the MA positions within a one-dimensional transmit region, which is challenging to be optimally solved. To drive essential insights, we first study a simplified case with a single user. Then, we analyze the performance gain of MAs over FPAs in the light-of-sight (LoS) BS-IRS channel and derive the conditions under which this gain becomes more or less significant. In addition, we propose an alternating optimization (AO) algorithm to solve the signal-to-noise ratio (SNR) maximization problem in the single-user case by combining the block coordinate descent (BCD) method and the graph-based method. For the general multi-user case, our performance analysis unveils that the performance gain of MAs over FPAs diminishes with typical transmit precoding strategies at the BS under certain conditions. We also propose a high-quality suboptimal solution to the sum-rate maximization problem by applying the AO algorithm that combines the weighted minimum mean square error (WMMSE) algorithm, manifold optimization method and discrete sampling method. Numerical results validate our theoretical analyses and demonstrate that the performance gain of MAs over FPAs may be reduced if the IRS passive beamforming is optimized."
1849,679d459debd8ffd557a2b5a6,cs.IT,https://arxiv.org/pdf/2501.15851,Coding for Strand Breaks in Composite DNA,"Frederik Walter, Yonatan Yehezkeally",Information Theory,"Even tough DNA can be considered as a very stable long term storage medium, errors must be expected during storage. From experiments it is evident that the most common error type due to storage are strand breaks. We address the problem of correcting strand breaks in DNA sequences resulting from composite DNA synthesis. We introduce a novel channel model with realistic assumptions about the errors resulting from long term storage. Our proposed coding scheme employs marker codes to correct single breaks. For this purpose, we generalize run-length-limited codes for the composite setting and derive bounds on the code size."
1850,679d459debd8ffd557a2b5a7,cs.IT,https://arxiv.org/pdf/2501.15729,Measurement-Based Non-Stationary Markov Tapped Delay Line Channel Model for 5G-Railways,"Xuejian Zhang, Ruisi He, Mi Yang, Jianwen Ding, Ruifeng Chen, Shuaiqi Gao, Ziyi Qi, Zhengyu Zhang, Bo Ai, Zhangdui Zhong",Information Theory,"5G for Railways (5G-R) is globally recognized as a promising next-generation railway communication system designed to meet increasing demands.
Channel modeling serves as foundation for communication system design, with tapped delay line (TDL) models widely utilized in system simulations due to their simplicity and practicality and serves as a crucial component of various standards like 3GPP.
However, existing TDL models applicable to 5G-R systems are limited.
Most fail to capture non-stationarity, a critical characteristic of railway communications, while others are unsuitable for the specific frequency bands and bandwidths of 5G-R.
In this paper, a channel measurement campaign for 5G-R dedicated network is carried out, resulting in a measurement-based 5-tap TDL model utilizing a first-order two-state Markov chain to represent channel non-stationarity. Key model parameters, including number of taps, statistical distribution of amplitude, phase and Doppler shift, and state transition probability matrix, are extracted.
The correlation between tap amplitudes are also obtained.
Finally, accuracy of model is validated through comparisons with measurement data and 3GPP model.
These findings are expected to offer valuable insights for design, optimization, and link-level simulation and validation of 5G-R systems."
1851,679d459debd8ffd557a2b5a8,cs.IT,https://arxiv.org/pdf/2501.15726,Vision-Aided Channel Prediction Based on Image Segmentation at Street Intersection Scenarios,"Xuejian Zhang, Ruisi He, Mi Yang, Ziyi Qi, Zhengyu Zhang, Bo Ai, Zhangdui Zhong","Information Theory, Signal Processing","Intelligent vehicular communication with vehicle-road collaboration capability is a key technology enabled by 6G, and the integration of various visual sensors on vehicles and infrastructures plays a crucial role.
Moreover, accurate channel prediction is foundational to realizing intelligent vehicular communication.
Traditional methods are still limited by the inability to balance accuracy and operability based on substantial spectrum resource consumption and highly refined description of environment.
Therefore, leveraging out-of-band information introduced by visual sensors provides a new solution and is increasingly applied across various communication tasks.
In this paper,
we propose a computer vision (CV)-based prediction model for vehicular communications, realizing accurate channel characterization prediction including path loss, Rice K-factor and delay spread based on image segmentation.
First, we conduct extensive vehicle-to-infrastructure measurement campaigns, collecting channel and visual data from various street intersection scenarios.
The image-channel dataset is generated after a series of data post-processing steps.
Image data consists of individual segmentation of target user using YOLOv8 network.
Subsequently, established dataset is used to train and test prediction network ResNet-32, where segmented images serve as input of network, and various channel characteristics are treated as labels or target outputs of network.
Finally, self-validation and cross-validation experiments are performed.
The results indicate that models trained with segmented images achieve high prediction accuracy and remarkable generalization performance across different streets and target users.
The model proposed in this paper offers novel solutions for achieving intelligent channel prediction in vehicular communications."
1852,679d459debd8ffd557a2b5a9,cs.IT,https://arxiv.org/pdf/2501.15717,Physics-Aware Decoding for Communication Channels Governed by Partial Differential Equations,"Tadashi Wadayama, Koji Igarashi, Takumi Takahashi",Information Theory,"Digital communication systems inherently operate through physical media governed by partial differential equations (PDEs). In this paper, we introduce a physics-aware decoding framework that integrates gradient descent-based error correcting algorithms with PDE-based channel modeling using differentiable PDE solvers. At the core of our approach is gradient flow decoding, which harnesses gradient information directly from the PDE solver to guide the decoding process. We validate our method through numerical experiments on both the heat equation and the nonlinear Schrödinger equation (NLSE), demonstrating significant improvements in decoding performance. The implications of this work extend beyond decoding applications, establishing a new paradigm for physics-aware signal processing that shows promise for various signal detection and signal recovery tasks."
1853,679d459debd8ffd557a2b5aa,cs.IT,https://arxiv.org/pdf/2501.15652,Rate Distortion Approach to Joint Communication and Sensing With Markov States: Open Loop Case,"Colton P. Lindstrom, Matthieu R. Bloch",Information Theory,"We investigate a joint communication and sensing (JCAS) framework in which a transmitter concurrently transmits information to a receiver and estimates a state of interest based on noisy observations.
The state is assumed to evolve according to a known dynamical model.
Past state estimates may then be used to inform current state estimates.
We show that Bayesian filtering constitutes the optimal sensing strategy.
We analyze JCAS performance under an open loop encoding strategy with results presented in terms of the tradeoff between asymptotic communication rate and expected per-block distortion of the state.
We illustrate the general result by specializing the analysis to a beam-pointing model with mobile state tracking.
Our results shed light on the relative performance of two beam control strategies, beam-switching and multi-beam."
1854,679d459debd8ffd557a2b5ab,cs.IT,https://arxiv.org/pdf/2501.15645,Individual Confidential Computing of Polynomials over Non-Uniform Information,"Saar Tarnopolsky, Zirui, Deng, Vinayak Ramkumar, Netanel Raviv, Alejandro Cohen",Information Theory,"In this paper, we address the problem of secure distributed computation in scenarios where user data is not uniformly distributed, extending existing frameworks that assume uniformity, an assumption that is challenging to enforce in data for computation. Motivated by the pervasive reliance on single service providers for data storage and computation, we propose a privacy-preserving scheme that achieves information-theoretic security guarantees for computing polynomials over non-uniform data distributions. Our framework builds upon the concept of perfect subset privacy and employs linear hashing techniques to transform non-uniform data into approximately uniform distributions, enabling robust and secure computation. We derive leakage bounds and demonstrate that information leakage of any subset of user data to untrusted service providers, i.e., not only to colluding workers but also (and more importantly) to the admin, remains negligible under the proposed scheme."
1855,679d459debd8ffd557a2b5ac,cs.IT,https://arxiv.org/pdf/2501.15576,First Real-Time Detection of Ambient Backscatters using Uplink Sounding Reference Signals of a Commercial 4G Smartphone,"Ahmed ElSanhoury, Islam Galal, Khaled AlKady, Aml ElKhodary, Ayman M. Hassan, Dinh-Thuy Phan-Huy",Information Theory,
1856,679d459debd8ffd557a2b5ad,cs.IT,https://arxiv.org/pdf/2501.15536,Intelligent Surface Assisted Radar Stealth Against Unauthorized ISAC,"Fan Xu, Wenhai Lai, Kaiming Shen","Information Theory, Signal Processing","The integration of radar sensors and communication networks as envisioned for the 6G wireless networks poses significant security risks, e.g., the user position information can be released to an unauthorized dual-functional base station (DFBS). To address this issue, we propose an intelligent surface (IS)-assisted radar stealth technology that prevents adversarial sensing. Specifically, we modify the wireless channels by tuning the phase shifts of IS in order to protect the target user from unauthorized sensing without jeopardizing the wireless communication link. In principle, we wish to maximize the distortion between the estimated angle-of-arrival (AoA) by the DFBS and the ground truth given the minimum signal-to-noise-radio (SNR) constraint for communication. Toward this end, we propose characterizing the problem as a game played by the DFBS and the IS, in which the DFBS aims to maximize a particular utility while the IS aims to minimize the utility. Although the problem is nonconvex, this paper shows that it can be optimally solved in closed form from a geometric perspective. According to the simulations, the proposed closed-form algorithm outperforms the baseline methods significantly in combating unauthorized sensing while limiting the impacts on wireless communications."
1857,679d459debd8ffd557a2b5ae,cs.IT,https://arxiv.org/pdf/2501.15488,Qualitative Mechanism Independence,"Oliver E Richardson, Spencer Peters, Joseph Y Halpern",Information Theory,"We define what it means for a joint probability distribution
to be(QIM-)compatiblewith a set of independent causal mechanisms,
at a qualitative level—or, more precisely, with a directed hypergraph𝒜𝒜\mathcal{A}caligraphic_A,
which is the qualitative structure of
a probabilistic dependency graph (PDG).
When𝒜𝒜\mathcal{A}caligraphic_Arepresents a qualitative Bayesian network,
QIM-compatibility with𝒜𝒜\mathcal{A}caligraphic_Areduces to satisfying the appropriate conditional independencies.
But giving semantics to hypergraphs using QIM-compatibility lets us do much more.
For one thing, we can capture functionaldependencies.
For another, QIM-compatibility captures important aspects of causality:
we can use compatibility to understand cyclic causal graphs, and
to demonstrate compatibility is essentially to produce a causal model.
Finally, compatibility has deep connections to
information theory.
Applying compatibility to cyclic structures helps to clarify a
longstanding conceptual issue
in information theory."
1858,679d459debd8ffd557a2b5af,cs.IT,https://arxiv.org/pdf/2501.15227,Detecting Unauthorized Drones with Cell-Free Integrated Sensing and Communication,"Xinyue Li, Zinat Behdad, Ozan Alp Topal, Ozlem Tugfe Demir, Cicek Cavdar","Information Theory, Signal Processing","Integrated sensing and communication (ISAC) boosts network efficiency by using existing resources for diverse sensing applications. In this work, we propose a cell-free massive MIMO (multiple-input multiple-output)-ISAC framework to detect unauthorized drones while simultaneously ensuring communication requirements. We develop a detector to identify passive aerial targets by analyzing signals from distributed access points (APs). In addition to the precision of the sensing, timeliness of the sensing information is also crucial due to the risk of drones leaving the area before the sensing procedure is finished. We introduce the age of sensing (AoS) and sensing coverage as our sensing performance metrics and propose a joint sensing blocklength and power optimization algorithm to minimize AoS and maximize sensing coverage while meeting communication requirements. Moreover, we propose an adaptive weight selection algorithm based on concave-convex procedure to balance the inherent trade-off between AoS and sensing coverage. Our numerical results show that increasing the communication requirements would significantly reduce both the sensing coverage and the timeliness of the sensing. Furthermore, the proposed adaptive weight selection algorithm can provide high sensing coverage and reduce the AoS by45%percent4545\%45 %compared to the fixed weights, demonstrating efficient utilization of both power and sensing blocklength."
1859,679d459debd8ffd557a2b5b0,cs.IT,https://arxiv.org/pdf/2501.15221,Performance analysis of tail-minimization and the linear rate of convergence of a proximal algorithm for sparse signal recovery,"Meng Huang, Shidong Li",Information Theory,"Recovery error bounds of tail-minimization and the rate of convergence of an efficient proximal alternating algorithm for sparse signal recovery are considered in this article. Tail-minimization focuses on minimizing the energy in the complementTcsuperscript𝑇𝑐T^{c}italic_T start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPTof an estimated supportT𝑇Titalic_T. Under the restricted isometry property (RIP) condition, we prove that tail-ℓ1subscriptℓ1\ell_{1}roman_ℓ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTminimization can exactly recover sparse signals in the noiseless case for a givenT𝑇Titalic_T. In the noisy case, two recovery results for the tail-ℓ1subscriptℓ1\ell_{1}roman_ℓ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTminimization and the tail-lasso models are established. Error bounds are improved over existing results. Additionally, we show that the RIP condition becomes surprisingly relaxed, allowing the RIP constant to approach1111as the estimationT𝑇Titalic_Tclosely approximates the true supportS𝑆Sitalic_S. Finally, an efficient proximal alternating minimization algorithm is introduced for solving the tail-lasso problem using Hadamard product parametrization. The linear rate of convergence is established using the Kurdyka-Łojasiewicz inequality. Numerical results demonstrate that the proposed algorithm significantly improves signal recovery performance compared to state-of-the-art techniques."
1860,679d459debd8ffd557a2b5b1,cs.IT,https://arxiv.org/pdf/2501.15207,Hybrid Near/Far-Field Frequency-Dependent Beamforming via Joint Phase-Time Arrays,"Yeyue Cai, Meixia Tao, Jianhua Mo, Shu Sun","Information Theory, Signal Processing","Joint phase-time arrays (JPTA) emerge as a cost-effective and energy-efficient architecture for frequency-dependent beamforming in wideband communications by utilizing both true-time delay units and phase shifters. This paper exploits the potential of JPTA to simultaneously serve multiple users in both near- and far-field regions with a single radio frequency chain. The goal is to jointly optimize JPTA-based beamforming and subband allocation to maximize overall system performance. To this end, we formulate a system utility maximization problem, including sum-rate maximization and proportional fairness as special cases. We develop a 3-step alternating optimization (AO) algorithm and an efficient deep learning (DL) method for this problem. The DL approach includes a 2-layer convolutional neural network, a 3-layer graph attention network (GAT), and a normalization module for resource and beamforming optimization. The GAT efficiently captures the interactions between resource allocation and analog beamformers. Simulation results confirm that JPTA outperforms conventional phased arrays (PA) in enhancing user rate and strikes a good balance between PA and fully-digital approach in energy efficiency. Employing a logarithmic utility function for user rates ensures greater fairness than maximizing sum-rates. Furthermore, the DL network achieves comparable performance to the AO approach, while having orders of magnitude lower computational complexity."
1861,679d459debd8ffd557a2b5b2,cs.IT,https://arxiv.org/pdf/2501.15172,DeepDIVE: Optimizing Input-Constrained Distributions for Composite DNA Storage via Multinomial Channel,"Adir Kobovich, Eitan Yaakobi, Nir Weinberger","Information Theory, Signal Processing","We address the challenge of optimizing the capacity-achieving input distribution for a multinomial channel under the constraint of limited input support size, which is a crucial aspect in the design of DNA storage systems. We propose an algorithm that further elaborates the Multidimensional Dynamic Assignment Blahut-Arimoto (M-DAB) algorithm[1]. Our proposed algorithm integrates variational autoencoder for determining the optimal locations of input distribution, into the alternating optimization of the input distribution locations and weights."
1862,679d459debd8ffd557a2b5b3,cs.IT,https://arxiv.org/pdf/2501.15165,A* Based Algorithm for Reduced Complexity ML Decoding of Tailbiting Codes,"Jorge Ortin, Paloma Garcia, Fernando Gutierrez, Antonio Valdovinos",Information Theory,"The A* algorithm is a graph search algorithm which has shown good results in terms of computational complexity for Maximum Likelihood (ML) decoding of tailbiting convolutional codes. The decoding of tailbiting codes with this algorithm is performed in two phases. In the first phase, a typical Viterbi decoding is employed to collect information regarding the trellis. The A* algorithm is then applied in the second phase, using the information obtained in the first one to calculate the heuristic function. The improvements proposed in this work decrease the computational complexity of the A* algorithm using further
information from the first phase of the algorithm. This information is used for obtaining a more accurate heuristic function and finding early terminating conditions for the A* algorithm. Simulation results show that the proposed modifications decrease the complexity of ML decoding with the A* algorithm in terms of the performed number of operations."
1863,679d459debd8ffd557a2b5b4,cs.IT,https://arxiv.org/pdf/2501.15091,Deep Reinforcement Learning for Energy Efficiency Maximization in RSMA-IRS-Assisted ISAC System,"Zhangfeng Ma, Ruichen Zhang, Bo Ai, Zhuxian Lian, Linzhou Zeng, Dusit Niyato","Information Theory, Signal Processing","This paper proposes a three-dimensional (3D) geometry-based channel model to accurately represent intelligent reflecting surfaces (IRS)-enhanced integrated sensing and communication (ISAC) networks using rate-splitting multiple access (RSMA) in practical urban environments. Based on this model, we formulate an energy efficiency (EE) maximization problem that incorporates transceiver beamforming constraints, IRS phase adjustments, and quality-of-service (QoS) requirements to optimize communication and sensing functions. To solve this problem, we use the proximal policy optimization (PPO) algorithm within a deep reinforcement learning (DRL) framework. Our numerical results confirm the effectiveness of the proposed method in improving EE and satisfying QoS requirements. Additionally, we observe that system EE drops at higher frequencies, especially under double-Rayleigh fading."
1864,679d459debd8ffd557a2b5b5,cs.IT,https://arxiv.org/pdf/2501.15013,An Information-Theoretic Efficient Capacity Region for Multi-User Interference Channel,"Sagnik Bhattacharya, Abhiram Rao Gorle, Muhammad Ali Mohsin, John M. Cioffi",Information Theory,"We investigate the capacity region of multi-user interference channels (IC), where each user encodes multiple sub-user components. By unifying chain-rule decomposition with the Entropy Power Inequality (EPI), we reason that single-user Gaussian codebooks suffice to achieve optimal performance, thus obviating any need for intricate auxiliary variables or joint typicality arguments. Our partial-MAC formulation enumerates sub-user decoding orders while only imposing constraints for sub-users actually decoded. This significantly reduces complexity relative to enumerating all subsets or bruteforcing over all successive interference cancellation (SIC) decoding order combinations at all receivers. This leads to a finite but comprehensive construction of all achievable rate tuples under sum-power constraints, while guaranteeing that each receiver fully recovers its intended sub-user signals. Consequently, known single-user Gaussian capacity results generalize naturally to multi-user scenarios, revealing a cohesive framework for analyzing multi-user IC. Our results thus offer a streamlined, tractable pathway for designing next-generation cell-free wireless networks that rely on IC mechanisms, efficiently exploiting interference structure while minimizing overhead. Overall, this provides a unifying perspective."
1865,679d459debd8ffd557a2b5b6,cs.IT,https://arxiv.org/pdf/2501.14941,On the Optimality of Gaussian Code-books for Signaling over a Two-Users Weak Gaussian Interference Channel,Amir K. Khandani,"Information Theory, Probability",
1866,679d459debd8ffd557a2b5b7,cs.IT,https://arxiv.org/pdf/2501.14921,Achieving uniform side information gain with multilevel lattice codes over the ring of integers,"Juliana G. F. Souza, Sueli I. R. Costa",Information Theory,"The index coding problem aims to optimise broadcast communication by taking advantage of receiver-side information to improve transmission efficiency. In this letter, we explore the application of ConstructionπAsubscript𝜋𝐴\pi_{A}italic_π start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPTlattices to index coding. We introduce a coding scheme, namedCRT lattice index coding, using ConstructionπAsubscript𝜋𝐴\pi_{A}italic_π start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPToverℤℤ\mathbb{Z}blackboard_Zto address the index coding problem. It is derived an upper bound for side information gain of a CRT lattice index code and conditions for the uniformity of this gain. The efficiency of this approach is shown through theoretical analysis and code design examples."
1867,679d459debd8ffd557a2b5b8,cs.IT,https://arxiv.org/pdf/2501.14861,A Deep-Unfolding-Optimized Coordinate-Descent Data-Detector ASIC for mmWave Massive MIMO,"Zixiao Li, Seyed Hadi Mirfarshbafan, Oscar Castañeda, Christoph Studer","Information Theory, Signal Processing","We present a 22 nm FD-SOI (fully depleted silicon-on-insulator) application-specific integrated circuit (ASIC) implementation of a novel soft-output Gram-domain block coordinate descent (GBCD) data detector for massive multi-user (MU) multiple-input multiple-output (MIMO) systems. The ASIC simultaneously addresses the high throughput requirements for millimeter wave (mmWave) communication, stringent area and power budget per subcarrier in an orthogonal frequency-division multiplexing (OFDM) system, and error-rate performance challenges posed by realistic mmWave channels.
The proposed GBCD algorithm utilizes a posterior mean estimate (PME) denoiser and is optimized using deep unfolding, which results in superior error-rate performance even in scenarios with highly correlated channels or where the number of user equipment (UE) data streams is comparable to the number of basestation (BS) antennas.
The fabricated GBCD ASIC supports up to 16 UEs transmitting QPSK to 256-QAM symbols to a 128-antenna BS, and achieves a peak throughput of 7.1 Gbps at 367 mW. The core area is only 0.97 mm2thanks to a reconfigurable array of processing elements that enables extensive resource sharing. Measurement results demonstrate that the proposed GBCD data-detector ASIC achieves best-in-class throughput and area efficiency."
1868,679d459debd8ffd557a2b5b9,cs.IT,https://arxiv.org/pdf/2501.14738,On strict ranking by pairwise comparisons,Jean-Pierre Magnot,Information Theory,"We attack the problem of getting a strict ranking (i.e. a ranking without equally ranked items) ofn𝑛nitalic_nitems from a pairwise comparisons matrix. Basic structures are described, a first heuristical approach based on a condition, theℛ−limit-fromℛ\mathcal{R}-caligraphic_R -condition, is proposed. Analyzing the limits of this ranking procedure, we finish with a minimization problem which can be applied to a wider class of pairwise comparisons matrices. If solved, it produces consistent pairwise comparisons that produce a strict ranking."
1869,679d459debd8ffd557a2b5ba,cs.IT,https://arxiv.org/pdf/2501.16218,Active Hypothesis Testing for Quantum Detection of Phase-Shift Keying Coherent States,"Yun-Feng Lo, Matthieu R. Bloch","Quantum Physics, Information Theory, Systems and Control","This paper explores the quantum detection of Phase-Shift Keying (PSK)-coded coherent states through the lens of active hypothesis testing, focusing on a Dolinar-like receiver with constraints on displacement amplitude and energy. With coherent state slicing, we formulate the problem as a controlled sensing task in which observation kernels have parameters shrinking with sample size. The constrained open-loop error exponent and a corresponding upper bound on the Bayesian error probability are proven. Surprisingly, the exponent-optimal open-loop policy for binary PSK with high dark counts is not simply time-sharing. This work serves as a first step towards obtaining analytical insights through the active hypothesis testing framework for designing resource-constrained quantum communication receivers."
1870,679d459debd8ffd557a2b5bb,cs.IT,https://arxiv.org/pdf/2501.15675,Joint Communication and Sensing with Bipartite Entanglement over Bosonic Channels,"Tuna Erdoğan, Shi-Yuan Wang, Shang-Jen Su, Matthieu Bloch","Quantum Physics, Information Theory","We consider a joint communication and sensing problem in an optical link in which a low-power transmitter attempts to communicate with a receiver while simultaneously identifying the range of a defect creating a backscattered signal. We model the system as a lossy thermal noise bosonic channel in which the location of the target, modeled as a beamsplitter, affects the timing of the backscattered signal. Motivated by the envisioned deployment of entanglement sharing quantum networks, we allow the transmitter to exploit entanglement to assist its sensing and communication. Since entanglement is known to enhance sensing, as known from quantum illumination, and increase communication rates, as known from the characterization of the entanglement-assisted capacity, the transmitter is faced with a trade-off and must judiciously allocate its entanglement resources. Our main result is a characterization of the trade-offs incurred in the form of an achievable rate/error-exponent region which can beat time-sharing in certain cases. The proof of our result relies on technical results of independent interests, by which we carefully show how to extend the known asymptotic characterization of multi-hypothesis testing Chernoff exponent in finite-dimensional spaces to infinite-dimensional spaces and provide a characterization of phase shift keying modulated displaced thermal states in Fock basis."
1871,679d459debd8ffd557a2b5bc,cs.IT,https://arxiv.org/pdf/2501.15465,Geometry of symplectic group and optimal EAQECC codes,"Ruihu Li, Yuezhen Ren, Chaofeng Guan, Yang Liu","Quantum Physics, Information Theory","A new type of link between geometry of symplectic group and
entanglement-assisted (EA) quantum error-correcting codes (EAQECCs) is
presented. Relations of symplectic subspaces and quaternary
additive codes concerning parameters of EAQECCs are described. Thus, parameters of EA stabilizer codes are revealed in the nomenclature of additive
codes. Our techniques enable us solve some open problems about
optimal EAQECCs and entanglement-assisted quantum minimum distance
separable (EAQMDS) codes, and are also useful for designing encoding
and decoding quantum circuit of EA stabilizer codes."
1872,679d459debd8ffd557a2b5bd,cs.IT,https://arxiv.org/pdf/2501.14984,The Cloud and Flock Polynomials of q-Matroids,"Heide Gluesing-Luerssen, Benjamin Jany","Combinatorics, Information Theory","We show that the Whitney function of aq𝑞qitalic_q-matroid can be determined from the cloud and flock polynomials associated to the
cyclic flats.
These polynomials capture information about the corank (resp., nullity) of certain spaces whose cyclic core (resp., closure) is the given cyclic flat.
Going one step further, we prove that the Whitney function, and in fact all cloud and flock polynomials, are determined by the configuration of theq𝑞qitalic_q-matroid, that is the abstract lattice of cyclic flats together with the corank-nullity data.
Examples illustrate that the converses of the above statements are not true.
This has the consequence that the Whitney function of a direct sum is not determined by the Whitney functions of the summands."
1873,679d459debd8ffd557a2b5be,cs.IT,https://arxiv.org/pdf/2501.14633,Channel Independent Precoder for OFDM-based Systems over Fading Channels,"Jorge Ortin, Paloma Garcia, Fernando Gutierrez, Antonio Valdovinos","Information Theory, Signal Processing","In this paper we propose an independent channel precoder for orthogonal frequency division multiplexing (OFDM) systems over fading channels. The design of the precoder is based on the information redistribution of the input modulated symbols amongst the output precoded symbols. The proposed precoder decreases the variance of the instantaneous noise power at the receiver produced by the channel variability. The employment of an interleaver together with a precoding matrix whose size does not depend on the number of data carriers in an OFDM symbol allows different configurations of time-frequency diversity which can be easily adapted to the channel conditions. The precoder is evaluated with a modified Zero Forcing (ZF) equalizer whose maximum gain is constrained by means of a clipping factor. Thus, the clipping factor limits the noise power transfer in the receiver deprecoding block in low SNR conditions."
1874,679d459debd8ffd557a2b5bf,cs.IT,https://arxiv.org/pdf/2501.14620,Strong Converse Exponent for Remote Lossy Source Coding,"Han Wu, Hamdi Joudeh",Information Theory,
1875,679d459debd8ffd557a2b5c0,cs.IT,https://arxiv.org/pdf/2501.14522,Information Age and Correctness for Energy Harvesting Devices with Random Access,"Khac-Hoang Ngo, Giuseppe Durisi, Petar Popovski",Information Theory,
1876,679d459debd8ffd557a2b5c1,cs.IT,https://arxiv.org/pdf/2501.14452,On the Rate-Exponent Region of Integrated Sensing and Communications With Variable-Length Coding,"Ioannis Papoutsidakis, George C. Alexandropoulos","Information Theory, Signal Processing","This paper considers the achievable rate-exponent region of integrated sensing and communication systems in the presence of variable-length coding with feedback. This scheme is fundamentally different from earlier studies, as the coding methods that utilize feedback impose different constraints on the codewords. The focus herein is specifically on the Gaussian channel, where three achievable regions are analytically derived and numerically evaluated. In contrast to a setting without feedback, we show that a trade-off exists between the operations of sensing and communications."
1877,679d459debd8ffd557a2b5c2,cs.IT,https://arxiv.org/pdf/2501.14313,Between Close Enough to Reveal and Far Enough to Protect: a New Privacy Region for Correlated Data,"Luis Maßny, Rawad Bitar, Fangwei Ye, Salim El Rouayheb",Information Theory,"When users make personal privacy choices, correlation between their data can cause inadvertent leakage about users who do not want to share their data by other users sharing their data.
As a solution, we consider local redaction mechanisms. As prior works proposed data-independent privatization mechanisms, we study the family of data-independent local redaction mechanisms and upper-bound their utility when data correlation is modeled by a stationary Markov process. In contrast, we derive a novel data-dependent mechanism, which improves the utility by leveraging a data-dependent leakage measure."
1878,679d459debd8ffd557a2b5c3,cs.IT,https://arxiv.org/pdf/2501.14270,Max-Min Fairness for IRS-Assisted Secure Two-Way Communications,"Harindu Jayarathne, Tharindu Wickremasinghe, Kasun T. Hemachandra, Tharaka Samarasinghe, Saman Atapattu",Information Theory,"This paper investigates an intelligent reflective surface (IRS) assisted secure multi-user two-way communication system. The aim of this paper is to enhance the physical layer security by optimizing the minimum secrecy-rate among all user-pairs in the presence of a malicious user. The optimization problem is converted into an alternating optimization problem consisting of two sub-problems. Transmit power optimization is handled using a fractional programming method, whereas IRS phase shift optimization is handled with semi-definite programming. The convergence of the proposed algorithm is investigated numerically. The performance gain in minimum secrecy-rate is quantified for four different user configurations in comparison to the baseline scheme. Results indicate a 3.6-fold gain in minimum secrecy rate over the baseline scheme when the IRS is positioned near a legitimate user, even when the malicious user is located close to the same legitimate user."
1879,679d459debd8ffd557a2b5c4,cs.IT,https://arxiv.org/pdf/2501.14081,Single-Letter Characterization of the Mismatched Distortion-Rate Function,"Maël Le Treust, Tristan Tomala",Information Theory,"The mismatched distortion-rate problem has remained open since its formulation by Lapidoth in 1997. In this paper, we characterize the mismatched distortion-rate function. Our single-letter solution highlights the adequate conditional distributions for the encoder and the decoder. The achievability result relies on a time-sharing argument that allows to convexify the upper bound of Lapidoth. We show that it is sufficient to consider two regimes, one with a large rate and another one with a small rate. Our main contribution is the converse proof. Suppose that the encoder selects a single-letter conditional distribution distinct from the one in the solution, we construct an encoding strategy that leads to the same expected cost for both encoder and decoder. This ensures that the encoder cannot gain by changing the single-letter conditional distribution. This argument relies on a careful identification of the sequence of auxiliary random variables. By building on Caratheodory’s Theorem we show that the cardinality of the auxiliary random variables is equal to the one of the source alphabet plus three."
1880,679d459debd8ffd557a2b5c5,cs.IT,https://arxiv.org/pdf/2501.14064,Switched Feedback for the Multiple-Access Channel,"Oliver Kosut, Michael Langberg, Michelle Effros",Information Theory,"A mechanism calledswitched feedbackis introduced; under switched feedback, each channel output goes forward to the receiver(s) or backwards to the transmitter(s) but never both. By studying the capacity of the Multiple Access Channel (MAC) with switched feedback, this work investigates the potential benefits of feedback in the MAC and explores strategies for maximizing that benefit under reliable and unreliable feedback scenarios. The study is motivated by an exploration of the tradeoffs between cooperation and transmission in the context of communication systems.
Results include upper and lower bounds on the capacity region of the MAC with switched feedback."
1881,679d459debd8ffd557a2b5c6,cs.IT,https://arxiv.org/pdf/2501.14053,The Redundancy of Non-Singular Channel Simulation,"Gergely Flamich, Sharang M. Sriramu, Aaron B. Wagner",Information Theory,"Channel simulation is an alternative to quantization and entropy coding for performing lossy source coding.
Recently, channel simulation has gained significant traction in both the machine learning and information theory communities, as it integrates better with machine learning-based data compression algorithms and has better rate-distortion-perception properties than quantization.
As the practical importance of channel simulation increases, it is vital to understand its fundamental limitations.
Recently, Sriramu and Wagner[1]have provided an almost-complete characterisation of the redundancy of channel simulation algorithms.
In this paper, we complete this characterisation.
First, we significantly extend a result of Li and El Gamal[2], and show that the redundancy of any instance of a channel simulation problem is lower bounded by the channel simulation divergence.
Second, we give two proofs that the asymptotic redundancy of simulating iid non-singular channels is lower-bounded by1/2121/21 / 2: one using a direct approach based on the asymptotic expansion of the channel simulation divergence and one using large deviations theory."
1882,679d459debd8ffd557a2b5c7,cs.IT,https://arxiv.org/pdf/2501.14358,CSI-Free Low-Complexity Remote State Estimation over Wireless MIMO Fading Channels using Semantic Analog Aggregation,"Minjie Tang, Photios A. Stavrou, Marios Kountouris","Systems and Control, Information Theory, Signal Processing","In this work, we investigate low-complexity remote system state estimation over wireless multiple-input-multiple-output (MIMO) channels without requiring prior knowledge of channel state information (CSI). We start by reviewing the conventional Kalman filtering-based state estimation algorithm, which typically relies on perfect CSI and incurs considerable computational complexity. To overcome the need for CSI, we introduce a novel semantic aggregation method, in which sensors transmit semantic measurement discrepancies to the remote state estimator through analog aggregation. To further reduce computational complexity, we introduce a constant-gain-based filtering algorithm that can be optimized offline using the constrained stochastic successive convex approximation (CSSCA) method. We derive a closed-form sufficient condition for the estimation stability of our proposed scheme via Lyapunov drift analysis. Numerical results showcase significant performance gains using the proposed scheme compared to several widely used methods."
1883,679d459debd8ffd557a2b5c8,cs.IT,https://arxiv.org/pdf/2501.14340,From Classical to Quantum: Explicit Classical Distributions Achieving Maximal Quantum $f$-Divergence,"Dimitri Lanier, Julien Béguinot, Olivier Rioul","Quantum Physics, Information Theory","THIS PAPER IS ELIGIBLE FOR THE STUDENT PAPER AWARD.
Explicit classical states achieving maximalf𝑓fitalic_f-divergence are given, allowing for a simple proof of Matsumoto’s Theorem, and the systematic extension of any inequality between classicalf𝑓fitalic_f-divergences
to quantumf𝑓fitalic_f-divergences. Our methodology is particularly simple as it does not require any elaborate matrix analysis machinery but only basic linear algebra. It is also effective, as illustrated by two examples improving existing bounds:
(i) an improved quantum Pinsker inequality is derived betweenχ2superscript𝜒2\chi^{2}italic_χ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTand trace norm, and leveraged to improve a bound in decoherence theory;
(ii) a new reverse quantum Pinsker inequality is derived for any quantumf𝑓fitalic_f-divergence, and compared to previous (Audenaert-Eisert and Hirche-Tomamichel) bounds."
1884,679d459debd8ffd557a2b5c9,cs.IT,https://arxiv.org/pdf/2501.14234,STAR-RIS-Enabled Multi-Path Beam Routing with Passive Beam Splitting,"Bonan An, Weidong Mei, Yuanwei Liu, Zhi Chen","Signal Processing, Information Theory","Reconfigurable intelligent surfaces (RISs) or intelligent reflecting surfaces (IRSs) can be densely deployed in the environment to create multi-reflection line-of-sight (LoS) links between base stations (BS) and users, thereby significantly enhancing the BS coverage. However, conventional reflection-only RISs can only achieve half-space reflection, which limits the LoS path diversity. In contrast, simultaneously transmitting and reflecting reconfigurable intelligent surfaces (STAR-RISs) can split incident signals into reflected and transmitted signals pointing to different half spaces simultaneously, thereby creating more LoS paths. Hence, in this paper, we study a new multi-STAR-RIS-aided communication system, where a multi-antenna BS transmits to multiple single-antenna users by exploiting the signal beam routing over a set of cascaded LoS paths each formed by multiple STAR-RISs. To reveal essential insights, we first consider a simplified single-user case, aiming to maximize its received signal power by jointly optimizing the active beamforming at the BS, the BS’s power allocation over different paths, the number of selected beam-routing paths, the selected STAR-RISs for each path, as well as their amplitude and phase shifts for transmission/reflection. However, this problem is particularly difficult to be optimally solved as different paths may be intricately coupled at their shared STAR-RISs. To tackle this difficulty, we first derive the optimal solutions to this problem in closed-form for a given set of paths. The clique-based approach in graph theory is then applied to solve the remaining multi-path selection problem efficiently. Next, we extend the proposed clique-based method to the more general multi-user case to maximize the minimum received signal power among all users, subject to additional constraints on the disjointness of the selected paths for different users. Simulation results show that our proposed STAR-RIS-enabled beam routing outperforms the conventional beam routing with reflection-only RISs in both single- and multi-user cases."
1885,679d459debd8ffd557a2b5ca,cs.IT,https://arxiv.org/pdf/2501.14164,WaveMax: Radar Waveform Design via Convex Maximization of FrFT Phase Retrieval,"Samuel Pinilla, Kumar Vijay Mishra, Brian M. Sadler","Signal Processing, Information Theory","The ambiguity function (AF) is a critical tool in radar waveform design, representing the two-dimensional correlation between a transmitted signal and its time-delayed, frequency-shifted version. Obtaining a radar signal to match a specified AF magnitude is a bi-variate variant of the well-known phase retrieval problem. Prior approaches to this problem were either limited to a few classes of waveforms or lacked a computable procedure to estimate the signal. Our recent work provided a framework for solving this problem for both band- and time-limited signals using non-convex optimization. In this paper, we introduce a novel approachWaveMaxthat formulates waveform recovery as aconvexoptimization problem by relying on the fractional Fourier transform (FrFT)-based AF. We exploit the fact that AF of the FrFT of the original signal is equivalent to a rotation of the original AF. In particular, we reconstruct the radar signal by solving a low-rank minimization problem, which approximates the waveform using the leading eigenvector of a matrix derived from the AF. Our theoretical analysis shows that unique waveform reconstruction is achievable with a sample size no more than three times the signal frequencies or time samples. Numerical experiments validate the efficacy ofWaveMaxin recovering signals from noiseless and noisy AF, including scenarios with randomly and uniformly sampled sparse data."
1886,679d459debd8ffd557a2b5cb,cs.RO,https://arxiv.org/pdf/2501.18564,SAM2Act: Integrating Visual Foundation Model with A Memory Architecture for Robotic Manipulation,"Haoquan Fang, Markus Grotz, Wilbert Pumacay, Yi Ru Wang, Dieter Fox, Ranjay Krishna, Jiafei Duan",Robotics,"Robotic manipulation systems operating in diverse, dynamic environments must exhibit three critical abilities: multitask interaction, generalization to unseen scenarios, and spatial memory. While significant progress has been made in robotic manipulation, existing approaches often fall short in generalization to complex environmental variations and addressing memory-dependent tasks. To bridge this gap, we introduceSAM2Act, a multi-view robotic transformer-based policy that leverages multi-resolution upsampling with visual representations from large-scale foundation model. SAM2Act achieves a state-of-the-art average success rate of86.8% across 18 tasksin the RLBench benchmark, and demonstrates robust generalization onThe Colosseumbenchmark, with only a4.3% performance gapunder diverse environmental perturbations. Building on this foundation, we proposeSAM2Act+, a memory-based architecture inspired by SAM2, which incorporates a memory bank, an encoder, and an attention mechanism to enhance spatial memory. To address the need for evaluating memory-dependent tasks, we introduceMemoryBench, a novel benchmark designed to assess spatial memory and action recall in robotic manipulation. SAM2Act+ achievescompetitive performance onMemoryBench, significantly outperforming existing approaches and pushing the boundaries of memory-enabled robotic systems."
1887,679d459debd8ffd557a2b5cc,cs.RO,https://arxiv.org/pdf/2501.18516,Learn from the Past: Language-conditioned Object Rearrangement with Large Language Models,"Guanqun Cao, Ryan Mckenna, John Oyekan",Robotics,"Object rearrangement is a significant task for collaborative robots, where they are directed to manipulate objects into a specified goal state. Determining the placement of objects is a major challenge that influences the efficiency of the rearrangement process.
Most current methods heavily rely on pre-collected datasets to train the model for predicting the goal position and are restricted to specific instructions, which limits their broader applicability and effectiveness.
In this paper, we propose a framework of language-conditioned object rearrangement based on the Large Language Model (LLM).
Particularly, our approach mimics human reasoning by using past successful experiences as a reference to infer the desired goal position.
Based on LLM’s strong natural language comprehension and inference ability, our method can generalise to handle various everyday objects and free-form language instructions in a zero-shot manner.
Experimental results demonstrate that our methods can effectively execute the robotic rearrangement tasks, even those involving long sequential orders."
1888,679d459debd8ffd557a2b5cd,cs.RO,https://arxiv.org/pdf/2501.18505,Path Planning and Optimization for Cuspidal 6R Manipulators,"Alexander J. Elias, John T. Wen",Robotics,"A cuspidal robot can move from one inverse kinematics (IK) solution to another without crossing a singularity. Multiple industrial robots are cuspidal. They tend to have a beautiful mechanical design, but they pose path planning challenges.
A task-space path may have a valid IK solution for each point along the path, but a continuous joint-space path may depend on the choice of the IK solution or even be infeasible.
This paper presents new analysis, path planning, and optimization methods to enhance the utility of cuspidal robots. We first demonstrate an efficient method to identify cuspidal robots and show, for the first time, that the ABB GoFa and certain robots with three parallel joint axes are cuspidal.
We then propose a new path planning method for cuspidal robots by finding all IK solutions for each point along a task-space path and constructing a graph to connect each vertex corresponding to an IK solution. Graph edges are weighted based on the optimization metric, such as minimizing joint velocity.
The optimal feasible path is the shortest path in the graph.
This method can find non-singular paths as well as smooth paths which pass through singularities.
Finally, this path planning method is incorporated into a path optimization algorithm. Given a fixed workspace toolpath, we optimize the offset of the toolpath in the robot base frame while ensuring continuous joint motion.
Code examples are available in a publicly accessible repository."
1889,679d459debd8ffd557a2b5ce,cs.RO,https://arxiv.org/pdf/2501.18351,Dual-BEV Nav: Dual-layer BEV-based Heuristic Path Planning for Robotic Navigation in Unstructured Outdoor Environments,"Jianfeng Zhang, Hanlin Dong, Jian Yang, Jiahui Liu, Shibo Huang, Ke Li, Xuan Tang, Xian Wei, Xiong You",Robotics,"Path planning with strong environmental adaptability plays a crucial role in robotic navigation in unstructured outdoor environments,
especially in the case of low-quality location and map information.
The path planning ability of a robot depends on
the identification of the traversability of global and local ground areas.
In real-world scenarios, the complexity of outdoor open environments makes it difficult for robots to identify the traversability of ground areas that lack a clearly defined structure.
Moreover, most existing methods have rarely analyzed
the integration of local and global traversability identifications in unstructured outdoor scenarios.
To address this problem,
we propose a novel method, Dual-BEV Nav, first introducing Bird’s Eye View (BEV) representations into local planning to generate high-quality traversable paths. Then,
these paths are projected onto the global traversability map generated by the global BEV planning model to obtain the optimal waypoints.
By integrating the traversability from both local and global BEV, we establish a dual-layer BEV heuristic planning paradigm, enabling long-distance navigation in unstructured outdoor environments.
We test our approach through both public dataset evaluations and real-world robot deployments, yielding promising results.
Compared to baselines, the Dual-BEV Nav improved temporal distance prediction accuracy by up to18.7%percent18.718.7\%18.7 %.
In the real-world deployment, under conditions significantly different from the training set and with notable occlusions in the global BEV, the Dual-BEV Nav successfully achieved a 65-meter-long outdoor navigation.
Further analysis demonstrates that the local BEV representation significantly enhances the rationality of the planning, while the global BEV probability map ensures the robustness of the overall planning."
1890,679d459debd8ffd557a2b5cf,cs.RO,https://arxiv.org/pdf/2501.18229,GPD: Guided Polynomial Diffusion for Motion Planning,"Ajit Srikanth, Parth Mahanjan, Kallol Saha, Vishal Mandadi, Pranjal Paul, Pawan Wadhwani, Brojeshwar Bhowmick, Arun Singh, Madhava Krishna",Robotics,"Diffusion-based motion planners are becoming popular due to their well-established performance improvements, stemming from sample diversity and the ease of incorporating new constraints directly during inference. However, a primary limitation of the diffusion process is the requirement for a substantial number of denoising steps, especially when the denoising process is coupled with gradient-based guidance. In this paper, we introduce, for the first time, diffusion in the parametric space of trajectories, where the parameters are represented as Bernstein coefficients. We show that this representation greatly improves the effectiveness of the cost-function guidance and the inference speed. We also introduce a novel stitching algorithm that leverages the diversity in diffusion-generated trajectories to produce collision-free trajectories with just a single cost function-guided model. We demonstrate that our approaches outperform current SOTA diffusion-based motion planners for manipulators and provide an ablation study on key components."
1891,679d459debd8ffd557a2b5d0,cs.RO,https://arxiv.org/pdf/2501.18220,On-Line Learning for Planning and Control of Underactuated Robots with Uncertain Dynamics,"Giulio Turrisi, Marco Capotondi, Claudio Gaz, Valerio Modugno, Giuseppe Oriolo, Alessandro De Luca",Robotics,"We present an iterative approach for planning and controlling motions of underactuated robots with uncertain dynamics. At its core, there is a learning process which estimates the perturbations induced by the model uncertainty on the active and passive degrees of freedom. The generic iteration of the algorithm makes use of the learned data in both the planning phase, which is based on optimization, and the control phase, where partial feedback linearization of the active dofs is performed on the model updated on-line. The performance of the proposed approach is shown by comparative simulations and experiments on a Pendubot executing various types of swing-up maneuvers. Very few iterations are typically needed to generate dynamically feasible trajectories and the tracking control that guarantees their accurate execution, even in the presence of large model uncertainties."
1892,679d459debd8ffd557a2b5d1,cs.RO,https://arxiv.org/pdf/2501.18075,Synthesizing Grasps and Regrasps for Complex Manipulation Tasks,"Aditya Patankar, Dasharadhan Mahalingam, Nilanjan Chakraborty",Robotics,"In complex manipulation tasks, e.g., manipulation by pivoting, the motion of the object being manipulated has to satisfy path constraints that can change during the motion. Therefore, a single grasp may not be sufficient for the entire path, and the object may need to be regrasped. Additionally, geometric data for objects from a sensor are usually available in the form of point clouds. The problem of computing grasps and regrasps from point-cloud representation of objects for complex manipulation tasks is a key problem in endowing robots with manipulation capabilities beyond pick-and-place.
In this paper, we formalize the problem of grasping/regrasping for complex manipulation tasks with objects represented by (partial) point clouds and present an algorithm to solve it. We represent a complex manipulation task as a sequence of constant screw motions. Using a manipulation plan skeleton as a sequence of constant screw motions, we use a grasp metric to find graspable regions on the object for every constant screw segment. The overlap of the graspable regions for contiguous screws are then used to determine when and how many times the object needs to be regrasped. We present experimental results on point cloud data collected from RGB-D sensors to illustrate our approach."
1893,679d459debd8ffd557a2b5d2,cs.RO,https://arxiv.org/pdf/2501.17968,Online Trajectory Replanner for Dynamically Grasping Irregular Objects,"Minh Nhat Vu, Florian Grander, Anh Nguyen",Robotics,
1894,679d459debd8ffd557a2b5d3,cs.RO,https://arxiv.org/pdf/2501.17963,Physics-Grounded Differentiable Simulation for Soft Growing Robots,"Lucas Chen, Yitian Gao, Sicheng Wang, Francesco Fuentes, Laura H. Blumenschein, Zachary Kingston",Robotics,"Soft-growing robots (i.e., vine robots) are a promising class of soft robots that allow for navigation and growth in tightly confined environments.
However, these robots remain challenging to model and control due to the complex interplay of the inflated structure and inextensible materials, which leads to obstacles for autonomous operation and design optimization.
Although there exist simulators for these systems that have achieved qualitative and quantitative success in matching high-level behavior, they still often fail to capture realistic vine robot shapes using simplified parameter models and have difficulties in high-throughput simulation necessary for planning and parameter optimization.
We propose a differentiable simulator for these systems, enabling the use of the simulator “in-the-loop” of gradient-based optimization approaches to address the issues listed above. With the more complex parameter fitting made possible by this approach, we experimentally validate and integrate a closed-form nonlinear stiffness model for thin-walled inflated tubes based on a first-principles approach to local material wrinkling.
Our simulator also takes advantage of data-parallel operations by leveraging existing differentiable computation frameworks, allowing multiple simultaneous rollouts.
We demonstrate the feasibility of using a physics-grounded nonlinear stiffness model within our simulator, and how it can be an effective tool in sim-to-real transfer. We provide our implementation open source."
1895,679d459debd8ffd557a2b5d4,cs.RO,https://arxiv.org/pdf/2501.17962,Agricultural Industry Initiatives on Autonomy: How collaborative initiatives of VDMA and AEF can facilitate complexity in domain crossing harmonization needs,"Georg Happich, Alexander Grever, Julius Schöning","Computers and Society, Robotics, Systems and Control",
1896,679d459debd8ffd557a2b5d5,cs.RO,https://arxiv.org/pdf/2501.17884,Ranging Performance Analysis in Automotive DToF Lidars,Xiao Guo,"Signal Processing, Robotics",
1897,679d459debd8ffd557a2b5d6,cs.RO,https://arxiv.org/pdf/2501.17851,UGSim: Autonomous Buoyancy-Driven Underwater Glider Simulator with LQR Control Strategy and Recursive Guidance System,"Zhizun Xu, Yang Song, Jiabao Zhu, Weichao Shi","Robotics, Software Engineering","This paper presents the UGSim, a simulator for buoyancy-driven gliders, with a LQR control strategy, and a recursive guidance system. Building on the top of the DAVE and the UUVsim, it is designed to address unique challenges that come from the complex hydrodynamic and hydrostatic impacts on buoyancy-driven gliders, which conventional robotics simulators can’t deal with. Since distinguishing features of the class of vehicles, general controllers and guidance systems developed for underwater robotics are infeasible. The simulator is provided to accelerate the development and the evaluation of algorithms that would otherwise require expensive and time-consuming operations at sea. It consists of a basic kinetic module, a LQR control module and a recursive guidance module, which allows the user to concentrate on the single problem rather than the whole robotics system and the software infrastructure. We demonstrate the usage of the simulator through an example, loading the configuration of the buoyancy-driven glider named Petrel-II, presenting its dynamics simulation, performances of the control strategy and the guidance system."
1898,679d459debd8ffd557a2b5d7,cs.RO,https://arxiv.org/pdf/2501.17773,SafePR: Unified Approach for Safe Parallel Robots by Contact Detection and Reaction with Redundancy Resolution,"Aran Mohammad, Tim-Lukas Habich, Thomas Seel, Moritz Schappler","Robotics, Systems and Control","Fast and safe motion is crucial for the successful deployment of physically interactive robots.
Parallel robots (PRs) offer the potential for higher speeds while maintaining the same energy limits due to their low moving masses.
However, they require methods for contact detection and reaction while avoiding singularities and self-collisions.
We address this issue and present SafePR — a unified approach for the detection and localization, including the distinction between collision and clamping to perform a reaction that is safe for humans and feasible for PRs.
Our approach uses information from the encoders and motor currents to estimate forces via a generalized-momentum observer.
Neural networks and particle filters classify and localize the contacts.
We introduce reactions with redundancy resolution to avoid type-II singularities and self-collisions.
Our approach detected and terminated 72 real-world collision and clamping contacts with end-effector speeds of up to 1.5 m/s, each within 25–275 ms.
The forces were below the thresholds from ISO/TS 15066.
By using built-in sensors, SafePR enables safe interaction with already assembled PRs without the need for new hardware components."
1899,679d459debd8ffd557a2b5d8,cs.RO,https://arxiv.org/pdf/2501.17666,An Intelligent System-on-a-Chip for a Real-Time Assessment of Fuel Consumption to Promote Eco-Driving,"Óscar Mata-Carballeira, Mikel Díaz-Rodríguez, Inés del Campo, Victoria Martínez",Robotics,"Pollution that originates ftom automobiles is a concern in the current world, not only because of global warming, but also due to the harmful effects on people’s health and lives. Despite regulations on exhaust gas emissions being applied, minimizing unsuitable driving habits that cause elevated fuel consumption and emissions would achieve further reductions. For that reason, this work proposes a self-organized map (SOM)-based intelligent system in order to provide drivers with eco-driving-intended driving style (DS) recommendations. The development of the DS advisor uses driving data from the Uyanik instrumented car. The system classifies drivers regarding the underlying causes of non-optimal DSs from the eco-driving viewpoint. When compared with other solutions, the main advantage of this approach is the personalization of the recommendations that are provided to motorists, comprising the handling of the pedals and the gearbox, with potential improvements in both fuel consumption and emissions ranging from the 9.5% to the 31.5%, or even higher for drivers that are strongly engaged with the system. It was successfully implemented using a field-programmable gate array (FPGA) device of the Xilinx ZynQ programmable system-on-a-chip (PSoC) family. This SOM-based system allows for real-time implementation, state-of-the-art timing performances, and low power consumption, which are suitable for developing advanced driving assistance systems (ADASs)."
1900,679d459debd8ffd557a2b5d9,cs.RO,https://arxiv.org/pdf/2501.17664,Analysis of the Motion Sickness and the Lack of Comfort in Car Passengers,"Estibaliz Asua, Jon Gutiérrez-Zaballa, Óscar Mata-Carballeira, Jon Ander Ruiz, Inés del Campo",Robotics,
1901,679d459debd8ffd557a2b5da,cs.RO,https://arxiv.org/pdf/2501.17661,Multi-Agent Path Finding Using Conflict-Based Search and Structural-Semantic Topometric Maps,"Scott Fredriksson, Yifan Bai, Akshit Saradagi, George Nikolakopoulos",Robotics,"As industries increasingly adopt large robotic fleets, there is a pressing need for computationally efficient, practical, and optimal conflict-free path planning for multiple robots. Conflict-Based Search (CBS) is a popular method for multi-agent path finding (MAPF) due to its completeness and optimality; however, it is often impractical for real-world applications, as it is computationally intensive to solve and relies on assumptions about agents and operating environments that are difficult to realize.
This article proposes a solution to overcome computational challenges and practicality issues of CBS by utilizing structural-semantic topometric maps. Instead of running CBS over large grid-based maps, the proposed solution runs CBS over a sparse topometric map containing structural-semantic cells representing intersections, pathways, and dead ends. This approach significantly accelerates the MAPF process and reduces the number of conflict resolutions handled by CBS while operating in continuous time.
In the proposed method, robots are assigned time ranges to move between topometric regions, departing from the traditional CBS assumption that a robot can move to any connected cell in a single time step. The approach is validated through real-world multi-robot path-finding experiments and benchmarking simulations. The results demonstrate that the proposed MAPF method can be applied to real-world non-holonomic robots and yields significant improvement in computational efficiency compared to traditional CBS methods while improving conflict detection and resolution in cases of corridor symmetries."
1902,679d459debd8ffd557a2b5db,cs.RO,https://arxiv.org/pdf/2501.17658,An eco-driving approach for ride comfort improvement,"Óscar Mata-Carballeira, Inés del Campo, Estibalitz Asua","Robotics, Computers and Society","New challenges on transport systems are emerging due to the advances that the current paradigm is experiencing. The breakthrough of the autonomous car brings concerns about ride comfort, while the pollution concerns have arisen in recent years. In the model of automated automobiles, drivers are expected to become passengers, so, they will be more prone to suffer from ride discomfort or motion sickness. Conversely, the eco-driving implications should not be set aside because of the influence of pollution on climate and people’s health. For that reason, a joint assessment of the aforementioned points would have a positive impact. Thus, this work presents a self-organized map (SOM)-based solution to assess ride comfort features of individuals considering their driving style (DS) from the viewpoint of eco-driving. For this purpose, a previously acquired dataset from an instrumented car was used to classify drivers regarding the causes of their lack of ride comfort and eco-friendliness. Once drivers are classified regarding their DS, natural-language-based recommendations are proposed to increase the engagement with the system. Hence, potential improvements of up to the 57.7% for ride comfort evaluation parameters, as well as up to the 47.1% in greenhouse-gasses (GHG) emissions are expected to be reached."
1903,679d459debd8ffd557a2b5dc,cs.RO,https://arxiv.org/pdf/2501.17437,Bayesian BIM-Guided Construction Robot Navigation with NLP Safety Prompts in Dynamic Environments,"Mani Amani, Reza Akhavian",Robotics,"Construction robotics increasingly relies on natural language processing for task execution, creating a need for robust methods to interpret commands in complex, dynamic environments. While existing research primarily focuses on what tasks robots should perform, less attention has been paid to how these tasks should be executed safely and efficiently. This paper presents a novel probabilistic framework that uses sentiment analysis from natural language commands to dynamically adjust robot navigation policies in construction environments. The framework leverages Building Information Modeling (BIM) data and natural language prompts to create adaptive navigation strategies that account for varying levels of environmental risk and uncertainty. We introduce an object-aware path planning approach that combines exponential potential fields with a grid-based representation of the environment, where the potential fields are dynamically adjusted based on the semantic analysis of user prompts. The framework employs Bayesian inference to consolidate multiple information sources: the static data from BIM, the semantic content of natural language commands, and the implied safety constraints from user prompts. We demonstrate our approach through experiments comparing three scenarios: baseline shortest-path planning, safety-oriented navigation, and risk-aware routing. Results show that our method successfully adapts path planning based on natural language sentiment, achieving a 50% improvement in minimum distance to obstacles when safety is prioritized, while maintaining reasonable path lengths. Scenarios with contrasting prompts, such as?dangerous?and?safe,?demonstrate the framework’s ability to modify paths based on. This approach provides a flexible foundation for integrating human knowledge and safety considerations into construction robot navigation."
1904,679d459debd8ffd557a2b5dd,cs.RO,https://arxiv.org/pdf/2501.17351,Realtime Limb Trajectory Optimization for Humanoid Running Through Centroidal Angular Momentum Dynamics,"Sait Sovukluk, Robert Schuller, Johannes Englsberger, Christian Ott",Robotics,"One of the essential aspects of humanoid robot running is determining the limb-swinging trajectories. During the flight phases, where the ground reaction forces are not available for regulation, the limb swinging trajectories are significant for the stability of the next stance phase. Due to the conservation of angular momentum, improper leg and arm swinging results in highly tilted and unsustainable body configurations at the next stance phase landing. In such cases, the robotic system fails to maintain locomotion independent of the stability of the center of mass trajectories. This problem is more apparent for fast and high flight time trajectories. This paper proposes a real-time nonlinear limb trajectory optimization problem for humanoid running. The optimization problem is tested on two different humanoid robot models, and the generated trajectories are verified using a running algorithm for both robots in a simulation environment."
1905,679d459debd8ffd557a2b5de,cs.RO,https://arxiv.org/pdf/2501.17349,An Efficient Numerical Function Optimization Framework for Constrained Nonlinear Robotic Problems,"Sait Sovukluk, Christian Ott","Robotics, Optimization and Control","This paper presents a numerical function optimization framework designed for constrained optimization problems in robotics. The tool is designed with real-time considerations and is suitable for online trajectory and control input optimization problems. The proposed framework does not require any analytical representation of the problem and works with constrained block-box optimization functions. The method combines first-order gradient-based line search algorithms with constraint prioritization through nullspace projections onto constraint Jacobian space. The tool is implemented in C++ and provided online for community use, along with some numerical and robotic example implementations presented in this paper."
1906,679d459debd8ffd557a2b5df,cs.RO,https://arxiv.org/pdf/2501.17313,Surena-V: A Humanoid Robot for Human-Robot Collaboration with Optimization-based Control Architecture,"Mohammad Ali Bazrafshani, Aghil Yousefi-Koma, Amin Amani, Behnam Maleki, Shahab Batmani, Arezoo Dehestani Ardakani, Sajedeh Taheri, Parsa Yazdankhah, Mahdi Nozari, Amin Mozayyan, Alireza Naeini, Milad Shafiee, Amirhosein Vedadi",Robotics,
1907,679d459debd8ffd557a2b5e0,cs.RO,https://arxiv.org/pdf/2501.17789,Propeller Motion of a Devil-Stick using Normal Forcing,"Aakash Khandelwal, Ranjan Mukherjee","Systems and Control, Robotics","The problem of realizing rotary propeller motion of a devil-stick in the vertical plane using forces purely normal to the stick is considered. This problem represents a nonprehensile manipulation task of an underactuated system. In contrast with previous approaches, the devil-stick is manipulated by controlling the normal force and its point of application. Virtual holonomic constraints are used to design the trajectory of the center-of-mass of the devil-stick in terms of its orientation angle, and conditions for stable propeller motion are derived. Intermittent large-amplitude forces are used to asymptotically stabilize a desired propeller motion. Simulations demonstrate the efficacy of the approach in realizing stable propeller motion without loss of contact between the actuator and devil-stick."
1908,679d459debd8ffd557a2b5e1,cs.RO,https://arxiv.org/pdf/2501.17754,Analysis of the navigation of magnetic microrobots through cerebral bifurcations,"Pedro G. Alves, Maria Pinto, Rosa Moreira, Derick Sivakumaran, Fabian C. Landers, Maria Guix, Bradley J. Nelson, Andreas D. Flouris, Salvador Pané, Josep Puigmartí-Luis, Tiago Sotto Mayor","Numerical Analysis, Robotics, Systems and Control, Biological Physics",
1909,679d459debd8ffd557a2b5e2,cs.RO,https://arxiv.org/pdf/2501.17173,Model Evaluation of a Transformable CubeSat for Nonholonomic Attitude Reorientation Using a Drop Tower,"Yuki Kubo, Tsubasa Ando, Hirona Kawahara, Shu Miyata, Naoya Uchiyama, Kazutoshi Ito, Yoshiki Sugawara","Instrumentation and Methods for Astrophysics, Robotics","This paper presents a design for a drop tower test to evaluate a numerical model for a structurally reconfigurable spacecraft with actuatable joints, referred to as a transformable spacecraft.
A mock-up robot for a 3U-sized transformable spacecraft is designed to fit in a limited time and space of the microgravity environment available in the drop tower. The robot performs agile reorientation, referred to as nonholonomic attitude control, by actuating joints in a particular manner. To adapt to the very short duration of microgravity in the drop tower test, a successive joint actuation maneuver is optimized to maximize the amount of attitude reorientation within the time constraint. The robot records the angular velocity history of all four bodies, and the data is analyzed to evaluate the accuracy of the numerical model. We confirm that the constructed numerical model sufficiently replicates the robot’s motion and show that the post-experiment model corrections further improve the accuracy of the numerical simulations. Finally, the difference between this drop tower test and the actual orbit demonstration is discussed to show the prospect."
1910,679d459debd8ffd557a2b5e3,cs.RO,https://arxiv.org/pdf/2501.17022,Mobile Manipulation Instruction Generation from Multiple Images with Automatic Metric Enhancement,"Kei Katsumata, Motonari Kambara, Daichi Yashima, Ryosuke Korekata, Komei Sugiura",Robotics,"We consider the problem of generating free-form mobile manipulation instructions based on a target object image and receptacle image.
Conventional image captioning models are not able to generate appropriate instructions because their architectures are typically optimized for single-image.
In this study, we propose a model that handles both the target object and receptacle to generate free-form instruction sentences for mobile manipulation tasks.
Moreover, we introduce a novel training method that effectively incorporates the scores from both learning-based and n-gram based automatic evaluation metrics as rewards.
This method enables the model to learn the co-occurrence relationships between words and appropriate paraphrases.
Results demonstrate that our proposed method outperforms baseline methods including representative multimodal large language models on standard automatic evaluation metrics.
Moreover, physical experiments reveal that using our method to augment data on language instructions improves the performance of an existing multimodal language understanding model for mobile manipulation."
1911,679d459debd8ffd557a2b5e4,cs.RO,https://arxiv.org/pdf/2501.17018,Six-Degree-of-Freedom Motion Emulation for Data-Driven Modeling of Underwater Vehicles,"Juliana Danesi Ruiz, Michael Swafford, Austin Krebill, Rachel Vitali, Casey Harwood",Robotics,
1912,679d459debd8ffd557a2b5e5,cs.RO,https://arxiv.org/pdf/2501.16973,Towards Open-Source and Modular Space Systems with ATMOS,"Pedro Roque, Sujet Phodapol, Elias Krantz, Jaeyoung Lim, Joris Verhagen, Frank Jiang, David Dorner, Roland Siegwart, Ivan Stenius, Gunnar Tibert, Huina Mao, Jana Tumova, Christer Fuglesang, Dimos V. Dimarogonas",Robotics,"In the near future, autonomous space systems will compose a large number of the spacecraft being deployed. Their tasks will involve autonomous rendezvous and proximity operations with large structures, such as inspections or assembly of orbiting space stations and maintenance and human-assistance tasks over shared workspaces. To promote replicable and reliable scientific results for autonomous control of spacecraft, we present the design of a space systems laboratory based on open-source and modular software and hardware. The simulation software provides a software-in-the-loop (SITL) architecture that seamlessly transfers simulated results to the ATMOS platforms, developed for testing of multi-agent autonomy schemes for microgravity. The manuscript presents the KTH space systems laboratory facilities and the ATMOS platform as open-source hardware and software contributions. Preliminary results showcase SITL and real testing."
1913,679d459debd8ffd557a2b5e6,cs.RO,https://arxiv.org/pdf/2501.16754,SSF-PAN: Semantic Scene Flow-Based Perception for Autonomous Navigation in Traffic Scenarios,"Yinqi Chen, Meiying Zhang, Qi Hao, Guang Zhou","Robotics, Computer Vision and Pattern Recognition","Vehicle detection and localization in complex traffic scenarios pose significant challenges due to the interference of moving objects.
Traditional methods often rely on outlier exclusions or semantic segmentations, which suffer from low computational efficiency and accuracy.
The proposed SSF-PAN can achieve the functionalities of LiDAR point cloud based object detection/localization and SLAM (Simultaneous Localization and Mapping)
with high computational efficiency and accuracy, enabling map-free navigation frameworks.
The novelty of this work is threefold:
1) developing a neural network which can achieve segmentation among static and dynamic objects within the scene flows
with different motion features, that is, semantic scene flow (SSF);
2) developing an iterative framework which can further optimize the quality of input scene flows and output segmentation results;
3) developing a scene flow-based navigation platform which can test the performance of the SSF perception system in the simulation environment.
The proposed SSF-PAN method is validated using the SUScape-CARLA111https://suscape.net/datasets/sceneflowand the KITTI[1]datasets, as well as on the CARLA simulator.
Experimental results demonstrate that the proposed approach outperforms traditional methods in terms of scene flow computation accuracy, moving object detection accuracy, computational efficiency, and autonomous navigation effectiveness."
1914,679d459debd8ffd557a2b5e7,cs.RO,https://arxiv.org/pdf/2501.16743,Hierarchical Trajectory (Re)Planning for a Large Scale Swarm,"Lishuo Pan, Yutong Wang, Nora Ayanian",Robotics,"We consider the trajectory replanning problem for a large-scale swarm in a cluttered environment. Our path planner replans for robots by utilizing a hierarchical approach, dividing the workspace, and computing collision-free paths for robots within each cell in parallel. Distributed trajectory optimization generates a deadlock-free trajectory for efficient execution and maintains the control feasibility even when the optimization fails. Our hierarchical approach combines the benefits of both centralized and decentralized methods, achieving a high task success rate while providing real-time replanning capability.
Compared to decentralized approaches, our approach effectively avoids deadlocks and collisions, significantly increasing the task success rate. We demonstrate the real-time performance of our algorithm with up to 142 robots in simulation, and a representative 24 physical Crazyflie nano-quadrotor experiment."
1915,679d459debd8ffd557a2b5e8,cs.RO,https://arxiv.org/pdf/2501.16728,Optimizing Efficiency of Mixed Traffic through Reinforcement Learning: A Topology-Independent Approach and Benchmark,"Chuyang Xiao, Dawei Wang, Xinzheng Tang, Jia Pan, Yuexin Ma",Robotics,"This paper presents a mixed traffic control policy
designed to optimize traffic efficiency across diverse road
topologies, addressing issues of congestion prevalent in urban
environments. A model-free reinforcement learning (RL) ap-
proach is developed to manage large-scale traffic flow, using
data collected by autonomous vehicles to influence human-
driven vehicles. A real-world mixed traffic control benchmark
is also released, which includes 444 scenarios from 20 countries,
representing a wide geographic distribution and covering a
variety of scenarios and road topologies. This benchmark
serves as a foundation for future research, providing a realistic
simulation environment for the development of effective policies.
Comprehensive experiments demonstrate the effectiveness and
adaptability of the proposed method, achieving better perfor-
mance than existing traffic control methods in both intersection
and roundabout scenarios. To the best of our knowledge, this
is the first project to introduce a real-world complex scenarios
mixed traffic control benchmark. Videos and code of our work
are available athttps://sites.google.com/berkeley.edu/mixedtrafficplus/home"
1916,679d459debd8ffd557a2b5e9,cs.RO,https://arxiv.org/pdf/2501.16719,Safety-Critical Control for Aerial Physical Interaction in Uncertain Environment,"Jeonghyun Byun, Yeonjoon Kim, Dongjae Lee, H. Jin Kim",Robotics,"Aerial manipulation for safe physical interaction with their environments is gaining significant momentum in robotics research.
In this paper, we present a disturbance-observer-based safety-critical control for a fully actuated aerial manipulator interacting with both static and dynamic structures.
Our approach centers on a safety filter that dynamically adjusts the desired trajectory of the vehicle’s pose, accounting for the aerial manipulator’s dynamics, the disturbance observer’s structure, and motor thrust limits.
We provide rigorous proof that the proposed safety filter ensures the forward invariance of the safety set—representing motor thrust limits—even in the presence of disturbance estimation errors.
To demonstrate the superiority of our method over existing control strategies for aerial physical interaction, we perform comparative experiments involving complex tasks, such as pushing against a static structure and pulling a plug firmly attached to an electric socket.
Furthermore, to highlight its repeatability in scenarios with sudden dynamic changes, we perform repeated tests of pushing a movable cart and extracting a plug from a socket.
These experiments confirm that our method not only outperforms existing methods but also excels in handling tasks with rapid dynamic variations."
1917,679d459debd8ffd557a2b5ea,cs.RO,https://arxiv.org/pdf/2501.16717,Strawberry Robotic Operation Interface: An Open-Source Device for Collecting Dexterous Manipulation Data in Robotic Strawberry Farming,"Linsheng Hou, Wenwu Lu, Yanan Wang, Chen Peng, Zhenghao Fei",Robotics,"The strawberry farming is labor-intensive, particularly in tasks requiring dexterous manipulation such as picking occluded strawberries. To address this challenge, we present the Strawberry Robotic Operation Interface (SROI), an open-source device designed for collecting dexterous manipulation data in robotic strawberry farming. The SROI features a handheld unit with a modular end effector, a stereo robotic camera, enabling the easy collection of demonstration data in field environments. A data post-processing pipeline is introduced to extract spatial trajectories and gripper states from the collected data. Additionally, we release an open-source dataset of strawberry picking demonstrations to facilitate research in dexterous robotic manipulation. The SROI represents a step toward automating complex strawberry farming tasks, reducing reliance on manual labor."
1918,679d459debd8ffd557a2b5eb,cs.RO,https://arxiv.org/pdf/2501.16590,Benchmarking Model Predictive Control and Reinforcement Learning Based Control for Legged Robot Locomotion in MuJoCo Simulation,"Shivayogi Akki, Tan Chen","Robotics, Systems and Control","Model Predictive Control (MPC) and Reinforcement Learning (RL) are two prominent strategies for controlling legged robots, each with unique strengths. RL learns control policies through system interaction, adapting to various scenarios, whereas MPC relies on a predefined mathematical model to solve optimization problems in real-time. Despite their widespread use, there is a lack of direct comparative analysis under standardized conditions. This work addresses this gap by benchmarking MPC and RL controllers on a Unitree Go1 quadruped robot within the MuJoCo simulation environment, focusing on a standardized task-straight walking at a constant velocity. Performance is evaluated based on disturbance rejection, energy efficiency, and terrain adaptability. The results show that RL excels in handling disturbances and maintaining energy efficiency but struggles with generalization to new terrains due to its dependence on learned policies tailored to specific environments. In contrast, MPC shows enhanced recovery capabilities from larger perturbations by leveraging its optimization-based approach, allowing for a balanced distribution of control efforts across the robot’s joints. The results provide a clear understanding of the advantages and limitations of both RL and MPC, offering insights into selecting an appropriate control strategy for legged robotic applications."
1919,679d459debd8ffd557a2b5ec,cs.RO,https://arxiv.org/pdf/2501.16485,Enhanced Position Estimation in Tactile Internet-Enabled Remote Robotic Surgery Using MOESP-Based Kalman Filter,"Muhammad Hanif Lashari, Wafa Batayneh, Ashfaq Khokhar, Shakil Ahmed","Robotics, Systems and Control","Accurately estimating the position of a patient’s side robotic arm in real-time during remote surgery is a significant challenge, especially within Tactile Internet (TI) environments. This paper presents a new, efficient method for position estimation using a Kalman Filter (KF) combined with the Multivariable Output-Error State Space (MOESP) method for system identification. Unlike traditional approaches that assume prior knowledge of the system’s dynamics, this study utilizes the JIGSAW dataset—a comprehensive collection of robotic surgical data—alongside input from the Master Tool Manipulator (MTM) to derive the state-space model directly. The MOESP method allows us to accurately model the Patient Side Manipulator (PSM) dynamics without prior system models, enhancing the KF’s performance under simulated network conditions, including delays, jitter, and packet loss. These conditions mimic the real-world challenges faced in Tactile Internet applications. Our findings demonstrate the KF’s enhanced resilience and accuracy in state estimation, achieving over 95% estimation accuracy despite the presence of uncertainties induced by the network."
1920,679d459debd8ffd557a2b5ed,cs.RO,https://arxiv.org/pdf/2501.16480,Modular Framework for Uncertainty Prediction in Autonomous Vehicle Motion Forecasting within Complex Traffic Scenarios,"Han Wang, Yuneil Yeo, Antonio R. Paiva, Jean Utke, Maria Laura Delle Monache","Robotics, Machine Learning, Signal Processing","We propose a modular modeling framework designed to enhance the capture and validation of uncertainty in autonomous vehicle (AV) trajectory prediction. Departing from traditional deterministic methods, our approach employs a flexible, end-to-end differentiable probabilistic encoder-decoder architecture. This modular design allows the encoder and decoder to be trained independently, enabling seamless adaptation to diverse traffic scenarios without retraining the entire system. Our key contributions include: (1) a probabilistic heatmap predictor that generates context-aware occupancy grids for dynamic forecasting, (2) a modular training approach that supports independent component training and flexible adaptation, and (3) a structured validation scheme leveraging uncertainty metrics to evaluate robustness under high-risk conditions. To highlight the benefits of our framework, we benchmark it against an end-to-end baseline, demonstrating faster convergence, improved stability, and flexibility. Experimental results validate these advantages, showcasing the capacity of the framework to efficiently handle complex scenarios while ensuring reliable predictions and robust uncertainty representation. This modular design offers significant practical utility and scalability for real-world autonomous driving applications."
1921,679d459debd8ffd557a2b5ee,cs.RO,https://arxiv.org/pdf/2501.16868,Event-Based Adaptive Koopman Framework for Optic Flow-Guided Landing on Moving Platforms,"Bazeela Banday, Chandan Kumar Sah, Jishnu Keshavan","Systems and Control, Robotics","This paper presents an optic flow-guided approach for achieving soft landings by resource-constrained unmanned aerial vehicles (UAVs) on dynamic platforms. An offline data-driven linear model based on Koopman operator theory is developed to describe the underlying (nonlinear) dynamics of optic flow output obtained from a single monocular camera that maps to vehicle acceleration as the control input. Moreover, a novel adaptation scheme within the Koopman framework is introduced online to handle uncertainties such as unknown platform motion and ground effect, which exert a significant influence during the terminal stage of the descent process. Further, to minimize computational overhead, an event-based adaptation trigger is incorporated into an event-driven Model Predictive Control (MPC) strategy to regulate optic flow and track a desired reference. A detailed convergence analysis ensures global convergence of the tracking error to a uniform ultimate bound while ensuring Zeno-free behavior. Simulation results demonstrate the algorithm’s robustness and effectiveness in landing on dynamic platforms under ground effect and sensor noise, which compares favorably to non-adaptive event-triggered and time-triggered adaptive schemes."
1922,679d459debd8ffd557a2b5ef,cs.RO,https://arxiv.org/pdf/2501.16006,Underactuated dexterous robotic grasping with reconfigurable passive joints,"Marek Kopicki, Sainul Islam Ansary, Simone Tolomei, Franco Angelini, Manolo Garabini, Piotr Skrzypczyński",Robotics,"We introduce a novel reconfigurable passive joint (RP-joint), which has been implemented and tested on an underactuated three-finger robotic gripper. RP-joint has no actuation, but instead it is lightweight and compact. It can be easily reconfigured by applying external forces and locked to perform complex dexterous manipulation tasks, but only after tension is applied to the connected tendon. Additionally, we present an approach that allows learning dexterous grasps from single examples with underactuated grippers and automatically configures the RP-joints for dexterous manipulation. This is enhanced by integrating kinaesthetic contact optimization, which improves grasp performance even further. The proposed RP-joint gripper and grasp planner have been tested on over 370 grasps executed on 42 IKEA objects and on the YCB object dataset, achieving grasping success rates of 80% and 87%, on IKEA and YCB, respectively."
1923,679d459debd8ffd557a2b5f0,cs.RO,https://arxiv.org/pdf/2501.15901,Robust Mobile Robot Path Planning via LLM-Based Dynamic Waypoint Generation,"Muhammad Taha Tariq, Congqing Wang, Yasir Hussain",Robotics,
1924,679d459debd8ffd557a2b5f1,cs.RO,https://arxiv.org/pdf/2501.15806,Autonomous Horizon-based Asteroid Navigation With Observability-constrained Maneuvers,"Aditya Arjun Anibha, Kenshiro Oguri","Robotics, Optimization and Control","Asteroid exploration is a pertinent challenge due to the varying complexity of their dynamical environments, shape, and communication delays due to distance. Thus, autonomous navigation methods are continually being developed and improved in current research to enable their safe exploration. These methods often involve using Optical Navigation (OpNav) to determine the spacecraft’s location, which relies on the horizon’s visibility. It is critical to ensure the reliability of this measurement so that the spacecraft may maintain an accurate state estimate throughout its mission. This paper presents an observability-constrained algorithm that generates orbital maneuvers for spacecraft to follow trajectories that allow continuously usable optical measurements to maintain system observability for safe navigation. This algorithm improves upon existing asteroid navigation capabilities by allowing the safe and robust autonomous targeting of various trajectories. The algorithm is adaptable to different asteroid scenarios, and allows a spacecraft to orbit at a wide range of distances within the optical measurement range. The paper presents a comprehensive framework that simulates asteroid dynamics, synthetic image generation, edge detection, horizon-based OpNav, filtering, and observability-constrained control.111A preliminary version of this work was previously presented during the 4th Space Imaging Workshop at Georgia Institute of Technology, Atlanta, GA."
1925,679d459debd8ffd557a2b5f2,cs.RO,https://arxiv.org/pdf/2501.15768,Error-State LQR Formulation for Quadrotor UAV Trajectory Tracking,Micah Reich,"Robotics, Systems and Control","This article presents an error-state Linear Quadratic Regulator (LQR) formulation for robust trajectory tracking in quadrotor Unmanned Aerial Vehicles (UAVs). The proposed approach leverages error-state dynamics and employs exponential coordinates to represent orientation errors, enabling a linearized system representation for real-time control. The control strategy integrates an LQR-based full-state feedback controller for trajectory tracking, combined with a cascaded bodyrate controller to handle actuator dynamics. Detailed derivations of the error-state dynamics, the linearization process, and the controller design are provided, highlighting the applicability of the method for precise and stable quadrotor control in dynamic environments."
1926,679d459debd8ffd557a2b5f3,cs.RO,https://arxiv.org/pdf/2501.15426,FAVbot: An Autonomous Target Tracking Micro-Robot with Frequency Actuation Control,"Zhijian Hao, Ashwin Lele, Yan Fang, Arijit Raychowdhury, Azadeh Ansari","Robotics, Systems and Control","Robotic autonomy at centimeter scale requires compact and miniaturization-friendly actuation integrated with sensing and neural network processing assembly within a tiny form factor. Applications of such systems have witnessed significant advancements in recent years in fields such as healthcare, manufacturing, and post-disaster rescue. The system design at this scale puts stringent constraints on power consumption for both the sensory front-end and actuation back-end and the weight of the electronic assembly for robust operation. In this paper, we introduce FAVbot, the first autonomous mobile micro-robotic system integrated with a novel actuation mechanism and convolutional neural network (CNN) based computer vision - all integrated within a compact 3-cm form factor. The novel actuation mechanism utilizes mechanical resonance phenomenon to achieve frequency-controlled steering with a single piezoelectric actuator. Experimental results demonstrate the effectiveness of FAVbot’s frequency-controlled actuation, which offers a diverse selection of resonance modes with different motion characteristics. The actuation system is complemented with the vision front-end where a camera along with a microcontroller supports object detection for closed-loop control and autonomous target tracking. This enables adaptive navigation in dynamic environments. This work contributes to the evolving landscape of neural network-enabled micro-robotic systems showing the smallest autonomous robot built using controllable multi-directional single-actuator mechanism."
1927,679d459debd8ffd557a2b5f4,cs.RO,https://arxiv.org/pdf/2501.15272,Safe and Agile Transportation of Cable-Suspended Payload via Multiple Aerial Robots,"Yongchao Wang, Junjie Wang, Xiaobin Zhou, Tiankai Yang, Chao Xu, Fei Gao",Robotics,"Transporting a heavy payload using multiple aerial robots (MARs) is an efficient manner to extend the load capacity of a single aerial robot. However, existing schemes for the multiple aerial robots transportation system (MARTS) still lack the capability to generate a collision-free and dynamically feasible trajectory in real-time and further track an agile trajectory especially when there are no sensors available to measure the states of payload and cable. Therefore, they are limited to low-agility transportation in simple environments. To bridge the gap, we propose complete planning and control schemes for the MARTS, achieving safe and agile aerial transportation (SAAT) of a cable-suspended payload in complex environments. Flatness maps for the aerial robot considering the complete kinematical constraint and the dynamical coupling between each aerial robot and payload are derived. To improve the responsiveness for the generation of the safe, dynamically feasible, and agile trajectory in complex environments, a real-time spatio-temporal trajectory planning scheme is proposed for the MARTS. Besides, we break away from the reliance on the state measurement for both the payload and cable, as well as the closed-loop control for the payload, and propose a fully distributed control scheme to track the agile trajectory that is robust against imprecise payload mass and non-point mass payload. The proposed schemes are extensively validated through benchmark comparisons, ablation studies, and simulations. Finally, extensive real-world experiments are conducted on a MARTS integrated by three aerial robots with onboard computers and sensors. The result validates the efficiency and robustness of our proposed schemes for SAAT in complex environments."
1928,679d459debd8ffd557a2b5f5,cs.RO,https://arxiv.org/pdf/2501.15214,Zero-shot Robotic Manipulation with Language-guided Instruction and Formal Task Planning,"Junfeng Tang, Zihan Ye, Yuping Yan, Ziqi Zheng, Ting Gao, Yaochu Jin","Robotics, Machine Learning","Robotic manipulation is often challenging due to the long-horizon tasks and the complex object relationships. A common solution is to develop a task and motion planning framework that integrates planning for high-level task and low-level motion. Recently, inspired by the powerful reasoning ability of Large Language Models (LLMs), LLM-based planning approaches have achieved remarkable progress. However, these methods still heavily rely on expert-specific knowledge, often generating invalid plans for unseen and unfamiliar tasks. To address this issue, we propose an innovative language-guided symbolic task planning (LM-SymOpt) framework with optimization. It is the first expert-free planning framework since we combine the world knowledge from LLMs with formal reasoning, resulting in improved generalization capability to new tasks. Specifically, differ to most existing work, our LM-SymOpt employs LLMs to translate natural language instructions into symbolic representations, thereby representing actions as high-level symbols and reducing the search space for planning. Next, after evaluating the action probability of completing the task using LLMs, a weighted random sampling method is introduced to generate candidate plans. Their feasibility is assessed through symbolic reasoning and their cost efficiency is then evaluated using trajectory optimization for selecting the optimal planning. Our experimental results show that LM-SymOpt outperforms existing LLM-based planning approaches."
1929,679d459debd8ffd557a2b5f6,cs.RO,https://arxiv.org/pdf/2501.15078,"Impact-resistant, autonomous robots inspired by tensegrity architecture","William R. Johnson III, Xiaonan Huang, Shiyang Lu, Kun Wang, Joran W. Booth, Kostas Bekris, Rebecca Kramer-Bottiglio",Robotics,
1930,679d459debd8ffd557a2b5f7,cs.RO,https://arxiv.org/pdf/2501.15071,Understanding via Gaze: Gaze-based Task Decomposition for Imitation Learning of Robot Manipulation,"Ryo Takizawa, Yoshiyuki Ohmura, Yasuo Kuniyoshi",Robotics,"In imitation learning for robotic manipulation, decomposing object manipulation tasks into multiple semantic actions is essential.
This decomposition enables the reuse of learned skills in varying contexts and the combination of acquired skills to perform novel tasks, rather than merely replicating demonstrated motions.
Gaze, an evolutionary tool for understanding ongoing events, plays a critical role in human object manipulation, where it strongly correlates with motion planning.
In this study, we propose a simple yet robust task decomposition method based on gaze transitions.
We hypothesize that an imitation agent’s gaze control—fixating on specific landmarks and transitioning between them—naturally segments demonstrated manipulations into sub-tasks.
Notably, our method achieves consistent task decomposition across all demonstrations, which is desirable in contexts such as machine learning.
Using teleoperation—a common modality in imitation learning for robotic manipulation—we collected demonstration data for various tasks, applied our segmentation method, and evaluated the characteristics and consistency of the resulting sub-tasks. Furthermore, through extensive testing across a wide range of hyperparameter variations, we demonstrated that the proposed method possesses the robustness necessary for application to different robotic systems."
1931,679d459debd8ffd557a2b5f8,cs.RO,https://arxiv.org/pdf/2501.15068,An Atomic Skill Library Construction Method for Data-Efficient Embodied Manipulation,"Dongjiang Li, Bo Peng, Chang Li, Ning Qiao, Qi Zheng, Lei Sun, Yusen Qin, Bangguo Li, Yifeng Luan, Yibing Zhan, Mingang Sun, Tong Xu, Lusong Li, Hui Shen, Xiaodong He",Robotics,"Embodied manipulation is a fundamental ability in the realm of embodied artificial intelligence. Although current embodied manipulation models show certain generalizations in specific settings, they struggle in new environments and tasks due to the complexity and diversity of real-world scenarios. The traditional end-to-end data collection and training manner leads to significant data demands, which we call “data explosion”. To address the issue, we introduce a three-wheeled data-driven method to build an atomic skill library. We divide tasks into subtasks using the Vision-Language Planning (VLP). Then, atomic skill definitions are formed by abstracting the subtasks. Finally, an atomic skill library is constructed via data collection and Vision-Language-Action (VLA) fine-tuning. As the atomic skill library expands dynamically with the three-wheel update strategy, the range of tasks it can cover grows naturally. In this way, our method shifts focus from end-to-end tasks to atomic skills, significantly reducing data costs while maintaining high performance and enabling efficient adaptation to new tasks. Extensive experiments in real-world settings demonstrate the effectiveness and efficiency of our approach."
1932,679d459debd8ffd557a2b5f9,cs.RO,https://arxiv.org/pdf/2501.14816,Jump Point Search Pathfinding in 4-connected Grids,Johannes Baum,Robotics,"This work introduces JPS4, a novel pathfinding algorithm for 4-connected grid maps. JPS4 builds upon the Jump Point Search (JPS8) algorithm, originally designed for 8-connected environments. To achieve efficient pathfinding on 4-connected grids, JPS4 employs a canonical ordering and a successor function that enable online graph pruning. This reduces the search space by minimizing unnecessary node expansions."
1933,679d459debd8ffd557a2b5fa,cs.RO,https://arxiv.org/pdf/2501.14819,A Comprehensive Mathematical and System-Level Analysis of Autonomous Vehicle Timelines,Paul Perrone,"Multiagent Systems, Robotics","Fully autonomous vehicles (AVs) continue to spark immense global interest, yet predictions on when they will operate safely and broadly remain heavily debated. This paper synthesizes two distinct research traditions—computational complexity and algorithmic constraintsversusreliability growth modeling and real-world testing—to form an integrated, quantitative timeline for future AV deployment. We propose a mathematical framework that unifies NP-hard multi-agent path planning analyses, high-performance computing (HPC) projections, and extensive Crow-AMSAA reliability growth calculations, factoring inoperational design domain (ODD)variations, severity, and partial vs. full domain restrictions."
1934,679d459debd8ffd557a2b5fb,cs.RO,https://arxiv.org/pdf/2501.14557,Optimizing Grasping Precision for Industrial Pick-and-Place Tasks Through a Novel Visual Servoing Approach,Khairidine Benali,Robotics,"The integration of robotic arm manipulators into industrial manufacturing lines has become common, thanks to their efficiency and effectiveness in executing specific tasks. With advancements in camera technology, visual sensors and perception systems have been incorporated to address more complex operations.
This study introduces a novel visual servoing control system designed for robotic operations in challenging environments, where accurate object pose estimation is hindered by factors such as vibrations, tool path deviations, and machining marks. To overcome these obstacles, our solution focuses on enhancing the accuracy of picking and placing tasks, ensuring reliable performance across various scenarios.
This is accomplished by a novel visual servoing method based on
the integration of two complementary methodologies: a technique for object localization and a separate approach for precise control through visual feedback, leveraging their strengths to address the challenges posed by the industrial context and thereby improving overall grasping accuracy. Our method employ feedback from perception sensors to adjust the control loop efficiently, enabling the robotic system to adeptly pick and place objects. We have introduced a controller capable of seamlessly managing the detection and manipulation of various shapes and types of objects within an industrial context, addressing numerous challenges that arise in such environments."
1935,679d459debd8ffd557a2b5fc,cs.RO,https://arxiv.org/pdf/2501.14526,Robustified Time-optimal Point-to-point Motion Planning and Control under Uncertainty,"Shuhao Zhang, Jan Swevers","Robotics, Systems and Control","This paper proposes a novel approach to formulate time-optimal point-to-point motion planning and control under uncertainty.
The approach defines a robustified two-stage Optimal Control Problem (OCP), in which stage 1, with a fixed time grid, is seamlessly stitched with stage 2, which features a variable time grid.
Stage 1 optimizes not only the nominal trajectory, but also feedback gains and corresponding state covariances, which robustify constraints in both stages.
The outcome is a minimized uncertainty in stage 1 and a minimized total motion time for stage 2, both contributing to the time optimality and safety of the total motion.
A timely replanning strategy is employed to handle changes in constraints and maintain feasibility, while a tailored iterative algorithm is proposed for efficient, real-time OCP execution."
1936,679d459debd8ffd557a2b5fd,cs.RO,https://arxiv.org/pdf/2501.14486,Visual-Lidar Map Alignment for Infrastructure Inspections,"Jake McLaughlin, Nicholas Charron, Sriram Narasimhan",Robotics,"Routine and repetitive infrastructure inspections present safety, efficiency, and consistency challenges as they are performed manually, often in challenging or hazardous environments. They can also introduce subjectivity and errors into the process, resulting in undesirable outcomes. Simultaneous localization and mapping (SLAM) presents an opportunity to generate high-quality 3D maps that can be used to extract accurate and objective inspection data. Yet, many SLAM algorithms are limited in their ability to align 3D maps from repeated inspections in GPS-denied settings automatically. This limitation hinders practical long-term asset health assessments by requiring tedious manual alignment for data association across scans from previous inspections. This paper introduces a versatile map alignment algorithm leveraging both visual and lidar data for improved place recognition robustness and presents an infrastructure-focused dataset tailored for consecutive inspections. By detaching map alignment from SLAM, our approach enhances infrastructure inspection pipelines, supports monitoring asset degradation over time, and invigorates SLAM research by permitting exploration beyond existing multi-session SLAM algorithms."
1937,679d459debd8ffd557a2b5fe,cs.RO,https://arxiv.org/pdf/2501.14377,Dream to Fly: Model-Based Reinforcement Learning for Vision-Based Drone Flight,"Angel Romero, Ashwin Shenai, Ismail Geles, Elie Aljalbout, Davide Scaramuzza",Robotics,"Autonomous drone racing has risen as a challenging robotic benchmark for testing the limits of learning, perception, planning, and control.
Expert human pilots are able to agilely fly a drone through a race track by mapping the real-time feed from a single onboard camera directly to control commands.
Recent works in autonomous drone racing attempting direct pixel-to-commands control policies (without explicit state estimation) have relied on either intermediate representations that simplify the observation space or performed extensive bootstrapping using Imitation Learning (IL).
This paper introduces an approach that learns policies from scratch, allowing a quadrotor to autonomously navigate a race track by directly mapping raw onboard camera pixels to control commands, just as human pilots do.
By leveraging model-based reinforcement learning (RL)—specifically DreamerV3—we train visuomotor policies capable of agile flight through a race track using only raw pixel observations.
While model-free RL methods such as PPO struggle to learn under these conditions, DreamerV3 efficiently acquires complex visuomotor behaviors.
Moreover, because our policies learn directly from pixel inputs, the perception-aware reward term employed in previous RL approaches to guide the training process is no longer needed.
Our experiments demonstrate in both simulation and real-world flight how the proposed approach can be deployed on agile quadrotors.
This approach advances the frontier of vision-based autonomous flight and shows that model-based RL is a promising direction for real-world robotics."
1938,679d459debd8ffd557a2b5ff,cs.RO,https://arxiv.org/pdf/2501.14280,Enhancing Robotic Precision in Construction: A Modular Factor Graph-Based Framework to Deflection and Backlash Compensation Using High-Accuracy Accelerometers,"Julien Kindle, Michael Loetscher, Andrea Alessandretti, Cesar Cadena, Marco Hutter",Robotics,"Accurate positioning is crucial in the construction industry, where labor shortages highlight the need for automation. Robotic systems with long kinematic chains are required to reach complex workspaces, including floors, walls, and ceilings. These requirements significantly impact positioning accuracy due to effects such as deflection and backlash in various parts along the kinematic chain. In this work, we introduce a novel approach that integrates deflection and backlash compensation models with high-accuracy accelerometers, significantly enhancing position accuracy. Our method employs a modular framework based on a factor graph formulation to estimate the state of the kinematic chain, leveraging acceleration measurements to inform the model. Extensive testing on publicly released datasets, reflecting real-world construction disturbances, demonstrates the advantages of our approach. The proposed method reduces the95%percent9595\%95 %error threshold in the xy-plane by50%percent5050\%50 %compared to the state-of-the-art Virtual Joint Method, and by31%percent3131\%31 %when incorporating base tilt compensation."
1939,679d459debd8ffd557a2b600,cs.RO,https://arxiv.org/pdf/2501.14151,RaccoonBot: An Autonomous Wire-Traversing Solar-Tracking Robot for Persistent Environmental Monitoring,"Efrain Mendez-Flores, Agaton Pourshahidi, Magnus Egerstedt",Robotics,"Environmental monitoring is used to characterize the health and relationship between organisms and their environments. In forest ecosystems, robots can serve as platforms to acquire such data, even in hard-to-reach places where wire-traversing platforms are particularly promising due to their efficient displacement. This paper presents the RaccoonBot, which is a novel autonomous wire-traversing robot for persistent environmental monitoring, featuring a fail-safe mechanical design with a self-locking mechanism in case of electrical shortage. The robot also features energy-aware mobility through a novel Solar tracking algorithm, that allows the robot to find a position on the wire to have direct contact with solar power to increase the energy harvested. Experimental results validate the electro-mechanical features of the RaccoonBot, showing that it is able to handle wire perturbations, different inclinations, and achieving energy autonomy."
1940,679d459debd8ffd557a2b601,cs.RO,https://arxiv.org/pdf/2501.14147,"HAMMER: Heterogeneous, Multi-Robot Semantic Gaussian Splatting","Javier Yu, Timothy Chen, Mac Schwager",Robotics,"3D Gaussian Splatting offers expressive scene reconstruction, modeling a broad range of visual, geometric, and semantic information. However, efficient real-time map reconstruction with data streamed from multiple robots and devices remains a challenge. To that end, we propose HAMMER, a server-based collaborative Gaussian Splatting method that leverages widely available ROS communication infrastructure to generate 3D, metric-semantic maps from asynchronous robot data-streams with no prior knowledge of initial robot positions and varying on-device pose estimators. HAMMER consists of (i) a frame alignment module that transforms local SLAM poses and image data into a global frame and requires no prior relative pose knowledge, and (ii) an online module for training semantic 3DGS maps from streaming data. HAMMER handles mixed perception modes, adjusts automatically for variations in image pre-processing among different devices, and distills CLIP semantic codes into the 3D scene for open-vocabulary language queries.
In our real-world experiments, HAMMER creates higher-fidelity maps (2x) compared to competing baselines and is useful for downstream tasks, such as semantic goal-conditioned navigation (e.g., “go to the couch”). Accompanying content available athammer-project.github.io."
1941,679d459debd8ffd557a2b602,cs.RO,https://arxiv.org/pdf/2501.14099,The Perceived Danger (PD) Scale: Development and Validation,"Jaclyn Molan, Laura Saad, Eileen Roesler, J. Malcolm McCurry, Nathaniel Gyory, J. Gregory Trafton",Robotics,"There are currently no psychometrically valid tools to measure the perceived danger of robots. To fill this gap, we provided a definition of perceived danger and developed and validated a 12-item bifactor scale through four studies.
An exploratory factor analysis revealed
four subdimensions of perceived danger: affective states, physical vulnerability, ominousness, and cognitive readiness.
A confirmatory factor analysis confirmed the bifactor model.
We then compared the perceived danger scale to the Godspeed perceived safety scale and found that the perceived danger scale is a better predictor of empirical data. We also validated the scale in an in-person setting and found that the perceived danger scale is sensitive to robot speed manipulations, consistent with previous empirical findings.
Results across experiments suggest that the perceived danger scale is reliable, valid, and an adequate predictor of both perceived safety and perceived danger in human-robot interaction contexts."
1942,679d459debd8ffd557a2b603,cs.RO,https://arxiv.org/pdf/2501.14672,Gaussian-Process-based Adaptive Tracking Control with Dynamic Active Learning for Autonomous Ground Vehicles,"Kristóf Floch, Tamás Péni, Roland Tóth","Systems and Control, Robotics","This article proposes an active-learning-based adaptive trajectory tracking control method for autonomous ground vehicles to compensate for modeling errors and unmodeled dynamics. The nominal vehicle model is decoupled into lateral and longitudinal subsystems, which are augmented with online Gaussian Processes (GPs), using measurement data. The estimated mean functions of the GPs are used to construct a feedbackcompensator, which, together with an LPV state feedback controller designed for the nominal system, gives the adaptive control structure. To assist exploration of the dynamics, the paper proposes a new, dynamic active learning method to collect the most informative samples to accelerate the training process. To analyze the performance of theoverall learning tool-chain provided controller, a novel iterative, counterexample-based algorithm is proposed for calculating the inducedℒ2subscriptℒ2\mathcal{L}_{2}caligraphic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTgain between the reference trajectory and the tracking error.The analysis can be executed for a set of possible realizations of the to-be-controlled system, giving robust performance certificate of the learning method under variation of the vehicle dynamics.The efficiency of the proposed control approach is shown on a high-fidelity physics simulator and in real experiments using a 1/10 scale F1TENTH electric car."
1943,679d459debd8ffd557a2b604,cs.RO,https://arxiv.org/pdf/2501.14616,QuIP: Experimental design for expensive simulators with many Qualitative factors via Integer Programming,"Yen-Chun Liu, Simon Mak","Applications, Robotics","The need to explore and/or optimize expensive simulators with many qualitative factors arises in broad scientific and engineering problems. Our motivating application lies in path planning – the exploration of feasible paths for navigation, which plays an important role in robotics, surgical planning and assembly planning. Here, the feasibility of a path is evaluated via expensive virtual experiments, and its parameter space is typically discrete and high-dimensional. A carefully selected experimental design is thus essential for timely decision-making. We propose here a novel framework, called QuIP, for experimental design of Qualitative factors via Integer Programming under a Gaussian process surrogate model with an exchangeable covariance function. For initial design, we show that its asymptotic D-optimal design can be formulated as a variant of the well-known assignment problem in operations research, which can be efficiently solved to global optimality using state-of-the-art integer programming solvers. For sequential design (specifically, for active learning or black-box optimization), we show that its design criterion can similarly be formulated as an assignment problem, thus enabling efficient and reliable optimization with existing solvers. We then demonstrate the effectiveness of QuIP over existing methods in a suite of path planning experiments and an application to rover trajectory optimization."
1944,679d459debd8ffd557a2b605,cs.RO,https://arxiv.org/pdf/2501.14451,MARL-OT: Multi-Agent Reinforcement Learning Guided Online Fuzzing to Detect Safety Violation in Autonomous Driving Systems,"Linfeng Liang, Xi Zheng","Software Engineering, Robotics","Autonomous Driving Systems (ADSs) are safety-critical, as real-world safety violations can result in significant losses. Rigorous testing is essential before deployment, with simulation testing playing a key role. However, ADSs are typically complex, consisting of multiple modules such as perception and planning, or well-trained end-to-end autonomous driving systems. Offline methods, such as the Genetic Algorithm (GA), can only generate predefined trajectories for dynamics, which struggle to cause safety violations for ADSs rapidly and efficiently in different scenarios due to their evolutionary nature. Online methods, such as single-agent reinforcement learning (RL), can quickly adjust the dynamics’ trajectory online to adapt to different scenarios, but they struggle to capture complex corner cases of ADS arising from the intricate interplay among multiple vehicles. Multi-agent reinforcement learning (MARL) has a strong ability in cooperative tasks. On the other hand, it faces its own challenges, particularly with convergence. This paper introducesMARL-OT, a scalable framework that leverages MARL to detect safety violations of ADS resulting from surrounding vehicles’ cooperation.MARL-OTemploys MARL for high-level guidance, triggering various dangerous scenarios for the rule-based online fuzzer to explore potential safety violations of ADS, thereby generating dynamic, realistic safety violation scenarios. Our approach improves the detected safety violation rate by up to 136.2% compared to the state-of-the-art (SOTA) testing technique."
1945,679d459debd8ffd557a2b606,cs.SI,https://arxiv.org/pdf/2501.18555,An Empirical Study of Dotfiles Repositories Containing User-Specific Configuration Files,"Wenhan Zhu, Michael W. Godfrey","Software Engineering, Social and Information Networks","Storing user-specific configuration files in a “dotfiles” repository is a common practice among software developers, with hundreds of thousands choosing to publicly host their repositories onGitHub.
This practice not only provides developers with a simple backup mechanism for their essential configuration files, but also facilitates sharing ideas and learning from others on how best to configure applications that are key to their daily workflows.
However, our current understanding of these repository sharing practices is limited and mostly anecdotal.
To address this gap, we conducted a study to delve deeper into this phenomenon.
Beginning with collecting and analyzing publicly-hosteddotfilesrepositories onGitHub, we discovered that maintainingdotfilesis widespread among developers.
Notably, we found that 25.8% of the top 500 most-starredGitHubusers maintain some form of publicly accessibledotfilesrepository.
Among these, configurations for text editors likeVimand shells such asbashandzshare the most commonly tracked.
Our analysis reveals that updatingdotfilesis primarily driven by the need to adjust configurations (63.3%) and project meta-management (25.4%).
Surprisingly, we found no significant difference in the types ofdotfilesobserved across code churn history patterns, suggesting that the frequency ofdotfilemodifications depends more on the developer than the properties of the specificdotfileand its associated application.
Finally, we discuss the challenges associated with managingdotfiles, including the necessity for a reliable and effective deployment mechanism, and how the insights gleaned fromdotfilescan inform tool designers by offering real-world usage information."
1946,679d459debd8ffd557a2b607,cs.SI,https://arxiv.org/pdf/2501.17951,An iterative spectral algorithm for digraph clustering,"James Martin, Tim Rogers, Luca Zanetti","Physics and Society, Social and Information Networks","Graph clustering is a fundamental technique in data analysis with applications in many different fields. While there is a large body of work on clustering undirected graphs, the problem of clustering directed graphs is much less understood. The analysis is more complex in the directed graph case for two reasons: the clustering must preserve directional information in the relationships between clusters, and directed graphs have non-Hermitian adjacency matrices whose properties are less conducive to traditional spectral methods. Here we consider the problem of partitioning the vertex set of a directed graph intok≥2𝑘2k\geq 2italic_k ≥ 2clusters so that edges between different clusters tend to follow the same direction. We present an iterative algorithm based on spectral methods applied to new Hermitian representations of directed graphs. Our algorithm performs favourably against the state-of-the-art, both on synthetic and real-world data sets. Additionally, it is able to identify a “meta-graph” ofk𝑘kitalic_kvertices that represents the higher-order relations between clusters in a directed graph. We showcase this capability on data sets pertaining food webs, biological neural networks, and the online card game Hearthstone.
graph clustering, directed graphs, spectral methods."
1947,679d459debd8ffd557a2b608,cs.SI,https://arxiv.org/pdf/2501.17042,Emergence of network communities driven by local rules,Alexei Vazquez,"Physics and Society, Disordered Systems and Neural Networks, Social and Information Networks","Natural systems are modeled by networks where nodes represent the system units and links their interactions. The networks nodes are often segregated into communities with different connectivity patterns. Node heterogeneity such as political affiliation in social networks or biological function in gene networks are highlighted as key factors driving the segregation of nodes into communities. Here I demonstrate that node heterogeneity is not a necessary requirement. Network communities are bound to emerge as a consequence of the local nature of the network evolution. To this end I introduce the Ramsey community number,rCsubscript𝑟𝐶r_{C}italic_r start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT, the minimum graph size that warranties the emergence of network communities with almost certainty. I show that the Watts-Strogatz, local search and duplication-split network models all have finiterCsubscript𝑟𝐶r_{C}italic_r start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPTvalues. In contrast, random graphs do not have emergent communities property and the Barabási-Albert model does not reach certainty for the emergence of communities. I conclude that network communities are an emergent property rooted on the local nature of the network evolution."
1948,679d459debd8ffd557a2b609,cs.SI,https://arxiv.org/pdf/2501.17831,TikTok's recommendations skewed towards Republican content during the 2024 U.S. presidential race,"Hazem Ibrahim, HyunSeok Daniel Jang, Nouar Aldahoul, Aaron R. Kaufman, Talal Rahwan, Yasir Zaki","Social and Information Networks, Computers and Society","TikTok is a major force among social media platforms with over a billion monthly active users worldwide and 170 million in the United States. The platform’s status as a key news source, particularly among younger demographics, raises concerns about its potential influence on politics in the U.S. and globally. Despite these concerns, there is scant research investigating TikTok’s recommendation algorithm for political biases. We fill this gap by conducting 323 independent algorithmic audit experiments testing partisan content recommendations in the lead-up to the 2024 U.S. presidential elections. Specifically, we create hundreds of “sock puppet” TikTok accounts in Texas, New York, and Georgia, seeding them with varying partisan content and collecting algorithmic content recommendations for each of them. Collectively, these accounts viewed∼similar-to\sim∼394,000 videos from April 30th to November 11th, 2024, which we label for political and partisan content. Our analysis reveals significant asymmetries in content distribution: Republican-seeded accounts received∼similar-to\sim∼11.8% more party-aligned recommendations compared to their Democratic-seeded counterparts, and Democratic-seeded accounts were exposed to∼similar-to\sim∼7.5% more opposite-party recommendations on average. These asymmetries exist across all three states and persist when accounting for video- and channel-level engagement metrics such as likes, views, shares, comments, and followers, and are driven primarily by negative partisanship content. Our findings provide insights into the inner workings of TikTok’s recommendation algorithm during a critical election period, raising fundamental questions about platform neutrality."
1949,679d459debd8ffd557a2b60a,cs.SI,https://arxiv.org/pdf/2501.17817,Improving community detection via community association strength scores,"Jordan Barrett, Ryan DeWolfe, Bogumił Kamiński, Paweł Prałat, Aaron Smith, François Théberge",Social and Information Networks,"Community detection methods play a central role in understanding complex networks by revealing highly connected subsets of entities. However, most community detection algorithms generate partitions of the nodes, thus (i) forcing every node to be part of a community and (ii) ignoring the possibility that some nodes may be part of multiple communities. In our work, we investigate three simple community association strength (CAS) scores and their usefulness as post-processing tools given some partition of the nodes. We show that these measures can be used to improve node partitions, detect outlier nodes (not part of any community), and help find nodes with multiple community memberships."
1950,679d459debd8ffd557a2b60b,cs.SI,https://arxiv.org/pdf/2501.17720,Parsimonious Hawkes Processes for temporal networks modelling,"Yuwei Zhu, Paolo Barucca","Social and Information Networks, Data Analysis, Statistics and Probability","Temporal networks are characterised by interdependent link events between nodes, forming ordered sequences of links that may represent specific information flows in the system.
Nevertheless, representing temporal networks using discrete snapshots in time partially cancels the effect of time-ordered links on each other, while continuous time models, such as Poisson or Hawkes processes, can describe the full influence between all the potential pairs of links at all times.
In this paper, we introduce a continuous Hawkes temporal network model which accounts both for a community structure of the aggregate network and a strong heterogeneity in the activity of individual nodes, thus accounting for the presence of highly heterogeneous clusters with isolated high-activity influencer nodes, communities and low-activity nodes.
Our model improves the prediction performance of previously available continuous time network models, and obtains a systematic increase in log-likelihood.
Characterising the direct interaction between influencer nodes and communities, we can provide a more detailed description of the system that can better outline the sequence of activations in the components of the systems represented by temporal networks."
1951,679d459debd8ffd557a2b60c,cs.SI,https://arxiv.org/pdf/2501.17774,Percolation and localisation: Sub-leading eigenvalues of the nonbacktracking matrix,"James Martin, Tim Rogers, Luca Zanetti","Physics and Society, Social and Information Networks","The spectrum of the nonbacktracking matrix associated to a network is known to contain fundamental information regarding percolation properties of the network. Indeed, the inverse of its leading eigenvalue is often used as an estimate for the percolation threshold.
However, for many networks with nonbacktracking centrality localised on a few nodes, such as networks with a core-periphery structure, this spectral approach badly underestimates the threshold.
In this work, we study networks that exhibit this localisation effect by looking beyond the leading eigenvalue and searching deeper into the spectrum of the nonbacktracking matrix.
We identify that, when localisation is present,
the threshold often more closely aligns with the inverse of one of the sub-leading real eigenvalues: the largest real eigenvalue with a “delocalised” corresponding eigenvector.
We investigate a core-periphery network model and determine, both theoretically and experimentally, a regime of parameters for which our approach closely approximates the threshold, while the estimate derived using the leading eigenvalue does not. We further present experimental results on large scale real-world networks that showcase the usefulness of our approach.

percolation; localisation; nonbacktracking spectrum"
1952,679d459debd8ffd557a2b60d,cs.SI,https://arxiv.org/pdf/2501.17300,Dilemmas and trade-offs in the diffusion of conventions,Lucas Gautheron,"Physics and Society, Social and Information Networks, Applications",
1953,679d459debd8ffd557a2b60e,cs.SI,https://arxiv.org/pdf/2501.16624,More Efficient Sybil Detection Mechanisms Leveraging Resistance of Users to Attack Requests,"Ali Safarpoor Dehkordi, Ahad N. Zehmakan",Social and Information Networks,"We investigate the problem of
sybil (fake account) detection in social networks from a graph algorithms perspective, where graph structural information is used to classify users as sybil and benign. We introduce the novel notion of user resistance to attack requests (friendship requests from sybil accounts). Building on this notion, we propose a synthetic graph data generation framework that supports various attack strategies. We then study the optimization problem where we are allowed to reveal the resistance of a subset of users with the aim to maximize the number of users which are discovered to be benign and the number of potential attack edges (connections from a sybil to a benign user). Furthermore, we devise efficient algorithms for this problem and investigate their theoretical guarantees.
Finally, through a large set of experiments, we demonstrate that our proposed algorithms improve detection performance notably when applied as a preprocessing step for different sybil detection algorithms.
The code and data used in this work are publicly available on GitHub.111GitHub repository:https://github.com/aSafarpoor/AAMAS2025-Paper/tree/main"
1954,679d459debd8ffd557a2b60f,cs.SI,https://arxiv.org/pdf/2501.16210,New Frontiers in Fighting Misinformation,"Harith Alani, Grégoire Burel",Social and Information Networks,
1955,679d459debd8ffd557a2b610,cs.SI,https://arxiv.org/pdf/2501.16193,Posting Patterns of Members of Parental Subreddits,"Nazanin Sabri, Mai Elsherief",Social and Information Networks,"Online forums (e.g., Reddit) are used by many parents to discuss their challenges, needs, and receive support. While studies have investigated the contents of posts made to popular parental subreddits revealing the family health concerns being expressed, little is known about parents’ posting patterns or other issues they engage in. In this study, we explore the posting activity of users of 55 parental subreddits. Exploring posts made by these users (667K) across Reddit (34M posts) reveals that over 85% of posters are not one-time users of Reddit and actively engage with the community. Studying cross-posting patterns also reveals the use of subreddits dedicated to other topics such as relationship and health advice (e.g., r/AskDocs, r/relationship_advice) by this population. As a result, for a comprehensive understanding of the type of information posters share and seek, future work should investigate sub-communities outside of parental-specific ones. Finally, we expand the list of parental subreddits, compiling a total of 115 subreddits that could be utilized in future studies of parental concerns."
1956,679d459debd8ffd557a2b611,cs.SI,https://arxiv.org/pdf/2501.16076,Minimizing Polarization and Disagreement in the Friedkin-Johnsen Model with Unknown Innate Opinions,"Federico Cinus, Atsushi Miyauchi, Yuko Kuroki, Francesco Bonchi",Social and Information Networks,"The bulk of the literature on opinion optimization in social networks adopts the Friedkin–Johnsen (FJ) opinion dynamics model, in which the innate opinions of all nodes are known: this is an unrealistic assumption.
In this paper, we study opinion optimization under the FJ model without the full knowledge of innate opinions. Specifically, we borrow from the literature a series of objective functions, aimed at minimizing polarization and/or disagreement, and we tackle the budgeted optimization problem, where we can query the innate opinions of only a limited number of nodes.
Given the complexity of our problem, we propose a framework based on three steps: (1) select the limited number of nodes we query, (2) reconstruct the innate opinions of all nodes based on those queried, and (3) optimize the objective function with the reconstructed opinions. For each step of the framework, we present and systematically evaluate several effective strategies. A key contribution of our work is a rigorous error propagation analysis that quantifies how reconstruction errors in innate opinions impact the quality of the final solutions.
Our experiments on various synthetic and real-world datasets show that we can effectively minimize polarization and disagreement even if we have quite limited information about innate opinions."
1957,679d459debd8ffd557a2b612,cs.SI,https://arxiv.org/pdf/2501.16004,Epidemics on the Move: How Public Transport Demand and Capacity Shape Disease Spread,"László Hajdu, Jovan Pavlović, Miklós Krész, András Bóta","Social and Information Networks, Physics and Society","Understanding the dynamics of passenger interactions and their epidemiological impact throughout public transportation systems is crucial for both service efficiency and public health. High passenger density and close physical proximity has been shown to accelerate the spread of infectious diseases. During the COVID-19 pandemic, many public transportation companies took measures to slow down and minimize disease spreading. One of these measures was introducing spacing and capacity constraints to public transit vehicles. Our objective is to explore the effects of demand changes and transportation measures from an epidemiological point of view, offering alternative measures to public transportation companies to keep the system alive while minimizing the epidemiological risk as much as possible."
1958,679d459debd8ffd557a2b613,cs.SI,https://arxiv.org/pdf/2501.15713,Modeling shared micromobility as a label propagation process for detecting the overlapping communities,"Peng Luo, Chengyu Song, Hao Li, Di Zhu, Fabio Duarte","Social and Information Networks, Applied Physics","Shared micro-mobility such as electric scooters (e-scooters) has gained significant popularity in many cities. While many studies have analyzed the spatiotemporal patterns of shared micro-mobility using individual-level trip data, the spatial structure of e-scooter mobility networks and their socio-economic implications remain underexplored. Examining these mobility networks through the lens of network science—such as analyzing their community structures—can provide valuable insights for urban policy and planning. For example, allocating e-scooters at the overlapping locations of two communities may improve the operational efficiency of e-scooter distribution. However, existing methods for detecting community structures in mobility networks often overlook potential overlaps between communities. In this study, we conceptualize shared micro-mobility in urban spaces as a process of information exchange, where locations are connected through e-scooters, facilitating the interaction and propagation of community affiliations. As a result, similar locations are assigned the same label. Based on this concept, we developed a Geospatial Interaction Propagation model (GIP) by designing a Speaker-Listener Label Propagation Algorithm (SLPA) that accounts for geographic distance decay, incorporating anomaly detection to ensure the derived community structures reflect meaningful spatial patterns.We applied this model to detect overlapping communities within the e-scooter system in Washington, D.C. The results demonstrate that our algorithm outperforms existing model of overlapping community detection in both efficiency and modularity. Additionally, we discovered significant social segregation within the overlapping communities: areas belong to multiple communities tend to be wealthier with shorter commute times. Our results provide a potential explanation for the community structure in human mobility networks and may offer insights for urban planning and policymaking aimed at creating a more equitable and accessible mobility system."
1959,679d459debd8ffd557a2b614,cs.SI,https://arxiv.org/pdf/2501.15539,Studying Behavioral Addiction by Combining Surveys and Digital Traces: A Case Study of TikTok,"Cai Yang, Sepehr Mousavi, Abhisek Dash, Krishna P. Gummadi, Ingmar Weber","Social and Information Networks, Computers and Society","Opaque algorithms disseminate and mediate the content that users consume on online social media platforms.
This algorithmic mediation serves users with contents of their liking, on the other hand, it may cause several inadvertent risks to society at scale.
While some of these risks, e.g., filter bubbles or dissemination of hateful content, are well studied in the community,behavioral addiction, designated by the Digital Services Act (DSA) as a potential systemic risk, has been understudied.
In this work, we aim to study if one can effectively diagnose behavioral addiction using digital data traces from social media platforms.
Focusing on the TikTok short-format video platform as a case study, we employ a novel mixed methodology of combining survey responses with data donations of behavioral traces.
We survey1590159015901590TikTok users and stratify them into three addiction groups (i.e., less/moderately/highly likely addicted).
Then, we obtain data donations from107107107107surveyed participants.
By analyzing users’ data we find that, among others, highly likely addicted users spend more time watching TikTok videos and keep coming back to TikTok throughout the day, indicating a compulsion to use the platform.
Finally, by using basic user engagement features, we train classifier models to identify highly likely addicted users withF1≥0.55subscript𝐹10.55F_{1}\geq 0.55italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≥ 0.55. The performance of the classifier models suggests predicting addictive users solely based on their usage is rather difficult."
1960,679d459debd8ffd557a2b615,cs.SI,https://arxiv.org/pdf/2501.15130,Community Detection in Large-Scale Complex Networks via Structural Entropy Game,"Yantuan Xian, Pu Li, Hao Peng, Zhengtao Yu, Yan Xiang, Philip S. Yu",Social and Information Networks,"Community detection is a critical task in graph theory, social network analysis, and bioinformatics, where communities are defined as clusters of densely interconnected nodes. However, detecting communities in large-scale networks with millions of nodes and billions of edges remains challenging due to the inefficiency and unreliability of existing methods. Moreover, many current approaches are limited to specific graph types, such as unweighted or undirected graphs, reducing their broader applicability.
To address these issues, we propose a novel heuristic community detection algorithm, termed CoDeSEG, which identifies communities by minimizing the two-dimensional (2D) structural entropy of the network within a potential game framework. In the game, nodes decide to stay in current community or move to another based on a strategy that maximizes the 2D structural entropy utility function.
Additionally, we introduce a structural entropy-based node overlapping heuristic for detecting overlapping communities, with a near-linear time complexity.
Experimental results on real-world networks demonstrate that CoDeSEG  is the fastest method available and achieves state-of-the-art performance in overlapping normalized mutual information (ONMI) and F1 score."
1961,679d459debd8ffd557a2b616,cs.SI,https://arxiv.org/pdf/2501.15048,YouTube Recommendations Reinforce Negative Emotions: Auditing Algorithmic Bias with Emotionally-Agentic Sock Puppets,"Hussam Habib, Rishab Nithyanand","Social and Information Networks, Computers and Society","Personalized recommendation algorithms, like those on YouTube, significantly shape online content consumption.
These systems aim to maximize engagement by learning users’ preferences and aligning content accordingly but may unintentionally reinforce impulsive and emotional biases.
Using a sock-puppet audit methodology, this study examines YouTube’s capacity to recognize and reinforce emotional preferences.
Simulated user accounts with assigned emotional preferences navigate the platform, selecting videos that align with their assigned preferences and recording subsequent recommendations.
Our findings reveal reveal that YouTube amplifies negative emotions, such as anger and grievance, by increasing their prevalence and prominence in recommendations.
This reinforcement intensifies over time and persists across contexts.
Surprisingly, contextual recommendations often exceed personalized ones in reinforcing emotional alignment.
These findings suggest the algorithm amplifies user biases, contributing to emotional filter bubbles and raising concerns about user well-being and societal impacts.
The study emphasizes the need for balancing personalization with content diversity and user agency."
1962,679d459debd8ffd557a2b617,cs.SI,https://arxiv.org/pdf/2501.14939,Principal Graph Encoder Embedding and Principal Community Detection,"Cencheng Shen, Yuexiao Dong, Carey E. Priebe, Jonathan Larson, Ha Trinh, Youngser Park","Social and Information Networks, Machine Learning","In this paper, we introduce the concept of principal communities and propose a principal graph encoder embedding method that concurrently detects these communities and achieves vertex embedding. Given a graph adjacency matrix with vertex labels, the method computes a sample community score for each community, ranking them to measure community importance and estimate a set of principal communities. The method then produces a vertex embedding by retaining only the dimensions corresponding to these principal communities. Theoretically, we define the population version of the encoder embedding and the community score based on a random Bernoulli graph distribution. We prove that the population principal graph encoder embedding preserves the conditional density of the vertex labels and that the population community score successfully distinguishes the principal communities. We conduct a variety of simulations to demonstrate the finite-sample accuracy in detecting ground-truth principal communities, as well as the advantages in embedding visualization and subsequent vertex classification. The method is further applied to a set of real-world graphs, showcasing its numerical advantages, including robustness to label noise and computational scalability."
1963,679d459debd8ffd557a2b618,cs.SI,https://arxiv.org/pdf/2501.14830,Sharp exact recovery threshold for two-community Euclidean random graphs,"Julia Gaudio, Charlie K. Guan","Social and Information Networks, Probability","This paper considers the problem of label recovery in random graphs and matrices. Motivated by transitive behavior in real-world networks (i.e., “the friend of my friend is my friend”), a recent line of work considers spatially-embedded networks, which exhibit transitive behavior. In particular, the Geometric Hidden Community Model (GHCM), introduced by Gaudio, Guan, Niu, and Wei, models a network as a labeled Poisson point process where every pair of vertices is associated with a pairwise observation whose distribution depends on the labels and positions of the vertices. The GHCM is in turn a generalization of the Geometric SBM (proposed by Baccelli and Sankararaman). Gaudio et al. provided a threshold below which exact recovery is information-theoretically impossible. Above the threshold, they provided a linear-time algorithm that succeeds in exact recovery under a certain “distinctness-of-distributions” assumption, which they conjectured to be unnecessary. In this paper, we partially resolve the conjecture by showing that the threshold is indeed tight for the two-community GHCM. We provide a two-phase, linear-time algorithm that explores the spatial graph in a data-driven manner in Phase I to yield an almost exact labeling, which is refined to achieve exact recovery in Phase II. Our results extend achievability to geometric formulations of well-known inference problems, such as the planted dense subgraph problem and submatrix localization, in which the distinctness-of-distributions assumption does not hold."
1964,679d459debd8ffd557a2b619,cs.SI,https://arxiv.org/pdf/2501.16070,Generalizing Egocentric Temporal Neighborhoods to probe for spatial correlations in temporal networks and infer their topology,Didier Le Bail,"Physics and Society, Social and Information Networks","Motifs are thought to be some fundamental components of social face-to-face interaction temporal networks.
However, the motifs previously considered are either limited to a handful of nodes and edges, or do not include triangles, which are thought to be of critical relevance to understand the dynamics of social systems.
Thus, we introduce a new class of motifs, that include these triangles, are not limited in their number of nodes or edges, and yet can be mined efficiently in any temporal network.
Referring to these motifs as the edge-centered motifs, we show analytically how they subsume the Egocentric Temporal Neighborhoods motifs of [A. Longa, G. Cencetti, B. Lepri, and A. Passerini, An efficient procedure for mining egocentric temporal motifs,Data Mining and Knowledge Discovery36, 355 (2022)].
We also confirm in empirical data that the edge-centered motifs bring relevant information with respect to the Egocentric motifs by using a
principle of maximum entropy.
Then, we show how mining for the edge-centered motifs in a network can be used to probe for spatial correlations in the underlying dynamics that have produced that network.
We deduce an approximate formula for the distribution of the edge-centered motifs in empirical networks of social face-to-face interactions.
In the last section of this paper, we explore how the statistics of the edge-centered motifs can be used to infer the complete topology of the network they were sampled from.
This leads to the needs of mathematical development, that we inaugurate here under the name of graph tiling theory."
1965,679d459debd8ffd557a2b61a,cs.SI,https://arxiv.org/pdf/2501.15920,Vienna Mosaic: Navigating Social Borders in a Melting Pot,"Marc Sadurní, Samuel Martin-Gutierrez, Ola Ali, Ana María Jaramillo, Rafael Prieto-Curiel, Fariba Karimi","Physics and Society, Social and Information Networks, Data Analysis, Statistics and Probability","Urban segregation poses a critical challenge in cities, exacerbating inequalities, social tensions, fears, and polarization. It emerges from a complex interplay of socioeconomic disparities and residential preferences, disproportionately impacting migrant communities. In this paper, using a comprehensive administrative data from Vienna, where nearly 40% of the population consists of international migrants, we analyse co-residence preferences between migrants and locals at the neighbourhood level. Our findings reveal two major clusters in Vienna shaped by wealth disparities, district diversity, and nationality-based homophily. These insights shed light on the underlying mechanisms of urban segregation and designing policies for better integration."
1966,679d459debd8ffd557a2b61b,cs.SI,https://arxiv.org/pdf/2501.15552,"Community-centric modeling of citation dynamics explains collective citation patterns in science, law, and patents","Sadamori Kojaku, Robert Mahari, Sandro Claudio Lera, Esteban Moro, Alex Pentland, Yong-Yeol Ahn","Physics and Society, Social and Information Networks","Many human knowledge systems, such as science, law, and invention, are built on documents and the citations that link them. Citations, while serving multiple purposes[1,2,3], primarily function as a way to explicitly document the use of prior work and thus have become central to the study of knowledge systems[4,5,6].
Analyzing citation dynamics has revealed statistical patterns that shed light on knowledge production, recognition, and formalization[7,5,8,9,10,11,12,13,14], and has helped identify key mechanisms driving these patterns[15,5,16].
However, most quantitative findings are confined to scientific citations, raising the question of universality of these findings.
Moreover, existing models of individual citation trajectories fail to explain phenomena such as delayed recognition, calling for a unifying framework.
Here, we analyze a newly available corpus of U.S. case law, in addition to scientific and patent citation networks, to show that they share remarkably similar citation patterns, including a heavy-tailed distribution of sleeping beauties. We propose a holistic model that captures the three core mechanisms driving collective dynamics and replicates the elusive phenomenon of delayed recognition.
We demonstrate that the model not only replicates observed citation patterns, but also better predicts future successes by considering the whole system. Our work offers insights into key mechanisms that govern large-scale patterns of collective human knowledge systems and may provide generalizable perspectives on discovery and innovation across domains."
1967,679d459debd8ffd557a2b61c,cs.SI,https://arxiv.org/pdf/2501.14762,Linked Data on Geo-annotated Events and Use Cases for the Resilience of Ukraine,"Manar Attar, Shuai Wang, Ronald Siebes, Eirik Kultorp, Zhisheng Huang, Tianyang Lu","Computers and Society, Social and Information Networks","The mission of resilience of Ukrainian cities calls for international collaboration with the scientific community to increase the quality of information by identifying and integrating information from various news and social media sources. Linked Data technology can be used to unify, enrich, and integrate data from multiple sources. In our work, we focus on datasets about damaging events in Ukraine due to Russia’s invasion between February 2022 and the end of April 2023. We convert two selected datasets to Linked Data and enrich them with additional geospatial information. Following that, we present an algorithm for the detection of identical events from different datasets. Our pipeline makes it easy to convert and enrich datasets to integrated Linked Data. The resulting dataset consists of 10K reported events covering damage to hospitals, schools, roads, residential buildings, etc. Finally, we demonstrate in use cases how our dataset can be applied to different scenarios for resilience purposes.111The paper is an extended version of our 2023 paper titled ‘Converting and Enriching Geo-annotated Event Data: Integrating Information for Ukraine Resilience’[sigspatial]presented at the ACM International Conference on Advances in Geographic Information Systems (SIGSPATIAL conference). It was included in theSIGSPATIAL ’23: Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems. The use cases were presented at the BNAIC-BeNeLearn Joint International Scientific Conferences on A.I. and Machine Learning. This long paper is the result of a merge of them with some minor extensions and additional explanations. Please cite our SIGSPATIAL paper instead. Its DOI is 10.1145/3589132.3625580. Related information and resources can be found on the Linked4Resilience platform (https://www.linked4resilience.eu)."
1968,679d459debd8ffd557a2b61d,cs.SI,https://arxiv.org/pdf/2501.14600,On the Homophily of Heterogeneous Graphs: Understanding and Unleashing,"Zhen Tao, Ziyue Qiao, Chaoqi Chen, Zhengyi Yang, Lun Du, Qingqiang Sun",Social and Information Networks,"Homophily, the tendency of similar nodes to connect, is a fundamental phenomenon in network science and a critical factor in the performance of graph neural networks (GNNs). While existing studies primarily explore homophily in homogeneous graphs, where nodes share the same type, real-world networks are often more accurately modeled as heterogeneous graphs (HGs) with diverse node types and intricate cross-type interactions. This structural diversity complicates the analysis of homophily, as traditional homophily metrics fail to account for distinct label spaces across node types. To address this limitation, we introduce the Cross-Type Homophily Ratio, a novel metric that quantifies homophily based on the similarity of target information across different node types. Furthermore, we introduce Cross-Type Homophily-guided Heterogeneous Graph Pruning, a method designed to selectively remove low-homophily cross-type edges, thereby enhancing the Cross-Type Homophily Ratio and boosting the performance of heterogeneous graph neural networks (HGNNs). Extensive experiments on five real-world HG datasets validate the effectiveness of our approach, which delivers up to 13.36% average relative performance improvement for HGNNs, offering a fresh perspective on cross-type homophily in heterogeneous graph learning."
1969,679d459debd8ffd557a2b61e,cs.SI,https://arxiv.org/pdf/2501.14637,The Paradox of Intervention: Resilience in Adaptive Multi-Role Coordination Networks,"Casper van Elteren, Vítor V. Vasconcelos, Mike H. Lees","Physics and Society, Social and Information Networks",
1970,679d459debd8ffd557a2b61f,cs.SI,https://arxiv.org/pdf/2501.14476,Avoiding Overfitting in Variable-Order Markov Models: a Cross-Validation Approach,"Valeria Secchini, Javier Garcia-Bernardo, Petr Janský","Physics and Society, Social and Information Networks, General Economics",
1971,679d459debd8ffd557a2b620,cs.SI,https://arxiv.org/pdf/2501.14196,PASER: A Physics-Inspired Theory for Stimulated Growth and Real-Time Optimization in On-Demand Platforms,Ioannis Dritsas,"Physics and Society, Social and Information Networks, Theoretical Economics",
1972,679d459debd8ffd557a2b621,cs.SE,https://arxiv.org/pdf/2501.18482,A Tool for In-depth Analysis of Code Execution Reasoning of Large Language Models,"Changshu Liu, Reyhaneh Jabbarvand",Software Engineering,"Code Executing Reasoning is becoming a new non-functional metric that assesses the ability of large language models (LLMs) in programming tasks. State-of-the-art frameworks (CodeMind or REval) and benchmarks (CruxEval) usually focus on LLM’s prediction of a given code’s input/output or intermediate variable states/values on limited programs. However, there is no tool for more in-depth analysis of the results. Without such a tool, the observations about LLM’s code execution reasoning cannot be generalized to more datasets, preventing the research community and practitioners from devising the next generation of LLMs with better code execution reasoning abilities. This paper introduces ExeRScope, a series of tools and heuristics to analyze the result of code execution reasoning frameworks to understand better the impact of code properties in the studied benchmarks on the code execution reasoning. With such tooling, analysis can be generalized to code with similar properties without the urgent need to design more benchmarks, which is a cumbersome effort."
1973,679d459debd8ffd557a2b622,cs.SE,https://arxiv.org/pdf/2501.18460,ExeCoder: Empowering Large Language Models with Executability Representation for Code Translation,"Minghua He, Fangkai Yang, Pu Zhao, Wenjie Yin, Yu Kang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang",Software Engineering,"Code translation is a crucial activity in the software development and maintenance process, and researchers have recently begun to focus on using pre-trained large language models (LLMs) for code translation. However, existing LLMs only learn the contextual semantics of code during pre-training, neglecting executability information closely related to the execution state of the code, which results in unguaranteed code executability and unreliable automated code translation. To address this issue, we propose ExeCoder, an LLM specifically designed for code translation, aimed at utilizing executability representations such as functional semantics, syntax structures, and variable dependencies to enhance the capabilities of LLMs in code translation. To evaluate the effectiveness of ExeCoder, we manually enhanced the widely used benchmark TransCoder-test, resulting in a benchmark called TransCoder-test-X that serves LLMs. Evaluation of TransCoder-test-X indicates that ExeCoder achieves state-of-the-art performance in code translation, surpassing existing open-source code LLMs by over 10.88% to 38.78% and over 27.44% to 42.97% on two metrics, and even outperforms the renowned closed-source LLM GPT-4o. Website:https://execoder4trans.github.io/"
1974,679d459debd8ffd557a2b623,cs.SE,https://arxiv.org/pdf/2501.18327,"PyExamine A Comprehensive, UnOpinionated Smell Detection Tool for Python","Karthik Shivashankar, Antonio Martini",Software Engineering,"The growth of Python adoption across diverse domains has led to increasingly complex codebases, presenting challenges in maintaining code quality. While numerous tools attempt to address these challenges, they often fall short in providing comprehensive analysis capabilities or fail to consider Python-specific contexts. PyExamine addresses these critical limitations through an approach to code smell detection that operates across multiple levels of analysis."
1975,679d459debd8ffd557a2b624,cs.SE,https://arxiv.org/pdf/2501.18257,DATCloud: A Model-Driven Framework for Multi-Layered Data-Intensive Architectures,"Moamin Abughazala, Henry Muccini",Software Engineering,"The complexity of multi-layered, data-intensive systems demands frameworks that ensure flexibility, scalability, and efficiency. DATCloud is a model-driven framework designed to facilitate the modeling, validation, and refinement of multi-layered architectures, addressing scalability, modularity, and real-world requirements. By adhering to ISO/IEC/IEEE 42010 standards, DATCloud leverages structural and behavioral meta-models and graphical domain-specific languages (DSLs) to enhance reusability and stakeholder communication. Initial validation through the VASARI system at the Uffizi Gallery demonstrates a 40% reduction in modeling time and a 32% improvement in flexibility compared to manual methods. While effective, DATCloud is a work in progress, with plans to integrate advanced code generation, simulation tools, and domain-specific extensions to further enhance its capabilities for applications in healthcare, smart cities, and other data-intensive domains."
1976,679d459debd8ffd557a2b625,cs.SE,https://arxiv.org/pdf/2501.18245,RESMETRIC: Analyzing Resilience to Enable Research on Antifragility,"Ferdinand Koenig, Marc Carwehl, Calum Imrie",Software Engineering,"A key feature in self-adaptive systems is resilience, which is an ongoing research topic.
Recently, the community started to explore antifragility, which describes the improvement of resilience over time.
While there are model-agnostic resilience metrics, there is currently no out-of-the-box tool for researchers and practitioners to determine to which degree their system is resilient.
To facilitate research on antifragility, we presentResMetric, a model-agnostic tool that calculates and visualizes various resilience metrics based on the quality of service over time. WithResMetric, researchers can evaluate their definition of resilience and antifragility.
This paper highlights howResMetriccan be employed by demonstrating its use in a case study on gas detection."
1977,679d459debd8ffd557a2b626,cs.SE,https://arxiv.org/pdf/2501.18230,Fast and Efficient What-If Analyses of Invocation Overhead and Transactional Boundaries to Support the Migration to Microservices,"Holger Knoche, Wilhelm Hasselbring",Software Engineering,"Improving agility and maintainability are common drivers for companies to adopt a microservice architecture for their existing software systems.
However, the existing software often relies heavily on the fact that it is executed within a single process space.
Therefore, decomposing existing software into out-of-process components like microservices can have a severe impact on non-functional properties, such as overall performance due to invocation overhead or data consistency."
1978,679d459debd8ffd557a2b627,cs.SE,https://arxiv.org/pdf/2501.18225,Toward Bundler-Independent Module Federations: Enabling Typed Micro-Frontend Architectures,"Billy Lando, Wilhelm Hasselbring",Software Engineering,"Modern web applications demand scalable and modular architectures, driving the adoption of micro-frontends. This paper introduces Bundler-Independent Module Federation (BIMF) as a New Idea, enabling runtime module loading without relying on traditional bundlers, thereby enhancing flexibility and team collaboration. This paper presents the initial implementation of BIMF, emphasizing benefits such as shared dependency management and modular performance optimization. We address key challenges, including debugging, observability, and performance bottlenecks, and propose solutions such as distributed tracing, server-side rendering, and intelligent prefetching. Future work will focus on evaluating observability tools, improving developer experience, and implementing performance optimizations to fully realize BIMF’s potential in micro-frontend architectures."
1979,679d459debd8ffd557a2b628,cs.SE,https://arxiv.org/pdf/2501.18160,RepoAudit: An Autonomous LLM-Agent for Repository-Level Code Auditing,"Jinyao Guo, Chengpeng Wang, Xiangzhe Xu, Zian Su, Xiangyu Zhang","Software Engineering, Programming Languages","Code auditing is a code review process with the goal of finding bugs. Large Language Models (LLMs) have shown substantial potential in this task,
offering the ability to analyze programs without compilation and enabling customized bug detection following specified prompts. However, applying LLMs to repository-level code auditing presents notable challenges. The inherent context limits and hallucinations of LLMs can lead to the low quality of bug reports. Meanwhile, the large size of software repositories introduces substantial time and token costs, hindering efficiency and scalability in real-world scenarios."
1980,679d459debd8ffd557a2b629,cs.SE,https://arxiv.org/pdf/2501.18145,Utilizing API Response for Test Refinement,"Devika Sondhi, Ananya Sharma, Diptikalyan Saha",Software Engineering,"Most of the web services are offered in the form of RESTful APIs. This has led to an active research interest in API testing to ensure the reliability of these services. While most of the testing techniques proposed in the past rely on the API specification to generate the test cases, a major limitation of such an approach is that in the case of an incomplete or inconsistent specification, the test cases may not be realistic in nature and would result in a lot of 4xx response due to invalid input. This is indicative of poor test quality. Learning-based approaches may learn about valid inputs but often require a large number of request-response pairs to learn the constraints, making it infeasible to be readily used in the industry. To address this limitation, this paper proposes a dynamic test refinement approach that leverages the response message. The response is used to infer the point in the API testing flow where a test scenario fix is required. Using an intelligent agent, the approach adds constraints to the API specification that are further used to generate a test scenario accounting for the learned constraint from the response. Following a greedy approach, the iterative learning and refinement of test scenarios are obtained from the API testing system. The proposed approach led to a decrease in the number of 4xx responses, taking a step closer to generating more realistic test cases with high coverage that would aid in functional testing. A high coverage was obtained from a lesser number of API requests, as compared with the state-of-the-art search-based API Testing tools."
1981,679d459debd8ffd557a2b62a,cs.SE,https://arxiv.org/pdf/2501.18514,Automating Physics-Based Reasoning for SysML Model Validation,"Candice Chambers, Summer Mueller, Parth Ganeriwala, Chiradeep Sen, Siddhartha Bhattacharyya","Systems and Control, Emerging Technologies, Software Engineering","System and software design benefits greatly from formal modeling, allowing for automated analysis and verification early in the design phase. Current methods excel at checking information flow and component interactions, ensuring consistency, and identifying dependencies within Systems Modeling Language (SysML) models. However, these approaches often lack the capability to perform physics-based reasoning about a system’s behavior represented in SysML models, particularly in the electromechanical domain. This significant gap critically hinders the ability to automatically and effectively verify the correctness and consistency of the model’s behavior against well-established underlying physical principles. Therefore, this paper presents an approach that leverages existing research on function representation, including formal languages, graphical representations, and reasoning algorithms, and integrates them with physics-based verification techniques. Four case studies (coffeemaker, vacuum cleaner, hairdryer, and wired speaker) are inspected to illustrate the model’s practicality and effectiveness in performing physics-based reasoning on systems modeled in SysML. This automated physics-based reasoning is broken into two main categories: (i) structural, which is performed on BDD and IBD, and (ii) functional, which is then performed on activity diagrams. This work advances the field of automated reasoning by providing a framework for verifying structural and functional correctness and consistency with physical laws within SysML models."
1982,679d459debd8ffd557a2b62b,cs.SE,https://arxiv.org/pdf/2501.17766,Formally Verified Binary-level Pointer Analysis,"Freek Verbeek, Ali Shokri, Daniel Engel, Binoy Ravindran",Software Engineering,"Binary-level pointer analysis can be of use in symbolic execution, testing, verification, and decompilation of software binaries.
In various such contexts, it is crucial that the result is trustworthy, i.e., it can be formally established that the pointer designations are overapproximative.
This paper presents an approach to formally proven correct binary-level pointer analysis.
A salient property of our approach is that it first generically considers what proof obligations a generic abstract domain for pointer analysis must satisfy.
This allows easy instantiation of different domains, varying in precision, while preserving the correctness of the analysis.
In the trade-off between scalability and precision, such customization allows “meaningful” precision (sufficiently precise to ensure basic sanity properties, such as that relevant parts of the stack frame are not overwritten during function execution) while also allowing coarse analysis when pointer computations have become too obfuscated during compilation for sound and accurate bounds analysis.
We experiment with three different abstract domains with high, medium, and low precision.
Evaluation shows that our approach is able to derive designations for memory writes soundly in COTS binaries, in a context-sensitive interprocedural fashion."
1983,679d459debd8ffd557a2b62c,cs.SE,https://arxiv.org/pdf/2501.17739,"Testing Research Software: An In-Depth Survey of Practices, Methods, and Tools","Nasir U. Eisty, Upulee Kanewala, Jeffrey C. Carver",Software Engineering,"ContextResearch software is essential for developing advanced tools and models to solve complex research problems and drive innovation across domains.
Therefore, it is essential to ensure its correctness.
Software testing plays a vital role in this task.
However, testing research software is challenging due to the software’s complexity and to the unique culture of the research software community.AimsBuilding on previous research, this study provides an in-depth investigation of testing practices in research software, focusing on test case design, challenges with expected outputs, use of quality metrics, execution methods, tools, and desired tool features. Additionally, we explore whether demographic factors influence testing processes.MethodWe survey research software developers to understand how they design test cases, handle output challenges, use metrics, execute tests, and select tools.ResultsResearch software testing varies widely.
The primary challenges are test case design, evaluating test quality, and evaluating the correctness of test outputs.
Overall, research software developers are not familiar with existing testing tools and have a need for new tools to support their specific needs.ConclusionAllocating human resources to testing and providing developers with knowledge about effective testing techniques are important steps toward improving the testing process of research software.
While many industrial testing tools exist, they are inadequate for testing research software due to its complexity, specialized algorithms, continuous updates, and need for flexible, custom testing approaches.
Access to a standard set of testing tools that address these special characteristics will increase level of testing in research software development and reduce the overhead of distributing knowledge about software testing."
1984,679d459debd8ffd557a2b62d,cs.SE,https://arxiv.org/pdf/2501.17678,Automated Repair of Cyber-Physical Systems,Pablo Valle,Software Engineering,"Cyber-Physical Systems (CPS) integrate digital technologies with physical processes and are common in different domains and industries, such as robotic systems, autonomous vehicles or satellites. Debugging and verification of CPS software consumes much of the development budget as it is often purely manual. To speed up this process, Automated Program Repair (APR) has been targeted for a long time. Although there have been advances in software APR and CPS verification techniques, research specifically on APR for CPSs is limited. This Ph.D. research project aims to develop scalable APR techniques for CPSs, addressing problems of fault localization, long test execution times, and fitness function limitations. A new method combining spectrum-based fault localization (SBFL) with patch generation and advanced artificial intelligence techniques will be investigated. The approach will be validated by empirical studies on open and industrial code bases of CPSs."
1985,679d459debd8ffd557a2b62e,cs.SE,https://arxiv.org/pdf/2501.17522,Toward Organizational Decoupling in Microservices Through Key Developer Allocation,"Xiaozhou Li, Noman Ahmad, Tomas Cerny, Andrea Janes, Valentina Lenarduzzi, Davide Taibi",Software Engineering,"With microservices continuously being popular in the software architecture domain, more practitioners and researchers have begun to pay attention to the degradation issue that diminishes its sustainability. One of the key factors that causes the degradation of software architecture is its organizational structure, according to Conway’s Law. However, the best practice of “One microservice per Team”, advocated widely by the industry, is not commonly adopted, especially when many developers contribute heavily across multiple microservices and create organizational coupling. Therein, many key developers, who are responsible for the majority of the project work and irreplaceable to the team, can also create the most coupling and be the primary cause of microservice degradation. Hence, to properly maintain microservice architecture in terms of its organizational structure, we shall identify these key developers and understand their connections to the organizational coupling within the project. We propose an approach to identify the key developers in microservice projects and investigate their connection to organizational coupling. The approach shall facilitate the maintenance and optimization of microservice projects against degradation by detecting and mitigating organizational coupling."
1986,679d459debd8ffd557a2b62f,cs.SE,https://arxiv.org/pdf/2501.17461,AugmenTest: Enhancing Tests with LLM-Driven Oracles,"Shaker Mahmud Khandaker, Fitsum Kifetew, Davide Prandi, Angelo Susi",Software Engineering,"Automated test generation is crucial for ensuring the reliability and robustness of software applications while at the same time reducing the effort needed. While significant progress has been made in test generation research, generating valid test oracles still remains an open problem."
1987,679d459debd8ffd557a2b630,cs.SE,https://arxiv.org/pdf/2501.17028,Approach Towards Semi-Automated Certification for Low Criticality ML-Enabled Airborne Applications,"Chandrasekar Sridhar, Vyakhya Gupta, Prakhar Jain, Karthik Vaidhyanathan",Software Engineering,"As Machine Learning (ML) makes its way into aviation, ML-enabled systems—including low-criticality systems—require a reliable certification process to ensure safety and performance. Traditional standards, like DO-178C, which are used for critical software in aviation, don’t fully cover the unique aspects of ML. This paper proposes a semi-automated certification approach, specifically for low-criticality ML systems, focusing on data and model validation, resilience assessment, and usability assurance while integrating manual and automated processes. Key aspects include structured classification to guide certification rigor on system attributes, an Assurance Profile that consolidates evaluation outcomes into a confidence measure the ML component, and methodologies for integrating human oversight into certification activities. Through a case study with a YOLOv8-based object detection system designed to classify military and civilian vehicles in real-time for reconnaissance and surveillance aircraft, we show how this approach supports the certification of ML systems in low-criticality airborne applications."
1988,679d459debd8ffd557a2b631,cs.SE,https://arxiv.org/pdf/2501.17026,Mitigating Omitted Variable Bias in Empirical Software Engineering,"Carlo A. Furia, Richard Torkar",Software Engineering,"Omitted variable bias occurs when a statistical model leaves out
variables that are relevant determinants of the effects under study.
This results in the model attributing the missing variables’ effect
to some of the included variables—hence over- or under-estimating
the latter’s true effect. Omitted variable bias presents a
significant threat to the validity of empirical research,
particularly in non-experimental studies such as those prevalent in
empirical software engineering."
1989,679d459debd8ffd557a2b632,cs.SE,https://arxiv.org/pdf/2501.17024,Automated Refactoring of Non-Idiomatic Python Code: A Differentiated Replication with LLMs,"Alessandro Midolo, Massimiliano Di Penta",Software Engineering,"In the Python ecosystem, the adoption of idiomatic constructs has been fostered because of their expressiveness, increasing productivity and even efficiency, despite controversial arguments concerning familiarity or understandability issues. Recent research contributions have proposed approaches—based on static code analysis and transformation—to automatically identify and enact refactoring opportunities of non-idiomatic code into idiomatic ones.
Given the potential recently offered by Large Language Models (LLMs) for code-related tasks, in this paper, we present the results of a replication study in which we investigate GPT-4 effectiveness in recommending and suggesting idiomatic refactoring actions. Our results reveal that GPT-4 not only identifies idiomatic constructs effectively but frequently exceeds the benchmark in proposing refactoring actions where the existing baseline failed. A manual analysis of a random sample shows the correctness of the obtained recommendations. Our findings underscore the potential of LLMs to achieve tasks where, in the past, implementing recommenders based on complex code analyses was required."
1990,679d459debd8ffd557a2b633,cs.SE,https://arxiv.org/pdf/2501.17004,Using Sustainability Impact Scores for Software Architecture Evaluation,"Iffat Fatima, Patricia Lago, Vasilios Andrikopoulos, Bram van der Waaij",Software Engineering,"For future regulatory compliance, organizations must assess and report on the state of sustainability in terms of its impacts over time. Sustainability, being a multidimensional concern, is complex to quantify. This complexity further increases with the interdependencies of the quality concerns across different sustainability dimensions. The research literature lacks a holistic way to evaluate sustainability at the software architecture level. With this study, our aim is to identify quality attribute (QA) trade-offs at the software architecture level and quantify the related sustainability impact. To this aim we present an improved version of the Sustainability Impact Score (SIS), building on our previous work. The SIS facilitates the identification and quantification of trade-offs in terms of their sustainability impact, leveraging a risk- and importance-based prioritization mechanism. To evaluate our approach, we apply it to an industrial case study involving a multi-model framework for integrated decision-making in the energy sector. Our study reveals that technical quality concerns have significant, often unrecognized impacts across sustainability dimensions. The SIS coupled with QA trade-offs can help practitioners make informed decisions that align with their sustainability goals. Early evaluations can help organizations mitigate sustainability risks by taking preventive actions."
1991,679d459debd8ffd557a2b634,cs.SE,https://arxiv.org/pdf/2501.16998,Large Language Models for Code Generation: The Practitioners Perspective,"Zeeshan Rasheed, Muhammad Waseem, Kai Kristian Kemell, Aakash Ahmad, Malik Abdul Sami, Jussi Rasku, Kari Systä, Pekka Abrahamsson",Software Engineering,"Large Language Models (LLMs) have emerged as coding assistants, capable of generating source code from natural language prompts. With the increasing adoption of LLMs in software development, academic research and industry-based projects are proposing and developing various tools, benchmarks, and metrics to evaluate the efficacy of LLM-generated code. However, there is a lack of solutions and tools evaluated through empirically grounded methods, incorporating practitioners’ perspectives, to assess functionality, syntax, and accuracy of LLM-generated code in real-world applications. To address this gap, we proposed and developed a multi-model unified platform to generate and execute code based on natural language prompts. We conducted a survey with 60 software developers – practitioners from 11 countries across 4 continents working in diverse professional roles and domains - to evaluate the usability, performance, strengths, and limitations of each model. The results of this study present practitioners’ feedback and insights into the use of LLMs in real-world software development contexts, including their strengths and weaknesses, key aspects missed by benchmarks and metrics of generated code, and enhance our understanding of the practical applicability of LLMs to real-world software development. The findings of this study can inform researchers and practitioners, facilitating knowledge transfer, for a systematic selection and usage of given LLMs in software development projects. Future research focuses on integrating more diverse models into the proposed system, incorporate additional case studies, and conducting developers’ interviews for deeper empirical insights on LLM-driven software development."
1992,679d459debd8ffd557a2b635,cs.SE,https://arxiv.org/pdf/2501.16857,Comparing Human and LLM Generated Code: The Jury is Still Out!,"Sherlock A. Licorish, Ansh Bajpai, Chetan Arora, Fanyu Wang, Kla Tantithamthavorn",Software Engineering,"Much is promised in relation to AI-supported software development. However, there has been limited evaluation effort in the research domain aimed at validating the true utility of such techniques, especially when compared to human coding outputs. We bridge this gap, where a benchmark dataset comprising 72 distinct software engineering tasks is used to compare the effectiveness of large language models (LLMs) and human programmers in producing Python software code. GPT-4 is used as a representative LLM, where for the code generated by humans and this LLM, we evaluate code quality and adherence to Python coding standards, code security and vulnerabilities, code complexity and functional correctness. We use various static analysis benchmarks, including Pylint, Radon, Bandit and test cases. Among the notable outcomes, results show that human-generated code recorded higher ratings for adhering to coding standards than GPT-4. We observe security flaws in code generated by both humans and GPT-4, however, code generated by humans shows a greater variety of problems, but GPT-4 code included more severe outliers. Our results show that although GPT-4 is capable of producing coding solutions, it frequently produces more complex code that may need more reworking to ensure maintainability. On the contrary however, our outcomes show that a higher number of test cases passed for code generated by GPT-4 across a range of tasks than code that was generated by humans. That said, GPT-4 frequently struggles with complex problem-solving that involve in-depth domain knowledge. This study highlights the potential utility of LLMs for supporting software development, however, tasks requiring comprehensive, innovative or unconventional solutions, and careful debugging and error correction seem to be better developed by human programmers. We plot an agenda for the software engineering community."
1993,679d459debd8ffd557a2b636,cs.SE,https://arxiv.org/pdf/2501.16712,Thinging Machines for Requirements Engineering: Superseding Flowchart-Based Modeling,Sabah Al-Fedaghi,Software Engineering,
1994,679d459debd8ffd557a2b637,cs.SE,https://arxiv.org/pdf/2501.16646,instancespace: a Python Package for Insightful Algorithm Testing through Instance Space Analysis,"Yusuf Berdan Güzel, Kushagra Khare, Nathan Harvey, Kian Dsouza, Dong Hyeog Jang, Junheng Chen, Cheng Ze Lam, Mario Andrés Muñoz",Software Engineering,"Instance Space Analysis is a methodology to evaluate algorithm performance across diverse problem fields. Through visualisation and exploratory data analysis techniques, Instance Space Analysis offers objective, data-driven insights into the diversity of test instances, algorithm behaviour, and algorithm strengths and weaknesses. As such, it supports automated algorithm selection and synthetic test instance generation, increasing testing reliability in optimisation, machine learning, and scheduling fields. This paper introducesinstancespace, a Python package that implements an automated pipeline for Instance Space Analysis. This package supports research by streamlining the testing process, providing unbiased metrics, and facilitating more informed algorithmic design and deployment decisions, particularly for complex and safety-critical systems."
1995,679d459debd8ffd557a2b638,cs.SE,https://arxiv.org/pdf/2501.16495,"Explaining GitHub Actions Failures with Large Language Models: Challenges, Insights, and Limitations","Pablo Valenzuela-Toledo, Chuyue Wu, Sandro Hernandez, Alexander Boll, Roman Machacek, Sebastiano Panichella, Timo Kehrer",Software Engineering,"GitHub Actions (GA) has become thede factotool that developers use to automate software workflows, seamlessly building, testing, and deploying code. Yet when GA fails, it disrupts development, causing delays and driving up costs. Diagnosing failures becomes especially challenging because error logs are often long, complex and unstructured.
Given these difficulties, this study explores the potential of large language models (LLMs) to generate correct, clear, concise, and actionable contextual descriptions (or summaries) for GA failures, focusing on developers’ perceptions of their feasibility and usefulness.
Our results show that over 80% of developers rated LLM explanations positively in terms of correctness for simpler/small logs.
Overall, our findings suggest that LLMs can feasibly assist developers in understanding common GA errors, thus, potentially reducing manual analysis.
However, we also found that improved reasoning abilities are needed to support more complex CI/CD scenarios. For instance, less experienced developers tend to be more positive on the described context, while seasoned developers prefer concise summaries.
Overall, our work offers key insights for researchers enhancing LLM reasoning, particularly in adapting explanations to user expertise."
1996,679d459debd8ffd557a2b639,cs.SE,https://arxiv.org/pdf/2501.16454,MoEVD: Enhancing Vulnerability Detection by Mixture-of-Experts (MoE),"Xu Yang, Shaowei Wang, Jiayuan Zhou, Wenhan Zhu",Software Engineering,"Deep Learning-based Vulnerability Detection (DLVD) techniques have garnered significant interest due to their ability to automatically learn vulnerability patterns from previously compromised code.
Despite the notable accuracy demonstrated by pioneering tools,
the broader application of DLVD methods in real-world scenarios is hindered by significant challenges.
A primary issue is the “one-for-all” design, where a single model is trained to handle all types of vulnerabilities.
This approach fails to capture the patterns of different vulnerability types, resulting in suboptimal performance,
particularly for less common vulnerabilities that are often underrepresented in training datasets.
To address these challenges, we propose MoEVD, which adopts the Mixture-of-Experts (MoE) framework for vulnerability detection.
MoEVD decomposes vulnerability detection into two tasks: CWE type classification and CWE-specific vulnerability detection.
By splitting the task, in vulnerability detection, MoEVD allows specific experts to handle distinct types of vulnerabilities instead of handling all vulnerabilities within one model.
Our results show that MoEVD achieves an F1-score of 0.44, significantly outperforming all studied state-of-the-art (SOTA) baselines by at least 12.8%.
MoEVD excels across almost all CWE types, improving recall over the best SOTA baseline by 9% to 77.8%.
Notably, MoEVD does not sacrifice performance on long-tailed CWE types; instead,
its MoE design enhances performance (F1-score) on these by at least 7.3%, addressing long-tailed issues effectively."
1997,679d459debd8ffd557a2b63a,cs.SE,https://arxiv.org/pdf/2112.07091,Simultaneous execution of quantum circuits on current and near-future NISQ systems,"Yasuhiro Ohkura, Takahiko Satoh, Rodney Van Meter","Quantum Physics, Software Engineering",
1998,679d459debd8ffd557a2b63b,cs.SE,https://arxiv.org/pdf/2501.16155,CITYWALK: Enhancing LLM-Based C++ Unit Test Generation via Project-Dependency Awareness and Language-Specific Knowledge,"Yuwei Zhang, Qingyuan Lu, Kai Liu, Wensheng Dou, Jiaxin Zhu, Li Qian, Chunxi Zhang, Zheng Lin, Jun Wei",Software Engineering,"Unit testing plays a pivotal role in the software development lifecycle, as it ensures code quality. However, writing high-quality unit tests remains a time-consuming task for developers in practice. More recently, the application of large language models (LLMs) in automated unit test generation has demonstrated promising results. Existing approaches primarily focus on interpreted programming languages (e.g., Java), while mature solutions tailored to compiled programming languages like C++ are yet to be explored. The intricate language features of C++, such as pointers, templates, and virtual functions, pose particular challenges for LLMs in generating both executable and high-coverage unit tests. To tackle the aforementioned problems, this paper introducesCITYWALK, a novel LLM-based framework for C++ unit test generation.CITYWALKenhances LLMs by providing a comprehensive understanding of the dependency relationships within the project under test via program analysis. Furthermore,CITYWALKincorporates language-specific knowledge about C++ derived from project documentation and empirical observations, significantly improving the correctness of the LLM-generated unit tests. We implementCITYWALKby employing the widely popular LLM GPT-4o. The experimental results show thatCITYWALKoutperforms current state-of-the-art approaches on a collection of eight popular C++ projects. Our findings demonstrate the effectiveness ofCITYWALKin generating high-quality C++ unit tests."
1999,679d459debd8ffd557a2b63c,cs.SE,https://arxiv.org/pdf/2501.16149,PATCH: Empowering Large Language Model with Programmer-Intent Guidance and Collaborative-Behavior Simulation for Automatic Bug Fixing,"Yuwei Zhang, Zhi Jin, Ying Xing, Ge Li, Fang Liu, Jiaxin Zhu, Wensheng Dou, Jun Wei",Software Engineering,"Bug fixing holds significant importance in software development and maintenance. Recent research has made substantial strides in exploring the potential of large language models (LLMs) for automatically resolving software bugs. However, a noticeable gap in existing approaches lies in the oversight of collaborative facets intrinsic to bug resolution, treating the process as a single-stage endeavor. Moreover, most approaches solely take the buggy code snippet as input for LLMs during the patch generation stage. To mitigate the aforementioned limitations, we introduce a novel stage-wise framework namedPATCH. Specifically, we first augment the buggy code snippet with corresponding dependence context and intent information to better guide LLMs in generating the correct candidate patches. Additionally, by taking inspiration from bug management practices, we decompose the bug-fixing task into four distinct stages: bug reporting, bug diagnosis, patch generation, and patch verification. These stages are performed interactively by LLMs, aiming to simulate the collaborative behavior of programmers during the resolution of software bugs. By harnessing these collective contributions,PATCHeffectively enhances the bug-fixing capability of LLMs. We implementPATCHby employing the powerful dialogue-based LLM ChatGPT. Our evaluation on the widely used bug-fixing benchmark BFP demonstrates thatPATCHhas achieved better performance than state-of-the-art LLMs."
2000,679d459debd8ffd557a2b63d,cs.SE,https://arxiv.org/pdf/2501.16044,MultiMend: Multilingual Program Repair with Context Augmentation and Multi-Hunk Patch Generation,"Reza Gharibi, Mohammad Hadi Sadreddini, Seyed Mostafa Fakhrahmad",Software Engineering,
2001,679d459debd8ffd557a2b63e,cs.SE,https://arxiv.org/pdf/2501.15934,Leveraging multi-task learning to improve the detection of SATD and vulnerability,"Barbara Russo, Jorge Melegati, Moritz Mock",Software Engineering,"Multi-task learning is a paradigm that leverages information from related tasks to improve the performance of machine learning. Self-Admitted Technical Debt (SATD) are comments in the code that indicate not-quite-right code introduced for short-term needs, i.e., technical debt (TD). Previous research has provided evidence of a possible relationship between SATD and the existence of vulnerabilities in the code.
In this work, we investigate if multi-task learning could leverage the information shared between SATD and vulnerabilities to improve the automatic detection of these issues.
To this aim, we implemented VulSATD, a deep learner that detects vulnerable and SATD code based on CodeBERT, a pre-trained transformers model. We evaluated VulSATD on MADE-WIC, a fused dataset of functions annotated for TD (through SATD) and vulnerability. We compared the results using single and multi-task approaches, obtaining no significant differences even after employing a weighted loss.
Our findings indicate the need for further investigation into the relationship between these two aspects of low-quality code. Specifically, it is possible that only a subset of technical debt is directly associated with security concerns. Therefore, the relationship between different types of technical debt and software vulnerabilities deserves future exploration and a deeper understanding."
2002,679d459debd8ffd557a2b63f,cs.SE,https://arxiv.org/pdf/2501.15919,Does Functional Package Management Enable Reproducible Builds at Scale? Yes,"Julien Malka, Stefano Zacchiroli, Théo Zimmermann",Software Engineering,"Reproducible Builds (R-B) guarantee that rebuilding a software package from source leads to bitwise identical artifacts.
R-B is a promising approach to increase the integrity of the software supply chain, when installing open source software built by third parties.
Unfortunately, despite success stories like high build reproducibility levels in Debian packages, uncertainty remains among field experts on the scalability of R-B to very large package repositories."
2003,679d459debd8ffd557a2b640,cs.SE,https://arxiv.org/pdf/2501.15854,Optimizing Deep Learning Models to Address Class Imbalance in Code Comment Classification,"Moritz Mock, Thomas Borsani, Giuseppe Di Fatta, Barbara Russo",Software Engineering,"Developers rely on code comments to document their work, track issues, and understand the source code. As such, comments provide valuable insights into developers’ understanding of their code and describe their various intentions in writing the surrounding code. Recent research leverages natural language processing and deep learning to classify comments based on developers’ intentions. However, such labelled data are often imbalanced, causing learning models to perform poorly.
This work investigates the use of different weighting strategies of the loss function to mitigate the scarcity of certain classes in the dataset. In particular, various RoBERTa-based transformer models are fine-tuned by means of a hyperparameter search to identify their optimal parameter configurations. Additionally, we fine-tuned the transformers with different weighting strategies for the loss function to address class imbalances.
Our approach outperforms the STACC baseline by 8.9 per cent on the NLBSE’25 Tool Competition dataset in terms of the average F1cscore, and exceeding the baseline approach in 17 out of 19 cases with a gain ranging from -5.0 to 38.2.
The source code is publicly available athttps://github.com/moritzmock/NLBSE2025."
2004,679d459debd8ffd557a2b641,cs.SE,https://arxiv.org/pdf/2501.15804,CodeImprove: Program Adaptation for Deep Code,"Ravishka Rathnasuriya, Zijie Zhao, Wei Yang",Software Engineering,"Leveraging deep learning (DL)-based code analysis tools to solve software engineering tasks is becoming increasingly popular.
Code models often suffer performance degradation due to various reasons (e.g., code data shifts).
Retraining is often required to address these issues, but frequent model updates are costly in labeling and deployment. In this paper, we explore an alternative solution: Adapting the program inputs to the code models. This can be achieved by two steps: 1) input validation that focuses on identifying whether an input is an out-of-scope input program that are beyond a model’s handling capability, and 2) input adaptation that adapts out-of-scope inputs to become in-scope inputs.
Validating program input is challenging, as current techniques focus on continuous inputs such as image data and fail with discrete inputs like code data, which have unique characteristics and are processed differently by deep learning models. Adapting out-of-scope programs is also challenging due to their vast search spaces.
Therefore, in this paper, we propose CodeImprove, which distinguishes out-of-scope from normal inputs and converts such out-of-scope inputs back to in-scope inputs through program transformation. In particular, we propose a validity score metric to identify out-of-scope inputs and leverage genetics algorithms to apply semantic preserving program transformation to convert out-of-scope inputs to in-scope inputs. Our experimental results show CodeImprove can enhance upto 8.78% of accuracy, and 51.28% of relative improvements in three code models on two SE tasks. Additionally, our input validation is promising in detecting out-of-scope inputs (AUC score of 0.924)."
2005,679d459debd8ffd557a2b642,cs.SE,https://arxiv.org/pdf/2501.15691,An Empirical Study on Decision-Making Aspects in Responsible Software Engineering for AI,"Lekshmi Murali Rani, Faezeh Mohammadi, Robert Feldt, Richard Berntsson Svensson",Software Engineering,"Incorporating responsible practices into software engineering for AI is essential to ensure ethical principles, societal impact, and accountability remain at the forefront of AI system design and deployment. This study investigates the ethical challenges and complexities inherent in responsible software engineering for AI, underscoring the need for practical, scenario-driven operational guidelines. Given the complexity of AI and the relative inexperience of professionals in this rapidly evolving field, continuous learning and market adaptation are crucial. Through qualitative interviews with seven practitioners (conducted until saturation), quantitative surveys of 51 practitioners and static validation of results with four industry experts in AI, this study explores how personal values, emerging roles, and awareness of AI’s societal impact influence responsible decision-making in responsible software engineering for AI. A key finding is the gap between the current state of the art and actual practice in responsible software engineering for AI, particularly in the failure to operationalize ethical and responsible decision-making within the software engineering life cycle for AI. While ethical issues in responsible software engineering for AI largely mirror those found in broader software engineering process, the study highlights a distinct lack of operational frameworks and resources to guide responsible software engineering practices for AI effectively. The results reveal that current ethical guidelines are insufficiently implemented at the operational level, reinforcing the complexity of embedding ethics throughout the software engineering life cycle. The study concludes that interdisciplinary collaboration, H-shaped competencies (Ethical-Technical dual competence), and a strong organizational culture of ethics are critical for fostering responsible software engineering practices for AI, with a particular focus on transparency and accountability."
2006,679d459debd8ffd557a2b643,cs.SE,https://arxiv.org/pdf/2501.15662,Retrospective: Data Mining Static Code Attributes to Learn Defect Predictors,Tim Menzies,Software Engineering,"Industry can get any research it wants, just by publishing a baseline result along with the data and scripts need to reproduce that work. For instance, the paper “Data Mining Static Code Attributes to Learn Defect Predictors” presented such a baseline, using static code attributes from NASA projects. Those result were enthusiastically embraced by a software engineering research community, hungry for data. At its peak (2016) this paper was SE’s most cited paper (per month). By 2018, twenty percent of leading TSE papers (according to Google Scholar Metrics), incorporated artifacts introduced and disseminated by this research. This brief note reflects on what we should remember, and what we should forget, from that paper."
2007,679d459debd8ffd557a2b644,cs.SE,https://arxiv.org/pdf/2501.15480,Exploring and Evaluating Interplays of BPpy with Deep Reinforcement Learning and Formal Methods,"Tom Yaacov, Gera Weiss, Adiel Ashrov, Guy Katz, Jules Zisser",Software Engineering,
2008,679d459debd8ffd557a2b645,cs.SE,https://arxiv.org/pdf/2501.15475,The Same Only Different: On Information Modality for Configuration Performance Analysis,"Hongyuan Liang, Yue Huang, Tao Chen",Software Engineering,"Configuration in software systems helps to ensure efficient operation and meet diverse user needs. Yet, some, if not all, configuration options have profound implications for the system’s performance. Configuration performance analysis, wherein the key is to understand (or infer) the configuration options’ relations and their impacts on performance, is crucial. Two major modalities exist that serve as the source information in the analysis: either the manual or source code. However, it remains unclear what roles they play in configuration performance analysis. Much work that relies on manuals claims their benefits of information richness and naturalness; while work that trusts the source code more prefers the structural information provided therein and criticizes the timeliness of manuals."
2009,679d459debd8ffd557a2b646,cs.SE,https://arxiv.org/pdf/2501.15392,Faster Configuration Performance Bug Testing with Neural Dual-level Prioritization,"Youpeng Ma, Tao Chen, Ke Li",Software Engineering,"As software systems become more complex and configurable, more performance problems tend to arise from the configuration designs. This has caused some configuration options to unexpectedly degrade performance which deviates from their original expectations designed by the developers. Such discrepancies, namely configuration performance bugs (CPBugs), are devastating and can be deeply hidden in the source code. Yet, efficiently testing CPBugs is difficult, not only due to the test oracle is hard to set, but also because the configuration measurement is expensive and there are simply too many possible configurations to test. As such, existing testing tools suffer from lengthy runtime or have been ineffective in detecting CPBugs when the budget is limited, compounded by inaccurate test oracle."
2010,679d459debd8ffd557a2b647,cs.SE,https://arxiv.org/pdf/2501.15387,Tracing the Lifecycle of Architecture Technical Debt in Software Systems: A Dependency Approach,"Edi Sutoyo, Paris Avgeriou, Andrea Capiluppi",Software Engineering,"Architectural technical debt (ATD) represents trade-offs in software architecture that accelerate initial development but create long-term maintenance challenges. ATD, in particular when self-admitted, impacts the foundational structure of software, making it difficult to detect and resolve."
2011,679d459debd8ffd557a2b648,cs.SE,https://arxiv.org/pdf/2501.15181,From Bugs to Benefits: Improving User Stories by Leveraging Crowd Knowledge with CrUISE-AC,"Stefan Schwedt, Thomas Ströder",Software Engineering,
2012,679d459debd8ffd557a2b649,cs.SE,https://arxiv.org/pdf/2501.15134,BitsAI-CR: Automated Code Review via LLM in Practice,"Tao Sun, Jian Xu, Yuanpeng Li, Zhao Yan, Ge Zhang, Lintao Xie, Lu Geng, Zheng Wang, Yueyan Chen, Qin Lin, Wenbo Duan, Kaixin Sui",Software Engineering,"Code review remains a critical yet resource-intensive process in software development, particularly challenging in large-scale industrial environments. While Large Language Models (LLMs) show promise for automating code review, existing solutions face significant limitations in precision and practicality.
This paper presentsBitsAI-CR, an innovative framework that enhances code review through a two-stage approach combining RuleChecker for initial issue detection and ReviewFilter for precision verification. The system is built upon a comprehensive taxonomy of review rules and implements a data flywheel mechanism that enables continuous performance improvement through structured feedback and evaluation metrics. Our approach introduces an Outdated Rate metric that can reflect developers’ actual adoption of review comments, enabling automated evaluation and systematic optimization at scale.
Empirical evaluation demonstratesBitsAI-CR’s effectiveness, achieving 75.0% precision in review comment generation. For the Go language which has predominant usage at ByteDance, we maintain an Outdated Rate of 26.7%. The system has been successfully deployed at ByteDance, serving over 12,000 Weekly Active Users (WAU).
Our work provides valuable insights into the practical application of automated code review and offers a blueprint for organizations seeking to implement automated code reviews at scale."
2013,679d459debd8ffd557a2b64a,cs.SE,https://arxiv.org/pdf/2501.15114,Does the Tool Matter? Exploring Some Causes of Threats to Validity in Mining Software Repositories,"Nicole Hoess, Carlos Paradis, Rick Kazman, Wolfgang Mauerer",Software Engineering,"Software repositories are an essential source of information for software engineering research on topics such as project evolution and developer collaboration.
Appropriate mining tools and analysis pipelines are therefore an indispensable precondition for many research activities.
Ideally, valid results should not depend on technical details of data collection and processing. It is, however, widely acknowledged that mining pipelines are complex, with a multitude of implementation decisions made by tool authors based on their interests and assumptions. This raises the questions if (and to what extent) tools agree on their results and are interchangeable.
In this study, we use two tools to extract and analyse ten large software projects, quantitatively and qualitatively comparing results and derived data to better understand this concern. We analyse discrepancies from a technical point of view, and adjust code and parametrisation to minimise replication differences. Our results indicate that despite similar trends, even simple metrics such as the numbers of commits and developers may differ by up to 500%. We find that such substantial differences are often caused by minor technical details. We show how tool-level and data post-processing changes can overcome these issues, but find they may require considerable efforts.
We summarise identified causes in our lessons learned to help researchers and practitioners avoid common pitfalls, and reflect on implementation decisions and their influence in ensuring obtained data meets explicit and implicit expectations. Our findings lead us to hypothesise that similar uncertainties exist in other analysis tools, which may limit the validity of conclusions drawn in tool-centric research."
2014,679d459debd8ffd557a2b64b,cs.SE,https://arxiv.org/pdf/2501.14983,"Code Change Intention, Development Artifact and History Vulnerability: Putting Them Together for Vulnerability Fix Detection by LLM","Xu Yang, Wenhan Zhu, Michael Pacheco, Jiayuan Zhou, Shaowei Wang, Xing Hu, Kui Liu",Software Engineering,"Detecting vulnerability fix commits in open-source software is crucial for maintaining software security. To help OSS identify vulnerability fix commits, several automated approaches are developed. However, existing approaches like VulFixMiner and CoLeFunDa, focus solely on code changes, neglecting essential context from development artifacts. Tools like Vulcurator, which integrates issue reports, fail to leverage semantic associations between different development artifacts (e.g., pull requests and history vulnerability fixes). Moreover, they miss vulnerability fixes in tangled commits and lack explanations, limiting practical use. Hence to address those limitations, we proposeLLM4VFD, a novel framework that leverages Large Language Models (LLMs) enhanced with Chain-of-Thought reasoning and In-Context Learning to improve the accuracy of vulnerability fix detection.LLM4VFDcomprises three components: (1) Code Change Intention, which analyzes commit summaries, purposes, and implications using Chain-of-Thought reasoning; (2) Development Artifact, which incorporates context from related issue reports and pull requests; (3) Historical Vulnerability, which retrieves similar past vulnerability fixes to enrich context. More importantly, on top of the prediction,LLM4VFDalso provides a detailed analysis and explanation to help security experts understand the rationale behind the decision.
We evaluatedLLM4VFDagainst state-of-the-art techniques, including Pre-trained Language Model-based approaches and vanilla LLMs, using a newly collected dataset, BigVulFixes.
Experimental results demonstrate thatLLM4VFDsignificantly outperforms the best-performed existing approach by 68.1%–145.4%.
Furthermore, We conducted a user study with security experts, showing that the analysis generated byLLM4VFDimproves the efficiency of vulnerability fix identification."
2015,679d459debd8ffd557a2b64c,cs.SE,https://arxiv.org/pdf/2501.14973,SecuRe -- An Approach to Recommending Security Design Patterns,"Alex R. Sabau, Dominik Lammers, Horst Lichter",Software Engineering,"Security is an important quality of software systems, but there is a huge lack of security experts. To overcome this gap, we aim to make security design knowledge reusable for architects by proposing theSecuRerecommendation approach to secure software design. It lifts design patterns and knowledge engineering concepts to security-related design recommendations for software architectures. This paper presents the central concepts of this approach, the overall recommendation process, and the first results from an initial case study."
2016,679d459debd8ffd557a2b64d,cs.SE,https://arxiv.org/pdf/2501.14890,Evaluation of MQTT Bridge Architectures in a Cross-Organizational Context,"Keila Lima, Tosin Daniel Oyetoyan, Rogardt Heldal, Wilhelm Hasselbring",Software Engineering,"The latest surveys estimate an increasing number of connected Internet-of-Things (IoT) devices (around 16 billion) despite the sector’s shortage of manufacturers. All these devices deployed into the wild will collect data to guide decision-making that can be made automatically by other systems, humans, or hybrid approaches.
In this work, we conduct an initial investigation of benchmark configuration options for IoT Platforms that process data ingested by such devices in real-time using the MQTT protocol. We identified metrics and related MQTT configurable parameters in the system’s component deployment for an MQTT bridge architecture. For this purpose, we benchmark a real-world IoT platform’s operational data flow design to monitor the surrounding environment remotely. We consider the MQTT broker solution and the system’s real-time ingestion and bridge processing portion of the platform to be the system under test. In the benchmark, we investigate two architectural deployment options for the bridge component to gain insights into the latency and reliability of MQTT bridge deployments in which data is provided in a cross-organizational context.
Our results indicate that the number of bridge components, MQTT packet sizes, and the topic name can impact the quality attributes in IoT architectures using MQTT protocol."
2017,679d459debd8ffd557a2b64e,cs.SE,https://arxiv.org/pdf/2501.14848,BEST: A Unified Business Process Enactment via Streams and Tables for Service Computing,"Ahmed Awad, Feras Awaysheh, Hugo A. López",Software Engineering,"Business process models are essential for the representation, analysis, and execution of organizational processes, serving as orchestration blueprints while relying on (web) services to implement individual tasks. At the representation level, there are two dominant paradigms: procedural (imperative) notations that specify the sequential flows within a process and declarative notations that capture the process as a set of constraints. Although each notation offers distinct advantages in representational clarity and cognitive effectiveness, they are seldom integrated, leading to compatibility challenges. In this paper, we set aside the imperative-declarative dichotomy to focus on orchestrating services that execute the underlying tasks. We propose an execution semantics based on the Continuous Query Language (CQL), where CQL statements respond dynamically to streams of events. As events unfold, these CQL statements update the execution state (tables) and can generate new events, effectively triggering (web) services that implement specific process tasks. By defining all executions around a unified event model, we achieve cross-language and cross-paradigm process enactment. We showcase how industrial process modeling languages, such as BPMN and DCR graphs, can be enacted through CQL queries, allowing seamless orchestration and execution of services across diverse modeling paradigms."
2018,679d459debd8ffd557a2b64f,cs.SE,https://arxiv.org/pdf/2501.14841,Insights from Publishing Open Data in Industry-Academia Collaboration,"Per Erik Strandberg, Philipp Peterseil, Julian Karoliny, Johanna Kallio, Johannes Peltola","Software Engineering, Computers and Society",
2019,679d459debd8ffd557a2b650,cs.SE,https://arxiv.org/pdf/2501.15895,Quantum Pattern Detection: Accurate State- and Circuit-based Analyses,"Julian Shen, Joshua Ammermann, Christoph König, Ina Schaefer","Quantum Physics, Software Engineering",
2020,679d459debd8ffd557a2b651,cs.SE,https://arxiv.org/pdf/2501.14683,An Empirical Study on LLM-based Classification of Requirements-related Provisions in Food-safety Regulations,"Shabnam Hassani, Mehrdad Sabetzadeh, Daniel Amyot",Software Engineering,"As Industry 4.0 transforms the food industry, the role of software in achieving compliance with food-safety regulations is becoming increasingly critical. Food-safety regulations, like those in many legal domains, have largely been articulated in a technology-independent manner to ensure their longevity and broad applicability. However, this approach leaves a gap between the regulations and the modern systems and software increasingly used to implement them. In this article, we pursue two main goals. First, we conduct a Grounded Theory study of food-safety regulations and develop a conceptual characterization of food-safety concepts that closely relate to systems and software requirements. Second, we examine the effectiveness of two families of large language models (LLMs) – BERT and GPT – in automatically classifying legal provisions based on requirements-related food-safety concepts.
Our results show that: (a) when fine-tuned, the accuracy differences between the best-performing models in the BERT and GPT families are relatively small. Nevertheless, the most powerful model in our experiments, GPT-4o, still achieves the highest accuracy, with an averagePrecisionof 89% and an averageRecallof 87%; (b) few-shot learning with GPT-4o increasesRecallto 97% but decreasesPrecisionto 65%, suggesting a trade-off between fine-tuning and few-shot learning; (c) despite our training examples being drawn exclusively from Canadian regulations, LLM-based classification performs consistently well on test provisions from the US, indicating a degree of generalizability across regulatory jurisdictions; and (d) for our classification task, LLMs significantly outperform simpler baselines constructed using long short-term memory (LSTM) networks and automatic keyword extraction."
2021,679d459debd8ffd557a2b652,cs.SE,https://arxiv.org/pdf/2501.14582,"""Estimating software project effort using analogies"": Reflections after 28 years",Martin Shepperd,Software Engineering,"Background: This invited paper is the result of an invitation to write a retrospective article on a “TSE most influential paper” as part of the journal’s 50th anniversary.Objective: To reflect on the progress of software engineering prediction research using the lens of a selected, highly cited research paper and 28 years of hindsight.Methods: The paper examines (i) what was achieved, (ii) what has endured and (iii) what could have been done differently with the benefit of retrospection.Conclusions: While many specifics of software project effort prediction have evolved, key methodological issues remain relevant. The original study emphasised empirical validation with benchmarks, out-of-sample testing and data/tool sharing. Four areas for improvement are identified: (i) stronger commitment to Open Science principles, (ii) focus on effect sizes and confidence intervals, (iii) reporting variability alongside typical results and (iv) more rigorous examination of threats to validity."
2022,679d459debd8ffd557a2b653,cs.SE,https://arxiv.org/pdf/2501.14465,Boundary Value Test Input Generation Using Prompt Engineering with LLMs: Fault Detection and Coverage Analysis,"Xiujing Guo, Chen Li, Tatsuhiro Tsuchiya",Software Engineering,"As software systems grow more complex, automated testing has become essential to ensuring reliability and performance. Traditional methods for boundary value test input generation can be time-consuming and may struggle to address all potential error cases effectively, especially in systems with intricate or highly variable boundaries. This paper presents a framework for assessing the effectiveness of large language models (LLMs) in generating boundary value test inputs for white-box software testing by examining their potential through prompt engineering. Specifically, we evaluate the effectiveness of LLM-based test input generation by analyzing fault detection rates and test coverage, comparing these LLM-generated test sets with those produced using traditional boundary value analysis methods. Our analysis shows the strengths and limitations of LLMs in boundary value generation, particularly in detecting common boundary-related issues. However, they still face challenges in certain areas, especially when handling complex or less common test inputs. This research provides insights into the role of LLMs in boundary value testing, underscoring both their potential and areas for improvement in automated testing methods."
2023,679d459debd8ffd557a2b654,cs.SE,https://arxiv.org/pdf/2501.14402,On the Effectiveness of Microservices Tactics and Patterns to Reduce Energy Consumption: An Experimental Study on Trade-Offs,"Xingwen Xiao, Chushu Gao, Justus Bogner",Software Engineering,"Context:Microservice-based systems have established themselves in the software industry.
However, sustainability-related legislation and the growing costs of energy-hungry software increase the importance of energy efficiency for these systems.
While some proposals for architectural tactics and patterns exist, their effectiveness as well as potential trade-offs on other quality attributes (QAs) remain unclear."
2024,679d459debd8ffd557a2b655,cs.SE,https://arxiv.org/pdf/2501.14326,Assessing Large Language Models in Comprehending and Verifying Concurrent Programs across Memory Models,"Ridhi Jain, Rahul Purandare",Software Engineering,"As concurrent programming becomes increasingly prevalent, effectively identifying and addressing concurrency issues such as data races and deadlocks is critical. This study evaluates the performance of several leading large language models (LLMs), including GPT-3.5-turbo, GPT-4, GPT-4o, GPT-4o-mini, and Mistral-AI’s Large2, in understanding and analyzing concurrency issues within software programs. Given that relaxed memory models, such as Total Store Order (TSO) and Partial Store Order (PSO), are widely implemented and adapted in modern systems—supported even by commodity architectures like ARM and x86—our evaluation focuses not only on sequentially consistent memory models but also on these relaxed memory models. Specifically, we assess two main aspects: the models’ capacity to detect concurrency problems under a sequentially consistent memory model and their ability to verify the correctness conditions of concurrent programs across both sequentially consistent and relaxed memory models. To do this, we leverage SV-COMP’spthreadtests and 25 ARM Litmus tests designed to evaluate Total Store Order (TSO) and Partial Store Order (PSO) memory models."
2025,679d459debd8ffd557a2b656,cs.SE,https://arxiv.org/pdf/2501.14257,C2SaferRust: Transforming C Projects into Safer Rust with NeuroSymbolic Techniques,"Vikram Nitin, Rahul Krishna, Luiz Lemos do Valle, Baishakhi Ray",Software Engineering,"In recent years, there has been a lot of interest in converting C code to Rust, to benefit from the memory and thread safety guarantees of Rust. C2Rust is a rule-based system that can automatically convert C code to functionally identical Rust, but the Rust code that it produces is non-idiomatic, i.e., makes extensive use of unsafe Rust, a subset of the language thatdoesn’thave memory or thread safety guarantees. At the other end of the spectrum are LLMs, which produce idiomatic Rust code, but these have the potential to make mistakes and are constrained in the length of code they can process. In this paper, we presentC2SaferRust, a novel approach to translate C to Rust that combines the strengths of C2Rust and LLMs. We first use C2Rust to convert C code to non-idiomatic, unsafe Rust. We then decompose the unsafe Rust code into slices that can be individually translated to safer Rust by an LLM. After processing each slice, we run end-to-end test cases to verify that the code still functions as expected. We also contribute a benchmark of 7 real-world programs, translated from C to unsafe Rust using C2Rust. Each of these programs also comes with end-to-end test cases. On this benchmark, we are able to reduce the number of raw pointers by up to 38%, and reduce the amount of unsafe code by up to 28%, indicating an increase in safety. The resulting programs still pass all test cases.C2SaferRustalso shows convincing gains in performance against two previous techniques for making Rust code safer."
2026,679d459debd8ffd557a2b657,cs.SE,https://arxiv.org/pdf/2501.14131,Refactoring for Dockerfile Quality: A Dive into Developer Practices and Automation Potential,"Emna Ksontini, Meriem Mastouri, Rania Khalsi, Wael Kessentini",Software Engineering,"Docker, the industry standard for packaging and deploying applications, leverages Infrastructure as Code (IaC) principles to facilitate the creation of images through Dockerfiles. However, maintaining Dockerfiles presents significant challenges. Refactoring, in particular, is often a manual and complex process."
2027,679d459debd8ffd557a2b658,cs.SY,https://arxiv.org/pdf/2501.18350,Joint Power and Spectrum Orchestration for D2D Semantic Communication Underlying Energy-Efficient Cellular Networks,"Le Xia, Yao Sun, Haijian Sun, Rose Qingyang Hu, Dusit Niyato, Muhammad Ali Imran",Systems and Control,"Semantic communication (SemCom) has been recently deemed a promising next-generation wireless technique to enable efficient spectrum savings and information exchanges, thus naturally introducing a novel and practical network paradigm where cellular and device-to-device (D2D) SemCom approaches coexist.
Nevertheless, the involved wireless resource management becomes complicated and challenging due to the unique semantic performance measurements and energy-consuming semantic coding mechanism.
To this end, this paper jointly investigates power control and spectrum reuse problems for energy-efficient D2D SemCom cellular networks.
Concretely, we first model the user preference-aware semantic triplet transmission and leverage a novel metric of semantic value to identify the semantic information importance conveyed in SemCom.
Then, we define the additional power consumption from semantic encoding in conjunction with basic power amplifier dissipation to derive the overall system energy efficiency (semantics/Joule).
Next, we formulate an energy efficiency maximization problem for joint power and spectrum allocation subject to several SemCom-related and practical constraints.
Afterward, we propose an optimal resource management solution by employing the fractional-to-subtractive problem transformation and decomposition while developing a three-stage method with theoretical analysis of its optimality guarantee and computational complexity.
Numerical results demonstrate the adequate performance superiority of our proposed solution compared with different benchmarks."
2028,679d459debd8ffd557a2b659,cs.SY,https://arxiv.org/pdf/2501.18318,Estimating unknown dynamics and cost as a bilinear system with Koopman-based Inverse Optimal Control,"Victor Nan Fernandez-Ayala, Shankar A. Deka, Dimos V. Dimarogonas","Systems and Control, Dynamical Systems","In this work, we address the challenge of approximating unknown system dynamics and costs by representing them as a bilinear system using Koopman-based Inverse Optimal Control (IOC). Using optimal trajectories, we construct a bilinear control system in transformed state variables through a modified Extended Dynamic Mode Decomposition with control (EDMDc) that maintains exact dynamical equivalence with the original nonlinear system. We derive Pontryagin’s Maximum Principle (PMP) optimality conditions for this system, which closely resemble those of the inverse Linear Quadratic Regulator (LQR) problem due to the consistent control input and state independence from the control. This similarity allows us to apply modified inverse LQR theory, offering a more tractable and robust alternative to nonlinear Inverse Optimal Control methods, especially when dealing with unknown dynamics. Our approach also benefits from the extensive analytical properties of bilinear control systems, providing a solid foundation for further analysis and application. We demonstrate the effectiveness of the proposed method through theoretical analysis, simulation studies and a robotic experiment, highlighting its potential for broader applications in the approximation and design of control systems."
2029,679d459debd8ffd557a2b65a,cs.SY,https://arxiv.org/pdf/2501.18179,Tunable Multilayer Surface Plasmon Resonance Biosensor for Trace-Level Toxin Detection,"Suripto Bhuiyan, Michael Geller",Systems and Control,"This paper presents a comprehensive study on a novel multilayer surface plasmon resonance (SPR) biosensor designed for detecting trace-level toxins in liquid samples with exceptional precision and efficiency. Leveraging the Kretschmann configuration, the proposed design integrates advanced two-dimensional materials, including black phosphorus (BP) and transition metal dichalcogenides (TMDs), to significantly enhance the performance metrics of the sensor. Key innovations include the optimization of sensitivity through precise material layering, minimization of full-width at half-maximum (FWHM) to improve signal resolution, and maximization of the figure of merit (FoM) for superior detection accuracy. Numerical simulations are employed to validate the structural and functional enhancements of the biosensor. The results demonstrate improved interaction between the evanescent field and the analyte, enabling detection at trace concentrations with higher specificity. This biosensor is poised to contribute to advancements in biochemical sensing, environmental monitoring, and other critical applications requiring high-sensitivity toxin detection."
2030,679d459debd8ffd557a2b65b,cs.SY,https://arxiv.org/pdf/2501.18130,Waste Animal Bone-derived Calcium Phosphate Particles with High Solar Reflectance,"Nathaniel LeCompte, Andrew Caratenuto, Yi Zheng",Systems and Control,
2031,679d459debd8ffd557a2b65c,cs.SY,https://arxiv.org/pdf/2501.18063,Impedance Trajectory Analysis during Power Swing for Grid-Forming Inverter with Different Current Limiters,"Yanshu Niu, Zhe Yang, Bikash C. Pal",Systems and Control,
2032,679d459debd8ffd557a2b65d,cs.SY,https://arxiv.org/pdf/2501.18583,Reducing Simulation Effort for RIS Optimization using an Efficient Far-Field Approximation,"Hans-Dieter Lang, Michel A. Nyffenegger, Heinz Mathis, Xingqi Zhang","Optimization and Control, Systems and Control","Optimization of Reconfigurable Intelligent Surfaces (RIS) via a previously introduced method is effective, but time-consuming, because multiport impedance or scatter matrices are required for each transmitter and receiver position, which generally must be obtained through full-wave simulation. Herein, a simple and efficient far-field approximation is introduced, to extrapolate scatter matrices for arbitrary receiver and transmitter positions from only a single simulation while still maintaining high accuracy suitable for optimization purposes. This is demonstrated through comparisons of the optimized capacitance values and further supported by empirical measurements."
2033,679d459debd8ffd557a2b65e,cs.SY,https://arxiv.org/pdf/2501.18385,Performance guarantees for optimization-based state estimation using turnpike properties,"Julian D. Schiller, Lars Grüne, and Matthias A. Müller","Optimization and Control, Systems and Control","In this paper, we develop novel accuracy and performance guarantees for optimal state estimation of general nonlinear systems (in particular, moving horizon estimation, MHE). Our results rely on a turnpike property of the optimal state estimation problem, which essentially states that the omniscient infinite-horizon solution involving all past and future data serves as turnpike for the solutions of finite-horizon estimation problems involving a subset of the data. This leads to the surprising observation that MHE problems naturally exhibit a leaving arc, which may have a strong negative impact on the estimation accuracy. To address this, we propose a delayed MHE scheme, and we show that the resulting performance (both averaged and non-averaged) is approximately optimal and achieves bounded dynamic regret with respect to the infinite-horizon solution, with error terms that can be made arbitrarily small by an appropriate choice of the delay. In various simulation examples, we observe that already a very small delay in the MHE scheme is sufficient to significantly improve the overall estimation error by 20–25 % compared to standard MHE (without delay). This finding is of great importance for practical applications (especially for monitoring, fault detection, and parameter estimation) where a small delay in the estimation is rather irrelevant but may significantly improve the estimation results."
2034,679d459debd8ffd557a2b65f,cs.SY,https://arxiv.org/pdf/2501.18355,Multilayered Intelligent Reflecting Surface for Long-Range Underwater Acoustic Communication,"Yu Luo, Lina Pu, Aijun Song","Audio and Speech Processing, Sound, Signal Processing, Systems and Control","This article introduces a multilayered acoustic reconfigurable intelligent surface (ML-ARIS) architecture designed for the next generation of underwater communications. ML-ARIS incorporates multiple layers of piezoelectric material in each acoustic reflector, with the load impedance of each layer independently adjustable via a control circuit. This design increases the flexibility in generating reflected signals with desired amplitudes and orthogonal phases, enabling passive in-phase and quadrature (IQ) modulation using a single acoustic reflector. Such a feature enables precise beam steering, enhancing sound levels in targeted directions while minimizing interference in surrounding environments. Extensive simulations and tank experiments were conducted to verify the feasibility of ML-ARIS. The experimental results indicate that implementing IQ modulation with a multilayer structure is indeed practical in real-world scenarios, making it possible to use a single reflection unit to generate reflected waves with high-resolution amplitudes and phases."
2035,679d459debd8ffd557a2b660,cs.SY,https://arxiv.org/pdf/2501.18203,Joint Design and Pricing of Extended Warranties for Multiple Automobiles with Different Price Bands,"Yajing Chen, Yanrong Li, Xiao-Lin Wang, Zhi-Sheng Ye","Optimization and Control, Systems and Control","Extended warranties (EWs) are significant source of revenue for capital-intensive products like automobiles. Such products consist of multiple subsystems, providing flexibility in EW customization, for example, bundling a tailored set of subsystems in an EW contract. This, in turn, enables the creation of a service menu with different EW contract options.
From the perspective of a third-party EW provider servicing a fleet of automobile brands, we develop a novel model to jointly optimize the design and pricing of EWs in order to maximize the profit.
Specifically, the problem is to determine which contracts should be included in the EW menu and identify the appropriate price for each contract.
As the complexity of the joint optimization problem increases exponentially with the number of subsystems, two solution approaches are devised to solve the problem.
The first approach is based on a mixed-integer second-order cone programming reformulation, which guarantees optimality but is applicable only for a small number of subsystems. The second approach utilizes a two-step iteration process, offering enhanced computational efficiency in scenarios with a large number of subsystems. Through numerical experiments, the effectiveness of our model is validated, particularly in scenarios characterized by high failure rates and a large number of subsystems."
2036,679d459debd8ffd557a2b661,cs.SY,https://arxiv.org/pdf/2501.18039,Online Nonstochastic Control with Convex Safety Constraints,"Nanfei Jiang, Spencer Hutchinson, Mahnoosh Alizadeh","Optimization and Control, Systems and Control","This paper considers the online nonstochastic control problem of a linear time-invariant system under convex state and input constraints that need to be satisfied at all times. We propose an algorithm called Online Gradient Descent with Buffer Zone for Convex Constraints (OGD-BZC), designed to handle scenarios where the system operates within general convex safety constraints. We demonstrate that OGD-BZC, with appropriate parameter selection, satisfies all the safety constraints under bounded adversarial disturbances. Additionally, to evaluate the performance of OGD-BZC, we define the regret with respect to the best safe linear policy in hindsight. We prove that OGD-BZC achieves𝒪~⁢(T)~𝒪𝑇\tilde{\mathcal{O}}(\sqrt{T})over~ start_ARG caligraphic_O end_ARG ( square-root start_ARG italic_T end_ARG )regret given proper parameter choices. Our numerical results highlight the efficacy and robustness of the proposed algorithm."
2037,679d459debd8ffd557a2b662,cs.SY,https://arxiv.org/pdf/2501.17867,Low-Thrust Many-Revolution Trajectory Design Under Operational Uncertainties for DESTINY+ Mission,"Naoya Ozaki, Yuki Akiyama, Akira Hatakeyama, Shota Ito, Takuya Chikazawa, Takayuki Yamamoto","Instrumentation and Methods for Astrophysics, Earth and Planetary Astrophysics, Systems and Control, Optimization and Control","DESTINY+is a planned JAXA medium-class Epsilon mission from Earth to deep space using a low-thrust, many-revolution orbit. Such a trajectory design is a challenging problem not only for trajectory design but also for flight operations, and in particular, it is essential to evaluate the impact of operational uncertainties to ensure mission success. In this study, we design the low-thrust trajectory from Earth orbit to a lunar transfer orbit by differential dynamic programming using the Sundman transformation. The results of Monte Carlo simulations with operational uncertainties confirm that the spacecraft can be successfully guided to the lunar transfer orbit by using the feedback control law of differential dynamic programming in the angular domain."
2038,679d459debd8ffd557a2b663,cs.SY,https://arxiv.org/pdf/2501.17808,"Replacing the Gallium Oxide Shell with Conductive Ag: Toward a Printable and Recyclable Composite for Highly Stretchable Electronics, Electromagnetic Shielding, and Thermal Interfaces","Abdollah Hajalilou, Elahe Parvini, Tiago A. Morgado, Pedro Alhais Lopes, M. Estrela Melo Jorge, Marta Freitas, Mahmoud Tavakoli",Systems and Control,
2039,679d459debd8ffd557a2b664,cs.SY,https://arxiv.org/pdf/2501.17804,Recyclable Thin-Film Soft Electronics for Smart Packaging and E-Skins,"Manuel Reis Carneiro, Anibal T. de Almeida, Mahmoud Tavakoli, Carmel Majidi","Systems and Control, Materials Science",
2040,679d459debd8ffd557a2b665,cs.SY,https://arxiv.org/pdf/2501.17648,Analysis and Control of Perturbed Density Systems,Igor Furtat,Systems and Control,"The paper investigates dynamical systems for which the derivative of some positive-definite function along the solutions of this system depends on so-called density function.
In turn, such dynamical systems are called density systems.
The density function sets the density of the space, where the system is evolved, and affects the behaviour of the original system.
For example, this function can define (un)stable regions and forbidden regions where there are no system solutions.
The density function can be used in the design of new adaptive control laws with the formulation of appropriate new control goals, e.g., stabilization in given bounded or semi-bounded sets.
To design a novel adaptive control law that ensures the system outputs in a given set, systems with known and unknown parameters under disturbances are considered.
All theoretical results and conclusions are illustrated by numerical simulations."
2041,679d459debd8ffd557a2b666,cs.SY,https://arxiv.org/pdf/2501.17621,Physics-Informed Neural Networks in Power System Dynamics: Improving Simulation Accuracy,"Ignasi Ventura Nadal, Rahul Nellikkath, Spyros Chatzivasileiadis",Systems and Control,"The importance and cost of time-domain simulations when studying power systems have exponentially increased in the last decades. With the growing share of renewable energy sources, the slow and predictable responses from large turbines are replaced by the fast and unpredictable dynamics from power electronics. The current existing simulation tools require new solutions designed for faster dynamics. Physics-Informed Neural Networks (PINNs) have recently emerged in power systems to accelerate such simulations. By incorporating knowledge during the up-front training, PINNs provide more accurate results over larger time steps than traditional numerical methods. This paper introduces PINNs as an alternative approximation method that seamlessly integrates with the current simulation framework. We replace a synchronous machine for a trained PINN in the IEEE 9-, 14-, and 30-bus systems and simulate several network disturbances. Including PINNs systematically boosts the simulations’ accuracy, providing more accurate results for both the PINN-modeled component and the whole multi-machine system states."
2042,679d459debd8ffd557a2b667,cs.SY,https://arxiv.org/pdf/2501.17614,Coalitional control: a bottom-up approach,"Filiberto Fele, José M. Maestre, Eduardo F. Camacho","Systems and Control, Computer Science and Game Theory, Optimization and Control","The recent major developments in information technologies have opened interesting possibilities for the effective management of multi-agent systems. In many cases, the important role of central control nodes can now be undertaken by several controllers in a distributed topology that suits better the structure of the system. This opens as well the possibility to promote cooperation between control agents in competitive environments, establishing links between controllers in order to adapt the exchange of critical information to the degree of subsystems’ interactions. In this paper a bottom-up approach tocoalitional controlis presented, where the structure of each agent’s model predictive controller is adapted to the time-variant coupling conditions, promoting the formation of coalitions — clusters of control agents where communication is essential to ensure the cooperation — whenever it can bring benefit to the overall system performance."
2043,679d459debd8ffd557a2b668,cs.SY,https://arxiv.org/pdf/2501.17597,Economic Nonlinear Model Predictive Control of Prosumer District Heating Networks: The Extended Version,"Max Sibeijn, Saeed Ahmed, Mohammad Khosravi, Tamás Keviczky",Systems and Control,"In this paper, we propose an economic nonlinear model predictive control (MPC) algorithm for district heating networks (DHNs). The proposed method features prosumers, multiple producers, and storage systems, which are essential components of 4th generation DHNs. These networks are characterized by their ability to optimize their operations, aiming to reduce supply temperatures, accommodate distributed heat sources, and leverage the flexibility provided by thermal inertia and storage—all crucial for achieving a fossil-fuel-free energy supply.
Developing a smart energy management system to accomplish these goals requires detailed models of highly complex nonlinear systems and computational algorithms able to handle large-scale optimization problems. To address this, we introduce a graph-based optimization-oriented model that efficiently integrates distributed producers, prosumers, storage buffers, and bidirectional pipe flows, such that it can be implemented in a real-time MPC setting. Furthermore, we conduct several numerical experiments to evaluate the performance of the proposed algorithms in closed-loop. Our findings demonstrate that the MPC methods achieved up to 9% cost improvement over traditional rule-based controllers while better maintaining system constraints."
2044,679d459debd8ffd557a2b669,cs.SY,https://arxiv.org/pdf/2501.17582,Coalitional Control: Cooperative game theory and control,"Filiberto Fele, José M. Maestre, Eduardo F. Camacho","Systems and Control, Computer Science and Game Theory, Optimization and Control",
2045,679d459debd8ffd557a2b66a,cs.SY,https://arxiv.org/pdf/2501.17561,Coalitional model predictive control of an irrigation canal,"Filiberto Fele, José M. Maestre, Mehdi Hashemy Shahdany, David Muñoz de la Peña, Eduardo F. Camacho","Systems and Control, Multiagent Systems, Optimization and Control","We present a hierarchical control scheme for large-scale systems whose components can exchange information through a data network. The main goal of the supervisory layer is to find the best compromise between control performance and communicational costs by actively modifying the network topology. The actions taken at the supervisory layer alter the control agents’ knowledge of the complete system, and the set of agents with which they can communicate. Each group of linked subsystems, orcoalition, is independently controlled based on a decentralized model predictive control (MPC) scheme, managed at the bottom layer. Hard constraints on the inputs are imposed, while soft constraints on the states are considered to avoid feasibility issues. The performance of the proposed control scheme is validated on a model of the Dez irrigation canal, implemented on the accurate simulator for water systems SOBEK. Finally, the results are compared with those obtained using a centralized MPC controller."
2046,679d459debd8ffd557a2b66b,cs.SY,https://arxiv.org/pdf/2501.17552,Efficient Calculation of Stabilization Parameters in RF Power Amplifiers,"Libe Mori, Ibone Lizarraga, Aitziber Anakabe, Juan-Mari Collantes, Vincent Armengaud, Geoffroy Soubercaze-Pun",Systems and Control,
2047,679d459debd8ffd557a2b66c,cs.SY,https://arxiv.org/pdf/2501.17544,Pole-Zero Identification: Unveiling the Critical Dynamics of Microwave Circuits Beyond Stability Analysis,"Juan-Mari Collantes, Libe Mori, Aitziber Anakabe, Nerea Otegi, Natanael Ayllon, Franco Ramirez, Vincent Armengaud, Geoffroy Soubercaze-Pun",Systems and Control,
2048,679d459debd8ffd557a2b66d,cs.SY,https://arxiv.org/pdf/2501.17529,Accelerated DC loadflow solver for topology optimization,"Nico Westerbeck, Joost van Dijk, Jan Viebahn, Christian Merz, Dirk Witthaut",Systems and Control,"We present a massively parallel solver that accelerates DC loadflow computations for power grid topology optimization tasks. Our approach leverages low-rank updates of the Power Transfer Distribution Factors (PTDFs) to represent substation splits, line outages, and reconfigurations without ever refactorizing the system. Furthermore, we implement the core routines on Graphics Processing Units (GPUs), thereby exploiting their high-throughput architecture for linear algebra. A two-level decomposition separates changes in branch topology from changes in nodal injections, enabling additional speed-ups by an in-the-loop brute force search over injection variations at minimal additional cost. We demonstrate billion-loadflow-per-second performance on power grids of varying sizes in workload settings which are typical for gradient-free topology optimization such as Reinforcement Learning or Quality Diversity methods. While adopting the DC approximation sacrifices some accuracy and prohibits the computation of voltage magnitudes, we show that this sacrifice unlocks new scales of computational feasibility, offering a powerful tool for large-scale grid planning and operational topology optimization."
2049,679d459debd8ffd557a2b66e,cs.SY,https://arxiv.org/pdf/2501.17499,A Sampling Complexity-aware Framework for Discrete-time Fractional-Order Dynamical System Identification,"Xiaole Zhang, Vijay Gupta, Paul Bogdan",Systems and Control,"A variety of complex biological, natural and man-made systems exhibit non-Markovian dynamics that can be modeled through fractional order differential equations, yet, we lack sample comlexity aware system identification strategies. Towards this end, we propose an affine discrete-time fractional order dynamical system (FoDS) identification algorithm and provide a detailed sample complexity analysis. The algorithm effectively addresses the challenges of FoDS identification in the presence of noisy data. The proposed algorithm consists of two key steps. Firstly, it avoids solving higher-order polynomial equations, which would otherwise result in multiple potential solutions for the fractional orders. Secondly, the identification problem is reformulated as a least squares estimation, allowing us to infer the system parameters. We derive the expectation and probabilistic bounds for the FoDS parameter estimation error, assuming prior knowledge of the functionsf𝑓fitalic_fandg𝑔gitalic_gin the FoDS model. The error decays at a rate ofN=O⁢(dϵ)𝑁𝑂𝑑italic-ϵN=O\left(\frac{d}{\epsilon}\right)italic_N = italic_O ( divide start_ARG italic_d end_ARG start_ARG italic_ϵ end_ARG ), whereN𝑁Nitalic_Nis the number of samples,d𝑑ditalic_dis the dimension of the state variable, andϵitalic-ϵ\epsilonitalic_ϵrepresents the desired estimation accuracy. Simulation results demonstrate that our theoretical bounds are tight, validating the accuracy and robustness of this algorithm."
2050,679d459debd8ffd557a2b66f,cs.SY,https://arxiv.org/pdf/2501.17484,Capacity Expansion Planning under Uncertainty subject to Expected Energy Not Served Constraints,"Marilena Zampara, Daniel Ávila, Anthony Papavasiliou",Systems and Control,"We present a method for solving a large-scale stochastic capacity expansion problem which explicitly considers reliability constraints, in particular constraints on expected energy not served. Our method tackles this problem by a Lagrange relaxation of the expected energy not served constraints. We solve the relaxed formulation in an iterative manner, using a subgradient-based method. Each iteration requires the solution of a stochastic capacity expansion problem, for which we implement a subgradient decomposition scheme in a high-performance computing infrastructure. We apply the proposed methodology on the Economic Viability Assessment model that is used by ENTSO-E in the annual European Resource Adequacy Assessment, extended to include explicit reliability constraints. The approach is able to solve this model achieving a 1.3%percent\%%optimality gap. We compare our approach against accounting for reliability through penalizing load shedding at VOLL, and find that the former results in 1.6%percent\%%savings in total cost. We are also able to quantify the cost savings from allowing some load curtailment in the capacity planning process, which ranges from 1.6 to 6%percent\%%in the cases analyzed."
2051,679d459debd8ffd557a2b670,cs.SY,https://arxiv.org/pdf/2501.17318,Floodgates up to contain the DeePC and limit extrapolation,"Mohammad Ramadan, Evan Toler, Mihai Anitescu",Systems and Control,"Behavioral data-enabled control approaches typically assume data-generating systems of linear dynamics. This may result in false generalization if the newly designed closed-loop system results in input-output distributional shifts beyond learning data. These shifts may compromise safety by activating harmful nonlinearities in the data-generating system not experienced previously in the data and/or not captured by the linearity assumption inherent in these approaches. This paper proposes an approach to slow down the distributional shifts and therefore enhance the safety of the data-enabled methods. This is achieved by introducing quadratic regularization terms to the data-enabled predictive control formulations. Slowing down the distributional shifts comes at the expense of slowing down the exploration, in a trade-off resembling the exploration vs exploitation balance in machine learning."
2052,679d459debd8ffd557a2b671,cs.SY,https://arxiv.org/pdf/1802.02140,Solve the General Constrained Optimal Control Problem with Common Integration Method,"Sheng Zhang, Jin-Mei Gao","Systems and Control, Optimization and Control",
2053,679d459debd8ffd557a2b672,cs.SY,https://arxiv.org/pdf/2501.17400,A Model-Free Data-Driven Algorithm for Continuous-Time Control,"Sean R. Bowerfind, Matthew R. Kirchner, Gary A. Hewer, D. Reed Robinson, Paula Chen, Alireza Farahmandi, Katia Estabridis","Optimization and Control, Systems and Control","Presented is an algorithm to synthesize an infinite-horizon LQR optimal feedback controller for continuous-time systems. The algorithm does not require knowledge of the system dynamics, but instead uses only a finite-length sampling of (possibly suboptimal) input-output data. The algorithm is based on a constrained optimization problem that enforces a necessary condition on the dynamics of the optimal value function along an arbitrary trajectory. This paper presents the derivation as well as shows examples applied to both linear and nonlinear systems inspired by air vehicles."
2054,679d459debd8ffd557a2b673,cs.SY,https://arxiv.org/pdf/2501.17105,Optimal control over Markovian wireless communication channels under generalized packet dropout compensation,"Yuriy Zacchia Lun, Francesco Smarra, Alessandro D'Innocenzo","Systems and Control, Optimization and Control","Control loops closed over wireless links greatly benefit from accurate estimates of the communication channel condition. To this end, the finite-state Markov channel model allows for reliable channel state estimation. This paper develops a Markov jump linear system representation for wireless networked control with persistent channel state observation, stochastic message losses, and generalized packet dropout compensation. With this model, we solve the finite- and infinite-horizon linear quadratic regulation problems and introduce an easy-to-test stability condition for any given infinite-horizon control law. We also thoroughly analyze the impact of a scalar general dropout compensation factor on the stability and closed-loop performance of a rotary inverted pendulum controlled remotely through a wireless link. Finally, we validate the results numerically via extensive Monte Carlo simulations, showing the benefits of the proposed control strategy."
2055,679d459debd8ffd557a2b674,cs.SY,https://arxiv.org/pdf/2501.16923,In-Circuit Characterization of Low-Frequency Stability Margins in Power Amplifiers,"Jose Manuel Gonzalez, Nerea Otegi, Aitziber Anakabe, Libe Mori, Asier Barcenilla, Juan-Mari Collantes",Systems and Control,
2056,679d459debd8ffd557a2b675,cs.SY,https://arxiv.org/pdf/2501.16921,Data-Efficient Extremum-Seeking Control Using Kernel-Based Function Approximation,"Wouter Weekers, Alessandro Saccon, Nathan van de Wouw",Systems and Control,"Existing extremum-seeking control (ESC) approaches typically rely on applying repeated perturbations to input parameters and performing measurements of the corresponding performance output. Performing these measurements can be costly in practical applications, e.g., due to the use of resources, making it desirable to reduce the number of performed measurements. Moreover, the required separation between the different timescales in the ESC loop typically results in slow convergence. With these challenges in mind, this work presents an approach aimed at both increasing the convergence rate and reducing the number of measurements that need to be performed. In the proposed approach, input-output data obtained during operation is used to construct online an approximation of the system’s underlying cost function. By using this approximation to perform parameter updates when a decrease in the cost can be guaranteed, instead of performing additional measurements to perform this update, more efficient use is made of the collected data. As a result, reductions in both the required number of measurements and update steps are indeed obtained. In addition, a stability analysis of the novel ESC approach is provided. The benefits of the synergy between kernel-based function approximation and standard ESC is demonstrated in simulation on a multi-input dynamical system."
2057,679d459debd8ffd557a2b676,cs.SY,https://arxiv.org/pdf/2501.16915,Understanding the Effect of Long-Term Memory Model Parameters in Pole-Zero Identification for Stability Analysis of Power Amplifiers,"Libe Mori, Aitziber Anakabe, Juan-Mari Collantes, Vincent Armengaud",Systems and Control,
2058,679d459debd8ffd557a2b677,cs.SY,https://arxiv.org/pdf/2501.16841,Toward Explainable NILM: Real-Time Event-Based NILM Framework for High-Frequency Data,"Grigorii Gerasimov, Ilia Kamyshev, Sahar Moghimian Hoosh, Elena Gryazina, Henni Ouerdane",Systems and Control,"Non-Intrusive Load Monitoring (NILM) is an advanced, and cost-effective technique for monitoring appliance-level energy consumption. However, its adaptability is hindered by the lack of transparency and explainability. To address this challenge, this paper presents an explainable, real-time, event-based NILM framework specifically designed for high-frequency datasets. The proposed framework ensures transparency at every stage by integrating a z-score-based event detector, appliance signature estimation, Fourier-based feature extraction, an XGBoost classifier, and post hoc SHAP analysis. The SHAP analysis further quantifies the contribution of individual features, such as cosine of specific harmonic phases, to appliance classification. The framework is trained and evaluated on the PLAID dataset, and achieved a classification accuracy of 90% while maintaining low computational requirements and a latency of less than one second."
2059,679d459debd8ffd557a2b678,cs.SY,https://arxiv.org/pdf/2501.16683,On Non-intrusive Data-driven Implementations of IRKA and Balanced Truncation,Umair Zulfiqar,Systems and Control,"Balanced truncation and the Iterative Rational Krylov Algorithm (IRKA) are two of the most significant model order reduction techniques, having stood the test of time as the most accurate methods in the field over the past two decades. The data-driven implementation of balanced truncation has been successfully achieved in the literature by approximating the integrals of Gramians using numerical quadrature. This formulation is non-intrusive, meaning it does not require access to the transfer function or state-space model for constructing reduced-order models. Instead, only samples of the transfer function and its derivative, or alternatively, samples of the impulse response and its derivative, are sufficient. Similarly, the data-driven formulation of IRKA also relies on samples of the transfer function and its derivatives, but unlike balanced truncation, the sampling frequencies are updated iteratively and are not known in advance. If the transfer function expression is available, new samples can be generated without needing the state-space model. However, if the transfer function is unavailable, IRKA must either pause until new samples are obtained through experiments or estimate new samples from existing data."
2060,679d459debd8ffd557a2b679,cs.SY,https://arxiv.org/pdf/2501.16639,Finite Sample Analysis of Subspace Identification Methods,"Jiabao He, Ingvar Ziemann, Cristian R. Rojas, S. Joe Qin, Håkan Hjalmarsson",Systems and Control,"As one of the mainstream approaches in system identification, subspace identification methods (SIMs) are known for their simple parameterization for MIMO systems and robust numerical properties. However, a comprehensive statistical analysis of SIMs remains an open problem. Amid renewed focus on identifying state-space models in the non-asymptotic regime, this work presents a finite sample analysis for a large class of open-loop SIMs. It establishes high-probability upper bounds for system matrices obtained via SIMs, and reveals that convergence rates for estimating Markov parameters and system matrices are𝒪⁢(1/N)𝒪1𝑁\mathcal{O}(1/\sqrt{N})caligraphic_O ( 1 / square-root start_ARG italic_N end_ARG )up to logarithmic terms, in line with classical asymptotic results. Following the key steps of SIMs, we arrive at the above results by a three-step procedure. In Step 1, we begin with a parsimonious SIM (PARSIM) that uses least-squares regression to estimate multiple high-order ARX models in parallel. Leveraging a recent analysis of an individual ARX model, we obtain a union error bound for a bank of ARX models. Step 2 involves model reduction via weighted singular value decomposition (SVD), where we consider different data-dependent weighting matrices and use robustness results for SVD to obtain error bounds on extended controllability and observability matrices, respectively. The final Step 3 focuses on deriving error bounds for system matrices, where two different realization algorithms, the MOESP type and the Larimore type, are considered. Although our study initially focuses on PARSIM, the methodologies apply broadly across many variants of SIMs."
2061,679d459debd8ffd557a2b67a,cs.SY,https://arxiv.org/pdf/2501.16487,Network Risk Estimation: A Risk Estimation Paradigm for Cyber Networks,"Arda Bayer, David Maluf, Behnaam Aazhang",Systems and Control,"Cyber networks are fundamental to many organization’s infrastructure, and the size of cyber networks is increasing rapidly. Risk measurement of the entities/endpoints that make up the network via available knowledge about possible threats has been the primary tool in cyber network security. However, the dynamic behavior of the entities and the sparsity of risk-measurable points are limiting factors for risk measurement strategies, which results in poor network visibility considering the volatility of cyber networks. This work proposes a new probabilistic risk estimation approach to network security, NRE, which operates on top of existing risk measurements. The proposed method NRE extracts relationships among system components from the network connection data, models risk propagation based on the learned relationships and refines the estimates whenever risk measurements are provided. In this work,(i)the risk estimation scheme is proposed,(ii)an application of quantitative risk estimates is devised,(iii)descriptiveness of the risk estimates are compared to a pure risk measurement alternative and(iv)low computational complexity of the proposed method is illustrated capable of real-time deployment. The proposed method, NRE, is ultimately a quantitative data-driven risk assessment tool that can be used to add security aspects to existing network functions, such as routing, and it provides a robust description of the network state in the presence of threats, capable of running in real-time."
2062,679d459debd8ffd557a2b67b,cs.SY,https://arxiv.org/pdf/2501.16473,Sensitivity Analysis of the Laser Power Control System to Measurement Noise in SLS 3D Printers,"Hamid Toshani, Janith Petangoda, Chatura Samarakoon, Phillip Stanley-Marbell",Systems and Control,"Uniform temperature distribution in Selective Laser Sintering (SLS) is essential for producing durable 3D prints. Achieving uniformity requires a laser power control system that minimises deviation of the printing temperatures from the target temperature. Because the estimate of the actual process temperature is an input to the laser power control, uncertainty in the estimate of the actual temperature can lead to fluctuations in laser power that affect the thermal performance of the SLS. This article investigates the sensitivity of a laser power control system to temperature measurement uncertainty. This article evaluates the effectiveness of two methods for quantifying the effect of input uncertainty on a SLS laser power control system: a recent innovation in uncertainty-tracked architecture and traditional Monte Carlo simulation. We show that recent advances in computer architecture for arithmatic on probability distributions make it possible for the first time, to perform control system uncertainty analysis with latencies under 30 ms, while achieving the same level of uncertainty analysis as Monte Carlo methods with latencies that are two orders of magnitude slower."
2063,679d459debd8ffd557a2b67c,cs.SY,https://arxiv.org/pdf/2501.16928,Detecting Critical Resonances in Microwave Amplifiers through Noise Simulations,"Juan-Mari Collantes, Nerea Otegi, Aitziber Anakabe, Libe Mori, Asier Barcenilla, Jose Manuel Gonzalez-Perez","Instrumentation and Detectors, Systems and Control",
2064,679d459debd8ffd557a2b67d,cs.SY,https://arxiv.org/pdf/2501.16167,A Dynamic Similarity Index for Assessing Voltage Source Behaviour in Power Systems,"Onur Alican, Dionysios Moutevelis, Josep Arevalo-Soler, Carlos Collados-Rodriguez, Jaume Amoros-Torrent, Oriol Gomis-Bellmunt, Marc Cheah-Mane, Eduardo Prieto-Araujo",Systems and Control,"Due to the fundamental transition to a power electronic dominated power system, the increasing diversity of dynamic elements underscores the need to assess their similarity to mature electrical engineering models.This article addresses the concept of the Dynamic Similarity Index (DSI) for its use in, power electronics-dominated networks.
The DSI is a multi-purpose tool developed to be used by different stakeholders (e.g., converter manufacturers and system operators).
Such an index is calculated per frequency, which serves to anticipatepotential differencesin particular frequency ranges of interest between the model under study and the reference model.
Within the scope of this study, the dynamic
similarity of inverter-based generators to an ideal voltage source
behind an impedance isassessed, due to the relevance of this
fundamental circuit in the representation of generation units in
power system studies.
The article presents two potential applications based
on this mathematical framework.First, for manufacturers to evaluate control performance compared to a reference model and second, it enables operators to diagnose buses with voltage vulnerability based on a user-defined referenceShort-Circuit Ratio(SCR) value. The DSI results for these
two case studies are validated using Matlab Simulink simulations."
2065,679d459debd8ffd557a2b67e,cs.SY,https://arxiv.org/pdf/2501.16128,Graphene-Assisted Chemical Stabilization of Liquid Metal Nano Droplets for Liquid Metal Based Energy Storage,"Afsaneh L. Sanati, Timur Nikitin, Rui Fausto, Carmel Majidi, Mahmoud Tavakoli","Systems and Control, Materials Science",
2066,679d459debd8ffd557a2b67f,cs.SY,https://arxiv.org/pdf/2501.15946,Impact of Lead Time on Aggregate EV Flexibility for Congestion Management Services,"Nanda Kishor Panda, Peter Palensky, Simon H. Tindemans",Systems and Control,"Increased electrification of energy end-usage can lead to network congestion during periods of high consumption. Flexibility of loads, such as aggregate smart charging of Electric Vehicles (EVs), is increasingly leveraged to manage grid congestion through various market-based mechanisms. Under such an arrangement, this paper quantifies the effect of lead time on the aggregate flexibility of EV fleets. Simulations using real-world charging transactions spanning over different categories of charging stations are performed for two flexibility products (redispatch and capacity limitations) when offered along with different business-as-usual (BAU) schedules. Results show that the variation of tradable flexibility depends mainly on the BAU schedules, the duration of the requested flexibility, and its start time. Further, the implication of these flexibility products on the average energy costs and emissions is also studied for different cases. Simulations show that bidirectional (V2G) charging outperforms unidirectional smart charging in all cases."
2067,679d459debd8ffd557a2b680,cs.SY,https://arxiv.org/pdf/2501.15924,Stabilization of an unstable reaction-diffusion PDE with input delay despite state and input quantization,"Florent Koudohode, Nikolaos Bekiaris-Liberis","Systems and Control, Analysis of PDEs","We solve the global asymptotic stability problem of an unstable reaction-diffusion Partial Differential Equation (PDE) subject to input delay and state quantization developing a switched predictor-feedback law. To deal with the input delay, we reformulate the problem as an actuated transport PDE coupled with the original reaction-diffusion PDE. Then, we design a quantized predictor-based feedback mechanism that employs a dynamic switching strategy to adjust the quantization range and error over time. The stability of the closed-loop system is proven properly combining backstepping with a small-gain approach and input-to-state stability techniques, for deriving estimates on solutions, despite the quantization effect and the system’s instability. We also extend this result to the input quantization case."
2068,679d459debd8ffd557a2b681,cs.SY,https://arxiv.org/pdf/2501.15899,Asynchronous distributed collision avoidance with intention consensus for inland autonomous ships,"Hoang Anh Tran, Nikolai Lauvås, Tor Arne Johansen, Rudy R. Negenborn",Systems and Control,"This paper focuses on the problem of collaborative collision avoidance for autonomous inland ships.
Two solutions are provided to solve the problem in a distributed manner.
We first present a distributed model predictive control (MPC) algorithm that allows ships to directly negotiate their intention to avoid collision in a synchronous communication framework.
Moreover, we introduce a new approach to shape the ship’s behavior to follow the waterway traffic regulations.
The conditional convergence toward a stationary solution of this algorithm is guaranteed by the theory of the Alternating Direction Method of Multipliers (ADMM).
To overcome the problem of asynchronous communication between ships, we adopt a new asynchronous nonlinear ADMM and present an asynchronous distributed MPC algorithm based on it.
Several simulations and field experiments show that the proposed algorithms can prevent ship collisions even in complex scenarios."
2069,679d459debd8ffd557a2b682,cs.SY,https://arxiv.org/pdf/2501.15897,MPC4RL -- A Software Package for Reinforcement Learning based on Model Predictive Control,"Dirk Reinhardt, Katrin Baumgärnter, Jonathan Frey, Moritz Diehl, Sebastien Gros",Systems and Control,"In this paper, we present an early software integrating Reinforcement Learning (RL) with Model Predictive Control
(MPC). Our aim is to make recent theoretical contributions from the literature more accessible to both the RL and MPC
communities. We combine standard software tools developed by the RL community, such asGymnasium,stable-baselines3,
orCleanRLwith theacadostoolbox, a widely-used software package for efficient MPC algorithms. Our core
contribution isMPC4RL, an open-sourcePythonpackage that supports learning-enhanced MPC schemes for existingacadosimplementations. The package is designed to be modular, extensible, and user-friendly, facilitating the
tuning of MPC algorithms for a broad range of control problems. It is available on GitHub."
2070,679d459debd8ffd557a2b683,cs.SY,https://arxiv.org/pdf/2501.15833,Mode Switching-Induced Instability of Multi-source Feed DC Microgrid,"Shanshan Jiang, Zelin Sun, Jiankun Zhang, Hua Geng",Systems and Control,"In DC microgrids (DCMGs), DC-bus signaling based control strategy is extensively used for power management, where mode switching plays a crucial role in achieving multi-source coordination.
However, few studies have noticed the impact of mode switching and switching strategies on system voltage stability. To fill this gap, this paper aims to provide a general analysis framework for mode switching-induced instability in multi-source DCMGs. First, manifold theory is employed to analyze the stability of the DCMG switched system. Subsequently, the instability mechanism and its physical interpretation are explored. The positive feedback activated by the decreasing DC bus voltage during the switching process leads to instability.
Switching strategy may inadvertently contribute to this instability.
To improve stability, a novel control method based on mode scheduling is proposed, by adjusting switching strategy and thereby correcting the system trajectory. Finally, both real-time simulations and experimental tests on a DCMG system verify the correctness and effectiveness of theoretical analysis results."
2071,679d459debd8ffd557a2b684,cs.SY,https://arxiv.org/pdf/2501.15742,Intuition and importance of feedback control through laboratory experiences,Aldo Jonathan Munoz-Vazquez,Systems and Control,
2072,679d459debd8ffd557a2b685,cs.SY,https://arxiv.org/pdf/2501.15611,Nuisance-free Automatic Ground Collision Avoidance System Design: Merging Exponential-CBF and Adaptive Sliding Manifolds,"Ege C. Altunkaya, Ibrahim Ozkol",Systems and Control,"The significance of the automatic ground collision avoidance system (Auto-GCAS) has been proven by considering the fatal crashes that have occurred over decades. Even though extensive efforts have been put forth to address the ground collision avoidance in the literature, the notion of being nuisance-free has not been sufficiently addressed. At this point, in this study, the Auto-GCAS design is formulated by merging exponential control barrier functions with sliding manifolds to manipulate the barrier function dynamics. The adaptive properties of the sliding manifolds are tailored to the key and governing flight parameters, ensuring that the nuisance-free requirement is satisfied. Furthermore, to ensure all safety requirements are met, a flight envelope protection algorithm is designed using control barrier functions to assess the commands generated by the Auto-GCAS. Eventually, the performance of the proposed methodology is demonstrated, focusing on authority-sharing, collision avoidance capability, and nuisance-free operation through various scenarios and Monte Carlo simulations."
2073,679d459debd8ffd557a2b686,cs.SY,https://arxiv.org/pdf/2501.15471,Dynamic Regressor Extension and Mixing-based Re-design of Adaptive Observer for Affine Systems,Mehdi Tavan,Systems and Control,
2074,679d459debd8ffd557a2b687,cs.SY,https://arxiv.org/pdf/2501.15339,"DER Hosting capacity for distribution networks: definitions, attributes, use-cases and challenges",Md Umar Hashmi,Systems and Control,"The rapid adoption of distributed energy resources (DERs) has outpaced grid modernization, leading to capacity limitations that challenge their further integration. Hosting Capacity Assessment (HCA) is a critical tool for evaluating how much DER capacity a grid can handle without breaching operational limits. HCA serves multiple goals: enabling higher DER penetration, accelerating grid connection times, guiding infrastructure upgrades or flexible resource deployment, and ensuring equitable policies. HCA lacks a universal definition, due to varying modelling approaches, uncertainty considerations, and objectives. This paper addresses five key questions to standardize HCA practices and applications.
First, it classifies HCA objectives associated with different stakeholders such as system operators, consumers, market operators and consumers.
Second, it examines model attributes, including modelling sophistication, data requirements, and uncertainty handling, thus balancing complexity with computational efficiency. Third, it explores HCA applications, such as planning grid investments or operational decisions, and summarizes use cases associated with HCA. Fourth, it emphasizes the need for periodic updates to reflect dynamic grid conditions, evolving technologies, and new DER installations. Finally, this work identifies challenges, such as ensuring data quality, managing computational demands, and aligning short-term and long-term goals.
By addressing these aspects, this paper provides a structured approach to perform and apply HCA, offering insights for engineers, planners, and policymakers to manage DER integration effectively."
2075,679d459debd8ffd557a2b688,cs.SY,https://arxiv.org/pdf/2501.15174,On Spectral Approach to the Synthesis of Shaping Filters,Konstantin A. Rybakov,"Systems and Control, Optimization and Control, Probability",
2076,679d459debd8ffd557a2b689,cs.SY,https://arxiv.org/pdf/2501.14971,Automatic Link Selection in Multi-Channel Multiple Access with Link Failures,"Mevan Wijewardena, Michael J. Neely",Systems and Control,"This paper focuses on the problem of automatic link selection in multi-channel multiple access control using bandit feedback. In particular, a controller assigns multiple users to multiple channels in a time slotted system, where in each time slot at most one user can be assigned to a given channel and at most one channel can be assigned to a given user. Given that useri𝑖iitalic_iis assigned to channelj𝑗jitalic_j, the transmission fails with a fixed probabilityfi,jsubscript𝑓𝑖𝑗f_{i,j}italic_f start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT. The failure probabilities are not known to the controller. The assignments are made dynamically using success/failure feedback. The goal is to maximize the time average utility, where we consider an arbitrary (possibly nonsmooth) concave and entrywise nondecreasing utility function. The problem of merely maximizing the total throughput has a solution of always assigning the same user-channel pairs and can be unfair to certain users, particularly when the number of channels is less than the number of users. Instead, our scheme allows various types of fairness, such as proportional fairness, maximizing the minimum, or combinations of these by defining the appropriate utility function. We propose two algorithms for this task. The first algorithm is adaptive and gets within𝒪⁢(log⁡(T)/T1/3)𝒪𝑇superscript𝑇13\mathcal{O}(\log(T)/T^{1/3})caligraphic_O ( roman_log ( start_ARG italic_T end_ARG ) / italic_T start_POSTSUPERSCRIPT 1 / 3 end_POSTSUPERSCRIPT )of optimality over any interval ofT𝑇Titalic_Tconsecutive slots over which the success probabilities do not change. The second algorithm has faster𝒪⁢(log⁡(T)/T)𝒪𝑇𝑇\mathcal{O}(\sqrt{\log(T)/T})caligraphic_O ( square-root start_ARG roman_log ( start_ARG italic_T end_ARG ) / italic_T end_ARG )performance over the firstT𝑇Titalic_Tslots, but does not adapt well if probabilities change."
2077,679d459debd8ffd557a2b68a,cs.SY,https://arxiv.org/pdf/2501.14906,What is a Relevant Signal-to-Noise Ratio for Numerical Differentiation?,"Shashank Verma, Mohammad Almuhaihi, Dennis S. Bernstein",Systems and Control,"In applications that involve sensor data, a useful measure of signal-to-noise ratio (SNR) is the ratio of the root-mean-squared (RMS) signal to the RMS sensor noise.
The present paper shows that, for numerical differentiation, the traditional SNR is ineffective.
In particular, it is shown that, for a harmonic signal with harmonic sensor noise, a natural and relevant SNR is given by the ratio of the RMS of the derivative of the signal to the RMS of the derivative of the sensor noise.
For a harmonic signal with white sensor noise, an effective SNR is derived.
Implications of these observations for signal processing are discussed."
2078,679d459debd8ffd557a2b68b,cs.SY,https://arxiv.org/pdf/2501.15991,Modeling and stability analysis of live systems with time-varying dimension,Andrii Mironchenko,"Optimization and Control, Systems and Control, Dynamical Systems","A major limitation of the classical control theory is the assumption that the state space and its dimension do not change with time. This prevents analyzing and even formalizing the stability and control problems for open multi-agent systems whose agents may enter or leave the network, industrial processes where the sensors or actuators may be exchanged frequently, smart grids, etc.
In this work, we propose a framework of live systems that covers a rather general class of systems with a time-varying state space. We argue that input-to-state stability is a proper stability notion for this class of systems, and many of the classic tools and results, such as Lyapunov methods and superposition theorems, can be extended to this setting."
2079,679d459debd8ffd557a2b68c,cs.SY,https://arxiv.org/pdf/2501.15206,Engineering-Oriented Design of Drift-Resilient MTJ Random Number Generator via Hybrid Control Strategies,"Ran Zhang, Caihua Wan, Yingqian Xu, Xiaohan Li, Raik Hoffmann, Meike Hindenberg, Shiqiang Liu, Dehao Kong, Shilong Xiong, Shikun He, Alptekin Vardar, Qiang Dai, Junlu Gong, Yihui Sun, Zejie Zheng, Thomas Kämpfe, Guoqiang Yu, Xiufeng Han","Applied Physics, Disordered Systems and Neural Networks, Systems and Control","In the quest for secure and reliable random number generation, Magnetic Tunnel Junctions (MTJs) have emerged as a promising technology due to their unique ability to exploit the stochastic nature of magnetization switching. This paper presents an engineering-oriented design of a drift-resilient MTJ-based True Random Number Generator (TRNG) utilizing a hybrid control strategy. We address the critical issue of switching probability drift, which can compromise the randomness and bias the output of MTJ-based TRNGs. Our approach combines a self-stabilization strategy, which dynamically adjusts the driving voltage based on real-time feedback, with pulse width modulation to enhance control over the switching probability. Through comprehensive experimental and simulation results, we demonstrate significant improvements in the stability, uniformity, and quality of the random numbers generated. The proposed system offers flexibility and adaptability for diverse applications, making it a reliable solution for high-quality randomness in cryptography, secure communications, and beyond."
2080,679d459debd8ffd557a2b68d,cs.SY,https://arxiv.org/pdf/2501.15164,UAV-Assisted MEC Architecture for Collaborative Task Offloading in Urban IoT Environment,"Subhrajit Barick, Chetna Singhal","Networking and Internet Architecture, Signal Processing, Systems and Control","Mobile edge computing (MEC) is a promising technology to meet the increasing demands and computing limitations of complex Internet of Things (IoT) devices. However, implementing MEC in urban environments can be challenging due to factors like high device density, complex infrastructure, and limited network coverage. Network congestion and connectivity issues can adversely affect user satisfaction. Hence, in this article, we use unmanned aerial vehicle (UAV)-assisted collaborative MEC architecture to facilitate task offloading of IoT devices
in urban environments. We utilize the combined capabilities of UAVs and ground edge servers (ESs) to maximize user satisfaction and thereby also maximize the service provider’s (SP) profit. We design IoT task-offloading as joint IoT-UAV-ES association and UAV-network topology optimization problem. Due to NP-hard nature, we break the problem into two subproblems: offload strategy optimization and UAV topology optimization. We develop a Three-sided Matching with Size and Cyclic preference (TMSC) based task offloading algorithm to find stable association between IoTs, UAVs, and ESs to achieve system objective. We also propose a K-means based iterative algorithm to decide the minimum number of UAVs and their positions to provide offloading services to maximum
IoTs in the system. Finally, we demonstrate the efficacy of the proposed task offloading scheme over benchmark schemes through
simulation-based evaluation. The proposed scheme outperforms by 19%, 12%, and 25% on average in terms of percentage of
served IoTs, average user satisfaction, and SP profit, respectively, with 25% lesser UAVs, making it an effective solution to support
IoT task requirements in urban environments using UAV-assisted MEC architecture."
2081,679d459debd8ffd557a2b68e,cs.SY,https://arxiv.org/pdf/2501.14720,Communication-Based Distributed Control of Large-Scale District Heating Networks,"Audrey Blizard, Stephanie Stockar",Systems and Control,"This paper presents a non-cooperative distributed model predictive controller for the control of large-scale District Heating Networks. To enable the design of this controller a novel information passing scheme and feasibility restoration method are created, allowing the local controllers to achieve a global consensus while minimizing a local cost function. The effectiveness of this controller is demonstrated on an 18-user District Heating Network decomposed into six subsystems. The results show that the developed control scheme effectively uses flexibility to manage the buildings’ heat demands reducing the total losses by 14% and the return temperature by 37%."
2082,679d459debd8ffd557a2b68f,cs.SY,https://arxiv.org/pdf/2501.14664,Predictive Position Estimation for Remote Surgery under Packet Loss Using the Informer Framework,"Muhammad Hanif Lashari, Shakil Ahmed, Wafa Batayneh, Ashfaq Khokhar",Systems and Control,"Accurate and real-time position estimation of the robotic arm on the patient’s side is crucial for the success of remote robotic surgery in Tactile Internet environments. This paper proposes a predictive approach using the computationally efficient Transformer-based Informer model for position estimation, combined with a Four-State Hidden Markov Model (4-State HMM) to simulate realistic packet loss scenarios. The method effectively addresses network-induced delays, jitter, and packet loss, ensuring reliable performance in remote robotic surgery. The study evaluates the Informer model on the JIGSAWS dataset, demonstrating its capability to handle sequential data challenges caused by network uncertainties. Key features, including ProbSparse attention and a generative-style decoder, enhance prediction accuracy, computational speed, and memory efficiency. Results indicate that the proposed method achieves over 90% accuracy across varying network conditions. Furthermore, the Informer framework outperforms traditional models, such as TCN, RNN, and LSTM, highlighting its suitability for real-time remote surgery applications."
2083,679d459debd8ffd557a2b690,cs.SY,https://arxiv.org/pdf/2501.14586,A sub-structuring approach for model reduction of frictionally clamped thin-walled structures,"Patrick Hippold, Johann Gross, Malte Krack",Systems and Control,"Thin-walled structures clamped by friction joints, such as aircraft skin panels are exposed to bending-stretching coupling and frictional contact.
We propose an original sub-structuring approach, where the system is divided into thin-walled and support regions, so that geometrically nonlinear behavior is relevant only in the former, and nonlinear contact behavior only in the latter.
This permits to derive reduced component models, in principle, with available techniques.
The Hurty-/Craig-Bampton method, combined with an interface reduction relying on an orthogonal polynomial series, is used to construct the reduction basis for each component.
To model geometrically nonlinear behavior, implicit condensation is used, where an original, engineering-oriented proposition is made for the delicate scaling of the static load cases required to estimate the coefficients of the nonlinear terms.
The proposed method is validated and its computational performance is assessed for the example of a plate with frictional clamping, using finite element analysis as reference.
The numerical results shed light into an interesting mutual interaction:
The extent of geometric hardening is limited by the reduced boundary stiffness when more sliding occurs in the clamping.
On the other hand, the frictional dissipation is increased by the tangential loading induced by membrane stretching."
2084,679d459debd8ffd557a2b691,cs.SY,https://arxiv.org/pdf/2501.14573,"A Transferable Physics-Informed Framework for Battery Degradation Diagnosis, Knee-Onset Detection and Knee Prediction","Huang Zhang, Xixi Liu, Faisal Altaf, Torsten Wik",Systems and Control,"The techno-economic and safety concerns of battery capacity knee occurrence call for developing online knee detection and prediction methods as an advanced battery management system (BMS) function.
To address this, a transferable physics-informed framework that consists of a histogram-based feature engineering method, a hybrid physics-informed model, and a fine-tuning strategy, is proposed for online battery degradation diagnosis and knee-onset detection. The hybrid model is first developed and evaluated using a scenario-aware pipeline in protocol cycling scenarios and then fine-tuned to create a local model deployed in a dynamic cycling scenario.
A 2D histogram-based feature set is found to be the best choice in both source and target scenarios. The fine-tuning strategy is proven to be effective in improving battery degradation mode estimation and degradation phase detection performance in the target scenario. Again, a strong linear correlation was found between the identified knee-onset and knee points.
As a result, advanced BMS functions, such as online degradation diagnosis and prognosis, online knee-onset detection and knee prediction, aging-aware battery classification, and second-life repurposing, can be enabled through a battery performance digital twin in the cloud."
2085,679d459debd8ffd557a2b692,cs.SY,https://arxiv.org/pdf/2501.14325,Joint Infrastructure Planning and Order Assignment for On-Demand Food-Delivery Services with Coordinated Drones and Human Couriers,"Yang Liu, Yitong Shang, Sen Li","Systems and Control, Optimization and Control","This paper investigates the optimal infrastructure planning and order assignment problem of an on-demand food-delivery platform with a mixed fleet of drones and human couriers. The platform serves the orders with two delivery modes: (a) ground delivery and (b) drone-assisted delivery (i.e., air delivery). In ground delivery, an order is collected at the restaurant and transported to the destination by a human courier. For air delivery, the delivery process is segmented into three legs: initially, a human courier picks up the order from the restaurant and transports it to a nearby launchpad. These launchpads are staffed by personnel who load the orders onto drones and replace batteries as needed. The loaded drone then transports the order from the launchpad to a kiosk, which is an automated facility designed to accommodate drone landings and securely store orders. Subsequently, another courier retrieves the order from the kiosk for delivery to the final destination. The platform must determine the optimal locations for launchpads and kiosks within a transportation network, and devise an order assignment strategy that allocates food-delivery orders between ground and air delivery, while considering the bundling probabilities of ground deliveries and the waiting times at launchpads and kiosks for air deliveries. We formulate the platform’s problem as a mixed-integer nonlinear program and develop a novel neural network-assisted optimization method to obtain high-quality solutions. The proposed model and algorithm are validated through a case study in Hong Kong, and the results reveal that the introduction of drone delivery will lead to reduced operational costs for the platform, a smaller courier fleet size, and increased opportunities for order bundling. Interestingly, we also find that the expansion of air delivery services may actually entail larger delivery times despite the significantly higher speeds of air compared to ground delivery. We attribute this phenomenon to the crucial trade-off between the travel time savings induced by the faster air delivery and the associated detours incurred by intermodal transfer and extra waiting times at launchpads and kiosks, which crucially depends on the distance of the orders and the sequence of activating long-distance air delivery routes versus short-distance ones."
2086,679d459debd8ffd557a2b693,cs.SY,https://arxiv.org/pdf/2501.14289,Higher-Order Meta Distribution Analysis of Wireless Systems with Application to the Reliability of UWB THz Networks,"Mehdi Monemi, Mehdi Rasti, S. Ali Mousavi, Matti Latva-aho, Martin Haenggi",Systems and Control,"Communication reliability, as defined by 3GPP, refers to the probability of providing a desired quality of service (QoS). This metric is typically quantified for wireless networks by averaging the QoS success indicator over spatial and temporal random variables. Recently, the meta distribution (MD) has emerged as a two-level performance analysis tool for wireless networks, offering a detailed examination of the outer level (i.e., system-level) reliability assessment versus the inner level (i.e., link-level) reliability thresholds. Most existing studies focus on first-order spatiotemporal MD reliability analyses, and the benefits of leveraging MD reliability for applications beyond this structure remain unexplored, a gap addressed in this paper. We present wireless application examples that can benefit the higher-order MD reliability analysis. Specifically, we provide the analysis and numerical results for a second-order spatial-spectral-temporal MD reliability of ultra-wideband THz communication. The results demonstrate the value of the hierarchical representation of MD reliability across three domains and the impact of the inner-layer target reliability on the overall MD reliability measure."
2087,679d459debd8ffd557a2b694,cs.SY,https://arxiv.org/pdf/2501.14259,Optimal Investment under Mutual Strategy Influence among Agents,"Huisheng Wang, H. Vicky Zhao","Systems and Control, Optimization and Control, Mathematical Finance, Portfolio Management","In financial markets, agents often mutually influence each other’s investment strategies and adjust their strategies to align with others. However, there is limited quantitative study of agents’ investment strategies in such scenarios. In this work, we formulate the optimal investment differential game problem to study the mutual influence among agents. We derive the analytical solutions for agents’ optimal strategies and propose a fast algorithm to find approximate solutions with low computational complexity. We theoretically analyze the impact of mutual influence on agents’ optimal strategies and terminal wealth. When the mutual influence is strong and approaches infinity, we show that agents’ optimal strategies converge to the asymptotic strategy. Furthermore, in general cases, we prove that agents’ optimal strategies are linear combinations of the asymptotic strategy and their rational strategies without others’ influence. We validate the performance of the fast algorithm and verify the correctness of our analysis using numerical experiments. This work is crucial to comprehend mutual influence among agents and design effective mechanisms to guide their strategies in financial markets."
2088,679d459debd8ffd557a2b695,cs.SY,https://arxiv.org/pdf/2501.14232,Learning-Augmented Online Control for Decarbonizing Water Infrastructures,"Jianyi Yang, Pengfei Li, Tongxin Li, Adam Wierman, Shaolei Ren",Systems and Control,"Water infrastructures are essential for drinking water supply, irrigation, fire protection, and other critical applications. However, water pumping systems, which are key to transporting water to the point of use, consume significant amounts of energy and emit millions of tons of greenhouse gases annually. With the wide deployment of digital water meters and sensors in these infrastructures, Machine Learning (ML) has the potential to optimize water supply control and reduce greenhouse gas emissions. Nevertheless, the inherent vulnerability of ML methods in terms of worst-case performance raises safety concerns when deployed in critical water infrastructures. To address this challenge, we propose a learning-augmented online control algorithm, termedLAOC, designed to dynamically schedule the activation and/or speed of water pumps. To ensure safety, we introduce a novel design of safe action sets for online control problems. By leveraging these safe action sets,LAOCcan provably guarantee safety constraints while utilizing ML predictions to reduce energy and environmental costs. Our analysis reveals the tradeoff between safety requirements and average energy/environmental cost performance.
Additionally, we conduct an experimental study on a building water supply system to demonstrate the empirical performance ofLAOC. The results indicate thatLAOCcan effectively reduce environmental and energy costs while guaranteeing safety constraints."
2089,679d459debd8ffd557a2b696,cs.SY,https://arxiv.org/pdf/2501.14193,Fabrication of Soft and Comfortable Pressure-Sensing Shoe Sole for Intuitive Monitoring of Human Quality Gaits,"Muhammad Adeel, Hasnain Ali, Afaque Manzoor Soomro, Muhammad Waqas",Systems and Control,
2090,679d459debd8ffd557a2b697,cs.SY,https://arxiv.org/pdf/2501.14115,Passivity-Based Robust Shape Control of a Cable-Driven Solar Sail Boom for the CABLESSail Concept,"Soojeong Lee, Ryan J. Caverly","Systems and Control, Space Physics","Solar sails provide a means of propulsion using solar radiation pressure, which offers the possibility of exciting new spacecraft capabilities. However, solar sails have attitude control challenges because of the significant disturbance torques that they encounter due to imperfections in the sail and its supporting structure, as well as limited actuation capabilities.
The Cable-Actuated Bio-inspired Lightweight Elastic Solar Sail (CABLESSail) concept was previously proposed to overcome these challenges by controlling the shape of the sail through cable actuation. The structural flexibility of CABLESSail introduces control challenges, which necessitate the design of a robust feedback controller for this system.
The purpose of the proposed research here is to design a robust controller to ensure precise and reliable control of CABLESSail’s boom. Taking into account the system dynamics and the dynamic properties of the CABLESSail concept, a passivity-based proportional-derivative (PD) controller for a single boom on the CABLESSail system is designed. To reach the nonzero desired setpoints, a feedforward input is additionally applied to the control law and a time-varying feedforward input is used instead of the constant one to effectively track a time-varying desired boom tip deflection. This control law is assessed by numerical simulations and by tests using a smaller-scale prototype of Solar Cruiser.
Both the simulation and the test results show that this PD control with the time-varying feedforward input robustly controls the flexible cable-actuated solar sail."
2091,679d459debd8ffd557a2b698,cs.SY,https://arxiv.org/pdf/2501.14696,Predictor-Feedback Stabilization of Globally Lipschitz Nonlinear Systems with State and Input Quantization,"Florent Koudohode, Nikolaos Bekiaris-Liberis","Optimization and Control, Systems and Control","We develop a switched nonlinear predictor-feedback control law to achieve global asymptotic stabilization for nonlinear systems with arbitrarily long input delay, under state quantization. The proposed design generalizes the nonlinear predictor-feedback framework by incorporating quantized measurements of both the plant and actuator states into the predictor state formulation. Due to the mismatch between the (inapplicable) exact predictor state and the predictor state constructed in the presence of state quantization, a global stabilization result is possible under a global Lipschitzness assumption on the vector field, as well as under the assumption of existence of a globally Lipschitz, nominal feedback law that achieves global exponential stability of the delay/quantization-free system. To address the constraints imposed by quantization, a dynamic switching strategy is constructed, adjusting the quantizer’s tunable parameter in a piecewise constant manner—initially increasing the quantization range, to capture potentially large system states and subsequently refining the precision to reduce quantization error. The global asymptotic stability of the closed-loop system is established through solutions estimates derived using backstepping transformations, combined with small-gain and input-to-state stability arguments. We also extend our approach to the case of input quantization."
2092,679d459debd8ffd557a2b699,cs.SY,https://arxiv.org/pdf/2501.14576,Dynamic Operation and Control of a Multi-Stack Alkaline Water Electrolysis System with Shared Gas Separators and Lye Circulation: A Model-Based Study,"Yiwei Qiu, Jiatong Li, Yangjun Zeng, Yi Zhou, Shi Chen, Xiaoyan Qiu, Buxiang Zhou, Ge He, Xu Ji, Wenying Li","Optimization and Control, Systems and Control","An emerging approach for large-scale hydrogen production using renewable energy is to integrate multiple alkaline water electrolysis (AWE) stacks into a single balance of plant (BoP) system, sharing components such as gas-lye separation and lye circulation. This configuration, termed theN𝑁Nitalic_N-in-1 AWE system, packsN𝑁Nitalic_Nstacks into a modular system, reducing land requirements, the complexity of plant topology, and overall capital costs. However, the coupling of these stacks through the shared BoP introduces challenges in dynamic operation under varying energy inputs, making their performance unclear compared to traditional 1-in-1 systems. To address this, we develop a state-space model of theN𝑁Nitalic_N-in-1 AWE system, capturing the dynamic behaviors of lye circulation, temperature, and HTO impurity, and their impact on energy conversion efficiency. We then propose a nonlinear model predictive controller (NMPC) to coordinately optimize inter-stack electrolytic current distribution, lye flow, and cooling, enabling the system to dynamically track varying load commands while maximizing efficiency, stabilizing temperature, and limiting HTO impurity accumulation. Simulation studies on a4,00040004,0004 , 000Nm3/h-rated 4-in-1 system verify the proposed controller under dynamic operation. Comparison with 4 independent 1-in-1 systems reveals that, with proper control, theN𝑁Nitalic_N-in-1 configuration offers comparable flexibility in accommodating real-world wind power inputs. The average differences in the root-mean-square errors (RMSEs) for load-tracking and stack temperature stabilization, and specific energy consumption are below0.0140.0140.0140.014MW,2.3562.3562.3562.356K, and0.0030.0030.0030.003kWh/Nm3."
2093,679d459debd8ffd557a2b69a,cs.SY,https://arxiv.org/pdf/2501.14442,New scenarios and trends in non-traditional laboratories from 2000 to 2020,"Ricardo M. Fernandez, Felix Garcia-Loro, Gustavo Alves, Africa Lopez-Rey, Russ Meier, Manuel Castro","Computers and Society, Systems and Control",
2094,679d459debd8ffd557a2b69b,cs.SY,https://arxiv.org/pdf/2501.14173,Constrained Fuel and Time Optimal 6DOF Powered Descent Guidance Using Indirect Optimization,"Nicholas P. Nurre, Ehsan Taheri","Optimization and Control, Systems and Control","Powered descent guidance (PDG) problems subject to six-degrees-of-freedom (6DOF) dynamics allow for enforcement of practical attitude constraints. However, numerical solutions to 6DOF PDG problems are challenging due to fast rotational dynamics coupled with translational dynamics, and the presence of highly nonlinear state/control path inequality constraints. In this work, constrained fuel- and time-optimal 6DOF PDG problems are solved leveraging a regularized indirect method, subject to inequality constraints on the thrust magnitude, thruster gimbal angle, rocket tilt angle, glideslope angle, and angular velocity magnitude. To overcome the challenges associated with solving the resulting multipoint boundary-value problems (MPBVPs), the state-only path inequality constraints (SOPICs) are enforced through an interior penalty function method, which embeds the resulting MPBVPs into a multi-parameter smooth neighboring families of two-point BVPs. Extremal solutions are obtained using an indirect multiple-shooting solution method with numerical continuation. Moreover, an empirical relation is derived for the directly-adjoined Lagrange multipliers associated with SOPICs. The fuel- and time-optimal trajectories are compared against solutions of DIDO — a capable pseudospectral-based software for solving practical constrained optimal control problems."
